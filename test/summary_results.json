[
    {
        "paper_id": "W11-2139",
        "summarized_text": "We have presented a summary of the enhancements made to a hierarchical phrase-based translation system for the WMT11 shared translation task. We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training. While this model is somewhat unusual (the conditional probability is backwards from a noisy channel model), it is a standard and effective technique for case restoration. model), but a number of changes we made were quite simple (OOV handling, using MT output to provide additional references for training) and also led to improved results. This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11).",
        "rouge_score": {}
    },
    {
        "paper_id": "D12-1126",
        "summarized_text": "Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts. In this paper, we present a method using dynamic features to tag POS of mixed texts. The underlying problem is how to tag part-of-speech (POS) for the English words involved. We would also like to investigate these features in more applications of natural language processing, such as name entity recognition, information extraction, etc. It should be noted that our method is also effective for the mixed texts of Chinese and any foreign languages since we use \u201cUnified Replacement\u201d. Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features To overcome the problem of the lack of annotated corpus on mixed texts, our features use both local and non-local information and take advantage of the characteristics of Chinese-English mixed texts. Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.",
        "rouge_score": {}
    },
    {
        "paper_id": "W01-0510",
        "summarized_text": "We have presented a data-driven approach to information extraction that, despite the small amount of training data used, is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad personal information management task. The task of template filling is cast as constrained parsing using the SLM. The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser. Training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage. The performance of the baseline model could be improved with more authoring effort, although this is expensive.",
        "rouge_score": {}
    },
    {
        "paper_id": "W06-2204",
        "summarized_text": "With more fields in the problem domain there is potentially more information on each of the candidate positions to constrain these decisions. We have also observed that the number of fields that are being extracted in the given domain affects the performance of our algorithm. We present a semi-supervised learning algorithm for information extraction that can acquire extraction patterns from a small amount of labelled text in conjunction with a large amount of unlabelled text. This definition allows TPLEX to perform well with very little training data in domains where other approaches that assume fragment redundancy would fail. We have described TPLEX, a semi-supervised algorithm for learning information extraction patterns. We are also exploring ideas for semi-supervised learning from the machine learning community. We contend that this is a result of our algorithm\u2019s ability to use the unlabelled test data to validate the patterns learned from the training data.",
        "rouge_score": {}
    },
    {
        "paper_id": "D08-1094",
        "summarized_text": "We argue that existing models for this task do not take syntactic structure sufficiently into account. present a novel vector space model that addresses these issues by incorporating the selectional preferences for words\u2019 argument positions. We have evaluated the SVS model on two datasets on the task of predicting the felicitousness of paraphrases in given contexts. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This makes it possible to integrate syntax into the computation of word meaning in context. We will explore the usability of vector space models of word meaning in NLP applications, formulated as the question of how to perform inferences on them in the context of the Textual Entailment task (Dagan et al., 2006).",
        "rouge_score": {}
    },
    {
        "paper_id": "W09-1210",
        "summarized_text": "Our system combines and implements efficient parsing techniques to get a high accuracy as well as very good parsing and training time. Therefore, a possible further accuracy and parsing speed improvement would be to select different features sets for different languages or to leave out some features. The semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro F1 score) in the joint task for syntactic and semantic dependency parsing. We think that also the development of systems can profit from this since one can perform more experiments in the given time. We provided a fast implementation with good parsing time and memory footprint. In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple languages.",
        "rouge_score": {}
    },
    {
        "paper_id": "C08-1081",
        "summarized_text": "We aregrateful to the Russian Foundation of Basic Re search for partial support of this research (grant no. 07-06-00339). We have presented the first results on parsing theSYNTAGRUS treebank of Russian using a data driven dependency parser. We hypothesize that this result can be generalized to other richly inflected languages, provided that sufficient amounts of data are available.Future work includes a deeper analysis of the in fluence of individual features, both morphological and lexical, as well as an evaluation of the parserunder more realistic conditions without gold stan dard annotation in the input. We present the first results on parsing the SYNTAGRUS treebank of Russian with a data-driven dependency parser, achieving a labeled attachment score of over 82% and an unlabeled attachment score of 89%. We conjecture that the latter result can be generalized to richly inflected languages in general, provided that sufficient amounts of training data are available.",
        "rouge_score": {}
    },
    {
        "paper_id": "D11-1132",
        "summarized_text": "Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches. This paper describes a novel approach to the semantic relation detection problem. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. Specifically, we detect a new semantic relation by projecting the new relation\u2019s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process.",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-1060",
        "summarized_text": "We believe that the approach has considerable theoretical as well as practical merits, and provides a simple and clean formulation for modeling phrasal and sentential semantics. While being deliberately vague in our working definition of motifs, we have presented simple efficient formulations to extract such motifs that uses both annotated as well as partially unannotated data. We have presented a new frequency-driven framework for distributional semantics of not only lexical items but also longer cohesive motifs. We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated. Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items.",
        "rouge_score": {}
    },
    {
        "paper_id": "W05-0104",
        "summarized_text": "This paper briefly outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way. The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system. There are certainly changes I will make when I teach this course again this fall. Using provided scaffolding, students built realistic tools with nearly state-of-theart performance in most cases.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-2802",
        "summarized_text": "Previous ap proaches to learning these grounded meaning representations require detailed annotations at training time. We presented a preliminary evaluation on a small corpus, demonstrating that the system isable to infer meanings for concrete noun phrases de spite having no direct supervision for these values. com mands given to a robotic forklift by untrained users. We assume the action policy takes a parametric form that factors based on the structure of the language, based on the G3 framework and use stochastic gradient ascentto optimize policy parameters. Toward Learning Perceptually Grounded Word Meanings from Unaligned Parallel Data as well as a mapping between specific phrases in the language and aspects of the external world; for example the mapping between the words ?the tire pallet? Weplan to train our system using a large dataset of language paired with robot actions in more complex en vironments, and on more than one robotic platform.",
        "rouge_score": {}
    },
    {
        "paper_id": "W04-0837",
        "summarized_text": "We intend to experiment further using a wider variety of grammatical relations, which we hope will improve performance for verbs, and with data from larger corpora, such as the Gigaword corpus and the web, which should allow us to cover a great many more words which do not occur in manually created resources such as SemCor. Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently. word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. We have demonstrated that it is possible to acquire predominant senses from raw textual corpora, and that these can be used as an unsupervised first sense heuristic that does not not rely on manually produced corpora such as SemCor.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-0610",
        "summarized_text": "We describe our data set, which consists of transcribed data from a widely used clinical diagnostic instrument (the ADOS) for children with autism, children with developmental language disorder, and typically developing children. When we examined the output of our error-type classifier, we noticed that many of the misclassified examples could be construed, upon closer inspection, as belonging to multiple error classes. We expect further improvement with additional data, features, and classification techniques. We then present methods for automatically extracting lexical and syntactic features from transcripts of children\u2019s speech to 1) identify certain syntactic and semantic errors that have previously been found to distinguish ASD language from that of children with typical development; and 2) perform diagnostic classification. Without knowing the context in which the sentence was uttered, it is not possible to determine the type of error through any manual or automatic means.",
        "rouge_score": {}
    },
    {
        "paper_id": "I08-1012",
        "summarized_text": "The unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words. There are many ways in which this research should be continued. We ex tract the information on short dependency relations 93in an automatically generated corpus parsed by a basic parser. The new parser achieves an absolute im provement of 1.24% over the state-of-the-art parser on Chinese Treebank (from 85.28% to 86.52%). Then we may collect more reliable information than the current one. We thentrain another parser which uses the informa tion on short dependency relations extractedfrom the output of the first parser. This paper presents an effective approach to improvedependency parsing by using unlabeled data. We then train a new parser with the information. This paper presents an effective dependencyparsing approach of incorporating short de pendency information from unlabeled data.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-2006",
        "summarized_text": "Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. This work introduces two extensions to the wellknown beam search algorithm for phrase-based machine translation. We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. We compare our decoder to Moses, reaching a similar highest BLEU score, but clearly outperforming it in terms of scalability with respect to the trade-off ratio between translation quality and speed. Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. Our software is part of the open source toolkit Jane.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-3706",
        "summarized_text": "The next steps would be to study the possibility to classify in more than two classes by using several language models. We present an accuracy above 81% for Spanish opinions in the financial products domain. Then the test opinions are compared to both models and a decision and confidence measure are calculated. The use of an external neutral corpus should also be considered in the future. There are other parameters which have to be experimentally tuned and they are not related to the positive or negative classification but to the subjective qualifier very/somewhat/little and to the confidence measure. We propose an approach based on the order of the words without using any syntactic and semantic information. To this end an Apertium morphological analyser would be necessary (30 languages are currently available) as well as a labeled data set of opinions.",
        "rouge_score": {}
    },
    {
        "paper_id": "P13-1092",
        "summarized_text": "To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches. This paper introduces grounded unsupervised semantic parsing, which leverages available database for indirect supervision and uses a grounded meaning representation to account for syntax-semantics mismatch in dependency-based semantic parsing. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM.",
        "rouge_score": {}
    },
    {
        "paper_id": "W07-2214",
        "summarized_text": "We present an example from natural language which seems to require both types of context sensitivity, and introduce partially ordered multisets (pomsets) mcfgs as a formalism which succintly expresses both. We introduced pomset mcfgs as a formalism for describing grammars with both types of context sensitivity, and outlined an informal proof of the its polynomialtime parsing complexity. In this paper we identified two types of context sensitivity, and provided a natural language example which exhibits both types of context sensitivity. This paper identifies two orthogonal dimensions of context sensitivity, the first being context sensitivity in concurrency and the second being structural context sensitivity.",
        "rouge_score": {}
    },
    {
        "paper_id": "P05-1022",
        "summarized_text": "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less. This result is only slightly higher than the highest reported result for this test-set, Bod\u2019s (.907) (Bod, 2003). This method generates 50-best lists that are of substantially higher quality than previously obtainable. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of parses. We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker.",
        "rouge_score": {}
    },
    {
        "paper_id": "C08-1074",
        "summarized_text": "They also seem to show that starting point selection by random walk is slightly superiorto uniform random selection. Random Restarts in Minimum Error Rate Training for Statistical Machine Translation We compare several ways of perform ing random restarts with MERT. We findthat all of our random restart methods out perform MERT without random restarts,and we develop some refinements of ran dom restarts that are superior to the most common approach with regard to resulting model quality and training time. We believe that our results show very convincingly that using random restarts in MERT improves the BLEU scores produced by the result ing models. The use of multiple randomized start ing points in MERT is a well-established practice, although there seems to be nopublished systematic study of its benefits.",
        "rouge_score": {}
    },
    {
        "paper_id": "P09-1065",
        "summarized_text": "We instead propose a method that combines multiple translation models in one decoder. In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the latticebased MERT (Macherey et al., 2008). Current SMT systems usually decode with single translation models and cannot benefit from the strengths of other models in phase. We have presented a framework for including multiple translation models in one decoder. Representing search space as a translation hypergraph, individual models are accessible to others via sharing nodes and even hyperedges. Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually. Therefore, one model can share translations and even derivations with other models. Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding.",
        "rouge_score": {}
    },
    {
        "paper_id": "D11-1094",
        "summarized_text": "The results might improve further if window-based context and dependency-based context are combined in an optimal way. This paper presents a novel method for the computation of word meaning in context. The evaluation on a lexical substitution task \u2013 carried out for both English and French \u2013 indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations. The factorization model allows us to determine which particular dimensions are important for a target word in a particular context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. Secondly, we would like to subject our approach to further evaluation, in particular on a number of different evaluation tasks, such as semantic compositionality.",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-2093",
        "summarized_text": "The results show that our methods outperform previous methods. In the future, we are interested in applying our methods into domain adaptation task of statistical machine translation in model level. When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points. We present three novel methods for translation model training data selection, which are based on the translation model and language model. In addition, our methods make full use of the limited in-domain data and are easily implemented. Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus. In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection.",
        "rouge_score": {}
    },
    {
        "paper_id": "P05-1063",
        "summarized_text": "The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm. The syntactic features provide an additional 0.3% reduction in test\u2013set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (signifiat < which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system. We follow where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second \u201creranking\u201d model is then used to choose an utterance from these 1000-best lists. We describe a method for discriminative training of a language model that makes use of syntactic features. We describe experiments on the Switchboard speech recognition task.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-2123",
        "summarized_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs. We have described two data structures for language modeling that achieve substantial reductions in time and memory cost. We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns. While we have minimized forward-looking state in Section 4.1, machine translation systems could also benefit by minimizing backward-looking state. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement. This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.",
        "rouge_score": {
            "rouge1": 0.4492607879924953,
            "rouge2": 0.1999946860943059,
            "rougeL": 0.30562445684396905
        }
    },
    {
        "paper_id": "D09-1092",
        "summarized_text": "Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora. We explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language. We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
        "rouge_score": {
            "rouge1": 0.6730423412610387,
            "rouge2": 0.5149904760678012,
            "rougeL": 0.394140623346735
        }
    },
    {
        "paper_id": "P13-1155",
        "summarized_text": "We show that the proposed unsupervised approach provides a normalization system with very high precision and a reasonable recall. When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. We compared the system with conventional correction approaches and with recent previous work; and we showed that it highly outperforms other systems. This shows a characteristic of the proposed approach; it is very conservative in proposing normalization which is desirable as a preprocessing step for NLP applications. This limitation can be marginalized by providing more data for generating the lexicon. We introduced a social media text normalization system that can be deployed as a preprocessor for MT and various NLP applications to handle social media text.",
        "rouge_score": {}
    },
    {
        "paper_id": "P08-1043",
        "summarized_text": "Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. We conjecture that this trend may continue by incorporating additional information, e.g., three-dimensional models as proposed by Tsarfaty and Sima\u2019an (2007). We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
        "rouge_score": {
            "rouge1": 0.48870082437124945,
            "rouge2": 0.3215201197672714,
            "rougeL": 0.32778474520091655
        }
    },
    {
        "paper_id": "N12-1052",
        "summarized_text": "The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%. We then showed that by using these cross-lingual word clusters, we can significantly improve on direct transfer of discriminative models for both parsing and NER. The first method works by projecting word clusters, induced from monolingual data, from a source language to a target language directly via word alignments. This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models. While previous work has focused primarily on English, we extend these results to other languages along two dimensions.",
        "rouge_score": {}
    },
    {
        "paper_id": "P07-1051",
        "summarized_text": "We present a new algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees. While U-DOP* performs worse than state-of-the-art supervised parsers on handannotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl. We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight. We believe that parsing, when separated from a task-based application, is mainly an academic exercise. We train both on Penn\u2019s WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set. We argued that this can be explained by the fact that U-DOP learns both constituents and (non-syntactic) phrases while supervised parsers learn constituents only.",
        "rouge_score": {}
    },
    {
        "paper_id": "P13-2112",
        "summarized_text": "We find that for all languages, accuracy rises quickly in the first 5\u20136 iterations, and then subsequently improves only slightly. We exemplify this in Figure 1 (right panel) for Dutch. We have proposed a method for unsupervised POS tagging that performs on par with the current stateof-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM). We examine the impact of self-training and revision over training iterations. We would like to thank Prokopis Prokopidis for providing us the Greek Treebank and Antonia Marti for the Spanish CoNLL 06 dataset.",
        "rouge_score": {}
    },
    {
        "paper_id": "W06-3120",
        "summarized_text": "Although local reordering is supposed to be included in the phrase structure, performing local reordering improves the translation quality. This paper reports translation results for the \u201cExploiting Parallel Texts for Statistical Machine Translation\u201d (HLT-NAACL Workshop on Parallel Texts 2006). In fact, local reordering, provided by the reordering approaches, allows for those generalizations which phrases could not achieve. We have studied different techniques to improve the standard Phrase-Based translation system. Reordering in the DeEn task is left as further work. TALP Phrase-Based Statistical Translation System For European Language Pairs Mainly we introduce two reordering approaches and add morphological information.",
        "rouge_score": {}
    },
    {
        "paper_id": "W05-1518",
        "summarized_text": "The unbalanced voting lead to the precision as high as 90.2 %, while the F-measure of 87.3 % outperforms the best result of balanced voting (87.0). We differ from them in exploring context features more deeply. To some extent, we are porting the ideas of Henderson and Brill (1999) to the world of dependency structures. We were able to significantly improve over the best parsing result for the given setting, known so far. The methods are language independent, though the amount of accuracy improvement may vary according to the performance of the available parsers. We have tested several approaches to combining of dependency parsers. Moreover, our experiments show that even parsers far below the state of the art can contribute to the total improvement. This paper explores the possibilities of improving parsing results by combining outputs of several parsers.",
        "rouge_score": {}
    },
    {
        "paper_id": "W10-1401",
        "summarized_text": "Thereis ample evidence that the application of read ily available statistical parsing models to suchlanguages is susceptible to serious performance degradation. We have shown that architectural, representational, and estimation issuesassociated with parsing MRLs are found to be chal lenging across languages and parsing frameworks. We further thank INRIA Al page team for their generous sponsorship. We synthesize the contributions of re searchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. We are finally grateful to our reviewers and authors for their dedicated work and individual contributions. This paper presents the synthesis of 11 contributionsto the first workshop on statistical parsing for mor phologically rich languages.",
        "rouge_score": {}
    },
    {
        "paper_id": "P13-1007",
        "summarized_text": "This work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision. We also present a general model for learning to build partial orders from a set of pairpreferences. We give an algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice. No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation. We develop the first statistical QSD model addressing the interaction of quantifiers with negation and the implicit universal of plurals, defining a baseline for this task on QuanText data (Manshadi et al., 2012). Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases.",
        "rouge_score": {}
    },
    {
        "paper_id": "P04-1013",
        "summarized_text": "We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model. We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative. This training method successfully satisfies the conflicting constraints that it be computationally tractable and that it be a good approximation to the theoretically optimal method. We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-3160",
        "summarized_text": "We showed that although thereare two competing strategies with comparable per formance, one is an unstable learner, and before weunderstand more regarding the nature of the insta bility, the preferred alternative is to use M?C as the hypothesis pair in optimization. tion selection methods, PB and M+C significantly underperforming it.Given that the translation performance of optimiz ing the loss functions represented by LU/MC and M?C selection is comparable on the evaluation sets for fr-en and cs-en, it may be premature to make a general recommendation for one over the other. We see a similar, albeit slightly less pronounced behavior on fr-en in Figure 2. We then used the presented al gorithm to empirically compare several widespreadloss functions and strategies for selecting hypothe ses for optimization.",
        "rouge_score": {}
    },
    {
        "paper_id": "W04-1505",
        "summarized_text": "Its parsing speed of about 300,000 words per hour is very good for a deep-linguistic parser and makes it fast enough for unlimited application. We present and evaluate an implemented statistical minimal parsing strategy exploiting DG charateristics to permit fast, robust, deeplinguistic analysis of unrestricted text, and compare its probability model to (Collins, 1999) and an adaptation, (Dubey and Keller, 2003). We show that DG allows for the expression of the majority of English LDDs in a context-free way and offers simple yet powerful statistical models. An evaluation shows that the performance of its implementation is state-of-the-art10.",
        "rouge_score": {}
    },
    {
        "paper_id": "N06-1045",
        "summarized_text": "Users of MT systems are generally interested in the string yield of those trees, and not the trees per se. We have shown that weighted determinization is useful for recovering -best unique trees from a weighted forest. We plan for our weighted determinization algorithm to be one component in a generally available tree automata package for intersection, composition, training, recognition, and generation of weighted and unweighted tree automata for research tasks such as the ones described above. We introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees. We also demonstrate our algorithm\u2019s effectiveness on two large-scale tasks. We have improved evaluation scores by incorporating the presented algorithm into our MT work and we believe that other NLP researchers working with trees can similarly benefit from this algorithm.",
        "rouge_score": {}
    },
    {
        "paper_id": "E09-1034",
        "summarized_text": "We have defined a parsing algorithm for wellnested dependency structures with bounded gap degree. When the gap degree is greater than 1, the time complexity goes up by a factor of n2 for each extra unit of gap degree, as in parsers for coupled context-free grammars. This set includes ill-nested structures verifying certain conditions, and can be parsed in O(n3k+4) with a variant of the parser for wellnested structures. Therefore, our O(n3k+4) parser can analyse all the gap degree k structures in these treebanks. The third case includes all the degree in a number of dependency treebanks. We present parsing algorithms for various mildly non-projective dependency formalisms.",
        "rouge_score": {}
    },
    {
        "paper_id": "P10-1044",
        "summarized_text": "This approach is capable of producing human interpretable classes, however, avoids the drawbacks of traditional class-based approaches (poor lexical coverage and ambiguity). We have presented an application of topic modeling to the problem of automatically computing selectional preferences. We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability. LDA-SP achieves state-of-the-art performance on predictive tasks such as pseudo-disambiguation, and filtering incorrect inferences. Our method, LDA-SP, learns a distribution over topics for each relation while simultaneously grouping related words into these topics. In the future, we wish to apply our model to automatically discover new inference rules and paraphrases. We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al.,",
        "rouge_score": {}
    },
    {
        "paper_id": "P06-1109",
        "summarized_text": "Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees. We investigate generalizations of the allsubtrees &quot;DOP&quot; approach to unsupervised parsing. We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data. Yet we should neither rule out the possibility that entirely unsupervised methods will in fact surpass semi-supervised methods. We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent. Whether such a massively maximalist approach is feasible can only be answered by empirical investigation in due time.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-2121",
        "summarized_text": "The new addition to the algorithm shows a clear advantage in parsing speed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The additional transition gives improvements to both parsing speed and accuracy, showing a linear time parsing speed with respect to sentence length. In the future, we will test the robustness of these approaches in more languages. We present two ways of improving transition-based, non-projective dependency parsing. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed.",
        "rouge_score": {}
    },
    {
        "paper_id": "N12-1090",
        "summarized_text": "We explored the under-investigated yet challenging task of performing coreference resolution for a language for which we have no coreference-annotated data and no linguistic knowledge of the language. To gain additional insights into our approach, we plan to pursue several directions. We believe that this approach has the potential to allow coreference technologies to be deployed across a larger number of languages than is currently possible, and that this is just the beginning of a new line of work. To build a coreference resolver for a new language, the typical approach is to first coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques. To alleviate this corpus annotation bottleneck, we examine a translation-based projection approach to multilingual coreference resolution.",
        "rouge_score": {}
    },
    {
        "paper_id": "E09-1018",
        "summarized_text": "We have presented a generative model of pronounanaphora in which virtually all of the parameters are learned by expectation maximization. We have compared it to several systems available on the web (all we have found so far). We find it of interest first as an example of one of the few tasks for which EM has been shown to be effective, and second as a useful program to be put in general use. The algorithm is fast and robust, and has been made publically available for downloading. The result is a very large fraction of first person pronouns are given incorrect antecedents. We present an algorithm for pronounanaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion. While EM frequently fails to find good models for the tasks to which it is set, in this case it works quite well.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1099",
        "summarized_text": "We showed that this approach can gain up to 2.2 BLEU points over its concatenation baseline and 0.39 BLEU points over a powerful mixture model. The model combination can be done using various mixture operations. Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. In this paper, we presented a new approach for domain adaptation using ensemble decoding. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. We will also add capability of supporting syntax-based ensemble decoding and experiment how a phrase-based system can benefit from syntax information present in a syntax-aware MT system.",
        "rouge_score": {}
    },
    {
        "paper_id": "W99-0613",
        "summarized_text": "The contextual rules are restricted and may not be applicable to every example, but the spelling rules are generally applicable and should have good coverage. Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules. The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function. The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). We are currently exploring other methods that employ similar ideas and their formal properties. The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.",
        "rouge_score": {
            "rouge1": 0.5411413109405175,
            "rouge2": 0.28356216458497285,
            "rougeL": 0.2621687498631101
        }
    },
    {
        "paper_id": "E03-1005",
        "summarized_text": "Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential processing time when the most probable parse tree is computed. While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.",
        "rouge_score": {
            "rouge1": 0.43794242126023913,
            "rouge2": 0.21665404913158445,
            "rougeL": 0.29186629171734363
        }
    },
    {
        "paper_id": "P13-2083",
        "summarized_text": "We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We outlined an approach that introduces structure into distributed semantic representations gives us an ability to compare the identity of two representations derived from supposedly semantically identical phrases with different surface realizations. We would also like to replace our syntactic relations to semantic relations and explore various ways of dimensionality reduction to solve this problem. We employed the task of event coreference to validate our representation and achieved significantly higher predictive accuracy than several baselines. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.",
        "rouge_score": {}
    },
    {
        "paper_id": "N12-1086",
        "summarized_text": "To achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in Gaussian and entropic pairwise Markovnetworks constructed from labeled and unla beled data. We present novel methods to construct compact natural language lexicons within a graph based semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data. This research was supported by Qatar Na tional Research Foundation grant NPRP 08-485-1-083, Google?s support of the Worldly Knowledge Project, andTeraGrid resources provided by the Pittsburgh Supercom puting Center under NSF grant number TG-DBS110003. We have presented a family of graph-based SSL objective functions that incorporate penalties encour aging sparse measures at each graph vertex. Sparse measures are desirable forhigh-dimensional multi-class learning problems such as the induction of labels on natu ral language types, which typically associate with only a few labels.",
        "rouge_score": {}
    },
    {
        "paper_id": "W08-0406",
        "summarized_text": "We also want to examine the relation between word alignment method and the extracted rules and the relationship between reordering and word selection. We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT. We achieve an absolute improvement in translation quality of 1.1 % BLEU. This is achieved by reordering the input string, but scoring on the output string. This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules. Unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based SMT. This way, all external reorderings are made possible, but only the rule-supported ones get promoted. We expect greater gains, due to the higher need for reordering between these less-related languages.",
        "rouge_score": {}
    },
    {
        "paper_id": "P09-2003",
        "summarized_text": "The crucial difference between previously proposed topdown RCG parsers and the new Earley-style algorithm is that while the former compute all clause instantiations during predict operations, the latter 4Of course, the use of constraints makes comparisons between items more complex and more expensive which means that for an efficient implementation, an integer-based representation of the constraints and adequate techniques for constraint solving are required. We have presented a new CYK and Earley parsing algorithms for the full class of RCG. The characteristic property of the Earley parser is that we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible. We plan to include this strategy in future work. We present a CYK and an Earley-style algorithm for parsing Range Concatenation Grammar (RCG), using the deductive parsing framework. This way, one could process predicates whose range boundaries are better known first.",
        "rouge_score": {}
    },
    {
        "paper_id": "N12-1049",
        "summarized_text": "We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference. We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. While most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a comof time This is used to construct a parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity. We hope to improve detection and explore system performance on multilingual and complex datasets in future work. We present a new approach to resolving temporal expressions, based on synchronous parsing of a fixed grammar with learned parameters and a compositional representation of time.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1013",
        "summarized_text": "We have shown that these differences can be quantified and tied to theoretical expectations of each model, which may provide insights leading to better models in the future. We have presented a thorough study of the dif ference in errors made between global exhaustivegraph-based parsing systems (MSTParser) and local greedy transition-based parsing systems (Malt Parser). We present a comparative error analysisof the two dominant approaches in datadriven dependency parsing: global, exhaus tive, graph-based models, and local, greedy, transition-based models. This analysisleads to new directions for parser develop ment. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models.",
        "rouge_score": {}
    },
    {
        "paper_id": "C10-1045",
        "summarized_text": "This paper is based on work supported in part by DARPA through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation er rors. Second, we show that although the PennArabic Treebank is similar to other tree banks in gross statistical terms, annotation consistency remains problematic. We are also grateful to Markus Dickinson, Ali Farghaly, Nizar Habash, Seth Kulick, David McCloskey, Claude Reichard, Ryan Roth, and Reut Tsarfaty for constructive discussions. Third,we develop a human interpretable grammar that is competitive with a latent vari able PCFG. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus.",
        "rouge_score": {}
    },
    {
        "paper_id": "H05-1094",
        "summarized_text": "Specifically, we perform joint decoding ofseparately-trained sequence models, preserv ing uncertainty between the tasks and allowinginformation from the new task to affect predic tions on the old task. Therefore, we wantto transfer learning from the old, general purpose subtask to a more specific new task, for which there is often less data. This paper has implications for how such standard tools are pack aged. The rich features afforded by aconditional model allow the new task to influence the pre 753 dictions of the old task, an effect that is only possible with joint decoding. While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old.",
        "rouge_score": {}
    },
    {
        "paper_id": "E09-1038",
        "summarized_text": "We show that using an external lexicon for dealing with rare lexical events greatly benefits a PCFG parser for Hebrew, and that results can be further improved by the incorporation of lexical probabilities estimated in a semi-supervised manner using a wide-coverage lexicon and a large unannotated corpus. This is achieved by defining a stochastic mapping layer between the two resources. We present a framework for interfacing a parser with an external lexicon following a different annotation scheme. Unlike other studies (Yang Huang et al., 2005; Szolovits, 2003) in which such interfacing is achieved by a restricted heuristic mapping, we propose a novel, stochastic approach, based on a layered representation. We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens.",
        "rouge_score": {}
    },
    {
        "paper_id": "C10-2096",
        "summarized_text": "Unlike previous works, which only focus onone module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-to string system. We integrate both lat tice and forest into a single tree-to-stringsystem, and explore the algorithms of lattice parsing, lattice-forest-based rule ex traction and decoding. The work is sup ported by National Natural Science Foundation of China, Contracts 90920004 and 60736014, and 863 State Key Project No. 2006AA010108 (H. M and Q. L.), and in part by DARPA GALE Contract No. Traditional 1-best translation pipelinessuffer a major drawback: the errors of 1 best outputs, inevitably introduced by each module, will propagate and accumulate along the pipeline. We have explored the algorithms of lattice parsing, rule extraction and decoding. The main advantage of our model is that we can make global decision to search for the best segmentation, parse-tree and translation in one step.",
        "rouge_score": {}
    },
    {
        "paper_id": "E12-1047",
        "summarized_text": "While they provide some efficiency gains, they do not help with the main problem of longer sentences. We have presented a new technique for largescale parsing with LCFRS, which makes it possible to parse sentences of any length, with favorable accuracies. Previous work on treebank parsing with discontinuous constituents using Linear Rewriting systems has been limited to sentences of up to 30 words, for reasons of computational complexity. There have been some results on an a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible. The resulting parser has been applied to a discontinuous treebank with favorable results. The availability of this technique may lead to a wider acceptance of LCFRS as a syntactic backbone in computational linguistics.",
        "rouge_score": {}
    },
    {
        "paper_id": "P05-1039",
        "summarized_text": "To test if this was indeed the case, we re-ran the final experiment, but excluded the SBAR transformation. While the SBAR transformation slightly reduces performance, recall that we argued the S GF transformation only made sense if the SBAR transformation is already in place. While we only presented results on the German NEGRA corpus, there is reason to believe that the techniques we presented here are also important to other languages where lexicalization provides little benefit: smoothing is a broadly-applicable technique, and if difficulties with lexicalization are due to sparse lexical data, then suffix analysis provides a useful way to get more information from lexical elements which were unseen while training. We did indeed find that applying S GF without the SBAR transformation reduced performance. What To Do When Lexicalization Fails: Parsing German With Suffix Analysis And Smoothing",
        "rouge_score": {}
    },
    {
        "paper_id": "A00-2005",
        "summarized_text": "This gain compares favorably with a bound on potential gain from increasing the corpus size. The economic basis for using ensemble methods will continue to improve with the increasing value (performance per price) of modern hardware. We have shown how to exploit the distribution created as a side-effect of the boosting algorithm to uncover inconsistencies in the training corpus. That baseline system is the best known Treebank parser. The best resulting system provides roughly as large of a gain in F-measure as doubling the corpus size. Perhaps the biggest advantage of this technique is that it requires no a priori notion of how the inconsistencies can be characterized. We have shown two methods, bagging and boosting, for automatically creating ensembles of parsers that produce better parses than any individual in the ensemble.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1091",
        "summarized_text": "We are currently exploringthese possibilities, for instance use of syntactic in formation in reordering and models with augmented input information.We have not addressed all computational problems of factored translation models. We presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes. These experiments demonstrate that within the frameworkof factored translation models additional informa tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase based statistical approach. The framework of factored translation models is very general. We reported on experiments that showed gains over standard phrase-based models, both in terms of automatic scores (gains of up to 2% BLEU), as well as a measure of grammatical coherence.",
        "rouge_score": {}
    },
    {
        "paper_id": "E06-1010",
        "summarized_text": "We investigate a series of graph-theoretic constraints on non-projective dependency and their effect on i.e. whether they allow naturally occurring syntactic constructions to be adequately and i.e. whether they reduce the search space for the parser. This suggests that the integration of such constraints into non-projective parsing algorithms will improve both accuracy and efficiency, but we have to leave the corroboration of this hypothesis as a topic for future research. This is a substantial improvement over the commonly used projective approximation (degree 0), which excludes 15\u201325% of the graphs. We have investigated a series of graph-theoretic constraints on dependency structures, aiming to find a better approximation than PROJECTIVITY for the structures found in naturally occurring data, while maintaining good parsing efficiency. This is a substantial improvement over the projective approximation, which only allows 75\u201385% of the dependency graphs to be captured exactly.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-3117",
        "summarized_text": "We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques. Unfortunately, we did not submit our best configuration for the shared task. We also presented further experiments using different machine learning techniques and we evaluated the impact of two sets of features - one set which is based on linguistic features extracted using POS tagging and parsing, and a second set which is based on topic modelling. This setup leads to a Mean Average Error of 0.62 and a Root Mean Squared Error of 0.78. Two sets features are proposed: one i.e. respecting the data limitation suggested by the organisers, and one i.e. using data or tools trained on data that was not provided by the workshop organisers. We presented in this paper our submission for the WMT12 Quality Estimation shared task. We plan to continue working on the task of machine translation quality estimation.",
        "rouge_score": {}
    },
    {
        "paper_id": "N01-1023",
        "summarized_text": "We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilistic treatment. Unlike previous approaches to unsupervised parsing our method can be trained and tested on the kind of representations and the complexity of sentences that are found in the Penn Treebank. We obtained 80.02% and 79.64% labeled bracketing precision and recall respectively. We then train a statistical parser on the combined set of labeled and unlabeled data. The baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and recall. We propose a novel Co-Training method for statistical parsing. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.",
        "rouge_score": {}
    },
    {
        "paper_id": "W09-1207",
        "summarized_text": "In EMNLP/CoNLL- Chang and Chih-Jen Lin, 2001. a for support vector Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li. In the future, we will study how to solve the domain adaptive problem and how to do joint learning between syntactic and semantic parsing. The additional in-domain (devel) SRL data can help the in-domain test. Our CoNLL 2009 Shared Task system is composed of three cascaded components. The pseudoprojective high-order syntactic dependency model outperforms our CoNLL 2008 model (in English).",
        "rouge_score": {}
    },
    {
        "paper_id": "N09-1061",
        "summarized_text": "Our results generalize previous work on Synchronous Context-Free Grammar, and are particularly relevant for machine translation from or to languages that require syntactic analyses with discontinuous constituents. To conclude this paper, we now discuss a number of aspects of the results that we have presented, including various other pieces of research that are particularly relevant to this paper. The parsing complexity of an is exponential in both the of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which productions have rank at most and has minimal fan-out. Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation. Optimal Reduction of Rule Length in Linear Context-Free Rewriting Systems",
        "rouge_score": {}
    },
    {
        "paper_id": "E09-1055",
        "summarized_text": "We have shown how to extract mildly contextsensitive grammars from dependency treebanks, and presented an efficient algorithm that attempts to convert these grammars into an efficiently parseable binary form. We take this number as an indicator for the usefulness of our result. We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms. The work of the first author was funded by the Swedish Research Council. This raises the question about the practical relevance of our technique. Well-nestedness is a property that implies the binarizability of the extracted grammar; however, the classes of wellnested trees and those whose corresponding productions can be binarized using our algorithm are incomparable\u2014in particular, there are well-nested productions that cannot be binarized in our framework. We see our results as promising first steps in a thorough exploration of the connections between non-projective and mildly context-sensitive parsing.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-2138",
        "summarized_text": "We also documented our experiments with several back-off techniques for English to Czech translation. We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words. We carried out experiments showing improvements in BLEU when using our method for translating into Czech, Finnish, German and Slovak with small parallel data. We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation. We showed that gains in BLEU score increase with growing size of monolingual data. We also provide a description of the systems we We introduced a technique for exploiting monolingual data to improve the quality of translation into morphologically rich languages. We discussed the issues of including similar translation models as separate components in MERT.",
        "rouge_score": {}
    },
    {
        "paper_id": "D12-1127",
        "summarized_text": "The methods outlined in the paper are standard and easy to replicate, yet highly accurate and should serve as baselines for more complex proposals. We achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised Penn We have shown that the Wiktionary can be used to train a very simple model to achieve state-ofart weakly-supervised and out-of-domain POS taggers. Use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy. These encouraging results show that using free, collaborative NLP resources can in fact produce results of the same level or better than using expensive annotations for many languages. It would be very interesting and perhaps necessary to incorporate this additional data in order to tackle challenges that arise across a larger number of language types, specifically non-European languages.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-2133",
        "summarized_text": "We also note that, unlike previous works involving topic modeling, we did not remove stop words and punctuation, but rather assumed that these features would have a relatively uniform topic distribution. We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task. This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training. Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models",
        "rouge_score": {}
    },
    {
        "paper_id": "D12-1069",
        "summarized_text": "We presented an algorithm for training a semantic parser in the form of a probabilistic Combinatory Categorial Grammar, using these two types of weak supervision. Together, these two experimental analyses suggest that the combination of syntactic and semantic weak supervision is indeed a sufficient basis for training semantic parsers for a diverse range of corpora and predicate ontologies. We used this algorithm to train a semantic parser for an ontology of 77 Freebase predicates, using Freebase itself as the weak semantic supervision. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences.",
        "rouge_score": {}
    },
    {
        "paper_id": "P08-1028",
        "summarized_text": "We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials. This paper proposes a framework for representing the meaning of phrases and sentences in vector space. The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here. We formulated composition as a function of two vectors and introduced several models based on addition and multiplication. We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al., 1988) and inductive inference (Heit and Rubinstein, 1994).",
        "rouge_score": {
            "rouge1": 0.501708599384097,
            "rouge2": 0.24429869147252278,
            "rougeL": 0.266352401070677
        }
    },
    {
        "paper_id": "W05-0636",
        "summarized_text": "So far, we have not been able to show improvement over selecting the 1-best parse tree. Our results with Collins-style reranking are too preliminary to draw definite conclusions, but the potential improvement does not appear to be great. To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser. In this paper, we have considered several methods for reranking parse trees using information from semantic role labeling. In this paper, we confirm these results with a MaxEnt-trained SRL model, and we extend them to show that weighting the probabilities does not help either. In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses. Our current results are negative, because a locallytrained SRL model can return inaccurate probability estimates.",
        "rouge_score": {}
    },
    {
        "paper_id": "A00-2018",
        "summarized_text": "We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head. We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank. We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100. We expect that as we experiment with other, more semantic conditioning information, the importance of this aspect of the model will increase.",
        "rouge_score": {
            "rouge1": 0.5215234468600441,
            "rouge2": 0.293492937473044,
            "rougeL": 0.3751173800133409
        }
    },
    {
        "paper_id": "N06-1039",
        "summarized_text": "As its key technique, we presented Unrestricted Relation Discovery that tries to find parallel correspondences between multiple entities in a document, and perform clustering using basic patterns as features. We presented the implementation of our preliminary system and its outputs. surface text patterns for a question answering system. of the 40th Annual Meeting of the As The number of the source articles that contained a mention of the hurricane is shown in the right column. In this paper we proposed Preemptive Information Extraction as a new direction of IE research. To increase the number of basic patterns, we used a cluster of comparable articles instead of a single document.",
        "rouge_score": {}
    },
    {
        "paper_id": "W06-2920",
        "summarized_text": "The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. We also hope that by combining parsers we can achieve even better performance, which in turn would facilitate the semi-automatic enlargement of existing treebanks and possibly the detection of remaining errors. There are many directions for interesting research building on the work done in this shared task. We also give an overview of the parsing approaches that participants took and the results that they achieved. We hope that the resources created and lessons learned during this shared task will be valuable for many years to come but also that they will be extended and improved by others in the future, and that the shared task website will grow into an informational hub on multilingual dependency parsing.",
        "rouge_score": {}
    },
    {
        "paper_id": "P05-1065",
        "summarized_text": "Reading Level Assessment Using Support Vector Machines And Statistical Language Models In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level. Statistical LMs were used to classify texts based on reading level, with trigram models being noticeably more accurate than bigrams and unigrams. This task can be addressed with natural language processing technology to assess reading level. Reading proficiency is a fundamental component of language competency. However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers. We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.",
        "rouge_score": {}
    },
    {
        "paper_id": "C04-1074",
        "summarized_text": "The paper aims at a deeper understanding of sev eral well-known algorithms and proposes ways to optimize them. The factors used in the algorithms and the algorithms themselves are evaluated on a Germancorpus annotated with syntactic and coreference in formation (Negra) (Skut et al, 1997). It describes and discusses factorsand strategies of factor interaction used in the algo rithms. A commonformat for pronoun resolution algorithms with sev eral open parameters is proposed, and the parameter settings optimal on the evaluation data are given.",
        "rouge_score": {}
    },
    {
        "paper_id": "I05-6010",
        "summarized_text": "The information gained from cor pus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and extraction tool for German text corpora. Moreover,some examples are given that under line the necessity of integrating somekind of information other than gram mar sensu stricto into the treebank. This article is devoted to the problem of quantifying noun groups in German.After a thorough description of the phenom ena, the results of corpus-based in vestigations are described. We argue that a more sophisticatedand fine-grained annotation in the tree bank would have very positve effects onstochastic parsers trained on the tree bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source ofdata for theoretical linguistic investigations. Some remarks on the Annotation of Quantifying Noun Groups in Treebanks",
        "rouge_score": {}
    },
    {
        "paper_id": "W04-0305",
        "summarized_text": "When accuracy is plotted as a function of k (figure 1), we found that there is a large increase in accuracy when the first word of lookahead is added (only 2.7% F-measure below non-deterministic search). When this pruning is applied directly after each word, there is a large reduction in accuracy (8.3% F-measure) as compared to the non-deterministic search. When expressed in terms of search, this means that the deterministic pruning is done k words behind a non-deterministic search for the best parse, based on a sum over the partial parses found by the non-deterministic search. We use a neural network to estimate the probabilities for an incremental history-based probability model based on leftcorner parsing. terministic parsing by characterizing these issues in terms of the search procedure used by a statistical parser.",
        "rouge_score": {}
    },
    {
        "paper_id": "D11-1006",
        "summarized_text": "We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm to adapt the transferred parsers to the respective target language, obtaining an additional 16% error reduction on average in a multi-source setting. We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders S\u00f8gaard for sharing early drafts of their recent related work.",
        "rouge_score": {}
    },
    {
        "paper_id": "P03-1013",
        "summarized_text": "This finding is at odds with what has been reported for parsing models trained on the Penn Treebank. We propose an alternative model that uses sister-head dependencies instead of head-head dependencies. This model uses lexical sisterhead dependencies, which makes it particularly suitable for parsing Negra\u2019s flat structures. We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German. This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra. We presented the first probabilistic full parsing model for German trained on Negra, a syntactically annotated corpus. This can the thought of as a way of binarizing the flat rules in Negra. This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.",
        "rouge_score": {}
    },
    {
        "paper_id": "W08-1007",
        "summarized_text": "We report a labeled attachment score close to 90% for dependency versions of the TIGER and T\u00a8uBa- D/Z treebanks. We can report state-of-the-art results for parsing the dependency versions of two German treebanks, and we have demonstrated, with promising results, how a dependency parser can parse full constituent structures by encoding the inverse mapping in complex arc labels of the dependency graph. Moreover, the parser is able to recover both constituent labels and grammatical functions with an F-Score over 75% for T\u00a8uBa-D/Z and over 65% for TIGER. We believe that this method can be improved by using, for example, head-finding rules. We have shown that a transition-based dependencydriven parser can be used for parsing German with both dependency and constituent representations. We present a dependency-driven parser that parses both dependency structures and constituent structures.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-1086",
        "summarized_text": "We hope that by modeling context in both axes, we will be able to completely replace composed-rule grammars with smaller minimal-rule grammars. Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both trainand decoding Here, we take the approach, where we only use minrules that cannot be formed out other rules), and instead rely on a model the derivation history to capture dependencies between minimal rules. We have developed a rule Markov model that captures vertical bigrams and trigrams of minimal rules, and tested it in the framework of treeto-string translation. Second, when we compare against vertically composed rules we are able to get about the same B score, but our model is much smaller and decoding with our model is faster.",
        "rouge_score": {}
    },
    {
        "paper_id": "N06-2033",
        "summarized_text": "We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We have presented a reparsing scheme that produces results with accuracy higher than the best individual parsers available by combining their results. By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score. In constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score. We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.",
        "rouge_score": {}
    },
    {
        "paper_id": "W10-2924",
        "summarized_text": "There are several possible directions for extending the current approach. We present a new method for joint entity and relation extraction using a graph we call a \u201ccard-pyramid.\u201d This graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes. We presented an efficient parsing algorithm for jointly labeling a card-pyramid using dynamic programming and beam search. The experiments demonstrated the benefit of our joint extraction method over a pipelined approach. We introduced a card-pyramid graph structure and presented a new method for jointly extracting entities and their relations from a sentence using it. The card-pyramid structure could be used to perform other languageprocessing tasks jointly with entity and relation extraction. Thus, it would be interesting to apply card-pyramid parsing to extract higher-order relations (such as causal or temporal relations).",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-1141",
        "summarized_text": "We gave good reasons why this should be done, and we presented an effective method showing how this could be done. We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications. The problem with this approach is that word structures and phrase structures are then not treated in a truly unified way, and besides the 33 part-of-speech tags originally contained in Penn Chinese Treebank, another 33 tags ending with \u2018f\u2019 are introduced. We leave this problem open for now and plan to address it in future work. We showed that word structures can be recovered with high precision, though there\u2019s still much room for improvement, especially for higher level constituent parsing. With the progress in statistical parsing technology and the development of large scale treebanks, the time has now come for this paradigm shift to happen.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1051",
        "summarized_text": "We have argued that tree transformation based semantic parsing can benefit from the literature on formal language theory and tree automata, and have taken a step in this direction by presenting a tree transducer based semantic parser. The new VB algorithm results in an overall performance improvement for the transducer over EM training, and the general effectiveness of the approach is further demonstrated by the Bayesian transducer achieving highest accuracy among other tree transformation based approaches. We demonstrate this by both building on previous work in training tree transducers using EM (Graehl et al., 2008), and describing a general purpose variational inference algorithm for adapting tree transducers to the Bayesian framework. This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions.",
        "rouge_score": {}
    },
    {
        "paper_id": "E12-1083",
        "summarized_text": "In this paper we analyze patents along the orthogonal dimensions of topic and textual structure. The most straightforward approach to improve machine translation performance on patents is to enlarge the training set to include all available data. We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters. Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents. A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs. We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance. We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning.",
        "rouge_score": {}
    },
    {
        "paper_id": "P07-1108",
        "summarized_text": "With a small bilingual corpus available for Lf-Le, we built a translation model, and interpolated it with the pivot model trained with the large Lf-Lp and Lp-Le bilingual corpora. Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly with 5,000 sentence pairs for French-Spanish translation. We also performed experiments using Lf-Lp and Lp-Le corpora of different sizes. To perform translation between Lf and Le, we bring in a pivot language Lp, via which the large corpora of Lf-Lp and Lp-Le can be used to induce a translation model for Lf-Le. Using only bilingual corpora, we can build a model for The advantage of this method lies in that we can perform between even if there is no bilingual corpus available for this language pair.",
        "rouge_score": {}
    },
    {
        "paper_id": "W03-1509",
        "summarized_text": "Though many approaches have been tried, the result is still not satisfactory. The F-measure of person names, location names, and organization names on the newswire test data for the 1999 IEER evaluation in Mandarin is 86.84%, 84.40% and 76.22% respectively. So in the future we will shift our algorithm to other languages. Our lab was mainly devoted to cross-language information processing and its application. Unfortunately, Chinese Named Entity Recognition (NER) is more difficult for the lack of capitalization information and the uncertainty in word segmentation. The main contribution of this paper is putting forward an approach which can make up for the limitation of using the statistical model or human knowledge purely by combining them organically. Thus we only need a relative small-sized labeled corpus (onemonth\u2019s Chinese People\u2019s Daily tagged with NER tags at Peking University) and human knowledge, but can achieve better performance.",
        "rouge_score": {}
    },
    {
        "paper_id": "P05-1053",
        "summarized_text": "This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.",
        "rouge_score": {}
    },
    {
        "paper_id": "E12-3010",
        "summarized_text": "Secondly, an overt pronoun cannot be restored from a finite impersonal verb without making the sentence ungrammatical; therefore, our approach is not useful for treating impersonal sentences. We then improved the rule-based system using a rule-based preprocessor to restore pro-drop as overt pronouns and a statistical post-editor to correct the translation. Results obtained from the second evaluation showed an improvement in the translation of both sorts of pronouns. Null subjects are non overtly expressed pronouns found in languages such as Italian and Spanish. We also evaluated its translation into a \u201cnon pro-drop\u201d language, that is, French, obtaining better results for personal pro-drop than for impersonal pro-drop, for both Its-2 and Moses, the two MT systems we tested. We use the Europarl corpus to evaluate two MT systems on their performance regarding null subject translation: Its-2, a rule-based system developed at LATL, and a statistical system built using the Moses toolkit.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-0314",
        "summarized_text": "ULISSE: an Unsupervised Algorithm for Detecting Reliable Dependency Parses ULISSE shows a promising performance against the output of two supervised parsers selected for their behavioral differences. This is the case, for instance, of the domain adaptation task in a self\u2013training scenario (McClosky et al., 2006), of the treebank construction process by minimizing the human annotators\u2019 efforts (Reichart and Rappoport, 2009b), of n\u2013best ranking methods for machine translation (Zhang, 2006). To our knowledge, it represents the first unsupervised ranking algorithm operating on dependency representations which are more and more gaining in popularity and are arguably more useful for some applications than constituency parsers. This study is being carried out with a specific view to NLP tasks which might benefit from the ULISSE algorithm. ULISSE is an unsupervised linguistically\u2013driven method to select reliable parses from the output of dependency parsers.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1015",
        "summarized_text": "This paper provides an algorithmic framework for learning statistical models involv ing directed spanning trees, or equivalently non-projective dependency structures. Xavier Carreraswas supported by the Catalan Ministry of Innova tion, Universities and Enterprise, and a grant from NTT, Agmt. We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff?s Matrix-Tree Theorem. The new training methods give improvements in accuracy over perceptron-trained models. This paper describes inference algorithms forspanning-tree distributions, focusing on the funda mental problems of computing partition functionsand marginals. To demonstrate an application of the method, we perform experiments which use the algorithm in training both log-linear and max-margin dependency parsers. The max-margin method gave improved/worse parses for 500/383 sentences. Terry Koo was funded by a grant from the NSF (DMS-0434222) and a grantfrom NTT, Agmt.",
        "rouge_score": {}
    },
    {
        "paper_id": "P13-1109",
        "summarized_text": "Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. Our results showed improvement over the baselines both in intrinsic evaluations and on BLEU. We presented a novel approach for inducing oov translations from a monolingual corpus on the source side and a parallel data using graph propagation. We also plan to explore different graph propagation objective functions. Regularizing these objective functions appropriately might let us scale to much larger data sets with an order of magnitude more nodes in the graph. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases.",
        "rouge_score": {}
    },
    {
        "paper_id": "P05-1013",
        "summarized_text": "We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. This leads to the best reported performance for robust non-projective parsing of Czech. We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques. Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.",
        "rouge_score": {
            "rouge1": 0.6359476552103717,
            "rouge2": 0.42978344022317444,
            "rougeL": 0.4055621902208824
        }
    },
    {
        "paper_id": "W05-1505",
        "summarized_text": "Our model, based on a MaxEnt classifier, improves overall dependency accuracy by .7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures. The goal is to recover non-projective dependency structures that are lost when using state-of-the-art constituencybased parsers; we show that our technique recovers over 50% of these dependencies. Our algorithm provides a simple framework for corrective modeling of dependency trees, making no prior assumptions about the trees. However, in the current model, we focus on trees with local errors. The continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees. We have presented a Maximum Entropy-based corrective model for dependency parsing. Overall, our technique improves dependency parsing and provides the necessary mechanism to recover non-projective structures. We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers.",
        "rouge_score": {}
    },
    {
        "paper_id": "C10-1151",
        "summarized_text": "In this paper, we focus on the challenge of con stituent syntactic parsing with treebanksof different annotations and propose a collaborative decoding (or co-decoding) ap proach to improve parsing accuracy byleveraging bracket structure consensus be tween multiple parsing decoders trainedon individual treebanks. Experimental results show the effectiveness of the proposed approach, which outperforms state of-the-art baselines, especially on long sentences. This paper proposed a co-decoding approach tothe challenge of heterogeneous parsing. However, such corpora are generally used independently due to distinctions in annotation standards. Experiments demonstratethe effectiveness of the co-decoding approach, es pecially the effectiveness on long sentences. We would like to thank our anonymous reviewers for their comments. There often exist multiple corpora for the same natural language processing (NLP)tasks. For the purpose of full use of readily available human annotations, it is significant to simultaneously utilize multiple corpora of different annotation standards.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1111",
        "summarized_text": "We list additional ideas that were not attempted due to time constraints, but that are likely to produce improved results. We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing. We apply this pars ing framework to both tracks of the CoNLL 2007 shared task, in each case taking ad vantage of multiple models trained with different learners. This work was supported in part by Grant-in-Aid for Specially Promoted Re search 18002007. There are several possible extensions and improvements to the approach we have described. We also thank the reviewers for their comments and suggestions, and Yusuke Miyao for insightful discussions. Parser actions are determined by a classifier, based on features that represent the current state of the parser.",
        "rouge_score": {}
    },
    {
        "paper_id": "W99-0623",
        "summarized_text": "Both of the switching techniques, as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments. Two general approaches are presented and two combination techniques are described for each approach. We have presented two general approaches to studying parser combination: parser switching and parse hybridization. We plan to explore more powerful techniques for exploiting the diversity of parsing methods. Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result. For each experiment we gave an nonparametric and a parametric technique for combining parsers. The resulting parsers surpass the best previously published performance results for the Penn Treebank.",
        "rouge_score": {
            "rouge1": 0.6293456540516839,
            "rouge2": 0.4518125736410928,
            "rougeL": 0.35738464677578746
        }
    },
    {
        "paper_id": "P09-1111",
        "summarized_text": "We have presented an algorithm for the binarization of a LCFRS with fan-out 2 that does not increase the fan-out, and have discussed how this can be applied to improve parsing efficiency in several practical applications. It still needs to be investigated whether the proposed technique, based on determinization of the choice of the reduction, can also be used for finding binarizations for LCFRS with fan-out larger than two, again without increasing the fan-out. Our algorithm has optimal time complexity, since it works in linear time with respect to the input production length. This is still an open problem; see (G\u00b4omez-Rodr\u00b4\u0131guez et al., 2009) for discussion. In the algorithm of Figure 1, we can modify line 14 to return R even in case of failure.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-0115",
        "summarized_text": "This work has intentionally left the data as raw as possible, in order to keep the noise present in the models at a realistic level. This is an extremely promising result, indicating that it is possible to generalize linear transformation functions beyond single lexical items in Distributional Semantics\u2019 spaces. This rather negative result may be due to its relatively smaller size, but this excuse may only be applied to PLS, the only model that relies on parameter estimation. This fact is expected from a theoretical point of view: since the Noun is the head of the AN pair, it is likely that the complex expression and its head share much of their distributional properties. This paper proposes an improved framework to model the compositionality of meaning in Distributional Semantics.",
        "rouge_score": {}
    },
    {
        "paper_id": "P07-1021",
        "summarized_text": "In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages. In previous work, several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense; the best-known but also most restrictive of these is projectivity. Most constraints are formulated on fully specified structures, which makes them hard to integrate into models where structures are composed from lexical information. Dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity.",
        "rouge_score": {}
    },
    {
        "paper_id": "P10-1074",
        "summarized_text": "While conventional wisdom says that adding more training data should always improve performance, this work is the first to our knowledge to incorporate singly-annotated data into a joint model, thereby providing a method for this additional data, which cannot be directly used by the non-hierarchical joint model, to help improve joint modeling performance. We performed experiments on joint parsing and named entity recognition, and found that our hierarchical joint model substantially outperformed a joint model which was trained on only the jointly annotated data. We built single-task models for the non-jointly labeled data, designing those single-task models so that they have features in common with the joint model, and then linked all of the different single-task and joint models via a hierarchical prior.",
        "rouge_score": {}
    },
    {
        "paper_id": "N06-1022",
        "summarized_text": "We use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level. What we actually did is to propose all possible constituents at the next level, and immediately rule out those lacking a corresponding constituent remaining at the previous level. We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results. We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG. Working bottom up, estimating the inside probability is easy (we just sum the probability of all the trees found to build this constituent).",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-2429",
        "summarized_text": "The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches). The most accurate approaches to Word Sense Disambiguation (WSD) for biomedical documents are based on supervised learning. Three techniques for determining the number of examples to use for training are explored. Surprisingly it was also found that using information from the MBR did not improve performance. It is found that a supervised approach (which makes use of manually labeled data) provides the best results. This paper describes the development of a large scale WSD system based on automatically labeled examples. We find that these examples can be generated for the majority of CUIs in the UMLS Metathesaurus. The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus.",
        "rouge_score": {}
    },
    {
        "paper_id": "E09-1053",
        "summarized_text": "We used our new dependency view to compare the strong generative capacity of PF-CCG with other mildly contextsensitive grammar formalisms. We propose a novel algorithm for extracting dependencies from the derivations of a large fragment of CCG. We would then be set to compare the behavior of wide-coverage statistical parsers for CCG with statistical dependency parsers. We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees \u2013 but the mechanisms they use to bring the words in these trees into a linear order are incomparable. We anticipate that our results about the strong generative capacity of PF-CCG will be useful to transfer algorithms and linguistic insights between formalisms.",
        "rouge_score": {}
    },
    {
        "paper_id": "W10-1407",
        "summarized_text": "In both treebanks, discontinuities are annotated with crossing branches. In future work, all of the presented evaluation methods will be investigated to greater detail. In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks. In order to do this, we will parse our data sets with current state-of-the-art systems. Last, since an algorithm is available which extracts LCFRSs from dependency structures (Kuhlmann and Satta, 2009), the parser is instantly ready for parsing them. Our evaluation, which used different metrics, showed that a PLCFRS parser can achieve competitive results. We have investigated the possibility of using Probabilistic Linear Context-Free Rewriting Systems for direct parsing of discontinuous constituents. Generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser.",
        "rouge_score": {}
    },
    {
        "paper_id": "P04-1042",
        "summarized_text": "We used customized features to compensate to some extent, but temporal annotation already exists in WSJ and could be used. We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks. We suspect this indicates that dislocated terminals are being usefully identified and mapped back to their proper governors, even if the syntactic projections of these terminals and governors are not being correctly identified by the parser. We note that Klein and Manning (2003) independently found retention of temporal NP marking useful for PCFG parsing. We use an algorithm based on loglinear classifiers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation. We were able to find a number of features to distinguish some cases, such as the presence of certain unambiguous relativeclause introducing complementizers beginning an S node, but much ambiguity remained.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-3131",
        "summarized_text": "We have finally discussed the impact of the availability of WMTscale data on system building decisions and provided comparative experimental results. We have presented the French-English translation system built for the NAACL WMT12 shared translation task, including descriptions of our data selection and text processing techniques. This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12). Experimental results have shown incremental improvement for each addition to our baseline system. We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building.",
        "rouge_score": {}
    },
    {
        "paper_id": "P04-1015",
        "summarized_text": "The improvement that was derived from the additional punctuation features demonstrates the flexibility of the approach in incorporating novel features in the model. When the training algorithm is provided the generative model scores as an additional feature, the resulting parser is quite competitive on this task. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent. This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. This paper was intended to compare search with the generative model and the perceptron model with roughly similar feature sets. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.",
        "rouge_score": {}
    },
    {
        "paper_id": "E99-1016",
        "summarized_text": "An em- pirical evaluation of the method yields very good results for NP/PP chunking of German ewspaper texts. This paper presents a new approach to partial parsing of context-free structures. Each layer of the resulting structure is represented byits own Markov Model, and output of a lower layer is passed as input to the next higher layer.",
        "rouge_score": {}
    },
    {
        "paper_id": "C08-1049",
        "summarized_text": "This paper describes a reranking strategy calledword lattice reranking. Using word- and POS- gram information, this reranking technique achieves an error reduction of 16.3% on Joint S&T, and 11.9% on segmentation, over the baseline classifier, and it also outperformsreranking on n-best list. With aperceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local fea tures that can?t be easily incorporated intothe perceptron baseline. We will also investigate the featureselection strategy under the word lattice architec ture, for effective use of non-local information. We show our special thanks to Liang Huang for his valuable suggestions. It confirms that word lattice reranking can effectively use non-local infor mation to select the best candidate result, from a relative small representation structure while with aquite high oracle F-measure. Word Lattice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging",
        "rouge_score": {}
    },
    {
        "paper_id": "P04-1054",
        "summarized_text": "We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a \u201cbag-of-words\u201d kernel. We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. These results contradict those given in Zelenko et al. (2003), where the sparse kernel achieves 2-3% better F1 performance than the contiguous kernel. While the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low. We have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel.",
        "rouge_score": {}
    },
    {
        "paper_id": "P08-1013",
        "summarized_text": "We have done a mum number of best hypotheses N. first step in this direction by estimating the probability of a parse tree. To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task. We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree. This obviously reduces the benefit of pure grammaticality information. We report a significant reduction in word error rate compared to a state-of-the-art baseline system. We have presented a language model based on a precise, linguistically motivated grammar, and we have successfully applied it to a difficult broad-domain task.",
        "rouge_score": {}
    },
    {
        "paper_id": "P10-1097",
        "summarized_text": "We solved the problems of data sparseness and incompatibility of dimensions which are inherent in this approach by modeling contextualization as an interplay between first- and second-order vectors. We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. We have presented a novel method for adapting the vector representations of words according to their context. We further showed that a weakly supervised heuristic, making use of WordNet sense ranks, can be significantly improved by incorporating information from our system. We studied the effect that context has on target words in a series of experiments, which vary the target word and keep the context constant.",
        "rouge_score": {}
    },
    {
        "paper_id": "P07-1083",
        "summarized_text": "This is the first research to apply discriminative string similarity to the task of cognate identification. This approach achieves exceptional performance; on nine separate cognate identification experiments using six language pairs, we more than double the precision of traditional orthographic measures like Longest Common Subsequence Ratio and Dice\u2019s Coefficient. We have introduced and successfully applied an alignment-based framework for discriminative similarity that consistently demonstrates improved performance in both bitext and dictionary-based cognate identification on six language pairs. We may also compare alignmentbased discriminative string similarity with a more complex discriminative model that learns the alignments as latent structure (McCallum et al., 2005). We have also made available our cognate identification data sets, which will be of interest to general string similarity researchers. We also show strong improvements over other recent discriminative and heuristic similarity functions. We gather features from substring pairs consistent with a character-based alignment of the two strings.",
        "rouge_score": {}
    },
    {
        "paper_id": "W10-3504",
        "summarized_text": "This method can then be used to expand corpora such as the Fracas test-suite (Cooper et al., 1996) which is more oriented to specific semantic phenomena. In this paper we proposed a method for expanding existing textual entailment corpora that leverages Wikipedia. We report empirical evidence that our method successfully expands existing textual entailment corpora. The method is extremely promising as it allows building corpora homogeneous to existing ones. The initial increase of performances is an interesting starting point. The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. The model we have presented is not strictly related to the RTE corpora.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-2074",
        "summarized_text": "It was shown that the same model structure can be effectively applied to feature combination, alignment combination and domain adaptation. In this paper we present a novel discriminative mixture model for statistical machine translation (SMT). The features in each region are tied together to share the same mixture weights that are optimized towards the maximum BLEU scores. It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task. This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT. We model the feature space with a log-linear combination of multiple mixture components. We also point out that it is straightforward to combine any of these three.",
        "rouge_score": {}
    },
    {
        "paper_id": "W10-1404",
        "summarized_text": "We have extended previous works, giving a finer grained description of morphosyntactic features on the learner\u2019s configuration, (FO: feature optimization; TP TC TS: Pseudo-projective, Coordination and Subordinated sentence transformations; SVG, SNP, SC: Stacking (feature passing) on Verb Groups, NPs and Coordination; SP, SCH, SGP: Stacking (category, features and dependency) on Parent, CHildren and GrandParent; *: statistically significant in McNemar's test, p < 0.005; **: statistically significant, p < 0.001) showing that it can significantly improve the results. We studied several proposals for improving a baseline system for parsing the Basque Treebank. We present a set of experiments on dependency parsing of the Basque Dependency Treebank (BDT). We have obtained the following main results: \u2022 Using rich morphological features.",
        "rouge_score": {}
    },
    {
        "paper_id": "E06-2025",
        "summarized_text": "However, the only current estimation methods that are consistent in the frequency-distribution test, have the linguistically undesirable property of converging to a distribution with all probability mass in complete parse trees. The STSG formalism, however, allows for many different derivations of the same parse tree, and for many different grammars to generate the same frequency-distribution. We have shown that DOP1 and methods based on correction factors also fail the weaker frequency-distribution test. In forthcoming work we therefore study yet another estimator, and the linguistically motivated evaluation criterion of convergence to a maximally general STSG consistent with the training data5. We show that all current estimation methods are inconsistent in the \u201cweight-distribution test\u201d, and argue that these results force us to rethink both the methods proposed and the criteria used. We analyze estimation methods for Data- Oriented Parsing, as well as the theoretical criteria used to evaluate them.",
        "rouge_score": {}
    },
    {
        "paper_id": "P08-1006",
        "summarized_text": "We also found that accuracy improvements vary when parsers are retrained with domainspecific data, indicating the importance of domain adaptation and the differences in the portability of parser training methods. This indicates that different results might be obtained with other tasks. The basic idea is to measure the accuracy improvements of the PPI extraction task by incorporating the parser output as statistical features of a machine learning classifier. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. This paper presents a comparative evaluation of several state-of-the-art English parsers based on different frameworks. We have presented our attempts to evaluate syntactic parsers and their representations that are based on different frameworks; dependency parsing, phrase structure parsing, or deep parsing.",
        "rouge_score": {}
    },
    {
        "paper_id": "D08-1008",
        "summarized_text": "To handle this situation fairly to both types of systems, we carried out a two-way evaluation: conversion of dependencies to segments for the dependency-basedsystem, and head-finding heuristics for segment based systems. We have described a dependency-based system1 for semantic role labeling of English in the PropBankframework. To tackle the problemof joint syntactic?semantic analysis, the system relies on a syntactic and a semantic subcomponent. Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently. Whether this is an advantage or a drawback will depend on the applica tion ? for instance, a template-filling system might need complete segments, while an SRL-based vectorspace representation for text categorization, or a rea soning application, might prefer using heads only. We present a PropBank semantic role label ing system for English that is integrated with a dependency parser.",
        "rouge_score": {}
    },
    {
        "paper_id": "W08-2107",
        "summarized_text": "This paper investigates, in a first stage, some methods for the automatic acquisition of verb-particle constructions (VPCs) taking into account their statistical properties and some regular patterns found in productive combinations of verbs and particles. We investigated the identification of VPCs using a combination of statistical methods and linguistic information, and whether there is a correlation between the productivity of VPCs and their semantics that could help us detect if a VPC is idiomatic or compositional. These rules are applied in a constrained way to verbs already in the lexicon, according to their semantic classes. The results obtained show that such combination can successfully be used to detect VPCs and distinguish idiomatic from compositional cases. This study can also be complemented with the results of investigations into the semantics of VPCs, as discussed by both Bannard (2005) and McCarthy et al. (2003).",
        "rouge_score": {}
    },
    {
        "paper_id": "P07-1055",
        "summarized_text": "It is realistic gression to train the local models yielded similar re- to imagine a training set where many examples do sults to those in Table 2. not have every level of sentiment annotated. Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions. 4 Future Work Finally we should note that experiments using One important extension to this work is to augment CRFs to train the structured models and logistic re- the models for partially labeled data. Experiments show that this method can significantly reduce classification error relative to models trained in isolation. In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another.",
        "rouge_score": {}
    },
    {
        "paper_id": "P08-1102",
        "summarized_text": "Under this model, many knowledge sources that may be intractable to be incorporated into the perceptron directly, can be utilized effectively in the outside-layer linear model. On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline. This is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large. We proposed a cascaded linear model for Chinese Joint S&T. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. We will investigate these problems in the following work.",
        "rouge_score": {
            "rouge1": 0.6250746250073925,
            "rouge2": 0.44596917498501526,
            "rougeL": 0.33563625535499925
        }
    },
    {
        "paper_id": "P11-2124",
        "summarized_text": "We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We demonstrated that the combination of lattice parsing with the PCFG-LA Berkeley parser is highly effective. We plan to address this point in future work. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.",
        "rouge_score": {}
    },
    {
        "paper_id": "S12-1097",
        "summarized_text": "This approach also makes use of information from WordNet. Two approaches for computing semantic similaritybetween sentences were explored. This paper describes the University of Sheffield?s submission to SemEval-2012 Task 6: Semantic Text Similarity. The results reported here show that the semantic text similarity task can be successfully approached using lexical overlap techniques augmented withlimited semantic information derived from Word Net. The first, unsu pervised approach, uses a vector space model andcomputes similarity between sentences by comparing vectors while the second is supervised and rep resents the sentences as sets of n-grams. We would also like to explore methods for improving performance of the n-gram over lap approach and making it more robust to different data sets.",
        "rouge_score": {}
    },
    {
        "paper_id": "W09-1218",
        "summarized_text": "We believe that there is still room for improvements since we used only two types of global features for the argument classifier. Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage Approximate Max-Margin Linear Models We would like to investigate syntactic-semantic joint approaches with reasonable time complexities. We attempted to perform Nbest based joint approach, however, the expensive computational cost of the 2nd-order projective parser discouraged it. To the best of our knowledge, three types of joint approaches have been proposed: N-best based approach (Johansson and Nugues, 2008), synchronous joint approach (Henderson et al., 2008), and a joint approach where parsing and SRL are performed simultaneously (Llu\u00b4\u0131s and M`arquez, 2008). The system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classifier, and an argument classifier for semantic dependency parsing. This paper describes a system for syntacticsemantic dependency parsing for multiple languages.",
        "rouge_score": {}
    },
    {
        "paper_id": "W09-2208",
        "summarized_text": "It is simple and, we believe not limited by the choice of the classifier. We also show that our algorithm achieves high accuracy when the training and test sets are from different domains. Such independent evidence is used to automatically extract highaccuracy and non-redundant data, leading to a much improved classifier at the next iteration. With only a small amount of training data, our algorithm can achieve a better NE tagging accuracy than a supervised algorithm with a large amount of training data. We presented a simple semi-supervised learning algorithm for NER using conditional random fields (CRFs). The algorithm is based on exploiting evidence that is independent from the features used for a classifier, which provides high-precision labels to unlabeled data. We show that our algorithm achieves an averimprovement of recall and precision compared to the supervised algorithm.",
        "rouge_score": {}
    },
    {
        "paper_id": "D11-1086",
        "summarized_text": "The objective in this workis to find matrices such that the inverse of the co variance matrix is sparse which has applications in Gaussian processes.In this paper, we tried sparsification in the con text of CCA only but our technique is general andcan be applied to its variants like OPCA. We are not aware of any NLP research that attempts to recover the sparseness of the covariance matrices to improve the projection directions. Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. This is cer tainly encouraging and in future we would like to explore more sophisticated techniques to recover the sparsity based on the training data itself. Their objective is to find projection directions such that the original documents are repre sented as a sparse vectors in the common sub-space.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1065",
        "summarized_text": "It also achieves accuracy comparable to DL-CoTrain, but does not require the features to be split into two independent views. This paper introduces a novel variant of the Yarowsky algorithm based on this view. We also believe that our method for adapting Collins and Singer (1999)\u2019s cautiousness to Yarowsky-prop can be applied to similar algorithms with other underlying classifiers, even to structured output models such as conditional random fields. The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets. It is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function. N Our novel algorithm achieves accuracy comparable to Yarowsky-cautious, but is better theoretically motivated by combining ideas from Haffari and Sarkar (2007) and Subramanya et al. (2010).",
        "rouge_score": {}
    },
    {
        "paper_id": "N03-2024",
        "summarized_text": "This approach will make better use of the Markov model, but it also requires work towards deeper semantic processing of the input. Referring expressions can be generated by recombining different pieces of the input rather than the currently used extraction of full NPs. One possible usage of the model which is not discussed in the paper but is the focus of current and ongoing work, is to generate realizations \u201con demand\u201d. Semantic information is needed in order to prevent the combination of almost synonymous premodifiers in the same NP and also for the identification of properties that are more central for the enity with respect to the focus of the input cluster. The interpretation of the probabilistic data helps us gain insight on how extractive summaries can be rewritten in an efficient manner to produce more fluent and easy-to-read text.",
        "rouge_score": {}
    },
    {
        "paper_id": "W07-0738",
        "summarized_text": "The reason is that, while MT quality aspects are its scope to the lexical dimension. We have shown, through empirical evidence, that linguistic features at more abstract levels may provide more reliable system rankings, specially when the systems under evaluation do not share the same lexicon. For instance, the following set could be used: { \u2018DP-HWCr-4\u2019, \u2018DP-Oc-*\u2019, \u2018DP-Ol-*\u2019, \u2018DP-Or-*\u2019, \u2018CPSTM-9\u2019, \u2018SR-Or-*\u2019, \u2018SR-Orv\u2019 } All these metrics are among the top-scoring in all the translation tasks studied. Moreover, relying on automatic processors implies two other important limitations. Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, may not be a reliable MT quality indicator. In this work, we suggest using metrics which take into account linguistic features at more abstract levels.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1045",
        "summarized_text": "Learning the semantics of language from the perceptual context in which it is uttered is a useful approach because only minimal human supervision is required. While the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets. Learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language. Recent work by Chen and Mooney (2011) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs. We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon\u2019s Mechanical Turk we can further improve the results. We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-2002",
        "summarized_text": "Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. We presented distance-based metrics defined for trees over lattices and applied them to evaluating parsers on joint morphological and syntactic disambiguation. The protocol uses distance-based metrics defined for the space of trees over lattices.",
        "rouge_score": {}
    },
    {
        "paper_id": "W07-0718",
        "summarized_text": "Understanding the exact causes of those differences still remains an important issue for future research. We measured the correlation of automatic evaluation metrics with human judgments. j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. This year\u2019s evaluation also measured the agreement between human assessors by computing the Kappa coefficient. We measured timing and intraand inter-annotator agreement for three types of subjective evaluation. We take the human judgments to be authoritative, and used them to evaluate the automatic metrics. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured correlation using Spearman\u2019s coefficient and found that three less frequently used metrics were stronger predictors of human judgments than Bleu.",
        "rouge_score": {}
    },
    {
        "paper_id": "W10-2009",
        "summarized_text": "We find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987). Thus, it may very well be the case that the NP-coordination preference that was present in our training corpus may have had a pragmatic origin related to topic-structure. We argued that our grammar showed an overall preference for NP-coordination but this preference was not necessarily reflected on each and every rule that dealt with coordinations. We found these results to be robust for a critical model parameter (beam size), which suggests that syntactic processing in human comprehension might be based on limited parallelism only. We argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference.",
        "rouge_score": {}
    },
    {
        "paper_id": "P06-1012",
        "summarized_text": "Using sense priors estimated by logistic regression further improves performance. We found that the WSD accuracies obtained with the method of (McCarthy et al., 2004) are on average 1.9% lower than our NBcal-EM method, and the difference is statistically significant. To gauge how well the sense priors are estimated, we measure the KL divergence between the true sense priors and the sense priors estimated by using the predictions of (uncalibrated) multiclass naive Bayes, calibrated naive Bayes, and logistic regression. This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations. We trained logistic regression classifiers and evaluated them on the 4 datasets. To elaborate, we first use the probability estimates of logistic regression in Equations (2) and (3) to estimate the sense priors .",
        "rouge_score": {}
    },
    {
        "paper_id": "J01-2004",
        "summarized_text": " Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model  A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers  A small recognition experiment also demonstrates the utility of the model  A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity  The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling",
        "rouge_score": {
            "rouge1": 0.4593909812037431,
            "rouge2": 0.15025563210848644,
            "rougeL": 0.20545481915518046
        }
    },
    {
        "paper_id": "N07-1050",
        "summarized_text": "We have shown that, for languages with a non-negligible proportion of non-projective structures, parsing accuracy can be improved significantly by allowing non-projective structures to be derived. We have also shown that the parsing time can be reduced substantially, with only a marginal loss in accuracy, by limiting the degree of nonprojectivity allowed during parsing. Using data from five different languages, we evaluate an incremental deterministic parser that derives non-projective depenstructures in supported by SVM classifiers for predicting the next parser action. Moreover, by restricting the class of permissible structures to limited degrees of non-projectivity, the parsing time can be reduced by up to 50% without a significant decrease in accuracy. The experiments show that unrestricted non-projective parsing gives a significant improvement in accuracy, compared to a strictly projective baseline, with up to 35% error reduction, leading to state-of-the-art results for the given data sets.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1053",
        "summarized_text": "It can be effectively constructed and the maximal rank of the nononly increases by Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves. Kepser and Rogers (2011) show that non-strict TAG are strongly equivalent to CFTG(1). G\u00b4omez-Rodriguez et al. (2010) show that wellnested LCFRS of maximal fan-out k can be parsed in time O(n2k+2), where n is the length of the input string w \u2208 A\u2217. Linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol. it was shown Tree-adjoining grammars are not closed unstrong Comput. It follows from Section 6 of Engelfriet et al. (1980) that the classes CFTG(k) with k \u2208 ICY induce an infinite hierarchy of string languages, but it remains an open problem whether the rank increase in our lexicalization construction is necessary.",
        "rouge_score": {}
    },
    {
        "paper_id": "W05-0602",
        "summarized_text": "A Statistical Semantic Parser That Integrates Syntax And Semantics It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. We present experimentalresults demonstrating that SCISSOR produces more accurate semantic representa tions than several previous approaches. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We introduce a learning semantic parser,SCISSOR, that maps natural-language sentences to a detailed, formal, meaning representation language.",
        "rouge_score": {}
    },
    {
        "paper_id": "P13-1126",
        "summarized_text": "This paper proposed a new approach to domain adaptation in statistical machine translation, based on vector space models (VSMs). Thus, we obtain a decoding feature whose value represents the phrase pair\u2019s closeness to the dev. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. This was done purely out of convenience: there are many, many ways to define a vector space in this situation. This is a simple, computationally cheap form of instance weighting for phrase pairs. Vector Space Model for Adaptation in Statistical Machine Translation",
        "rouge_score": {}
    },
    {
        "paper_id": "P13-2073",
        "summarized_text": "We presented an experiment showing the effect of using two language independent features, source connectivity score and target connectivity score, to improve the quality of pivot-based SMT. We showed that these features help improving the overall translation quality. We also plan to explore language specific features which could be extracted from some seed parallel data, e.g., syntactic and morphological compatibility of the source and target phrase pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table.",
        "rouge_score": {}
    },
    {
        "paper_id": "N06-1037",
        "summarized_text": "We may integrate more features (such as head words or WordNet semantics) into nodes of parse trees. We can also benefit from the learning algorithm to study how to solve the data imbalance and sparseness issues from the learning algorithm viewpoint. This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction. We may adopt a two-step method (Culotta and Sorensen, 2004) to separately model the relation detection and characterization issues. We also would like to thank the three anonymous reviewers for their invaluable suggestions. We conclude that: 1) the relations between entities can be well represented by parse trees with carefully calibrating effective portions of parse trees; 2) the syntactic features embedded in a parse tree are particularly effective for relation extraction; 3) the convolution tree kernel can effectively capture the syntactic features for relation extraction.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-3407",
        "summarized_text": "We performed an additional test using the partial dependency analyzer\u2019s gold dependency relations as input to MaltParser. When performing this task, we found the problem of matching the treebank tokens with those obtained from the analyzers, as there were divergences on the treatment of multiword units, mostly coming from Named Entities, verb compounds and complex postpositions (formed with morphemes appearing at two different words). We will experiment the effect of using the output of the knowledge-based analyzers as input to the data-driven parsers in a stacked learning scheme. Yeh (2000) used the output of several baseline diverse parsers to increase the performance of a second transformation-based parser. We must take into account that this evaluation was performed on the gold POS tags, rather than on automatically assigned POS tasks, as in the present experiment.",
        "rouge_score": {}
    },
    {
        "paper_id": "P87-1015",
        "summarized_text": "We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems, and classified these formalisms on the basis of two features: path complexity; and path independence. We address the question of whether or not a formalism can generate only structural descriptions with independent paths. We contrasted formalisms such as CFG's, HG's, TAG's and MCTAG's, with formalisms such as IG's and unificational systems such as LFG's and FUG's. In order to observe the similarity between these constrained systems, it is crucial to abstract away from the details of the structures and operations used by the system. The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets. This property reflects an important aspect of the underlying linguistic theory associated with the formalism.",
        "rouge_score": {
            "rouge1": 0.504986642596354,
            "rouge2": 0.14955575957634984,
            "rougeL": 0.2384911165033761
        }
    },
    {
        "paper_id": "D09-1034",
        "summarized_text": "This work contributes to the important, developing enterprise of leveraging data-driven NLP approaches to derive new measures of high utility for psycholinguistic and neuropsychological studies. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. We have presented novel methods for teasing apart syntactic and lexical surprisal from a fully lexicalized parser, as well as for extending the operation of a predictive parser to capture novel entropy measures that are also shown to be relevant to psycholinguistic modeling. Such automatic methods provide psycholinguistically relevant measures that are intractable to calculate by hand. The empirical validation presented here demonstrated that the new measures \u2013 particularly syntactic entropy and syntactic surprisal \u2013 have high utility for modeling human reading time data.",
        "rouge_score": {}
    },
    {
        "paper_id": "W05-0638",
        "summarized_text": "Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ. In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs. In this paper, we add more full parsing features to argument classification models, and represent full parsing information as constraints in ILPs to resolve labeling inconsistencies. In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs. We also integrate two argument classification models, ME and SVM, by combining their argument classification results and applying them to the above-mentioned ILPs. The results show full parsing information increases the total F-score by 1.34%. The ensemble of SVM and ME also boosts the F-score by 0.77%.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-3145",
        "summarized_text": "In of the Sixth on Statistical Machine pages 187\u2013197. We obtained a moderate gain of 0.4 BLEU points with the ensemble decoding over the baseline system in newstest-2011. In French-English, we experimented the ensemble decoding framework that effectively utilizes the small amount of news genre data to improve the performance in the testset belonging to the same genre. We submitted systems in two language pairs FrenchEnglish and English-Czech for WMT-12 shared task. For newstest-2012, it performs comparably to that of the baseline and we are presently investigating the lack of improvement in newstest-2012. models for morpheme segmentation and morphology Transactions on Speech and Language 4(1):3:1\u20133:34, February. For Cz-En, We found that the BLEU scores do not substantially differ from each other and also the minor differences are not consistent for Test-11 and Test-12. Kriya - The SFU System for Translation Task at WMT-12",
        "rouge_score": {}
    },
    {
        "paper_id": "P04-1036",
        "summarized_text": "Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We use an automatically acquired thesaurus and a WordNet Similarity measure. Whilst we have used WordNet as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbours and the senses. word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al., 2004).",
        "rouge_score": {
            "rouge1": 0.48963658362527307,
            "rouge2": 0.21945944489255031,
            "rougeL": 0.22989879061156443
        }
    },
    {
        "paper_id": "N10-1091",
        "summarized_text": "In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. Second, we showed that well-formed dependency trees can be guaranteed without significant performance loss by linear-time approximate re-parsing algorithms. Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking. This study unearthed several non-intuitive yet important observations about ensemble models for dependency parsing. This study proves that fast and accurate ensemble parsers can be built with minimal effort.",
        "rouge_score": {}
    },
    {
        "paper_id": "E12-1055",
        "summarized_text": "We expand on work by (Foster et al., 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research. While techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately. We envision that a weighted combination could be useful to deal with noisy datasets, or applied after a clustering of training data. We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT).",
        "rouge_score": {}
    },
    {
        "paper_id": "D11-1033",
        "summarized_text": "Translation models trained on data selected in this way consistently outperformed the general-domain baseline while using as few as 35k out of 12 million sentences. This paper has also explored three simple yet effective methods for extracting these pseudo indomain sentences from a general-domain corpus. We have also shown in passing that the linear interpolation of translation models may work less well for translation model adaptation than the multiple paths decoding technique of (Birch et al., 2007). We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. This fast and simple technique for discarding over 99% of the general-domain training corpus resulted in an increase of 1.8 BLEU points. These sentences may be selected with simple cross-entropy based methods, of which we present three.",
        "rouge_score": {}
    },
    {
        "paper_id": "P10-1021",
        "summarized_text": "The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. This means that semantic costs are a significant predictor of reading time in addition to the well-known syntactic surprisal. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous analyses of semantic constraint have been conducted on different eye-tracking corpora (Dundee and Embra Corpus) and on different languages (English, French). Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure",
        "rouge_score": {}
    },
    {
        "paper_id": "E12-1014",
        "summarized_text": "We propose a novel algorithm to estimate reordering probabilities from monolingual data. We estimate the parameters of a phrasebased statistical machine translation sysfrom instead of a corpus. We extend existing research on bilingual lexicon induction estimate and phrasal translation probabilities for MT-scale phrasetables. We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone. We evaluated the performance of our algorithms in a full end-to-end translation system. Thus our techniques have stand-alone efficacy when large bilingual corpora are not available and also make a significant contribution to combined ensemble performance when they are. We report translation results for an end-to-end translation system using these monolingual features alone. We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features.",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-2022",
        "summarized_text": "We contribute a faster decoding algorithm for phrase-based machine translation. This leads to two contributions: hiding irrelevant state from features and an incremental refinement algorithm to find high-scoring combinations. Since our algorithm and cube pruning are both approximate, improvement can be used to increase speed or accuracy. When tuned to attain the same accuracy, our algorithm is 4.0\u20137.7 times as fast as the Moses decoder with cube pruning. We have contributed a new phrase-based search algorithm based on the principle that the language model cares the most about boundary words. Translation hypotheses keep track of state, such as context for the language model and coverage of words in the source sentence. This algorithm is implemented in a new fast phrase-based decoder, which we release as open-source under the LGPL at kheafield.com/code/.",
        "rouge_score": {}
    },
    {
        "paper_id": "N07-2041",
        "summarized_text": "Simultaneous Identification of Biomedical Named-Entity and Functional Relation Using Statistical Parsing Techniques We made use of a large unlabeled data set to train our relation extraction model. We also plan to train a graphical model based on all extracted BP, PL and BPL relations to infer relations from multiple sentences and documents. We build a parser that derives both syntactic and domain-dependent semantic information achieves an F-score of the relation extraction task. We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research. We have implemented a discriminative model (Liu et al., 2007) which takes as input the examples with gold named entities and identifies BPL relations on them.",
        "rouge_score": {}
    },
    {
        "paper_id": "P13-2009",
        "summarized_text": "The parser performs comparably to several recent purpose-built semantic parsers on the GeoQuery dataset, while training considerably faster than state-of-the-art systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. The second is the use of large, composed rules (rather than rules which trigger on only one lexical item, or on tree portions of limited depth (Lu et al., 2008)) in order to \u201cmemorize\u201d frequently-occurring largescale structures. We have presented a semantic parser which uses techniques from machine translation to learn mappings from natural language to variable-free meaning representations. The results also demonstrate the usefulness of two techniques which are crucial for successful MT, but which are not widely used in semantic parsing.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-2147",
        "summarized_text": "For English?German we attempted to improve the trans lation tables with a combination of standard statistical word alignments and phrase-basedword alignments. For German?English trans lation we tried to make the German text moresimilar to the English text by normalizing Ger man morphology and performing rule-basedclause reordering of the German text. This resulted in small improvements for both transla tion directions. Experiments with word alignment normalization and clause reordering for SMT between English and German This paper presents the LIU system for theWMT 2011 shared task for translation be tween German and English.",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-1008",
        "summarized_text": "An inference engine is built to achieve inference on abstract denotations. In this paper, we equip the DCS framework logical inference, by defining abdenotations an abstraction of the computing process of denotations in original DCS. Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics. Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation. Experiments on FraCaS and PASCAL RTE datasets show promising results.",
        "rouge_score": {}
    },
    {
        "paper_id": "C10-1132",
        "summarized_text": "Subword-based Tagging for Con Yi scores: How much im Yu f ACL, pages 840-847, cknowledgement The authors extend sincere thanks to Wenbing Jiang for his helps with our experiments. Last, it is found that weighting various features differently would give better result. Program) of China under Grant No. 2006AA010108-4 as well. The character-based tagging approach is a dominant technique for Chinese word segmentation, and both discrimi native and generative models can be adopted in that framework. To take advantage of these two ap proaches, a joint model is thus proposed to combine them. Moreover, closed tests on the second SIGHAN Bakeoff corpora show that this joint model significantly outperforms all the state of-the-art systems reported in the literature. In Proceedings of GHAN Workshop on Chinese Lan St Th Jia W Jo Hw Fu ction using conditional random fields.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-1089",
        "summarized_text": "However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. This model may be refined by incorporating richer features and improved decoding. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection. We have proposed a discriminative model that jointly infers morphological properties and syntactic structures. Further evaluation on other morphological systems would also be desirable. In particular, we would like to experiment with higher-order features (\u00a76), and with maximum a posteriori decoding, via max-product BP or (relaxed) integer linear programming. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the \u201cpipeline\u201d approach, assuming that morphological information has been separately obtained. Most previous studies of morphological disambiguation and dependency parsing have been pursued independently.",
        "rouge_score": {}
    },
    {
        "paper_id": "W06-0508",
        "summarized_text": "We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases. We suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text. The relations extracted can be used for various tasks, including semantic web annotation and ontology learning. We will then carry experiments with our corpus of newsletters in order to evaluate the approach. We presented a hybrid approach for the extraction of semantic relations from text.",
        "rouge_score": {}
    },
    {
        "paper_id": "C10-1061",
        "summarized_text": "We evaluate our parser with a gram mar extracted from the German NeGratreebank. To speed up parsing, we use context summary estimates for parse items. Therefore, our approach demon strates convincingly that PLCFRS is a natural and tractable alternative for data-driven parsing which takes non-local dependencies into consideration. Our experiments show that data driven LCFRS parsing is feasible with a reasonable speed and yields output of competitive quality. This paper presents a first efficient imple mentation of a weighted deductive CYKparser for Probabilistic Linear ContextFree Rewriting Systems (PLCFRS), to gether with context-summary estimatesfor parse items used to speed up parsing. We have presented the first parser for unrestrictedProbabilistic Linear Context-Free Rewriting Sys tems (PLCFRS), implemented as a CYK parser with weighted deductive parsing.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1025",
        "summarized_text": "The appropriate application of heterogeneous annotations leads to a significant improvement (a relative error reduction of 11%) over the best performance for this task. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations We employ stacking models to incorporate features derived from heterogeneous analysis and apply them to convert heterogeneous labeled data for re-training. Penn Chinese Treebank (CTB) and PKU\u2019s People\u2019s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e.",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-1012",
        "summarized_text": "Secondly, compared with the baseline phrase features Xi, our introduced input original phrase features X significantly improve the performance of not only our DAE features but also the DBN features. Using the unsupervised pre-trained deep belief net (DBN) to initialize DAE\u2019s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features. Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning. On two Chinese-English translation tasks, the results demonstrate that our solutions solve the two aforementioned shortcomings successfully. The results also demonstrate that DNN (DAE and HCDAE) features are complementary to the original features for SMT, and adding them together obtain statistically significant improvements of 3.16 (IWSLT) and 2.06 (NIST) BLEU points over the baseline features.",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-2110",
        "summarized_text": "We Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis. Learning Polylingual Topic Models from Code-Switched Social Media Documents Code-switched documents are in social media, providing evidence polylingual topic models to infer aligned topics across languages. We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators.",
        "rouge_score": {}
    },
    {
        "paper_id": "D11-1140",
        "summarized_text": "Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. The factored lexical representation also has significant potential for lexical transfer learning, where we would need to learn new lexemes for each target application, but much of the information in the templates could, potentially, be ported across domains. We argued that factored CCG lexicons, which include both lexemes and lexical templates, provide a compact representation of lexical knowledge that can have advantages for learning. We also described a complete approach for inducing factored, probabilistic CCGs for semantic parsing, and demonstrated strong performance across a wider range of benchmark datasets that any previous approach. We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model.",
        "rouge_score": {}
    },
    {
        "paper_id": "P01-1018",
        "summarized_text": "We have proposed a more constrained version of Joshi\u2019s question, \u201cHow much strong generative power can be squeezed out of a formal system without increasing its weak generative power,\u201d and shown that within these constraints, a variant of TAG called MMTAG characterizes the limit of how much strong generative power can be squeezed out of CFG. Moreover, using the notion of a meta-level grammar, this result is extended to formalisms beyond CFG. This way of approaching Joshi\u2019s question is by no means the only way, but we hope that this work will contribute to a better understanding of the strong generative capacity of constrained grammar formalisms as well as reveal more powerful formalisms for linguistic analysis and natural language processing. We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar.",
        "rouge_score": {}
    },
    {
        "paper_id": "D12-1046",
        "summarized_text": "Previous work often used a pipeline method \u2013 Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. \u2193 and \u2191 mean the error number reduced or increased by the joint model. parsing method. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. Our experiments demonstrate the advantage of the joint models. This work is partly supported by DARPA under Contract No. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. The sub-models are independently trained for the three tasks to reduce model complexity and optimize individual sub-models. We will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-2058",
        "summarized_text": "We propose a novel heuristic algorithm for Cube Pruning running in linear time in the beam size. Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy.",
        "rouge_score": {}
    },
    {
        "paper_id": "W10-2803",
        "summarized_text": "While dimensions of distributional models are usually not individually interpretable, there are some first models (Almuhareb and Poesio, 2005; Baroni et al., 2010) that use patterns to extract meaningful dimensions from corpus data. We have argued that it makes sense to look to cognitive models of mental concept representation. What Is Word Meaning Really? (And How Can Distributional Models Help Us Describe It?) We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models. We have argued that distributional models are a good match for word meaning models following hypotheses (A)(C): They can represent individual word usages as points in vector space, and they can also represent dictionary senses in a way that allows for graded membership and overlapping senses, and we have discussed some existing models, both prototypebased and exemplar-based.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1048",
        "summarized_text": "In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. This paper presents a novel method for SMT system adaptation by making use of the monolingual corpora in new domains. To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information In the future, we will verify our method on other language pairs, for example, Chinese to Japanese.",
        "rouge_score": {}
    },
    {
        "paper_id": "P04-1040",
        "summarized_text": "Thus, we are able to provide virtually all information available in the corpus, without modifying the parser, viewing it, indeed, as a black box. This is very close to the performance reported by Carroll et al. (2003) for the parser specifically designed for the extraction of grammatical relations. We presented a method to automatically enrich the output of a parser with information that is not provided by the parser itself, but is available in a treebank. We describe a method for enriching the output of a parser with information available in a corpus. Using the method with two state of the art statistical parsers and the Penn Treebank allowed us to recover functional tags (grammatical and semantic), empty nodes and traces. TiMBL performed well on tasks where structured, more complicated and task-specific statistical models have been used previously (Blaheta and Charniak, 2000).",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-1004",
        "summarized_text": "We have shown our models yield superior performance both qualitatively and quantitatively. We apply them to two real, non-trivial datasets: human-computer spoken dialogues in bus query service, and humanhuman text-based chats from a live technical support service. We evaluated on two very different corpora\u2014logs from spoken, human-computer dialogues about bus time, and logs of textual, humanhuman dialogues about technical support. We show that our models extract meaningful state representations and dialogue structures consistent with human annotations. We propose three new unsupervised models to discover latent structures in task-oriented dialogues. Quantitatively, we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task. Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states). Parallelization (Asuncion et al., 2012) or online learning (Doucet et al., 2001) could significantly speed up inference.",
        "rouge_score": {}
    },
    {
        "paper_id": "D10-1004",
        "summarized_text": "The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. We presented a unified view of two recent approximate dependency parsers, by stating their underlying factor graphs and by deriving the variational problems that they address. We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). We provided an aggressive online algorithm for training the models with a broad family of losses. We introduced new hard constraint factors, along with formulae for their messages, local belief constraints, and entropies. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-3029",
        "summarized_text": "This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodifier relationships are recorded. We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published. The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax. The annotations are produced automatically with statistical models that are specifically adapted to historical text.",
        "rouge_score": {}
    },
    {
        "paper_id": "P01-1005",
        "summarized_text": "Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. While it is encouraging that there is a vast amount of on-line text, much work remains to be done if we are to learn how best to exploit this resource to improve natural language processing. We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets. We propose that a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections, while deemphasizing the focus on comparing different learning techniques trained only on small training corpora.",
        "rouge_score": {}
    },
    {
        "paper_id": "D12-1108",
        "summarized_text": "We start by describing the decoding algorithm and the state operations used by our decoder, then we present empirical results demonstrating the effectiveness of our approach and its usability with a document-level semantic language model, and finally we discuss some related work. We have presented preliminary results on a cross-sentence semantic language model addressing the problem of lexical cohesion to demonstrate that this kind of models is worth exploring further. We believe that SMT research has reached a point of maturity where discourse phenomena should not be ignored any longer, and we consider our decoder to be a step towards this goal. any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. This setup gives us complete freedom to define scoring functions over the entire document.",
        "rouge_score": {}
    },
    {
        "paper_id": "P08-1108",
        "summarized_text": "Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference. Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing. In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model. Moreover, a comparative error analysis reveals that the improvements are largely predictable from theoretical properties of the two models, in particular the tradeoff between global learning and inference, on the one hand, and rich feature representations, on the other.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1002",
        "summarized_text": "We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learnalgorithm that applies regularization for joint feature selection over distributed stochastic learning processes. We presented an approach to scaling discriminative learning for SMT not only to large feature sets but also to large sets of parallel training data. Ultimately, since our algorithms are inspired by multi-task learning, we would like to apply them to scenarios where a natural definition of tasks is given. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data.",
        "rouge_score": {}
    },
    {
        "paper_id": "N03-1004",
        "summarized_text": "The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques. In this paper, we introduced a multi-strategy and multisource approach to question answering that enables combination of answering agents adopting different strategies and consulting multiple knowledge sources. Our experiments showed that our best performing algorithms achieved a 35.0% relative improvement in the number of correct answers and a 32.8% improvement in average precision on a previously unseen test set. We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels. Motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multistrategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora.",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-1110",
        "summarized_text": "The model demonstrated substantial improvements on the three tasks over the pipeline combination of the state-of-the-art joint segmentation and POS tagging model, and dependency parser. We conducted some comparison experiments of the partially joint and full joint models. We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Particularly, results showed that the accuracies of POS tagging and dependency parsing were remarkably improved by 0.6% and 2.4%, respectively corresponding to 8.3% and 10.2% error reduction. We also perform comparison experiments with the partially joint models. More efficient decoding would also allow the use of the look-ahead features (Hatori et al., 2011) and richer parsing features (Zhang and Nivre, 2011). We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework.",
        "rouge_score": {}
    },
    {
        "paper_id": "D10-1069",
        "summarized_text": "We propose an in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). What is less known is that some parsers suffer more from domain shifts than others. We presented a method for domain adaptation of deterministic shift-reduce parsers. Uptraining with large amounts of unlabeled data gives similar improvements as having access to 2,000 labeled sentences from the target domain. We evaluated multiple state-of-the-art parsers on a question corpus and showed that parsing accuracies degrade substantially on this out-of-domain task. We then proposed a simple, yet very effective uptraining method for domainadaptation. With 2,000 labeled questions and a large amount of unlabeled questions, uptraining is able to close the gap between in-domain and out-of-domain accuracy. We show that dependency parsers have more difficulty parsing questions than constituency parsers.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-2084",
        "summarized_text": "In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources. We have proven that topical knowledge is useful and improves the quality of word translations. Our next steps involve experiments with other topic models and other corpora, and combining this unsupervised approach with other tools for lexicon extraction and synonymy detection from unrelated and comparable corpora. Identifying Word Translations from Comparable Corpora Using Latent Topic Models The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported. We have presented a generic, language-independent framework for mining translations of words from latent topic models. The quality of translations depends only on the quality of a topic model and its ability to find latent relations between words.",
        "rouge_score": {}
    },
    {
        "paper_id": "N09-2064",
        "summarized_text": "Third, we extend these parser combination from multiple outputs to muloutputs. While constituent recombination results in the highest f-score of the methods explored, contextfree production recombination produces trees which better preserve the syntactic structure of the individual parses. the output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (Henderson and Brill, 1999; Sagae and Lavie, 2006). Second, we propose an efficient lineartime algorithm for computing expected f-score using Minimum Bayes Risk parse selection. We propose three ways to improve upon existing methods for parser combination. We present results on WSJ section 23 and also on the English side of a Chinese-English parallel corpus. We have presented an algorithm for parse hybridization by recombining context-free productions. We have also presented an efficient linear-time algorithm for selecting the parse with maximum expected f-score. Results are shown in Tables 2, 3, and 4.",
        "rouge_score": {}
    },
    {
        "paper_id": "W03-1022",
        "summarized_text": "We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. Within this framework it is possible to use the information contained in WordNet to improve classification and define a more realistic evaluation than standard cross-validation. We used a fixed set 26 semantic labels, which we called su- These are the labels used by lexicographers developing WordNet. We also define a more realistic evaluation procedure than cross-validation. We present a new framework for classifying common nouns that extends namedentity classification. We presented a new framework for word sense classification, based on the WordNet lexicographer classes, that extends named-entity classification.",
        "rouge_score": {}
    },
    {
        "paper_id": "D11-1022",
        "summarized_text": "We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. Since exact decoding is intractable, we solve an LP relaxation through a recently proposed consensus algorithm, DD-ADMM, which is suitable for problems with many overlapping components. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. We study the empirical runtime and convergence properties of DD-ADMM, complementing the theoretical treatment in Martins et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011).",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1092",
        "summarized_text": "Recently, Stroppa and Yvon (2005) have shown how analogical learning alone deals nicely with morphology in differentlanguages. This would relieve usfrom requiring the training material while translat ing, and would allow us to compare our approachwith other methods proposed for unsupervised mor phology acquisition.Acknowledgement We are grateful to the anony mous reviewers for their useful suggestions and to Pierre Poulin for his fruitful comments. On the contrary to several lines of work, our approach does not rely on massive additional resources but capitalizes instead on an information which is inherently pertaining tothe language. We measured that roughly 80% of or dinary unknown French words can receive a valid translation into English with our approach. Unknown words are a well-known hindranceto natural language applications. Second, we are investigat ing how a systematic enrichment of a phrase-transfer table will impact a phrase-based statistical machine translation engine.",
        "rouge_score": {}
    },
    {
        "paper_id": "W06-3114",
        "summarized_text": " Evaluation was done automatically using the BLEU score and manually on fluency and adequacy Replacing this with an ranked evaluation seems to be more suitable. The manual evaluation of scoring translation on a graded scale from 1\u20135 seems to be very hard to perform. While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems. Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data. Human judges also pointed out difficulties with the evaluation of long sentences. We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back",
        "rouge_score": {
            "rouge1": 0.7370061853118813,
            "rouge2": 0.5741746694474579,
            "rougeL": 0.44894709514322234
        }
    },
    {
        "paper_id": "D09-1161",
        "summarized_text": "For feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. We verify our method by combining the two representative parsing models, lexicalized model and un-lexicalized model, on both Chinese and English. Experimental results show our method is very effective and advance the state-of-the-art results on both Chinese and English syntax parsing. In this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers. The proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model. In the future, we will explore more features and study the forest-based combination methods for syntactic parsing.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-3401",
        "summarized_text": "We tested a standard approach to lexical generalization for parsing that has been previously explored, where a word is mapped to a single cluster or synset. We use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets. We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach. We built a distributional thesaurus from an automatically-parsed large text corpus, using it to generate word clusters and perform WordNet ASR. We have investigated the use of probabilistic lexical target spaces for reducing lexical data sparseness in a transition-based dependency parser for French.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-1310",
        "summarized_text": "Exemplar-Based Word-Space Model for Compositionality Detection: Shared Task System Description This model is tested for compositionality detection and it is found to outperform existing prototype-based models. Also, we use multiple evidences for compositionality detection rather than basing our judgement on a single evidence. We have participated in the shared task (Biemann and Giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations. We showed exemplar-based WSM is effective in dealing with polysemy. Most models represent each word as a single prototype-based vector without addressing polysemy. In this paper, we highlight the problems of polysemy in word space models of compositionality detection. We propose an exemplar-based model which is designed to handle polysemy. Overall, performance of the Exemplar-based models of compositionality detection is found to be superior to prototype-based models.",
        "rouge_score": {}
    },
    {
        "paper_id": "A00-2030",
        "summarized_text": "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. A single model proved capable of performing all necessary sentential processing, both syntactic and semantic. We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model. In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.",
        "rouge_score": {
            "rouge1": 0.5485010743162461,
            "rouge2": 0.3633425031487658,
            "rougeL": 0.26859755756293585
        }
    },
    {
        "paper_id": "W12-3134",
        "summarized_text": "Acknowledgements This research was supported by in part by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme), and by the NSF under grant IIS-0713448. We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. Our system has been extended towards efficiently supporting large-scale experiments in parsing-based machine translation and text-to-text generation: Joshua 4.0 supports compactly represented large grammars with its packed grammars, as well as large language models via KenLM and BerkeleyLM.We include an implementation of PRO, allowing for stable and fast tuning of large feature sets, and extend our toolkit beyond pure translation applications by extending Thrax with a large-scale paraphrase extraction module. We present a new iteration of the Joshua machine translation toolkit.",
        "rouge_score": {}
    },
    {
        "paper_id": "W04-1506",
        "summarized_text": "We have evaluated the application of the grammar to corpus, measuring the linking of the verb with its dependents, with satisfactory results. We have shown how dependency grammar approach provides a good solution for deeper syntactic analysis, being at this moment the best alternative for morphologically complex languages. The Dependency syntactic model is applied to establish the dependency-based grammatical relations between the components within the clause. The manually tagged corpus has been used to evaluate the accuracy of the parser. We present the Dependency Parser, for the linguistic processing of Basque, which can serve as a representative of agglutinative languages that are also characterized by the free order of its constituents. This scheme was used both to the manual tagging of the corpus and to develop the parser.",
        "rouge_score": {}
    },
    {
        "paper_id": "P04-1006",
        "summarized_text": "We presented a parsing technique that shifts the attention of a word-lattice parser in order to ensure syntactic analyses for all lattice paths. The parser\u2019s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model. Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice.",
        "rouge_score": {}
    },
    {
        "paper_id": "P10-1155",
        "summarized_text": "Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD. Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation. Many supervised WSD systems have been built, but the effort of creatthe training corpus sense corpora has always been a matter of concern. We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora. We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain.",
        "rouge_score": {}
    },
    {
        "paper_id": "N12-1007",
        "summarized_text": "With more data, we might be able to relax the constraints and use an exchangeable DPMM, which might be more effective. The instance-level constraints represent tendencies that could be learned from larger amounts of data. When co-referent text mentions appear in different languages, these techniques cannot be easily applied. This work was started during the SCALE 2010 summer workshop at Johns Hopkins. This result was achieved without a knowledge base, which is required by previous approaches to cross-lingual entity linking. The first author is supported by a National Science Foundation Graduate Fellowship. We proposed pipeline models that make clustering decisions based on cross-lingual similarity. The best model improved upon the cross-lingual entity baseline by 24.3% F1. We investigated two methods for mapping documents in different languages to a common representation: MT and the PLTM.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1026",
        "summarized_text": "The basic assumption is that if a word is entailed by another in a given context, then some of the contexts of the entailed word should be similar tothat of the word to be disambiguated. Our tech nique is effective, as it largely surpasses both the random and most frequent baselines. The approach is fully unsupervised and based on kernel methods. We demonstrate the effectiveness of our technique largelysurpassing both the random and most fre quent baselines and outperforming current state-of-the-art unsupervised approaches ona benchmark ontology available in the liter ature. In this paper, we presented a novel unsupervised technique for recognizing lexical entailment in texts, namely instance based lexical entailment, and we exploited it to approach an ontology population task. Ontology population is only one of the possible applications of lexical entailment.",
        "rouge_score": {}
    },
    {
        "paper_id": "P05-1061",
        "summarized_text": "We presented a method for complex relation extraction, the core of which was to factorize complex relations into sets of binary relations, learn to identify binary relations and then reconstruct the complex relations by finding maximal cliques in graphs that represent relations between pairs of entities. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. We mentioned earlier that the only restriction of our complex relation definition is that the arity of the relation must be known in advance. complex relation is any relation in which some of the arguments may be be unspecified. We showed that such a method can be successful with an empirical evaluation on a large set of biomedical data annotated with genomic variation relations. We present here a simple two-stage method for extracting complex relations between named entities in text.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-2067",
        "summarized_text": "Speculations as to cause have suggested the parser, the data, or other factors. We have shown that a lack of overall improvement using reordering-aspreprocessing need not be due to the usual suspects, language pair and reordering process. We have systematically varied several aspects of the Howlett and Dras (2010) system and reproduced results close to both papers, plus a full range in between. We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT? The reimplementation of this system by Howlett and Dras (2010) came to the opposite conclusion. We conclude that effort is best directed at determining for which sentences the improvement will appear. There are a number of systems that use a syntax-based reordering step prior to phrasebased statistical MT.",
        "rouge_score": {}
    },
    {
        "paper_id": "N10-1035",
        "summarized_text": "The normal form takes up linear space with respect to grammar size, and the algorithm is based on a bottom-up process that can be applied to any LCFRS, achieving O(cp \u00b7 |G |\u00b7 |w|2cp+2) time complexity when applied to LCFRS of fan-out cp in our normal form. This complexity is an asymptotic improvement over existing results for this class, both from parsers specifically geared to well-nested LCFRS or equivalent formalisms (Hotz and Pitsch, 1996) and from applying general LCFRS parsing techniques to the well-nested case (Seki et al., 1991). The use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order. We present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1122",
        "summarized_text": "Then we present evaluation results and error analyses focusing on Chinese. We used Nivre?smethod to produce the dependency arcs and the se quence labeler to produce the dependency labels. The experimental results showed that our system can provide good performance for all languages. For four languages with different values of ROOT, we design some spe cial features for the ROOT labeler. The parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem. We present a two-stage multilingual de pendency parsing system submitted to the Multilingual Track of CoNLL-2007.",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-1078",
        "summarized_text": "Our model is built upon a medical corpus containing 80M sentences (11 gigabyte text) and designed to accurately and efficiently detect the key medical relations that can facilitate clinical decision making. To provide users with more flexibility, we also take label weight into consideration. We also present a new manifold model to efficiently extract these relations from text. In this paper, we present a manifold model for medical relation extraction. Our model is developed to utilize both labeled and unlabeled examples. We apply the new model to construct a relation knowledge base (KB), and use it as a complement to the existing manually created KBs. In this paper, we identify a list of key relations that can facilitate clinical decision making. Our approach integrates domain specific parsing and typing systems, and can utilize labeled as well as unlabeled examples. Effectiveness of the new model is demonstrated both theoretically and experimentally.",
        "rouge_score": {}
    },
    {
        "paper_id": "C10-1135",
        "summarized_text": "While using lattices to offer more alternatives to translation systems have elegantly alleviated this prob lem, we take a further step to tokenize and translate jointly. We have presented a novel method for joint tok enization and translation which directly combines the tokenization model into the decoding phase.Allowing tokenization and translation to collaborate with each other, tokenization can be opti mized for translation, while translation also makes contribution to tokenization performance under a supervised way. We believe that our approach can be applied to other string-based model such asphrase-based model (Koehn et al, 2003), stringto-tree model (Galley et al, 2006) and string-to dependency model (Shen et al, 2008). We are also grateful to Wen bin Jiang, Zhiyang Wang and Zongcheng Ji for their helpful feedback.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1030",
        "summarized_text": "We presented a method using the existing RBMT system as a black box to produce synthetic bilin gual corpus, which was used as training data for the SMT system. We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006). We used the existing RBMT system to translate the monolingual corpus into a syn thetic bilingual corpus. We found that 930 phrase pairs, which are also in the phrase table of the standard model, are used by the interpolated model for translation but not used by the standard model. With the synthetic bilingual corpus, we could build an SMT system even if there is no real bilingual corpus.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-1061",
        "summarized_text": "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010). Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models. Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language. We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.",
        "rouge_score": {
            "rouge1": 0.6605833735825398,
            "rouge2": 0.5390633836378317,
            "rougeL": 0.2988976304563452
        }
    },
    {
        "paper_id": "W06-2932",
        "summarized_text": "Second, we plan on integrating any available morphological features in a more principled manner. The current system simply includes all morphological bi-gram features. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English. The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis. present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.",
        "rouge_score": {
            "rouge1": 0.6156096063871832,
            "rouge2": 0.41273641855189386,
            "rougeL": 0.27007641745435595
        }
    },
    {
        "paper_id": "D12-1129",
        "summarized_text": "We have here presented a new framework for domain Word Sense Disambiguation. While we selected 30 domains for this study, nothing would prevent us from using a smaller or larger set of these domains, or a set of completely different domains. We depart from the use of general-purpose sense inventories like WordNet and propose a bootstrapping approach to the acquisition of sense inventories for virtually any domain. The multi-domain glossary (and sense inventory) together with the seeds used for bootstrapping are available from http://lcl.uniroma1. it/dwsd. We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD.",
        "rouge_score": {}
    },
    {
        "paper_id": "D11-1039",
        "summarized_text": "When something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals. This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system. The loss was defined over the conversational context, without requiring annotation of user utterances meaning. The overall approach assumes that, in aggregate, the conversations contain sufficient signal (remediations such as clarification, etc.) to learn effectively. We demonstrate learning without any explicit annotation of the meanings of user utterances. We presented a loss-driven learning approach that induces the lexicon and parameters of a CCG parser for mapping sentences to logical forms. This would include designing remediation strategies that allow for the most effective learning and considering how similar techniques could be used simultaneously for other dialog subproblems.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-1060",
        "summarized_text": "In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.",
        "rouge_score": {
            "rouge1": 0.5256372143458835,
            "rouge2": 0.3431123557499029,
            "rougeL": 0.35394932077668106
        }
    },
    {
        "paper_id": "W07-1712",
        "summarized_text": "We show that it is feasible to boost performance by considering several heuristics and patterns acquired from the Web data. The approach we follow uses a restricted number of features. To improve the results that we get by employing orthographic and contextual features, we add patterns extracted from the Web and use a similarity measure to find the named entities similar to the NEs in the training set. The other problem which we have not considered up to now is the ambiguity of some named entities. The former might be explained by the size of the corpus we use and by the characteristics of the language. The results we received are, in general, lower than the performance of NER systems in other languages but higher than both baselines.",
        "rouge_score": {}
    },
    {
        "paper_id": "D10-1025",
        "summarized_text": "This paper presents two different methods for creating discriminative projections: OPCA and CPLSA. When compared to other techniques, OPCA had the highest accuracy while still having a run-time that allowed scaling to large data sets. We therefore recommend the use of OPCA as a pre-processing step for large-scale comparable document retrieval or cross-language text categorization. The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines. We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA). We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space. We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters. The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1066",
        "summarized_text": "test andtraining sets and cross-treebank and -language con trolled error insertion experiments. We have shown that different treebank annotation schemes have a strong impact on parsing results forsimilar input data with similar (simulated) parser er rors. We showed that PAR SEVAL cannot be used to compare the output ofPCFG parsers trained on different treebank anno tation schemes, because the results correlate withthe ratio of non-terminal/terminal nodes. We use thePARSEVAL metric, the Leaf-Ancestor metric as well as a dependency-based evaluation, and present novel approaches measur ing the effect of controlled error insertion on treebank trees and parser output. We will attempt to pursue this in further research.",
        "rouge_score": {}
    },
    {
        "paper_id": "W06-3119",
        "summarized_text": "We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop\u2019s baseline system. While no improvements were available at submission time, our subsequent performance highlights the importance of tight integration of n-gram language modeling within the syntax driven parsing environment. We present translation results on the shared task \u201dExploiting Parallel Texts for Statistical Machine Translation\u201d generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. Our work reaffirms the feasibility of parsing approaches to machine translation in a large data setting, and illustrates the impact of adding syntactic categories to drive and constrain the structured search space.",
        "rouge_score": {}
    },
    {
        "paper_id": "D07-1119",
        "summarized_text": "For the multi lingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language. We describe our experiments using the DeSR parser in the multilingual and do main adaptation tracks of the CoNLL 2007 shared task. We used a second order averaged perceptron as classifier and achieved accuracy scores quite above the average in all lan guages. For the domain adaptation track we applied a tree revision method which learns how to correct the mistakes made by the base parser on the adaptation domain. The technique achieved quite promising results and it also offers the interesting possibility of being iterated, allowing the parser to incorporate language knowledge from additional domains. Since the technique is applicable to any parser, we plan to test it also with more accurate English parsers.",
        "rouge_score": {}
    },
    {
        "paper_id": "C02-1154",
        "summarized_text": "These names exhibitcertain properties that make their identi ca tion more complex than that of regular propernames. Nomen uses a novel form of bootstrap ping to grow sets of textual instances and of their contextual patterns. The algorithm makes use of competing evidence to boost the learning of several categories of names simultaneously. We present an algorithm, Nomen, for learning generalized names in text. We also investigate the relative merits of several evaluation strategies. We present results of the algorithm on a large corpus. Examples of these are names of diseases and infectious agents, such as bacteria and viruses.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-0131",
        "summarized_text": "This paper has introduced a structured vectorial semantic (SVS) framework in which vector composition and syntactic parsing are a single, interactive process. It was found that relationallyclustered SVS outperformed the simpler lexicalized model and syntax-only models, and that additional clusters had a mildly positive effect. Two standard parsing techniques were defined within SVS and evaluated: headword-lexicalization SVS (bilexical parsing) and relational-clustering SVS (latent annotations). It is hoped that this flexible framework will enable new generations of interactive interpretation models that deal with the syntax\u2013semantics interface in a plausible manner. The framework thus fully integrates distributional semantics with traditional syntactic models of language. Evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.",
        "rouge_score": {}
    },
    {
        "paper_id": "A97-1014",
        "summarized_text": "We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation. These differences can be illustrated by a comparison with the Penn Treebank annotation scheme. The following features of our formalism are then of particular importance: The current tagset comprises only 16 node labels and 34 function tags, yet a. finely grinned classification will take place in the near future. Syntactically annotated corpora of German have been missing until now. We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- \u2022lar representational strata. We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.",
        "rouge_score": {
            "rouge1": 0.456578106690275,
            "rouge2": 0.2496977259883807,
            "rougeL": 0.24116964492921789
        }
    },
    {
        "paper_id": "W10-1403",
        "summarized_text": "These methods can be thought as different paradigms of modularity. We systematically explored the effect of various linguistic features in Hindi dependency parsing. We then investigated the best way to incorporate this information during dependency parsing. We then described 2 methods to incorporate such features during the parsing process. This paper was also the first attempt at complete sentence level parsing for Hindi. Using gold-standard semantic features, they showed considerable improvement in the core inter-chunk dependency accuracy. We plan to see their effect in complete sentence parsing using automatic shallow parser information also. We first explored which information provided by the shallow parser is useful and showed that local morphosyntactic features in the form of chunk type, head/non-head info, chunk boundary info, distance to the end of the chunk and suffix concatenation are very crucial for Hindi dependency parsing. Two Methods to Incorporate &rsquo;Local Morphosyntactic&rsquo; Features in Hindi Dependency Parsing",
        "rouge_score": {}
    },
    {
        "paper_id": "W06-2207",
        "summarized_text": "Our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an Expectation Maximization clustering algorithm that uses the text words as attributes. In this paper we present a hybrid approach for the acquisition of syntacticosemantic patterns from raw text. We show that the combination of the two methods always outperforms the decision list learner alone. The direct evaluation of the acquired patterns by the human experts validates these results. Our results indicate that co-training the Expectation Maximization algorithm with the decision list learner tailored to acquire only high precision patterns is by far the best solution. This paper introduces a hybrid, lightly-supervised method for the acquisition of syntactico-semantic patterns for Information Extraction. Furthermore, we customize the decision list learner with up to four criteria for pattern selection, which is the most important component of the acquisition algorithm.",
        "rouge_score": {}
    },
    {
        "paper_id": "W09-0402",
        "summarized_text": "The results also showed that the syntax-oriented metrics are competitive with the widely used evaluation measures BLEU, METEOR and TER. We carried out an extensive analysis of the Spearman\u2019s rank correlation coefficients between the syntactic evaluation metrics and the human judgments. We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the the on the detailed tags as well as the precision, recall and F-measure obtained We also introduced Fbased on both word and grams. We proposed several syntax-oriented evaluation metrics based on the detailed POS tags: the POSBLEU score and POS-n-gram precision, recall and F-measure, i.e. the POSP, POSR, and POSF score. The results presented in this article suggest that the syntactic information has the potential to strenghten automatic evaluation metrics, and there are many possible directions for future work.",
        "rouge_score": {}
    },
    {
        "paper_id": "C04-1180",
        "summarized_text": "Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation. This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.",
        "rouge_score": {}
    },
    {
        "paper_id": "D12-1133",
        "summarized_text": "The error analysis reveals improvements in tagging accuracy for syntactically central categories, mainly verbs, with improvement in syntactic accuracy for core grammatical functions as a result. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. Evaluation on four languages shows consistent improvements in both tagging and parsing accuracy over a pipeline system with state-of-the-art results across the board. In future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features. A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.",
        "rouge_score": {}
    },
    {
        "paper_id": "P14-1126",
        "summarized_text": "We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resourcerich language. We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems. The experimental results on three data sets across ten target languages show that our approach achieves significant improvement over previous studies. We perform experiments on three Data sets \u2014 Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization",
        "rouge_score": {}
    },
    {
        "paper_id": "P12-3012",
        "summarized_text": "In this paper we present an API for programmatic access to BabelNet \u2013 a wide-coverage multilingual lexical knowledge base \u2013 and multilingual knowledge-rich Word Sense Disambiguation (WSD). Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. Multilingual WSD with Just a Few Lines of Code: the BabelNet API",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-2205",
        "summarized_text": "Using PoS tagging as our case study, we examined recent attempts of evaluating unsupervised approaches and showed that a lot of confusion is caused due to evaluating their output against a labeled gold standard. Using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods. Inwe argue that the rarely used evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied. The primary advantage of these methods is that they do not require annotated data to learn a model. The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research.",
        "rouge_score": {}
    },
    {
        "paper_id": "W12-3154",
        "summarized_text": "\u201cfill in\u201d) the phrase-table, and using the scores provided by out-of-domain data has a tendency to be harmful to translation quality. This explains why approaches which try to weight the outof-domain data in some way (e.g. corpus weighting or instance weighting) can be more successful than simply concatenating data sets. The precision results of Section 3.5 show out-of-domain data (when it is simply added to the training set) mainly helping with the low frequency words, and having a neutral or harmful effect for higher frequency words. Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words. This can be detected by the differences in word distribution and out-ofvocabulary rates observed in Figure 2, and is reflected by the differing translation results in Figure 3.",
        "rouge_score": {}
    },
    {
        "paper_id": "P11-1144",
        "summarized_text": "We showed that graph-based label propagation and resulting smoothed frame distributions over unseen targets significantly improved the coverage of a state-of-the-art semantic frame disambiguation model to previously unseen predicates, also improving the quality of full framesemantic parses. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement full frame-semantic parsing on a blind test set, over a state-of-the-art supervised baseline. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. We have presented a semi-supervised strategy to improve the coverage of a frame-semantic parsing model. We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data.",
        "rouge_score": {}
    },
    {
        "paper_id": "W09-1116",
        "summarized_text": "pronouns like reflect the gender and number of the entities to which they refer. We have shown how noun-pronoun co-occurrence counts can be used to automatically annotate the gender of millions of nouns in unlabeled text. While this provides useful statistics for frequent nouns, many infrequent nouns cannot be classified using this method. Rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model. Training from these examples produced a classifier that clearly exceeds the state-of-the-art in gender classification. We incorporated thousands of useful but previously unexplored indicators of noun gender as features in our classifier. Pronoun resolution systems can use this fact to filter noun candidates that do not agree with the pronoun gender. Previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class.",
        "rouge_score": {}
    },
    {
        "paper_id": "W11-1002",
        "summarized_text": "We then show that the correlation of HMEANT, the human variant of MEANT, can be greatly improved by introducing a simple length-based weighting scheme that approximates the degree of contribution of each semantic frame to the overall sentence. Structured vs. Flat Semantic Role Representations for Machine Translation Evaluation The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame\u2019s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER. We argue that failing to capture the degree of contribution of each semantic frame in a sentence explains puzzling results in recent work on the MEANT family of semantic MT evaluation metrics, which have disturbingly indicated that dissociating semantic roles and fillers from their predicates actually improves correlation with human adequacy judgments even though, intuitively, properly segregating event frames should more accurately reflect the preservation of meaning.",
        "rouge_score": {}
    },
    {
        "paper_id": "W06-2905",
        "summarized_text": "We explore a novel computational approach to identifying \u201cconstructions\u201d or \u201cmulti-word expressions\u201d (MWEs) in an annotated corpus. What Are The Productive Units Of Natural Language Grammar? A DOP Approach To The Automatic Identification Of Constructions We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank. With the added operation of adjunction, equation (8) is not valid anymore. Which correlations are so strong that it is reasonable to think of the correlated phrases as a single unit? We presented a new algorithm for estimating weights of an STSG from a corpus, and reported promising empirical results on a small corpus. We report quantitative results on the ATIS corpus of phrase-structure annotated sentences, and give examples of the MWEs extracted from this corpus.",
        "rouge_score": {}
    },
    {
        "paper_id": "D10-1044",
        "summarized_text": "We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora. We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines. We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.",
        "rouge_score": {
            "rouge1": 0.5214292521056232,
            "rouge2": 0.3236745415793154,
            "rougeL": 0.23099807256502236
        }
    },
    {
        "paper_id": "I08-2105",
        "summarized_text": "We present experiments that analyze the necessity of using a highly interconnectedword/sense graph for unsupervised all words word sense disambiguation. We especially note good performancewhen disambiguating verbs with grammati cally constrained links. We show that allowing only grammatically related words to influence each other?s senses leads to disambiguation results on a par with thebest graph-based systems, while greatly reducing the computation load. We have studied the impact of grammatical in formation for constraining and guiding the word sense disambiguation process in an unsupervised all-words setup. We explored a new method for estimating sense association strength from a sense-untagged corpus.Disambiguation when using sense relatedness com puted from WordNet is very close in performance with disambiguation based on sense association strength computed from the British National Corpus,and on a par with state-of-the-art unsupervised systems on Senseval-2.",
        "rouge_score": {}
    },
    {
        "paper_id": "N10-1002",
        "summarized_text": "We have proposed a chart mining technique for lexical acquisition based on partial parsing with precision grammars. The main advantage, though, of chart mining is that parsing with precision grammars does not any longer have to assume complete coverage, as has traditionally been the case. We applied the proposed method to the task of extracting English verb particle constructions from a prescribed set of corpus instances. With MWE extraction, e.g., even though some MWEs are fixed and have no internal syntactic variability, such as ad hoc, there is a very large proportion of idioms that allow various degrees of internal variability, and with a variable number of elements. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features.",
        "rouge_score": {}
    }
]