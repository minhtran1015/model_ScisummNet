textsum % python -m scripts.train
Using device: mps
âœ… Extracted sentences and adjacency matrices saved to data/processed/extracted_sentences.json
Training on 468 samples, validating on 52 samples
Sample 0, Loss: 0.2779
Sample 10, Loss: 0.2967
Sample 20, Loss: 0.1488
Sample 30, Loss: 0.1403
Sample 40, Loss: 0.1665
Sample 50, Loss: 0.0574
Sample 60, Loss: 0.1665
Sample 70, Loss: 0.1593
Sample 80, Loss: 0.1172
Sample 90, Loss: 0.1810
Sample 100, Loss: 0.1403
Sample 110, Loss: 0.1739
Sample 120, Loss: 0.1838
Sample 130, Loss: 0.1665
Sample 140, Loss: 0.1524
Sample 150, Loss: 0.1810
Sample 160, Loss: 0.1739
Sample 170, Loss: 0.1810
Sample 180, Loss: 0.1859
Sample 190, Loss: 0.1859
Sample 200, Loss: 0.0000
Sample 210, Loss: 0.1810
Sample 220, Loss: 0.1665
Sample 230, Loss: 0.1739
Sample 240, Loss: 0.0000
Sample 250, Loss: 0.1838
Sample 260, Loss: 0.1593
Sample 270, Loss: 0.1810
Sample 280, Loss: 0.1524
Sample 290, Loss: 0.1810
Sample 300, Loss: 0.1665
Sample 310, Loss: 0.0883
Sample 320, Loss: 0.1810
Sample 330, Loss: 0.1859
Sample 340, Loss: 0.1810
Sample 350, Loss: 0.1524
Sample 360, Loss: 0.1195
Sample 370, Loss: 0.1593
Sample 380, Loss: 0.0904
Sample 390, Loss: 0.1810
Sample 400, Loss: 0.1593
Sample 410, Loss: 0.1593
Sample 420, Loss: 0.1810
Sample 430, Loss: 0.1810
Sample 440, Loss: 0.1859
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1739
Epoch 1/50, Avg Train Loss: 0.1581
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1325
Sample 10, Loss: 0.1859
Sample 20, Loss: 0.0000
Sample 30, Loss: 0.1665
Sample 40, Loss: 0.1810
Sample 50, Loss: 0.0000
Sample 60, Loss: 0.1838
Sample 70, Loss: 0.1403
Sample 80, Loss: 0.1838
Sample 90, Loss: 0.1739
Sample 100, Loss: 0.0000
Sample 110, Loss: 0.1665
Sample 120, Loss: 0.1810
Sample 130, Loss: 0.1859
Sample 140, Loss: 0.1665
Sample 150, Loss: 0.0787
Sample 160, Loss: 0.1665
Sample 170, Loss: 0.1739
Sample 180, Loss: 0.1739
Sample 190, Loss: 0.0000
Sample 200, Loss: 0.1859
Sample 210, Loss: 0.1211
Sample 220, Loss: 0.1810
Sample 230, Loss: 0.1810
Sample 240, Loss: 0.1838
Sample 250, Loss: 0.1403
Sample 260, Loss: 0.1810
Sample 270, Loss: 0.1665
Sample 280, Loss: 0.1810
Sample 290, Loss: 0.1810
Sample 300, Loss: 0.1739
Sample 310, Loss: 0.1461
Sample 320, Loss: 0.1524
Sample 330, Loss: 0.1810
Sample 340, Loss: 0.1739
Sample 350, Loss: 0.1838
Sample 360, Loss: 0.1665
Sample 370, Loss: 0.1859
Sample 380, Loss: 0.1524
Sample 390, Loss: 0.1810
Sample 400, Loss: 0.1739
Sample 410, Loss: 0.1810
Sample 420, Loss: 0.0574
Sample 430, Loss: 0.1739
Sample 440, Loss: 0.0000
Sample 450, Loss: 0.1859
Sample 460, Loss: 0.1593
Epoch 2/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1567
Sample 10, Loss: 0.1739
Sample 20, Loss: 0.1859
Sample 30, Loss: 0.1739
Sample 40, Loss: 0.1665
Sample 50, Loss: 0.1461
Sample 60, Loss: 0.1739
Sample 70, Loss: 0.1739
Sample 80, Loss: 0.0787
Sample 90, Loss: 0.1593
Sample 100, Loss: 0.1810
Sample 110, Loss: 0.1810
Sample 120, Loss: 0.1665
Sample 130, Loss: 0.1225
Sample 140, Loss: 0.1859
Sample 150, Loss: 0.1810
Sample 160, Loss: 0.1566
Sample 170, Loss: 0.1593
Sample 180, Loss: 0.1593
Sample 190, Loss: 0.1665
Sample 200, Loss: 0.1838
Sample 210, Loss: 0.1665
Sample 220, Loss: 0.1810
Sample 230, Loss: 0.1838
Sample 240, Loss: 0.0641
Sample 250, Loss: 0.1593
Sample 260, Loss: 0.1810
Sample 270, Loss: 0.1810
Sample 280, Loss: 0.1838
Sample 290, Loss: 0.0991
Sample 300, Loss: 0.1739
Sample 310, Loss: 0.0000
Sample 320, Loss: 0.1859
Sample 330, Loss: 0.1838
Sample 340, Loss: 0.1524
Sample 350, Loss: 0.1739
Sample 360, Loss: 0.1039
Sample 370, Loss: 0.1011
Sample 380, Loss: 0.1739
Sample 390, Loss: 0.1665
Sample 400, Loss: 0.1859
Sample 410, Loss: 0.1665
Sample 420, Loss: 0.1076
Sample 430, Loss: 0.1248
Sample 440, Loss: 0.1665
Sample 450, Loss: 0.1859
Sample 460, Loss: 0.1566
Epoch 3/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1665
Sample 10, Loss: 0.1810
Sample 20, Loss: 0.1810
Sample 30, Loss: 0.0851
Sample 40, Loss: 0.1810
Sample 50, Loss: 0.1810
Sample 60, Loss: 0.1810
Sample 70, Loss: 0.1859
Sample 80, Loss: 0.1524
Sample 90, Loss: 0.1859
Sample 100, Loss: 0.1739
Sample 110, Loss: 0.1566
Sample 120, Loss: 0.1524
Sample 130, Loss: 0.1810
Sample 140, Loss: 0.1859
Sample 150, Loss: 0.1859
Sample 160, Loss: 0.1461
Sample 170, Loss: 0.1739
Sample 180, Loss: 0.1011
Sample 190, Loss: 0.1739
Sample 200, Loss: 0.1859
Sample 210, Loss: 0.1349
Sample 220, Loss: 0.1665
Sample 230, Loss: 0.1739
Sample 240, Loss: 0.1810
Sample 250, Loss: 0.1810
Sample 260, Loss: 0.1739
Sample 270, Loss: 0.1810
Sample 280, Loss: 0.1593
Sample 290, Loss: 0.0000
Sample 300, Loss: 0.1739
Sample 310, Loss: 0.1248
Sample 320, Loss: 0.1739
Sample 330, Loss: 0.1739
Sample 340, Loss: 0.1524
Sample 350, Loss: 0.1593
Sample 360, Loss: 0.1665
Sample 370, Loss: 0.1859
Sample 380, Loss: 0.0264
Sample 390, Loss: 0.1739
Sample 400, Loss: 0.1593
Sample 410, Loss: 0.1859
Sample 420, Loss: 0.1810
Sample 430, Loss: 0.1859
Sample 440, Loss: 0.1461
Sample 450, Loss: 0.1739
Sample 460, Loss: 0.1039
Epoch 4/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1403
Sample 10, Loss: 0.0000
Sample 20, Loss: 0.1810
Sample 30, Loss: 0.1810
Sample 40, Loss: 0.1810
Sample 50, Loss: 0.1593
Sample 60, Loss: 0.1810
Sample 70, Loss: 0.1403
Sample 80, Loss: 0.1566
Sample 90, Loss: 0.1739
Sample 100, Loss: 0.1859
Sample 110, Loss: 0.1859
Sample 120, Loss: 0.1403
Sample 130, Loss: 0.1810
Sample 140, Loss: 0.1739
Sample 150, Loss: 0.1403
Sample 160, Loss: 0.1739
Sample 170, Loss: 0.1665
Sample 180, Loss: 0.1665
Sample 190, Loss: 0.1524
Sample 200, Loss: 0.1739
Sample 210, Loss: 0.1810
Sample 220, Loss: 0.0836
Sample 230, Loss: 0.0783
Sample 240, Loss: 0.1461
Sample 250, Loss: 0.1665
Sample 260, Loss: 0.1665
Sample 270, Loss: 0.1665
Sample 280, Loss: 0.0914
Sample 290, Loss: 0.1566
Sample 300, Loss: 0.1201
Sample 310, Loss: 0.1524
Sample 320, Loss: 0.0000
Sample 330, Loss: 0.1665
Sample 340, Loss: 0.0264
Sample 350, Loss: 0.1859
Sample 360, Loss: 0.1810
Sample 370, Loss: 0.1810
Sample 380, Loss: 0.1810
Sample 390, Loss: 0.1349
Sample 400, Loss: 0.0000
Sample 410, Loss: 0.1739
Sample 420, Loss: 0.1665
Sample 430, Loss: 0.1593
Sample 440, Loss: 0.1593
Sample 450, Loss: 0.1665
Sample 460, Loss: 0.0000
Epoch 5/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1810
Sample 10, Loss: 0.1665
Sample 20, Loss: 0.1665
Sample 30, Loss: 0.1566
Sample 40, Loss: 0.1665
Sample 50, Loss: 0.1665
Sample 60, Loss: 0.1810
Sample 70, Loss: 0.1739
Sample 80, Loss: 0.1665
Sample 90, Loss: 0.1859
Sample 100, Loss: 0.1810
Sample 110, Loss: 0.1665
Sample 120, Loss: 0.1859
Sample 130, Loss: 0.1593
Sample 140, Loss: 0.1810
Sample 150, Loss: 0.1859
Sample 160, Loss: 0.1810
Sample 170, Loss: 0.1838
Sample 180, Loss: 0.1859
Sample 190, Loss: 0.1665
Sample 200, Loss: 0.1739
Sample 210, Loss: 0.1665
Sample 220, Loss: 0.1810
Sample 230, Loss: 0.0000
Sample 240, Loss: 0.1461
Sample 250, Loss: 0.0264
Sample 260, Loss: 0.1739
Sample 270, Loss: 0.1739
Sample 280, Loss: 0.1172
Sample 290, Loss: 0.1810
Sample 300, Loss: 0.1665
Sample 310, Loss: 0.1665
Sample 320, Loss: 0.1739
Sample 330, Loss: 0.1838
Sample 340, Loss: 0.1810
Sample 350, Loss: 0.1739
Sample 360, Loss: 0.1593
Sample 370, Loss: 0.1524
Sample 380, Loss: 0.1739
Sample 390, Loss: 0.0000
Sample 400, Loss: 0.1810
Sample 410, Loss: 0.1403
Sample 420, Loss: 0.1810
Sample 430, Loss: 0.1859
Sample 440, Loss: 0.1739
Sample 450, Loss: 0.1403
Sample 460, Loss: 0.0000
Epoch 6/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1593
Sample 10, Loss: 0.1524
Sample 20, Loss: 0.1739
Sample 30, Loss: 0.1349
Sample 40, Loss: 0.1739
Sample 50, Loss: 0.1593
Sample 60, Loss: 0.1299
Sample 70, Loss: 0.1810
Sample 80, Loss: 0.1810
Sample 90, Loss: 0.1739
Sample 100, Loss: 0.1211
Sample 110, Loss: 0.0819
Sample 120, Loss: 0.1739
Sample 130, Loss: 0.1810
Sample 140, Loss: 0.0936
Sample 150, Loss: 0.0000
Sample 160, Loss: 0.1665
Sample 170, Loss: 0.1810
Sample 180, Loss: 0.1665
Sample 190, Loss: 0.1665
Sample 200, Loss: 0.1566
Sample 210, Loss: 0.1810
Sample 220, Loss: 0.1810
Sample 230, Loss: 0.1593
Sample 240, Loss: 0.1739
Sample 250, Loss: 0.1593
Sample 260, Loss: 0.1665
Sample 270, Loss: 0.1810
Sample 280, Loss: 0.1838
Sample 290, Loss: 0.1810
Sample 300, Loss: 0.1810
Sample 310, Loss: 0.1593
Sample 320, Loss: 0.1739
Sample 330, Loss: 0.1739
Sample 340, Loss: 0.1859
Sample 350, Loss: 0.0758
Sample 360, Loss: 0.1810
Sample 370, Loss: 0.1593
Sample 380, Loss: 0.1739
Sample 390, Loss: 0.1810
Sample 400, Loss: 0.1303
Sample 410, Loss: 0.1810
Sample 420, Loss: 0.1665
Sample 430, Loss: 0.1403
Sample 440, Loss: 0.1859
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1524
Epoch 7/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.0000
Sample 10, Loss: 0.0000
Sample 20, Loss: 0.1665
Sample 30, Loss: 0.0000
Sample 40, Loss: 0.1859
Sample 50, Loss: 0.1859
Sample 60, Loss: 0.1593
Sample 70, Loss: 0.1349
Sample 80, Loss: 0.1211
Sample 90, Loss: 0.1524
Sample 100, Loss: 0.1810
Sample 110, Loss: 0.1069
Sample 120, Loss: 0.1810
Sample 130, Loss: 0.0000
Sample 140, Loss: 0.1665
Sample 150, Loss: 0.1665
Sample 160, Loss: 0.0758
Sample 170, Loss: 0.1810
Sample 180, Loss: 0.1810
Sample 190, Loss: 0.1739
Sample 200, Loss: 0.1739
Sample 210, Loss: 0.1810
Sample 220, Loss: 0.1739
Sample 230, Loss: 0.1810
Sample 240, Loss: 0.1299
Sample 250, Loss: 0.1665
Sample 260, Loss: 0.1665
Sample 270, Loss: 0.1810
Sample 280, Loss: 0.0985
Sample 290, Loss: 0.1665
Sample 300, Loss: 0.1810
Sample 310, Loss: 0.1859
Sample 320, Loss: 0.1739
Sample 330, Loss: 0.1810
Sample 340, Loss: 0.1859
Sample 350, Loss: 0.1859
Sample 360, Loss: 0.0758
Sample 370, Loss: 0.1566
Sample 380, Loss: 0.1665
Sample 390, Loss: 0.1810
Sample 400, Loss: 0.1838
Sample 410, Loss: 0.1665
Sample 420, Loss: 0.1838
Sample 430, Loss: 0.1859
Sample 440, Loss: 0.1665
Sample 450, Loss: 0.1838
Sample 460, Loss: 0.1810
Epoch 8/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1739
Sample 10, Loss: 0.1810
Sample 20, Loss: 0.1810
Sample 30, Loss: 0.0000
Sample 40, Loss: 0.1524
Sample 50, Loss: 0.0000
Sample 60, Loss: 0.1859
Sample 70, Loss: 0.1524
Sample 80, Loss: 0.1665
Sample 90, Loss: 0.1859
Sample 100, Loss: 0.1665
Sample 110, Loss: 0.1810
Sample 120, Loss: 0.1810
Sample 130, Loss: 0.1739
Sample 140, Loss: 0.1739
Sample 150, Loss: 0.1739
Sample 160, Loss: 0.1810
Sample 170, Loss: 0.1665
Sample 180, Loss: 0.1810
Sample 190, Loss: 0.1665
Sample 200, Loss: 0.1665
Sample 210, Loss: 0.1810
Sample 220, Loss: 0.1593
Sample 230, Loss: 0.1739
Sample 240, Loss: 0.1593
Sample 250, Loss: 0.1810
Sample 260, Loss: 0.1838
Sample 270, Loss: 0.1461
Sample 280, Loss: 0.1859
Sample 290, Loss: 0.1810
Sample 300, Loss: 0.1810
Sample 310, Loss: 0.1739
Sample 320, Loss: 0.1859
Sample 330, Loss: 0.1739
Sample 340, Loss: 0.1838
Sample 350, Loss: 0.1810
Sample 360, Loss: 0.1810
Sample 370, Loss: 0.1665
Sample 380, Loss: 0.1665
Sample 390, Loss: 0.1739
Sample 400, Loss: 0.1859
Sample 410, Loss: 0.0000
Sample 420, Loss: 0.1859
Sample 430, Loss: 0.1593
Sample 440, Loss: 0.1665
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1739
Epoch 9/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1810
Sample 10, Loss: 0.1665
Sample 20, Loss: 0.0000
Sample 30, Loss: 0.1810
Sample 40, Loss: 0.1665
Sample 50, Loss: 0.0000
Sample 60, Loss: 0.1299
Sample 70, Loss: 0.1810
Sample 80, Loss: 0.1524
Sample 90, Loss: 0.1524
Sample 100, Loss: 0.1665
Sample 110, Loss: 0.1810
Sample 120, Loss: 0.1859
Sample 130, Loss: 0.1838
Sample 140, Loss: 0.1739
Sample 150, Loss: 0.1810
Sample 160, Loss: 0.1665
Sample 170, Loss: 0.1859
Sample 180, Loss: 0.0000
Sample 190, Loss: 0.1665
Sample 200, Loss: 0.1403
Sample 210, Loss: 0.1039
Sample 220, Loss: 0.1810
Sample 230, Loss: 0.1739
Sample 240, Loss: 0.1665
Sample 250, Loss: 0.1524
Sample 260, Loss: 0.1739
Sample 270, Loss: 0.1739
Sample 280, Loss: 0.1739
Sample 290, Loss: 0.1461
Sample 300, Loss: 0.1810
Sample 310, Loss: 0.1810
Sample 320, Loss: 0.1810
Sample 330, Loss: 0.1593
Sample 340, Loss: 0.1665
Sample 350, Loss: 0.1665
Sample 360, Loss: 0.1810
Sample 370, Loss: 0.0062
Sample 380, Loss: 0.1566
Sample 390, Loss: 0.1739
Sample 400, Loss: 0.1838
Sample 410, Loss: 0.1859
Sample 420, Loss: 0.1810
Sample 430, Loss: 0.1593
Sample 440, Loss: 0.1859
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1739
Epoch 10/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1810
Sample 10, Loss: 0.1739
Sample 20, Loss: 0.1810
Sample 30, Loss: 0.1859
Sample 40, Loss: 0.1739
Sample 50, Loss: 0.1810
Sample 60, Loss: 0.1403
Sample 70, Loss: 0.1838
Sample 80, Loss: 0.1665
Sample 90, Loss: 0.1461
Sample 100, Loss: 0.1076
Sample 110, Loss: 0.1859
Sample 120, Loss: 0.1838
Sample 130, Loss: 0.1810
Sample 140, Loss: 0.1566
Sample 150, Loss: 0.1299
Sample 160, Loss: 0.1739
Sample 170, Loss: 0.1739
Sample 180, Loss: 0.1810
Sample 190, Loss: 0.1838
Sample 200, Loss: 0.1838
Sample 210, Loss: 0.1593
Sample 220, Loss: 0.1524
Sample 230, Loss: 0.0264
Sample 240, Loss: 0.1593
Sample 250, Loss: 0.0000
Sample 260, Loss: 0.0000
Sample 270, Loss: 0.1403
Sample 280, Loss: 0.1739
Sample 290, Loss: 0.1665
Sample 300, Loss: 0.1665
Sample 310, Loss: 0.1739
Sample 320, Loss: 0.0860
Sample 330, Loss: 0.1810
Sample 340, Loss: 0.1665
Sample 350, Loss: 0.1810
Sample 360, Loss: 0.1001
Sample 370, Loss: 0.1810
Sample 380, Loss: 0.1810
Sample 390, Loss: 0.1859
Sample 400, Loss: 0.1838
Sample 410, Loss: 0.1665
Sample 420, Loss: 0.1810
Sample 430, Loss: 0.1211
Sample 440, Loss: 0.1810
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1665
Epoch 11/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.0264
Sample 10, Loss: 0.1810
Sample 20, Loss: 0.1859
Sample 30, Loss: 0.0575
Sample 40, Loss: 0.1859
Sample 50, Loss: 0.1039
Sample 60, Loss: 0.1174
Sample 70, Loss: 0.1593
Sample 80, Loss: 0.1665
Sample 90, Loss: 0.1665
Sample 100, Loss: 0.1739
Sample 110, Loss: 0.1023
Sample 120, Loss: 0.1859
Sample 130, Loss: 0.1859
Sample 140, Loss: 0.1810
Sample 150, Loss: 0.1665
Sample 160, Loss: 0.1739
Sample 170, Loss: 0.1859
Sample 180, Loss: 0.1859
Sample 190, Loss: 0.1859
Sample 200, Loss: 0.1859
Sample 210, Loss: 0.0000
Sample 220, Loss: 0.0803
Sample 230, Loss: 0.1665
Sample 240, Loss: 0.1403
Sample 250, Loss: 0.1461
Sample 260, Loss: 0.1810
Sample 270, Loss: 0.1665
Sample 280, Loss: 0.1810
Sample 290, Loss: 0.1739
Sample 300, Loss: 0.1665
Sample 310, Loss: 0.1810
Sample 320, Loss: 0.1739
Sample 330, Loss: 0.1739
Sample 340, Loss: 0.1810
Sample 350, Loss: 0.1810
Sample 360, Loss: 0.1810
Sample 370, Loss: 0.1739
Sample 380, Loss: 0.0883
Sample 390, Loss: 0.1665
Sample 400, Loss: 0.1665
Sample 410, Loss: 0.1566
Sample 420, Loss: 0.0000
Sample 430, Loss: 0.1665
Sample 440, Loss: 0.1665
Sample 450, Loss: 0.1739
Sample 460, Loss: 0.1403
Epoch 12/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
No improvement in validation loss. Patience: 1/5
--------------------------------------------------
Sample 0, Loss: 0.1461
Sample 10, Loss: 0.1665
Sample 20, Loss: 0.1461
Sample 30, Loss: 0.1859
Sample 40, Loss: 0.1739
Sample 50, Loss: 0.1810
Sample 60, Loss: 0.0904
Sample 70, Loss: 0.1739
Sample 80, Loss: 0.1739
Sample 90, Loss: 0.1739
Sample 100, Loss: 0.1810
Sample 110, Loss: 0.1810
Sample 120, Loss: 0.1859
Sample 130, Loss: 0.1739
Sample 140, Loss: 0.1739
Sample 150, Loss: 0.1299
Sample 160, Loss: 0.1859
Sample 170, Loss: 0.0914
Sample 180, Loss: 0.1859
Sample 190, Loss: 0.1739
Sample 200, Loss: 0.0936
Sample 210, Loss: 0.1810
Sample 220, Loss: 0.1665
Sample 230, Loss: 0.1810
Sample 240, Loss: 0.1039
Sample 250, Loss: 0.1593
Sample 260, Loss: 0.1403
Sample 270, Loss: 0.1739
Sample 280, Loss: 0.1566
Sample 290, Loss: 0.1838
Sample 300, Loss: 0.1461
Sample 310, Loss: 0.1810
Sample 320, Loss: 0.1838
Sample 330, Loss: 0.1859
Sample 340, Loss: 0.1593
Sample 350, Loss: 0.0985
Sample 360, Loss: 0.1665
Sample 370, Loss: 0.1810
Sample 380, Loss: 0.1810
Sample 390, Loss: 0.1201
Sample 400, Loss: 0.1810
Sample 410, Loss: 0.1859
Sample 420, Loss: 0.1566
Sample 430, Loss: 0.1810
Sample 440, Loss: 0.1524
Sample 450, Loss: 0.0000
Sample 460, Loss: 0.1859
Epoch 13/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
New best validation loss: 0.1606
--------------------------------------------------
Sample 0, Loss: 0.1152
Sample 10, Loss: 0.1859
Sample 20, Loss: 0.1739
Sample 30, Loss: 0.0783
Sample 40, Loss: 0.1665
Sample 50, Loss: 0.1739
Sample 60, Loss: 0.1859
Sample 70, Loss: 0.1859
Sample 80, Loss: 0.1665
Sample 90, Loss: 0.1403
Sample 100, Loss: 0.1593
Sample 110, Loss: 0.1665
Sample 120, Loss: 0.0000
Sample 130, Loss: 0.1810
Sample 140, Loss: 0.1859
Sample 150, Loss: 0.0000
Sample 160, Loss: 0.1101
Sample 170, Loss: 0.1739
Sample 180, Loss: 0.1810
Sample 190, Loss: 0.1524
Sample 200, Loss: 0.1859
Sample 210, Loss: 0.1403
Sample 220, Loss: 0.1810
Sample 230, Loss: 0.1859
Sample 240, Loss: 0.1665
Sample 250, Loss: 0.1211
Sample 260, Loss: 0.1859
Sample 270, Loss: 0.1810
Sample 280, Loss: 0.1665
Sample 290, Loss: 0.1039
Sample 300, Loss: 0.1859
Sample 310, Loss: 0.1838
Sample 320, Loss: 0.0893
Sample 330, Loss: 0.1739
Sample 340, Loss: 0.1739
Sample 350, Loss: 0.1810
Sample 360, Loss: 0.1739
Sample 370, Loss: 0.1665
Sample 380, Loss: 0.1739
Sample 390, Loss: 0.1665
Sample 400, Loss: 0.1593
Sample 410, Loss: 0.0000
Sample 420, Loss: 0.1665
Sample 430, Loss: 0.1859
Sample 440, Loss: 0.1248
Sample 450, Loss: 0.1461
Sample 460, Loss: 0.1739
Epoch 14/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
No improvement in validation loss. Patience: 1/5
--------------------------------------------------
Sample 0, Loss: 0.1859
Sample 10, Loss: 0.1810
Sample 20, Loss: 0.1810
Sample 30, Loss: 0.0783
Sample 40, Loss: 0.1524
Sample 50, Loss: 0.1859
Sample 60, Loss: 0.1810
Sample 70, Loss: 0.1593
Sample 80, Loss: 0.1593
Sample 90, Loss: 0.1838
Sample 100, Loss: 0.1403
Sample 110, Loss: 0.1810
Sample 120, Loss: 0.1810
Sample 130, Loss: 0.1810
Sample 140, Loss: 0.1739
Sample 150, Loss: 0.1665
Sample 160, Loss: 0.1665
Sample 170, Loss: 0.1566
Sample 180, Loss: 0.1810
Sample 190, Loss: 0.1810
Sample 200, Loss: 0.1810
Sample 210, Loss: 0.1665
Sample 220, Loss: 0.1593
Sample 230, Loss: 0.1739
Sample 240, Loss: 0.1739
Sample 250, Loss: 0.1810
Sample 260, Loss: 0.0000
Sample 270, Loss: 0.1076
Sample 280, Loss: 0.0641
Sample 290, Loss: 0.1403
Sample 300, Loss: 0.1810
Sample 310, Loss: 0.1810
Sample 320, Loss: 0.1665
Sample 330, Loss: 0.1859
Sample 340, Loss: 0.0000
Sample 350, Loss: 0.1593
Sample 360, Loss: 0.1739
Sample 370, Loss: 0.1810
Sample 380, Loss: 0.1566
Sample 390, Loss: 0.1593
Sample 400, Loss: 0.0574
Sample 410, Loss: 0.1739
Sample 420, Loss: 0.1859
Sample 430, Loss: 0.1739
Sample 440, Loss: 0.0000
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1739
Epoch 15/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
No improvement in validation loss. Patience: 2/5
--------------------------------------------------
Sample 0, Loss: 0.1524
Sample 10, Loss: 0.1524
Sample 20, Loss: 0.1739
Sample 30, Loss: 0.1665
Sample 40, Loss: 0.1739
Sample 50, Loss: 0.1739
Sample 60, Loss: 0.1461
Sample 70, Loss: 0.1665
Sample 80, Loss: 0.1859
Sample 90, Loss: 0.1739
Sample 100, Loss: 0.1665
Sample 110, Loss: 0.1859
Sample 120, Loss: 0.1859
Sample 130, Loss: 0.0000
Sample 140, Loss: 0.1665
Sample 150, Loss: 0.1810
Sample 160, Loss: 0.1810
Sample 170, Loss: 0.1810
Sample 180, Loss: 0.1739
Sample 190, Loss: 0.1859
Sample 200, Loss: 0.0787
Sample 210, Loss: 0.0991
Sample 220, Loss: 0.1403
Sample 230, Loss: 0.1665
Sample 240, Loss: 0.1859
Sample 250, Loss: 0.1859
Sample 260, Loss: 0.1566
Sample 270, Loss: 0.1859
Sample 280, Loss: 0.0000
Sample 290, Loss: 0.1810
Sample 300, Loss: 0.1859
Sample 310, Loss: 0.1810
Sample 320, Loss: 0.1810
Sample 330, Loss: 0.1859
Sample 340, Loss: 0.1810
Sample 350, Loss: 0.1349
Sample 360, Loss: 0.0000
Sample 370, Loss: 0.1665
Sample 380, Loss: 0.1593
Sample 390, Loss: 0.1739
Sample 400, Loss: 0.1810
Sample 410, Loss: 0.0000
Sample 420, Loss: 0.1810
Sample 430, Loss: 0.1739
Sample 440, Loss: 0.1299
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1859
Epoch 16/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
No improvement in validation loss. Patience: 3/5
--------------------------------------------------
Sample 0, Loss: 0.1665
Sample 10, Loss: 0.1593
Sample 20, Loss: 0.1739
Sample 30, Loss: 0.1665
Sample 40, Loss: 0.1859
Sample 50, Loss: 0.0707
Sample 60, Loss: 0.1039
Sample 70, Loss: 0.1810
Sample 80, Loss: 0.1665
Sample 90, Loss: 0.1739
Sample 100, Loss: 0.1665
Sample 110, Loss: 0.1665
Sample 120, Loss: 0.0000
Sample 130, Loss: 0.1195
Sample 140, Loss: 0.1524
Sample 150, Loss: 0.1810
Sample 160, Loss: 0.1739
Sample 170, Loss: 0.1665
Sample 180, Loss: 0.1810
Sample 190, Loss: 0.1665
Sample 200, Loss: 0.1349
Sample 210, Loss: 0.1665
Sample 220, Loss: 0.0836
Sample 230, Loss: 0.1665
Sample 240, Loss: 0.1739
Sample 250, Loss: 0.1665
Sample 260, Loss: 0.1593
Sample 270, Loss: 0.1739
Sample 280, Loss: 0.1101
Sample 290, Loss: 0.0264
Sample 300, Loss: 0.0000
Sample 310, Loss: 0.1810
Sample 320, Loss: 0.1665
Sample 330, Loss: 0.1838
Sample 340, Loss: 0.1739
Sample 350, Loss: 0.1810
Sample 360, Loss: 0.1524
Sample 370, Loss: 0.0936
Sample 380, Loss: 0.1810
Sample 390, Loss: 0.1859
Sample 400, Loss: 0.1810
Sample 410, Loss: 0.1461
Sample 420, Loss: 0.1665
Sample 430, Loss: 0.1810
Sample 440, Loss: 0.1739
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1810
Epoch 17/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
No improvement in validation loss. Patience: 4/5
--------------------------------------------------
Sample 0, Loss: 0.1566
Sample 10, Loss: 0.1739
Sample 20, Loss: 0.1810
Sample 30, Loss: 0.0914
Sample 40, Loss: 0.1299
Sample 50, Loss: 0.1810
Sample 60, Loss: 0.1810
Sample 70, Loss: 0.1739
Sample 80, Loss: 0.1665
Sample 90, Loss: 0.1665
Sample 100, Loss: 0.1665
Sample 110, Loss: 0.1859
Sample 120, Loss: 0.0836
Sample 130, Loss: 0.1859
Sample 140, Loss: 0.1593
Sample 150, Loss: 0.1566
Sample 160, Loss: 0.1859
Sample 170, Loss: 0.1593
Sample 180, Loss: 0.1566
Sample 190, Loss: 0.1069
Sample 200, Loss: 0.0893
Sample 210, Loss: 0.1838
Sample 220, Loss: 0.1665
Sample 230, Loss: 0.1810
Sample 240, Loss: 0.1739
Sample 250, Loss: 0.1859
Sample 260, Loss: 0.1211
Sample 270, Loss: 0.1810
Sample 280, Loss: 0.1665
Sample 290, Loss: 0.1859
Sample 300, Loss: 0.1403
Sample 310, Loss: 0.1810
Sample 320, Loss: 0.1739
Sample 330, Loss: 0.1461
Sample 340, Loss: 0.1859
Sample 350, Loss: 0.0000
Sample 360, Loss: 0.0783
Sample 370, Loss: 0.1593
Sample 380, Loss: 0.1739
Sample 390, Loss: 0.1859
Sample 400, Loss: 0.1739
Sample 410, Loss: 0.1739
Sample 420, Loss: 0.1739
Sample 430, Loss: 0.1739
Sample 440, Loss: 0.1593
Sample 450, Loss: 0.1810
Sample 460, Loss: 0.1859
Epoch 18/50, Avg Train Loss: 0.1540
Validation Loss: 0.1606
No improvement in validation loss. Patience: 5/5
Early stopping triggered at epoch 18