{
    "W11-2139": {
        "input_sentences": [
            "Abstract",
            "This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11).",
            "The CMU-ARK German-English Translation System",
            "We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11).": 0,
            "The CMU-ARK German-English Translation System": 0,
            "We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.37632490414910114,
                0.03478782856295148
            ],
            [
                0.0,
                0.37632490414910114,
                1.0,
                0.03784999541878948
            ],
            [
                0.0,
                0.03478782856295148,
                0.03784999541878948,
                1.0
            ]
        ]
    },
    "D12-1126": {
        "input_sentences": [
            "Abstract",
            "Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features",
            "Dynamic Features",
            "The underlying problem is how to tag part-of-speech (POS) for the English words involved.",
            "In this paper, we present a method using dynamic features to tag POS of mixed texts.",
            "Experiments show that our method achieves higher performance than traditional sequence labeling methods.",
            "Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts.",
            "Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.",
            "In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature.",
            "Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, \u201cforeign words\u201d."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features": 0,
            "Dynamic Features": 0,
            "The underlying problem is how to tag part-of-speech (POS) for the English words involved.": 0,
            "In this paper, we present a method using dynamic features to tag POS of mixed texts.": 0,
            "Experiments show that our method achieves higher performance than traditional sequence labeling methods.": 0,
            "Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts.": 0,
            "Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.": 0,
            "In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature.": 0,
            "Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, \u201cforeign words\u201d.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.5053551827781617,
                0.21262147699514686,
                0.3827897404642627,
                0.0,
                0.35576635883623525,
                0.3294265508132168,
                0.11800127648061418,
                0.052124226142999025
            ],
            [
                0.0,
                0.5053551827781617,
                1.0,
                0.0,
                0.3996713065560648,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.21262147699514686,
                0.0,
                1.0,
                0.19951300790777698,
                0.0,
                0.06036132148786439,
                0.09772564569744747,
                0.1159065997125784,
                0.18471581728937633
            ],
            [
                0.0,
                0.3827897404642627,
                0.3996713065560648,
                0.19951300790777698,
                1.0,
                0.07306312974357668,
                0.15652842291718255,
                0.25178861193332636,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.07306312974357668,
                1.0,
                0.0,
                0.19038521555442572,
                0.0,
                0.0
            ],
            [
                0.0,
                0.35576635883623525,
                0.0,
                0.06036132148786439,
                0.15652842291718255,
                0.0,
                1.0,
                0.1561424773763962,
                0.10215235969943612,
                0.04512334829605968
            ],
            [
                0.0,
                0.3294265508132168,
                0.0,
                0.09772564569744747,
                0.25178861193332636,
                0.19038521555442572,
                0.1561424773763962,
                1.0,
                0.05836329436680251,
                0.0
            ],
            [
                0.0,
                0.11800127648061418,
                0.0,
                0.1159065997125784,
                0.0,
                0.0,
                0.10215235969943612,
                0.05836329436680251,
                1.0,
                0.13956027037214888
            ],
            [
                0.0,
                0.052124226142999025,
                0.0,
                0.18471581728937633,
                0.0,
                0.0,
                0.04512334829605968,
                0.0,
                0.13956027037214888,
                1.0
            ]
        ]
    },
    "W01-0510": {
        "input_sentences": [
            "Information Extraction Using The Structured Language Model",
            "Abstract",
            "Training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage.",
            "The model is automatically trained from a set of sentences annotated with frame/slot labels and spans.",
            "The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser.",
            "The task of template filling is cast as constrained parsing using the SLM.",
            "Despite the small amount of training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad personal information management task."
        ],
        "authority_scores": {
            "Information Extraction Using The Structured Language Model": 0,
            "Abstract": 0,
            "Training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage.": 0,
            "The model is automatically trained from a set of sentences annotated with frame/slot labels and spans.": 0,
            "The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser.": 0,
            "The task of template filling is cast as constrained parsing using the SLM.": 0,
            "Despite the small amount of training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad personal information management task.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.03352282262418517,
                0.07291247242593155,
                0.514272243870114,
                0.11312055399751217,
                0.09798686896931907
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.03352282262418517,
                0.0,
                1.0,
                0.1557587780460249,
                0.10271763287421695,
                0.09107719030596993,
                0.1643547967972304
            ],
            [
                0.07291247242593155,
                0.0,
                0.1557587780460249,
                1.0,
                0.037496860800601634,
                0.0,
                0.08825258888168074
            ],
            [
                0.514272243870114,
                0.0,
                0.10271763287421695,
                0.037496860800601634,
                1.0,
                0.29704416300629644,
                0.0838178631745121
            ],
            [
                0.11312055399751217,
                0.0,
                0.09107719030596993,
                0.0,
                0.29704416300629644,
                1.0,
                0.06655442482166087
            ],
            [
                0.09798686896931907,
                0.0,
                0.1643547967972304,
                0.08825258888168074,
                0.0838178631745121,
                0.06655442482166087,
                1.0
            ]
        ]
    },
    "W06-2204": {
        "input_sentences": [
            "Abstract",
            "First, the algorithm does not require redundancy in the fragments to be extracted, but only redundancy of the extraction patterns themselves.",
            "Compared to previous work, two novel features.",
            "The requirement for large labelled training corpora is widely recognized as a key bottleneck in the use of learning algorithms for extraction.",
            "Transductive Pattern Learning For Information Extraction",
            "Second, most bootstrapping methods identify the highest quality fragments in the unlabelled data and then assume that they are as reliable as manually labelled data in subsequent iterations. contrast, scoring mechanism prevents errors from snowballing by recording the reliability of fragments extracted from unlabelled data.",
            "We present a semi-supervised learning algorithm for information extraction that can acquire extraction patterns from a small amount of labelled text in conjunction with a large amount of unlabelled text.",
            "Our experiments with several demonstrate that usually competitive with various fully-supervised algorithms when very little labelled training data is available."
        ],
        "authority_scores": {
            "Abstract": 0,
            "First, the algorithm does not require redundancy in the fragments to be extracted, but only redundancy of the extraction patterns themselves.": 0,
            "Compared to previous work, two novel features.": 0,
            "The requirement for large labelled training corpora is widely recognized as a key bottleneck in the use of learning algorithms for extraction.": 0,
            "Transductive Pattern Learning For Information Extraction": 0,
            "Second, most bootstrapping methods identify the highest quality fragments in the unlabelled data and then assume that they are as reliable as manually labelled data in subsequent iterations. contrast, scoring mechanism prevents errors from snowballing by recording the reliability of fragments extracted from unlabelled data.": 0,
            "We present a semi-supervised learning algorithm for information extraction that can acquire extraction patterns from a small amount of labelled text in conjunction with a large amount of unlabelled text.": 0,
            "Our experiments with several demonstrate that usually competitive with various fully-supervised algorithms when very little labelled training data is available.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.041010181811105856,
                0.06955400315935216,
                0.12264358530760344,
                0.1833987395845558,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.041010181811105856,
                0.0,
                1.0,
                0.15036328741515304,
                0.021987680112525,
                0.18969127194173202,
                0.16705073220358235
            ],
            [
                0.0,
                0.06955400315935216,
                0.0,
                0.15036328741515304,
                1.0,
                0.0,
                0.2685234005484584,
                0.0
            ],
            [
                0.0,
                0.12264358530760344,
                0.0,
                0.021987680112525,
                0.0,
                1.0,
                0.08043159223084566,
                0.1323794180775436
            ],
            [
                0.0,
                0.1833987395845558,
                0.0,
                0.18969127194173202,
                0.2685234005484584,
                0.08043159223084566,
                1.0,
                0.0831189234485537
            ],
            [
                0.0,
                0.0,
                0.0,
                0.16705073220358235,
                0.0,
                0.1323794180775436,
                0.0831189234485537,
                1.0
            ]
        ]
    },
    "D08-1094": {
        "input_sentences": [
            "Abstract",
            "A Structured Vector Space Model for Word Meaning in Context",
            "In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.",
            "This makes it possible to integrate syntax into the computation of word meaning in context.",
            "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context.",
            "We argue that existing models for this task do not take syntactic structure sufficiently into account. present a novel vector space model that addresses these issues by incorporating the selectional preferences for words\u2019 argument positions.",
            "This task is a crucial step towards a robust, vector-based compositional account of sentence meaning."
        ],
        "authority_scores": {
            "Abstract": 0,
            "A Structured Vector Space Model for Word Meaning in Context": 0,
            "In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.": 0,
            "This makes it possible to integrate syntax into the computation of word meaning in context.": 0,
            "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context.": 0,
            "We argue that existing models for this task do not take syntactic structure sufficiently into account. present a novel vector space model that addresses these issues by incorporating the selectional preferences for words\u2019 argument positions.": 0,
            "This task is a crucial step towards a robust, vector-based compositional account of sentence meaning.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.08888299224438508,
                0.2824357134358599,
                0.3737241642859834,
                0.16559100373895358,
                0.13857038818177173
            ],
            [
                0.0,
                0.08888299224438508,
                1.0,
                0.0,
                0.0,
                0.04005301246686281,
                0.0
            ],
            [
                0.0,
                0.2824357134358599,
                0.0,
                1.0,
                0.17548424344204644,
                0.0,
                0.053252479013257616
            ],
            [
                0.0,
                0.3737241642859834,
                0.0,
                0.17548424344204644,
                1.0,
                0.10288575640253957,
                0.14320710607252835
            ],
            [
                0.0,
                0.16559100373895358,
                0.04005301246686281,
                0.0,
                0.10288575640253957,
                1.0,
                0.12933269319136162
            ],
            [
                0.0,
                0.13857038818177173,
                0.0,
                0.053252479013257616,
                0.14320710607252835,
                0.12933269319136162,
                1.0
            ]
        ]
    },
    "W09-1210": {
        "input_sentences": [
            "Abstract",
            "The semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro F1 score) in the joint task for syntactic and semantic dependency parsing.",
            "In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple languages.",
            "For the subtask of syntactic dependency parsing, we could reach the second place with an accuracy in average of 85.68 which is only 0.09 points behind the first ranked system.",
            "Efficient Parsing of Syntactic and Semantic Dependency Structures",
            "For the applications of syntactic and semantic parsing, the parsing time and memory footprint are very important.",
            "For this task, our system has the highest accuracy for English with 89.88, German with 87.48 and the out-of-domain data in average with 78.79.",
            "Our system combines and implements efficient parsing techniques to get a high accuracy as well as very good parsing and training time.",
            "We think that also the development of systems can profit from this since one can perform more experiments in the given time."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro F1 score) in the joint task for syntactic and semantic dependency parsing.": 0,
            "In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple languages.": 0,
            "For the subtask of syntactic dependency parsing, we could reach the second place with an accuracy in average of 85.68 which is only 0.09 points behind the first ranked system.": 0,
            "Efficient Parsing of Syntactic and Semantic Dependency Structures": 0,
            "For the applications of syntactic and semantic parsing, the parsing time and memory footprint are very important.": 0,
            "For this task, our system has the highest accuracy for English with 89.88, German with 87.48 and the out-of-domain data in average with 78.79.": 0,
            "Our system combines and implements efficient parsing techniques to get a high accuracy as well as very good parsing and training time.": 0,
            "We think that also the development of systems can profit from this since one can perform more experiments in the given time.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.26817541405359957,
                0.1976542762393916,
                0.29054398597393205,
                0.17933205560265086,
                0.039749290282465655,
                0.047842820495640176,
                0.0
            ],
            [
                0.0,
                0.26817541405359957,
                1.0,
                0.10225545764668223,
                0.41542873556591997,
                0.16738323034528804,
                0.04914530959282534,
                0.059152005189119006,
                0.0
            ],
            [
                0.0,
                0.1976542762393916,
                0.10225545764668223,
                1.0,
                0.18505371193810755,
                0.10600118132228442,
                0.10704252300879312,
                0.11079888914834939,
                0.0
            ],
            [
                0.0,
                0.29054398597393205,
                0.41542873556591997,
                0.18505371193810755,
                1.0,
                0.30291672253438734,
                0.0,
                0.2483060851042223,
                0.0
            ],
            [
                0.0,
                0.17933205560265086,
                0.16738323034528804,
                0.10600118132228442,
                0.30291672253438734,
                1.0,
                0.0,
                0.21543484515154937,
                0.07777915902450919
            ],
            [
                0.0,
                0.039749290282465655,
                0.04914530959282534,
                0.10704252300879312,
                0.0,
                0.0,
                1.0,
                0.050619744212930196,
                0.0
            ],
            [
                0.0,
                0.047842820495640176,
                0.059152005189119006,
                0.11079888914834939,
                0.2483060851042223,
                0.21543484515154937,
                0.050619744212930196,
                1.0,
                0.06593609801317635
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07777915902450919,
                0.0,
                0.06593609801317635,
                1.0
            ]
        ]
    },
    "C08-1081": {
        "input_sentences": [
            "Abstract",
            "We present the first results on parsing the SYNTAGRUS treebank of Russian with a data-driven dependency parser, achieving a labeled attachment score of over 82% and an unlabeled attachment score of 89%.",
            "A feature analysis shows that high parsing accuracy is crucially dependent on the use of both lexical and morphological features.",
            "Parsing the SynTagRus Treebank of Russian",
            "We conjecture that the latter result can be generalized to richly inflected languages in general, provided that sufficient amounts of training data are available."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We present the first results on parsing the SYNTAGRUS treebank of Russian with a data-driven dependency parser, achieving a labeled attachment score of over 82% and an unlabeled attachment score of 89%.": 0,
            "A feature analysis shows that high parsing accuracy is crucially dependent on the use of both lexical and morphological features.": 0,
            "Parsing the SynTagRus Treebank of Russian": 0,
            "We conjecture that the latter result can be generalized to richly inflected languages in general, provided that sufficient amounts of training data are available.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.028890339305226883,
                0.3377312534024911,
                0.03988554181515318
            ],
            [
                0.0,
                0.028890339305226883,
                1.0,
                0.08554239210665184,
                0.0
            ],
            [
                0.0,
                0.3377312534024911,
                0.08554239210665184,
                1.0,
                0.0
            ],
            [
                0.0,
                0.03988554181515318,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "D11-1132": {
        "input_sentences": [
            "Abstract",
            "The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.",
            "This paper describes a novel approach to the semantic relation detection problem.",
            "Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors.",
            "Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations.",
            "First, we construct a large relation repository of more than 7,000 relations from Wikipedia.",
            "Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations.",
            "Relation Extraction with Relation Topics",
            "Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations.",
            "Specifically, we detect a new semantic relation by projecting the new relation\u2019s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.": 0,
            "This paper describes a novel approach to the semantic relation detection problem.": 0,
            "Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors.": 0,
            "Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations.": 0,
            "First, we construct a large relation repository of more than 7,000 relations from Wikipedia.": 0,
            "Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations.": 0,
            "Relation Extraction with Relation Topics": 0,
            "Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations.": 0,
            "Specifically, we detect a new semantic relation by projecting the new relation\u2019s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09332705183627799,
                0.04688698701108858,
                0.10838382967786535,
                0.2524138073673013,
                0.05633044188683733,
                0.16251772200270398,
                0.055116545336106555,
                0.055009362100974735
            ],
            [
                0.0,
                0.09332705183627799,
                1.0,
                0.03993118536322442,
                0.03877049406276753,
                0.030306390909063716,
                0.023118416690743798,
                0.09001758603323438,
                0.0226202248545138,
                0.11566401671653959
            ],
            [
                0.0,
                0.04688698701108858,
                0.03993118536322442,
                1.0,
                0.06239481820547934,
                0.04877321780253426,
                0.16269694518755273,
                0.14486869594044133,
                0.03640358091076918,
                0.3132944295285684
            ],
            [
                0.0,
                0.10838382967786535,
                0.03877049406276753,
                0.06239481820547934,
                1.0,
                0.27021131404158744,
                0.18403930074754635,
                0.21627032074228217,
                0.2579511645001479,
                0.11435566283508283
            ],
            [
                0.0,
                0.2524138073673013,
                0.030306390909063716,
                0.04877321780253426,
                0.27021131404158744,
                1.0,
                0.19876667077399204,
                0.1099503380558493,
                0.10129200240222762,
                0.057222350378257396
            ],
            [
                0.0,
                0.05633044188683733,
                0.023118416690743798,
                0.16269694518755273,
                0.18403930074754635,
                0.19876667077399204,
                1.0,
                0.17404653568302453,
                0.12258687963414237,
                0.19088143088460607
            ],
            [
                0.0,
                0.16251772200270398,
                0.09001758603323438,
                0.14486869594044133,
                0.21627032074228217,
                0.1099503380558493,
                0.17404653568302453,
                1.0,
                0.17029590845100862,
                0.1699647399010535
            ],
            [
                0.0,
                0.055116545336106555,
                0.0226202248545138,
                0.03640358091076918,
                0.2579511645001479,
                0.10129200240222762,
                0.12258687963414237,
                0.17029590845100862,
                1.0,
                0.15346539391859323
            ],
            [
                0.0,
                0.055009362100974735,
                0.11566401671653959,
                0.3132944295285684,
                0.11435566283508283,
                0.057222350378257396,
                0.19088143088460607,
                0.1699647399010535,
                0.15346539391859323,
                1.0
            ]
        ]
    },
    "P14-1060": {
        "input_sentences": [
            "Abstract",
            "We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated.",
            "In this work, we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohelineal constituents, or The framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents, and circumvents some problems of data sparsity by design.",
            "Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.",
            "Vector space semantics with frequency-driven motifs",
            "Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated.": 0,
            "In this work, we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohelineal constituents, or The framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents, and circumvents some problems of data sparsity by design.": 0,
            "Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.": 0,
            "Vector space semantics with frequency-driven motifs": 0,
            "Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.13625125921064493,
                0.0,
                0.0,
                0.027950252087105776
            ],
            [
                0.0,
                0.13625125921064493,
                1.0,
                0.04645048737663035,
                0.04399029929421255,
                0.14676661670902477
            ],
            [
                0.0,
                0.0,
                0.04645048737663035,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.04399029929421255,
                0.0,
                1.0,
                0.04700467735299054
            ],
            [
                0.0,
                0.027950252087105776,
                0.14676661670902477,
                0.0,
                0.04700467735299054,
                1.0
            ]
        ]
    },
    "W05-0104": {
        "input_sentences": [
            "Abstract",
            "A Core-Tools Statistical NLP Course",
            "In the fall term of 2004, I taught a new statistical NLP course focusing on core tools and machine-learning algorithms.",
            "The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system.",
            "Using provided scaffolding, students built realistic tools with nearly state-of-theart performance in most cases.",
            "This paper briefly outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way."
        ],
        "authority_scores": {
            "Abstract": 0,
            "A Core-Tools Statistical NLP Course": 0,
            "In the fall term of 2004, I taught a new statistical NLP course focusing on core tools and machine-learning algorithms.": 0,
            "The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system.": 0,
            "Using provided scaffolding, students built realistic tools with nearly state-of-theart performance in most cases.": 0,
            "This paper briefly outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.4682904729813428,
                0.14728527921502757,
                0.06666815478570393,
                0.13299972809539504
            ],
            [
                0.0,
                0.4682904729813428,
                1.0,
                0.0689722930667944,
                0.031220061737390656,
                0.06228250557618252
            ],
            [
                0.0,
                0.14728527921502757,
                0.0689722930667944,
                1.0,
                0.06106257005348405,
                0.08183534639388546
            ],
            [
                0.0,
                0.06666815478570393,
                0.031220061737390656,
                0.06106257005348405,
                1.0,
                0.0
            ],
            [
                0.0,
                0.13299972809539504,
                0.06228250557618252,
                0.08183534639388546,
                0.0,
                1.0
            ]
        ]
    },
    "W12-2802": {
        "input_sentences": [
            "Abstract",
            "as well as a mapping between specific phrases in the language and aspects of the external world; for example the mapping between the words ?the tire pallet?",
            "and a specific object in the environment.",
            "In this paper, we present an approach which is capable of jointly learninga policy for following natural language com mands such as ?Pick up the tire pallet,?",
            "com mands given to a robotic forklift by untrained users.",
            "In order for robots to effectively understand natural language commands, they must be ableto acquire a large vocabulary of meaning rep resentations that can be mapped to perceptualfeatures in the external world.",
            "Previous ap proaches to learning these grounded meaning representations require detailed annotations at training time.",
            "We assume the action policy takes a parametric form that factors based on the structure of the language, based on the G3 framework and use stochastic gradient ascentto optimize policy parameters.",
            "Toward Learning Perceptually Grounded Word Meanings from Unaligned Parallel Data",
            "Our prelimi nary evaluation demonstrates the effectivenessof the model on a corpus of ?pick up?"
        ],
        "authority_scores": {
            "Abstract": 0,
            "as well as a mapping between specific phrases in the language and aspects of the external world; for example the mapping between the words ?the tire pallet?": 0,
            "and a specific object in the environment.": 0,
            "In this paper, we present an approach which is capable of jointly learninga policy for following natural language com mands such as ?Pick up the tire pallet,?": 0,
            "com mands given to a robotic forklift by untrained users.": 0,
            "In order for robots to effectively understand natural language commands, they must be ableto acquire a large vocabulary of meaning rep resentations that can be mapped to perceptualfeatures in the external world.": 0,
            "Previous ap proaches to learning these grounded meaning representations require detailed annotations at training time.": 0,
            "We assume the action policy takes a parametric form that factors based on the structure of the language, based on the G3 framework and use stochastic gradient ascentto optimize policy parameters.": 0,
            "Toward Learning Perceptually Grounded Word Meanings from Unaligned Parallel Data": 0,
            "Our prelimi nary evaluation demonstrates the effectivenessof the model on a corpus of ?pick up?": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.1261630954749166,
                0.1534115697327828,
                0.0,
                0.13420722660520923,
                0.0,
                0.02665493303638215,
                0.0,
                0.0
            ],
            [
                0.0,
                0.1261630954749166,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.1534115697327828,
                0.0,
                1.0,
                0.16104863681824003,
                0.08120186056613109,
                0.0,
                0.1127034595115597,
                0.0,
                0.07356405465935874
            ],
            [
                0.0,
                0.0,
                0.0,
                0.16104863681824003,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.13420722660520923,
                0.0,
                0.08120186056613109,
                0.0,
                1.0,
                0.05351551775198901,
                0.022898956904635694,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05351551775198901,
                1.0,
                0.0,
                0.15850127058588157,
                0.0
            ],
            [
                0.0,
                0.02665493303638215,
                0.0,
                0.1127034595115597,
                0.0,
                0.022898956904635694,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.15850127058588157,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.07356405465935874,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "W04-0837": {
        "input_sentences": [
            "Abstract",
            "The first (or predominant) sense heuristic assumes the availability of handtagged data.",
            "We evaluate on the and English alldata.",
            "In this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text.",
            "Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently.",
            "For accurate first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough.",
            "Using Automatically Acquired Predominant Senses For Word Sense Disambiguation",
            "In this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from SemCor many systems in",
            "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The first (or predominant) sense heuristic assumes the availability of handtagged data.": 0,
            "We evaluate on the and English alldata.": 0,
            "In this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text.": 0,
            "Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently.": 0,
            "For accurate first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough.": 0,
            "Using Automatically Acquired Predominant Senses For Word Sense Disambiguation": 0,
            "In this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from SemCor many systems in": 0,
            "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.1870492216479757,
                0.0,
                0.12670572375593261,
                0.16363109920032076,
                0.07634478349104613,
                0.12728464230736486
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.1870492216479757,
                0.0,
                1.0,
                0.0,
                0.10073532769474954,
                0.39006521479616285,
                0.3419809003822796,
                0.15794790281925317
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.06323859053553922,
                0.0,
                0.08283022203314287
            ],
            [
                0.0,
                0.12670572375593261,
                0.0,
                0.10073532769474954,
                0.0,
                1.0,
                0.05237453494288719,
                0.07318959952117035,
                0.1220242113433204
            ],
            [
                0.0,
                0.16363109920032076,
                0.0,
                0.39006521479616285,
                0.06323859053553922,
                0.05237453494288719,
                1.0,
                0.2417355764568301,
                0.4018663916010243
            ],
            [
                0.0,
                0.07634478349104613,
                0.0,
                0.3419809003822796,
                0.0,
                0.07318959952117035,
                0.2417355764568301,
                1.0,
                0.10571376567494037
            ],
            [
                0.0,
                0.12728464230736486,
                0.0,
                0.15794790281925317,
                0.08283022203314287,
                0.1220242113433204,
                0.4018663916010243,
                0.10571376567494037,
                1.0
            ]
        ]
    },
    "W11-0610": {
        "input_sentences": [
            "Abstract",
            "We describe our data set, which consists of transcribed data from a widely used clinical diagnostic instrument (the ADOS) for children with autism, children with developmental language disorder, and typically developing children.",
            "We expect further improvement with additional data, features, and classification techniques.",
            "Classification of Atypical Language in Autism",
            "Our classifiers achieve results well above chance, demonstrating the potential for using NLP techniques to enhance neurodevelopmental diagnosis and atypical language analysis.",
            "In this paper, we discuss previous work identifying language errors associated with atypical language in ASD and describe a procedure for reproducing those results.",
            "We then present methods for automatically extracting lexical and syntactic features from transcripts of children\u2019s speech to 1) identify certain syntactic and semantic errors that have previously been found to distinguish ASD language from that of children with typical development; and 2) perform diagnostic classification.",
            "Atypical or idiosyncratic language is a characteristic of autism spectrum disorder (ASD)."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We describe our data set, which consists of transcribed data from a widely used clinical diagnostic instrument (the ADOS) for children with autism, children with developmental language disorder, and typically developing children.": 0,
            "We expect further improvement with additional data, features, and classification techniques.": 0,
            "Classification of Atypical Language in Autism": 0,
            "Our classifiers achieve results well above chance, demonstrating the potential for using NLP techniques to enhance neurodevelopmental diagnosis and atypical language analysis.": 0,
            "In this paper, we discuss previous work identifying language errors associated with atypical language in ASD and describe a procedure for reproducing those results.": 0,
            "We then present methods for automatically extracting lexical and syntactic features from transcripts of children\u2019s speech to 1) identify certain syntactic and semantic errors that have previously been found to distinguish ASD language from that of children with typical development; and 2) perform diagnostic classification.": 0,
            "Atypical or idiosyncratic language is a characteristic of autism spectrum disorder (ASD).": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.12534470824229235,
                0.12555857791089306,
                0.01463154141199088,
                0.03141553982299722,
                0.21784230835256904,
                0.13439482834437586
            ],
            [
                0.0,
                0.12534470824229235,
                1.0,
                0.16916379995296657,
                0.08192113210469175,
                0.0,
                0.10285032471627177,
                0.0
            ],
            [
                0.0,
                0.12555857791089306,
                0.16916379995296657,
                1.0,
                0.13843271845335153,
                0.20555381024242456,
                0.11810638061848054,
                0.3879812645032711
            ],
            [
                0.0,
                0.01463154141199088,
                0.08192113210469175,
                0.13843271845335153,
                1.0,
                0.13187487231398765,
                0.013763124971565425,
                0.07762054680068463
            ],
            [
                0.0,
                0.03141553982299722,
                0.0,
                0.20555381024242456,
                0.13187487231398765,
                1.0,
                0.10205724774511372,
                0.18212353577990814
            ],
            [
                0.0,
                0.21784230835256904,
                0.10285032471627177,
                0.11810638061848054,
                0.013763124971565425,
                0.10205724774511372,
                1.0,
                0.06622337512894727
            ],
            [
                0.0,
                0.13439482834437586,
                0.0,
                0.3879812645032711,
                0.07762054680068463,
                0.18212353577990814,
                0.06622337512894727,
                1.0
            ]
        ]
    },
    "I08-1012": {
        "input_sentences": [
            "Abstract",
            "We thentrain another parser which uses the informa tion on short dependency relations extractedfrom the output of the first parser.",
            "Our proposed approach achieves an unlabeled at tachment score of 86.52, an absolute 1.24% improvement over the baseline system on the data set of Chinese Treebank.",
            "Dependency Parsing with Short Dependency Relations in Unlabeled Data",
            "The unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words.",
            "This paper presents an effective dependencyparsing approach of incorporating short de pendency information from unlabeled data."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We thentrain another parser which uses the informa tion on short dependency relations extractedfrom the output of the first parser.": 0,
            "Our proposed approach achieves an unlabeled at tachment score of 86.52, an absolute 1.24% improvement over the baseline system on the data set of Chinese Treebank.": 0,
            "Dependency Parsing with Short Dependency Relations in Unlabeled Data": 0,
            "The unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words.": 0,
            "This paper presents an effective dependencyparsing approach of incorporating short de pendency information from unlabeled data.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.2881659320284415,
                0.20359345559664951,
                0.037313444000216654
            ],
            [
                0.0,
                0.0,
                1.0,
                0.08613497552914895,
                0.055454739274520215,
                0.12286715403836027
            ],
            [
                0.0,
                0.2881659320284415,
                0.08613497552914895,
                1.0,
                0.2791822424167421,
                0.16581745847419083
            ],
            [
                0.0,
                0.20359345559664951,
                0.055454739274520215,
                0.2791822424167421,
                1.0,
                0.10675528576354017
            ],
            [
                0.0,
                0.037313444000216654,
                0.12286715403836027,
                0.16581745847419083,
                0.10675528576354017,
                1.0
            ]
        ]
    },
    "P12-2006": {
        "input_sentences": [
            "Abstract",
            "We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed.",
            "Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation",
            "At a speed of roughly 70 words per second, Moses 17.2% whereas our approach yields 20.0% with identical models.",
            "In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions.",
            "Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors.",
            "Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed.": 0,
            "Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation": 0,
            "At a speed of roughly 70 words per second, Moses 17.2% whereas our approach yields 20.0% with identical models.": 0,
            "In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions.": 0,
            "Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors.": 0,
            "Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.03472289717174186,
                0.19440153436390656,
                0.02122090634018364,
                0.06739936640760663,
                0.10481883816331648
            ],
            [
                0.0,
                0.03472289717174186,
                1.0,
                0.0,
                0.35099580110075296,
                0.14962016511017284,
                0.20356753760504714
            ],
            [
                0.0,
                0.19440153436390656,
                0.0,
                1.0,
                0.0,
                0.039097548235006094,
                0.06746352571140075
            ],
            [
                0.0,
                0.02122090634018364,
                0.35099580110075296,
                0.0,
                1.0,
                0.09144039723130376,
                0.12441034594700011
            ],
            [
                0.0,
                0.06739936640760663,
                0.14962016511017284,
                0.039097548235006094,
                0.09144039723130376,
                1.0,
                0.13417011548759256
            ],
            [
                0.0,
                0.10481883816331648,
                0.20356753760504714,
                0.06746352571140075,
                0.12441034594700011,
                0.13417011548759256,
                1.0
            ]
        ]
    },
    "W12-3706": {
        "input_sentences": [
            "Abstract",
            "We propose an approach based on the order of the words without using any syntactic and semantic information.",
            "In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards.",
            "It consists of building one probabilistic model for the positive and another one for the negative opinions.",
            "Then the test opinions are compared to both models and a decision and confidence measure are calculated.",
            "The classification of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach.",
            "Opinum: statistical sentiment analysis for opinion classification",
            "We present an accuracy above 81% for Spanish opinions in the financial products domain."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We propose an approach based on the order of the words without using any syntactic and semantic information.": 0,
            "In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards.": 0,
            "It consists of building one probabilistic model for the positive and another one for the negative opinions.": 0,
            "Then the test opinions are compared to both models and a decision and confidence measure are calculated.": 0,
            "The classification of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach.": 0,
            "Opinum: statistical sentiment analysis for opinion classification": 0,
            "We present an accuracy above 81% for Spanish opinions in the financial products domain.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.08043811576938974,
                0.0,
                0.0,
                0.156669186161381,
                0.0,
                0.0
            ],
            [
                0.0,
                0.08043811576938974,
                1.0,
                0.0,
                0.0,
                0.07272996726972251,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0783189732254673,
                0.18321954723350445,
                0.0,
                0.0783189732254673
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0783189732254673,
                1.0,
                0.0,
                0.0,
                0.06952106848944942
            ],
            [
                0.0,
                0.156669186161381,
                0.07272996726972251,
                0.18321954723350445,
                0.0,
                1.0,
                0.19187978225624547,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.19187978225624547,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0783189732254673,
                0.06952106848944942,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P13-1092": {
        "input_sentences": [
            "Abstract",
            "Grounded Unsupervised Semantic",
            "Grounded Unsupervised Semantic Parsing",
            "To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision.",
            "On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches.",
            "Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM.",
            "We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Grounded Unsupervised Semantic": 0,
            "Grounded Unsupervised Semantic Parsing": 0,
            "To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision.": 0,
            "On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches.": 0,
            "Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM.": 0,
            "We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.8173085975132055,
                0.10900781745413778,
                0.0,
                0.08354281803775586,
                0.216312161842785
            ],
            [
                0.0,
                0.8173085975132055,
                1.0,
                0.08909302640141686,
                0.0,
                0.06828026344273917,
                0.3147671861900417
            ],
            [
                0.0,
                0.10900781745413778,
                0.08909302640141686,
                1.0,
                0.0362006212199834,
                0.03326973771044591,
                0.1013507415747642
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0362006212199834,
                1.0,
                0.03680612605796076,
                0.16818564379442075
            ],
            [
                0.0,
                0.08354281803775586,
                0.06828026344273917,
                0.03326973771044591,
                0.03680612605796076,
                1.0,
                0.028375515018618222
            ],
            [
                0.0,
                0.216312161842785,
                0.3147671861900417,
                0.1013507415747642,
                0.16818564379442075,
                0.028375515018618222,
                1.0
            ]
        ]
    },
    "W07-2214": {
        "input_sentences": [
            "Abstract",
            "Pomset mcfgs",
            "This paper identifies two orthogonal dimensions of context sensitivity, the first being context sensitivity in concurrency and the second being structural context sensitivity.",
            "We present an example from natural language which seems to require both types of context sensitivity, and introduce partially ordered multisets (pomsets) mcfgs as a formalism which succintly expresses both."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Pomset mcfgs": 0,
            "This paper identifies two orthogonal dimensions of context sensitivity, the first being context sensitivity in concurrency and the second being structural context sensitivity.": 0,
            "We present an example from natural language which seems to require both types of context sensitivity, and introduce partially ordered multisets (pomsets) mcfgs as a formalism which succintly expresses both.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.12255132163600413
            ],
            [
                0.0,
                0.0,
                1.0,
                0.21955315742347717
            ],
            [
                0.0,
                0.12255132163600413,
                0.21955315742347717,
                1.0
            ]
        ]
    },
    "P05-1022": {
        "input_sentences": [
            "Abstract",
            "This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).",
            "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.",
            "A discriminative reranker requires a source of candidate parses for each sentence.",
            "Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking",
            "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).",
            "This method generates 50-best lists that are of substantially higher quality than previously obtainable."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).": 0,
            "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.": 0,
            "A discriminative reranker requires a source of candidate parses for each sentence.": 0,
            "Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking": 0,
            "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).": 0,
            "This method generates 50-best lists that are of substantially higher quality than previously obtainable.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07113492897701715,
                0.05833309031801102,
                0.2194117033772753,
                0.17804451303186747,
                0.14436590946624858
            ],
            [
                0.0,
                0.07113492897701715,
                1.0,
                0.19612617844537855,
                0.09467049656119805,
                0.0,
                0.024734087777161906
            ],
            [
                0.0,
                0.05833309031801102,
                0.19612617844537855,
                1.0,
                0.10072434990566387,
                0.07634952715544875,
                0.0
            ],
            [
                0.0,
                0.2194117033772753,
                0.09467049656119805,
                0.10072434990566387,
                1.0,
                0.1948449342944911,
                0.060177310672596435
            ],
            [
                0.0,
                0.17804451303186747,
                0.0,
                0.07634952715544875,
                0.1948449342944911,
                1.0,
                0.06051419542991648
            ],
            [
                0.0,
                0.14436590946624858,
                0.024734087777161906,
                0.0,
                0.060177310672596435,
                0.06051419542991648,
                1.0
            ]
        ]
    },
    "C08-1074": {
        "input_sentences": [
            "Abstract",
            "We findthat all of our random restart methods out perform MERT without random restarts,and we develop some refinements of ran dom restarts that are superior to the most common approach with regard to resulting model quality and training time.",
            "The use of multiple randomized start ing points in MERT is a well-established practice, although there seems to be nopublished systematic study of its benefits.",
            "We compare several ways of perform ing random restarts with MERT.",
            "Och?s (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights instatistical machine translation (SMT) models.",
            "Random Restarts in Minimum Error Rate Training for Statistical Machine Translation"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We findthat all of our random restart methods out perform MERT without random restarts,and we develop some refinements of ran dom restarts that are superior to the most common approach with regard to resulting model quality and training time.": 0,
            "The use of multiple randomized start ing points in MERT is a well-established practice, although there seems to be nopublished systematic study of its benefits.": 0,
            "We compare several ways of perform ing random restarts with MERT.": 0,
            "Och?s (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights instatistical machine translation (SMT) models.": 0,
            "Random Restarts in Minimum Error Rate Training for Statistical Machine Translation": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.02250623786550378,
                0.3023051923658024,
                0.07125841943517847,
                0.2206507285111092
            ],
            [
                0.0,
                0.02250623786550378,
                1.0,
                0.13691510529286435,
                0.024888284884630473,
                0.0
            ],
            [
                0.0,
                0.3023051923658024,
                0.13691510529286435,
                1.0,
                0.039998894098870724,
                0.18447676097787194
            ],
            [
                0.0,
                0.07125841943517847,
                0.024888284884630473,
                0.039998894098870724,
                1.0,
                0.43992467703012245
            ],
            [
                0.0,
                0.2206507285111092,
                0.0,
                0.18447676097787194,
                0.43992467703012245,
                1.0
            ]
        ]
    },
    "P09-1065": {
        "input_sentences": [
            "Abstract",
            "Joint Decoding with Multiple Translation Models",
            "Therefore, one model can share translations and even derivations with other models.",
            "Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding.",
            "Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually.",
            "Joint Decoding",
            "Current SMT systems usually decode with single translation models and cannot benefit from the strengths of other models in phase.",
            "We instead propose a method that combines multiple translation models in one decoder."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Joint Decoding with Multiple Translation Models": 0,
            "Therefore, one model can share translations and even derivations with other models.": 0,
            "Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding.": 0,
            "Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually.": 0,
            "Joint Decoding": 0,
            "Current SMT systems usually decode with single translation models and cannot benefit from the strengths of other models in phase.": 0,
            "We instead propose a method that combines multiple translation models in one decoder.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.10306433499709079,
                0.26790087536883,
                0.38633766696541194,
                0.653644599250337,
                0.21481164595125388,
                0.3457207211539403
            ],
            [
                0.0,
                0.10306433499709079,
                1.0,
                0.0,
                0.05249914520291542,
                0.0,
                0.09288818042721149,
                0.06221135703048743
            ],
            [
                0.0,
                0.26790087536883,
                0.0,
                1.0,
                0.03788927265388329,
                0.4098570930993458,
                0.0,
                0.0
            ],
            [
                0.0,
                0.38633766696541194,
                0.05249914520291542,
                0.03788927265388329,
                1.0,
                0.1447110757863302,
                0.10942124443329948,
                0.2758475613355517
            ],
            [
                0.0,
                0.653644599250337,
                0.0,
                0.4098570930993458,
                0.1447110757863302,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.21481164595125388,
                0.09288818042721149,
                0.0,
                0.10942124443329948,
                0.0,
                1.0,
                0.1296639036283243
            ],
            [
                0.0,
                0.3457207211539403,
                0.06221135703048743,
                0.0,
                0.2758475613355517,
                0.0,
                0.1296639036283243,
                1.0
            ]
        ]
    },
    "D11-1094": {
        "input_sentences": [
            "Abstract",
            "Latent Vector Weighting for Word Meaning in Context",
            "The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly.",
            "This paper presents a novel method for the computation of word meaning in context.",
            "We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions.",
            "The evaluation on a lexical substitution task \u2013 carried out for both English and French \u2013 indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Latent Vector Weighting for Word Meaning in Context": 0,
            "The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly.": 0,
            "This paper presents a novel method for the computation of word meaning in context.": 0,
            "We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions.": 0,
            "The evaluation on a lexical substitution task \u2013 carried out for both English and French \u2013 indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.22831725938232475,
                0.2728679790105563,
                0.1464402048533234,
                0.048717365252502226
            ],
            [
                0.0,
                0.22831725938232475,
                1.0,
                0.09606190501553862,
                0.2946922048017521,
                0.0
            ],
            [
                0.0,
                0.2728679790105563,
                0.09606190501553862,
                1.0,
                0.03829321454780142,
                0.037078097517338725
            ],
            [
                0.0,
                0.1464402048533234,
                0.2946922048017521,
                0.03829321454780142,
                1.0,
                0.0
            ],
            [
                0.0,
                0.048717365252502226,
                0.0,
                0.037078097517338725,
                0.0,
                1.0
            ]
        ]
    },
    "P14-2093": {
        "input_sentences": [
            "Abstract",
            "In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection.",
            "Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest.",
            "By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model.",
            "When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points.",
            "Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus.",
            "Effective Selection of Translation Model Training Data",
            "The results show that our methods outperform previous methods."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection.": 0,
            "Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest.": 0,
            "By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model.": 0,
            "When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points.": 0,
            "Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus.": 0,
            "Effective Selection of Translation Model Training Data": 0,
            "The results show that our methods outperform previous methods.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.15734390077085791,
                0.07041066341523418,
                0.06726893593079615,
                0.33224963501852167,
                0.20658702054117367,
                0.12980190983813572
            ],
            [
                0.0,
                0.15734390077085791,
                1.0,
                0.06065175694442089,
                0.025456273846832442,
                0.16363790414886448,
                0.2896383966449512,
                0.0
            ],
            [
                0.0,
                0.07041066341523418,
                0.06065175694442089,
                1.0,
                0.11816556375185272,
                0.15657898563432032,
                0.25959714809040446,
                0.0
            ],
            [
                0.0,
                0.06726893593079615,
                0.025456273846832442,
                0.11816556375185272,
                1.0,
                0.10012218347251949,
                0.04543067754498493,
                0.10140687334907186
            ],
            [
                0.0,
                0.33224963501852167,
                0.16363790414886448,
                0.15657898563432032,
                0.10012218347251949,
                1.0,
                0.14601863772865678,
                0.08512576985468516
            ],
            [
                0.0,
                0.20658702054117367,
                0.2896383966449512,
                0.25959714809040446,
                0.04543067754498493,
                0.14601863772865678,
                1.0,
                0.0
            ],
            [
                0.0,
                0.12980190983813572,
                0.0,
                0.0,
                0.10140687334907186,
                0.08512576985468516,
                0.0,
                1.0
            ]
        ]
    },
    "P05-1063": {
        "input_sentences": [
            "Abstract",
            "The syntactic features provide an additional 0.3% reduction in test\u2013set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (signifiat < which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system.",
            "Discriminative Syntactic Language Modeling For Speech Recognition",
            "The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.",
            "We describe experiments on the Switchboard speech recognition task.",
            "We follow where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second \u201creranking\u201d model is then used to choose an utterance from these 1000-best lists.",
            "We describe a method for discriminative training of a language model that makes use of syntactic features."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The syntactic features provide an additional 0.3% reduction in test\u2013set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (signifiat < which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system.": 0,
            "Discriminative Syntactic Language Modeling For Speech Recognition": 0,
            "The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.": 0,
            "We describe experiments on the Switchboard speech recognition task.": 0,
            "We follow where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second \u201creranking\u201d model is then used to choose an utterance from these 1000-best lists.": 0,
            "We describe a method for discriminative training of a language model that makes use of syntactic features.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.03163850811476974,
                0.15272028473513075,
                0.057929697164129657,
                0.050369474754394836,
                0.19440815036683604
            ],
            [
                0.0,
                0.03163850811476974,
                1.0,
                0.06345641672252904,
                0.3360162229019465,
                0.0,
                0.37412429302415245
            ],
            [
                0.0,
                0.15272028473513075,
                0.06345641672252904,
                1.0,
                0.0,
                0.0745489198778529,
                0.4354891403327371
            ],
            [
                0.0,
                0.057929697164129657,
                0.3360162229019465,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.050369474754394836,
                0.0,
                0.0745489198778529,
                0.0,
                1.0,
                0.03370266333720936
            ],
            [
                0.0,
                0.19440815036683604,
                0.37412429302415245,
                0.4354891403327371,
                0.0,
                0.03370266333720936,
                1.0
            ]
        ]
    },
    "W11-2123": {
        "input_sentences": [
            "Abstract",
            "The structure uses linear probing hash tables and is designed for speed.",
            "Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.",
            "Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.",
            "This paper describes the several performance techniques used and presents benchmarks against alternative implementations.",
            "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "KenLM: Faster and Smaller Language Model Queries"
        ],
        "authority_scores": {
            "Abstract": 0,
            "The structure uses linear probing hash tables and is designed for speed.": 0,
            "Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.": 0,
            "Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.": 0,
            "This paper describes the several performance techniques used and presents benchmarks against alternative implementations.": 0,
            "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.": 0,
            "KenLM: Faster and Smaller Language Model Queries": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.08652933685732783,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.08652933685732783,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.368590733672213
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.368590733672213,
                1.0
            ]
        ]
    },
    "D09-1092": {
        "input_sentences": [
            "Abstract",
            "Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.",
            "Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.",
            "We introduce a polylingual topic model that discovers topics aligned across multiple languages.",
            "Polylingual Topic Models",
            "We explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.": 0,
            "Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.": 0,
            "We introduce a polylingual topic model that discovers topics aligned across multiple languages.": 0,
            "Polylingual Topic Models": 0,
            "We explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.054097090116774924,
                0.03896540412920627,
                0.23322753292515866,
                0.07688447168563078
            ],
            [
                0.0,
                0.054097090116774924,
                1.0,
                0.09707033339965308,
                0.0,
                0.13161452296986548
            ],
            [
                0.0,
                0.03896540412920627,
                0.09707033339965308,
                1.0,
                0.2935628177359411,
                0.1873335637619313
            ],
            [
                0.0,
                0.23322753292515866,
                0.0,
                0.2935628177359411,
                1.0,
                0.068377968249451
            ],
            [
                0.0,
                0.07688447168563078,
                0.13161452296986548,
                0.1873335637619313,
                0.068377968249451,
                1.0
            ]
        ]
    },
    "P13-1155": {
        "input_sentences": [
            "Abstract",
            "When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%.",
            "The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text.",
            "The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.",
            "We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4).",
            "Social Text Normalization using Contextual Graph Random Walks",
            "The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus.",
            "We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text."
        ],
        "authority_scores": {
            "Abstract": 0,
            "When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%.": 0,
            "The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text.": 0,
            "The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.": 0,
            "We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4).": 0,
            "Social Text Normalization using Contextual Graph Random Walks": 0,
            "The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus.": 0,
            "We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.03144127694318666,
                0.2228502577411377,
                0.0,
                0.08911254188675241,
                0.021399820099649705,
                0.4698464706764564
            ],
            [
                0.0,
                0.03144127694318666,
                1.0,
                0.08838312963650623,
                0.058912023043353585,
                0.14276993885429187,
                0.1568223096794174,
                0.1179734685020534
            ],
            [
                0.0,
                0.2228502577411377,
                0.08838312963650623,
                1.0,
                0.10802648750383793,
                0.09597359920345579,
                0.10842786314698245,
                0.506021441080271
            ],
            [
                0.0,
                0.0,
                0.058912023043353585,
                0.10802648750383793,
                1.0,
                0.0,
                0.09225663607694415,
                0.0
            ],
            [
                0.0,
                0.08911254188675241,
                0.14276993885429187,
                0.09597359920345579,
                0.0,
                1.0,
                0.3847111610290877,
                0.22885688026492704
            ],
            [
                0.0,
                0.021399820099649705,
                0.1568223096794174,
                0.10842786314698245,
                0.09225663607694415,
                0.3847111610290877,
                1.0,
                0.03922188370947291
            ],
            [
                0.0,
                0.4698464706764564,
                0.1179734685020534,
                0.506021441080271,
                0.0,
                0.22885688026492704,
                0.03922188370947291,
                1.0
            ]
        ]
    },
    "P08-1043": {
        "input_sentences": [
            "Abstract",
            "These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.",
            "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.",
            "Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.",
            "A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.": 0,
            "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.": 0,
            "Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.": 0,
            "Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.": 0,
            "A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.09821935144927076,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.034764289943667565,
                0.07220874904808894,
                0.09604468891830532
            ],
            [
                0.0,
                0.09821935144927076,
                0.034764289943667565,
                1.0,
                0.06191482542259787,
                0.08235276508092304
            ],
            [
                0.0,
                0.0,
                0.07220874904808894,
                0.06191482542259787,
                1.0,
                0.4626878962536718
            ],
            [
                0.0,
                0.0,
                0.09604468891830532,
                0.08235276508092304,
                0.4626878962536718,
                1.0
            ]
        ]
    },
    "N12-1052": {
        "input_sentences": [
            "Abstract",
            "While previous work has focused primarily on English, we extend these results to other languages along two dimensions.",
            "Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.",
            "Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.",
            "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.",
            "First, we show that these results hold true for a number of languages across families.",
            "Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure",
            "When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%."
        ],
        "authority_scores": {
            "Abstract": 0,
            "While previous work has focused primarily on English, we extend these results to other languages along two dimensions.": 0,
            "Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.": 0,
            "Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.": 0,
            "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.": 0,
            "First, we show that these results hold true for a number of languages across families.": 0,
            "Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure": 0,
            "When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.10188787230211577,
                0.0,
                0.0,
                0.19038705179470286,
                0.0,
                0.0
            ],
            [
                0.0,
                0.10188787230211577,
                1.0,
                0.14914749789825887,
                0.08664432268218131,
                0.053564239877775595,
                0.22538423301182645,
                0.13109854770991247
            ],
            [
                0.0,
                0.0,
                0.14914749789825887,
                1.0,
                0.28368524662337224,
                0.0,
                0.4506554299209405,
                0.0
            ],
            [
                0.0,
                0.0,
                0.08664432268218131,
                0.28368524662337224,
                1.0,
                0.0,
                0.2679318402419458,
                0.0
            ],
            [
                0.0,
                0.19038705179470286,
                0.053564239877775595,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.22538423301182645,
                0.4506554299209405,
                0.2679318402419458,
                0.0,
                1.0,
                0.15417041554023989
            ],
            [
                0.0,
                0.0,
                0.13109854770991247,
                0.0,
                0.0,
                0.0,
                0.15417041554023989,
                1.0
            ]
        ]
    },
    "P07-1051": {
        "input_sentences": [
            "Abstract",
            "We present a new algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees.",
            "How far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?",
            "We train both on Penn\u2019s WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set.",
            "We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight.",
            "While U-DOP* performs worse than state-of-the-art supervised parsers on handannotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl.",
            "Is the End of Supervised Parsing in Sight?"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We present a new algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees.": 0,
            "How far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?": 0,
            "We train both on Penn\u2019s WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set.": 0,
            "We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight.": 0,
            "While U-DOP* performs worse than state-of-the-art supervised parsers on handannotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl.": 0,
            "Is the End of Supervised Parsing in Sight?": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09205995694446104,
                0.03274898474036551,
                0.02400237753460082,
                0.10928267559117975,
                0.06682881616569034
            ],
            [
                0.0,
                0.09205995694446104,
                1.0,
                0.11014619833552819,
                0.02949112089896079,
                0.0,
                0.08211089481596957
            ],
            [
                0.0,
                0.03274898474036551,
                0.11014619833552819,
                1.0,
                0.0,
                0.06425378466671508,
                0.0
            ],
            [
                0.0,
                0.02400237753460082,
                0.02949112089896079,
                0.0,
                1.0,
                0.2860006210415143,
                0.43913295366315486
            ],
            [
                0.0,
                0.10928267559117975,
                0.0,
                0.06425378466671508,
                0.2860006210415143,
                1.0,
                0.14687208863154474
            ],
            [
                0.0,
                0.06682881616569034,
                0.08211089481596957,
                0.0,
                0.43913295366315486,
                0.14687208863154474,
                1.0
            ]
        ]
    },
    "P13-2112": {
        "input_sentences": [
            "Interestingly, our method achieves higher accuracies on Germanic languages \u2014 the family of our source language, English \u2014 while Das and Petrov perform better on Romance languages.",
            "Unsupervised part-of-speech tagging with bilingual projections.",
            "2009.",
            "It took over a day to complete this step on an eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, but only 15 minutes for our model. in fact have tried EM, but it did not help.",
            "This might be because our model relies on alignments, which might be more accurate for more-related languages, whereas Das and Petrov additionally rely on label propagation.",
            "References Thorsten Brants.",
            "2011.",
            "(Findings are similar for other languages.)",
            "The poor performance on unknown words is expected because we do not use any language-specific rules to handle this case.",
            "In of the 23rd Pacific Asia Conference on Language, Information",
            "Abstract",
            "Pascal Denis and Benoit Sagot.",
            "Finally, we thank Siva Reddy and Spandana Gella for many discussions and suggestions.",
            "On individual languages, self training and revision and the method of Das and Petrov are split \u2014 each performs better on half of the cases.",
            "TnT: A statistical part-oftagger.",
            "We would like to thank Prokopis Prokopidis for providing us the Greek Treebank and Antonia Marti for the Spanish CoNLL 06 dataset.",
            "P103/12/G084).",
            "6 Conclusion We have proposed a method for unsupervised POS tagging that performs on par with the current stateof-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM). complexity of our algorithm is to that of Das and Petrov 637 where the size of training We our code are available for In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy.",
            "This might be because selftraining with revision already found the local maximal point. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language - Volume 1 (ACL pages 600\u2013609.",
            "Figure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of known tokens for Italian (left) and Dutch (right).",
            "Our model performs poorly on unknown words as indicated by the low accuracy on unknown words, and high accuracy on known words compared to the overall accuracy.",
            "One way to improve the performance of our tagger might be to reduce the proportion of unknown words by using a larger training corpus, as Das and Petrov did.",
            "We find that for all languages, accuracy rises quickly in the first 5\u20136 iterations, and then subsequently improves only slightly.",
            "Dipanjan Das and Slav Petrov.",
            "Self training and revision improve the accuracy for every language over the seed model, and gives an average improvement of roughly two percentage points.",
            "Moreover, on average for the final model, approximately 10% of the test data tokens are unknown.",
            "In many cases, GS outperformed other methods, thus we would like to try GS first for our model.",
            "In of re-implemented label propagation from Das and Petrov (2011).",
            "Portland, Oregon, USA.",
            "We exemplify this in Figure 1 (right panel) for Dutch.",
            "The overall performance dropped slightly.",
            "Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less effort.",
            "Table 2 shows results for our seed model, self training and revision, and the results reported by Das and Petrov.",
            "Although accuracy does not increase much in later iterations, they may still have some benefit as the vocabulary size continues to grow.",
            "2000.",
            "Simpler unsupervised POS tagging with bilingual projections",
            "We examine the impact of self-training and revision over training iterations.",
            "Figure 1 (left panel) shows accuracy, accuracy on known words, accuracy on unknown words, and proportion of known tokens for each iteration of our model for Italian; iteration 0 is the seed model, and iteration 31 is the final model.",
            "7 Acknowledgements This work is funded by Erasmus Mundus European Masters Program in Language and Communication Technologies (EM-LCT) and by the Czech Science Foundation (grant no.",
            "Seattle, Washington, USA.",
            "Compared to Das and Petrov, our model performs poorest on Italian, in terms of percentage point difference in accuracy.",
            "Given the improvements of our model over that of Das and Petrov on languages from the same family as our source language, and the observation of Snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language.",
            "Using our final model with unsupervised HMM methods might improve the final performance too, i.e. use our final model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging.",
            "The average accuracy of self training and revision is on par with that reported by Das and Petrov.",
            "In of the sixth conference on Applied natural language processing pages 224\u2013231."
        ],
        "authority_scores": {
            "Interestingly, our method achieves higher accuracies on Germanic languages \u2014 the family of our source language, English \u2014 while Das and Petrov perform better on Romance languages.": 0,
            "Unsupervised part-of-speech tagging with bilingual projections.": 0,
            "2009.": 0,
            "It took over a day to complete this step on an eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, but only 15 minutes for our model. in fact have tried EM, but it did not help.": 0,
            "This might be because our model relies on alignments, which might be more accurate for more-related languages, whereas Das and Petrov additionally rely on label propagation.": 0,
            "References Thorsten Brants.": 0,
            "2011.": 0,
            "(Findings are similar for other languages.)": 0,
            "The poor performance on unknown words is expected because we do not use any language-specific rules to handle this case.": 0,
            "In of the 23rd Pacific Asia Conference on Language, Information": 0,
            "Abstract": 0,
            "Pascal Denis and Benoit Sagot.": 0,
            "Finally, we thank Siva Reddy and Spandana Gella for many discussions and suggestions.": 0,
            "On individual languages, self training and revision and the method of Das and Petrov are split \u2014 each performs better on half of the cases.": 0,
            "TnT: A statistical part-oftagger.": 0,
            "We would like to thank Prokopis Prokopidis for providing us the Greek Treebank and Antonia Marti for the Spanish CoNLL 06 dataset.": 0,
            "P103/12/G084).": 0,
            "6 Conclusion We have proposed a method for unsupervised POS tagging that performs on par with the current stateof-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM). complexity of our algorithm is to that of Das and Petrov 637 where the size of training We our code are available for In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy.": 0,
            "This might be because selftraining with revision already found the local maximal point. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language - Volume 1 (ACL pages 600\u2013609.": 0,
            "Figure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of known tokens for Italian (left) and Dutch (right).": 0,
            "Our model performs poorly on unknown words as indicated by the low accuracy on unknown words, and high accuracy on known words compared to the overall accuracy.": 0,
            "One way to improve the performance of our tagger might be to reduce the proportion of unknown words by using a larger training corpus, as Das and Petrov did.": 0,
            "We find that for all languages, accuracy rises quickly in the first 5\u20136 iterations, and then subsequently improves only slightly.": 0,
            "Dipanjan Das and Slav Petrov.": 0,
            "Self training and revision improve the accuracy for every language over the seed model, and gives an average improvement of roughly two percentage points.": 0,
            "Moreover, on average for the final model, approximately 10% of the test data tokens are unknown.": 0,
            "In many cases, GS outperformed other methods, thus we would like to try GS first for our model.": 0,
            "In of re-implemented label propagation from Das and Petrov (2011).": 0,
            "Portland, Oregon, USA.": 0,
            "We exemplify this in Figure 1 (right panel) for Dutch.": 0,
            "The overall performance dropped slightly.": 0,
            "Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less effort.": 0,
            "Table 2 shows results for our seed model, self training and revision, and the results reported by Das and Petrov.": 0,
            "Although accuracy does not increase much in later iterations, they may still have some benefit as the vocabulary size continues to grow.": 0,
            "2000.": 0,
            "Simpler unsupervised POS tagging with bilingual projections": 0,
            "We examine the impact of self-training and revision over training iterations.": 0,
            "Figure 1 (left panel) shows accuracy, accuracy on known words, accuracy on unknown words, and proportion of known tokens for each iteration of our model for Italian; iteration 0 is the seed model, and iteration 31 is the final model.": 0,
            "7 Acknowledgements This work is funded by Erasmus Mundus European Masters Program in Language and Communication Technologies (EM-LCT) and by the Czech Science Foundation (grant no.": 0,
            "Seattle, Washington, USA.": 0,
            "Compared to Das and Petrov, our model performs poorest on Italian, in terms of percentage point difference in accuracy.": 0,
            "Given the improvements of our model over that of Das and Petrov on languages from the same family as our source language, and the observation of Snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language.": 0,
            "Using our final model with unsupervised HMM methods might improve the final performance too, i.e. use our final model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging.": 0,
            "The average accuracy of self training and revision is on par with that reported by Das and Petrov.": 0,
            "In of the sixth conference on Applied natural language processing pages 224\u2013231.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.14564442856842033,
                0.0,
                0.0,
                0.16581771053765484,
                0.03656768684354663,
                0.04768103265844199,
                0.0,
                0.0,
                0.0,
                0.27845003870362306,
                0.0,
                0.0,
                0.0,
                0.08675643598275244,
                0.02680018943943174,
                0.0,
                0.0,
                0.055805186830389895,
                0.1036239671022311,
                0.10631250356648787,
                0.03558030363228244,
                0.0,
                0.0,
                0.08681771220652762,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05534347642641864,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.027306238358139317,
                0.0,
                0.060810955755342296,
                0.3030771575070263,
                0.03422090907064613,
                0.078858381162189,
                0.03843302699175461
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0961510017702722,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09542952299555654,
                0.0,
                0.0,
                0.0,
                0.6821576976823215,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.14098119210199575,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.02482634384129499,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.017591706886661788,
                0.06054312747366323,
                0.0,
                0.0,
                0.024180207921136055,
                0.028481150396957117,
                0.02614508229770233,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.023705504185451504,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04173658014933267,
                0.04003175615700063,
                0.0,
                0.02604741261774867,
                0.013097336032310593,
                0.07308532994364707,
                0.0,
                0.0
            ],
            [
                0.14564442856842033,
                0.0,
                0.0,
                0.02482634384129499,
                1.0,
                0.0,
                0.0,
                0.10336155182557294,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1305605813971831,
                0.0,
                0.0,
                0.0,
                0.07026448121909035,
                0.0,
                0.0,
                0.025600787581628006,
                0.06957170849847515,
                0.06459342618638117,
                0.13253862101296573,
                0.03518887454508236,
                0.04144793260202287,
                0.038048308928746215,
                0.3823756623760705,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10349414887544764,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06073824044878767,
                0.0,
                0.0,
                0.1137185177835588,
                0.13429220439089942,
                0.033687758542280524,
                0.09831187060715597,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13573535688119395,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.4181706810613262,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.16581771053765484,
                0.0,
                0.0,
                0.0,
                0.10336155182557294,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10632987570296495,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.12215122766088321,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05452921242590832,
                0.0,
                0.0,
                0.0
            ],
            [
                0.03656768684354663,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.05950299952943608,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0241939436185034,
                0.03344498998221597,
                0.03582672041097661,
                0.21556090984411377,
                0.1792963901504284,
                0.0,
                0.0,
                0.044402033098879495,
                0.05708653516419446,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11531362755421255,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10435417395559149,
                0.03407650794424143,
                0.0,
                0.0,
                0.09620237343045629,
                0.07451511580279034,
                0.0,
                0.04796205659778813
            ],
            [
                0.04768103265844199,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05950299952943608,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04360931186121037,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05789632796154076,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04443275548510048,
                0.0,
                0.0,
                0.12543939486745864,
                0.0,
                0.0,
                0.18819656383951613
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.08190215034278438,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.27845003870362306,
                0.0,
                0.0,
                0.0,
                0.1305605813971831,
                0.0,
                0.0,
                0.10632987570296495,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.195083971666858,
                0.041249745188589884,
                0.0,
                0.04968548713600954,
                0.12080700935999836,
                0.06644841197060346,
                0.13634484824621496,
                0.16534871751800959,
                0.0,
                0.09913777529200142,
                0.111342950253068,
                0.0,
                0.0,
                0.0,
                0.0,
                0.233080126337122,
                0.0,
                0.0,
                0.0,
                0.28121593778916465,
                0.0,
                0.0,
                0.0,
                0.15155704220942065,
                0.11117600049851045,
                0.0,
                0.3321136045449061,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08190215034278438,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08086191124963885,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.08675643598275244,
                0.0961510017702722,
                0.0,
                0.0,
                0.07026448121909035,
                0.0,
                0.13573535688119395,
                0.0,
                0.0241939436185034,
                0.0,
                0.0,
                0.0,
                0.0,
                0.195083971666858,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.15691657987070046,
                0.1011129097647702,
                0.3599267424697225,
                0.024114807715132684,
                0.12888138193568247,
                0.09923339498412995,
                0.06574684619148859,
                0.0,
                0.16200862517602016,
                0.0,
                0.0,
                0.0,
                0.14861394974524283,
                0.1132492811971638,
                0.06493605963471129,
                0.0,
                0.13647311708206558,
                0.12307341127961102,
                0.08499085958755526,
                0.033256984591801694,
                0.0,
                0.12989965292905936,
                0.06054077676164524,
                0.1347036565770165,
                0.24966483075195378,
                0.0
            ],
            [
                0.02680018943943174,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03344498998221597,
                0.04360931186121037,
                0.0,
                0.0,
                0.0,
                0.041249745188589884,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07159652935799214,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03828788841429091,
                0.0,
                0.0,
                0.0,
                0.051045524642274895,
                0.0,
                0.024974422698578212,
                0.0,
                0.07043571688490859,
                0.07050601377905272,
                0.0,
                0.054556039725545985,
                0.10578008238368444
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03582672041097661,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.15691657987070046,
                0.0,
                1.0,
                0.36270750518780787,
                0.08170998206643942,
                0.10712860467035713,
                0.0,
                0.08829048411772128,
                0.20995689157753775,
                0.0,
                0.0,
                0.0,
                0.2755390322329701,
                0.09483662830791573,
                0.0,
                0.0,
                0.09070355583884175,
                0.0,
                0.0,
                0.0,
                0.5080434508232072,
                0.0,
                0.0,
                0.1539818891534187,
                0.0,
                0.0,
                0.12333447378807406,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.017591706886661788,
                0.025600787581628006,
                0.0,
                0.0,
                0.0,
                0.21556090984411377,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04968548713600954,
                0.0,
                0.0,
                0.0,
                0.1011129097647702,
                0.0,
                0.36270750518780787,
                1.0,
                0.20733826729608223,
                0.10543444867319697,
                0.0,
                0.11182873423925337,
                0.11019052384188976,
                0.02696066333755999,
                0.0,
                0.0,
                0.0,
                0.09333686040659102,
                0.0,
                0.02444498396730018,
                0.0892691492808457,
                0.0,
                0.0,
                0.0,
                0.4567905789547512,
                0.0,
                0.0,
                0.23917009130027997,
                0.013505900014590955,
                0.0238708195509115,
                0.12138403450934353,
                0.0
            ],
            [
                0.055805186830389895,
                0.0,
                0.0,
                0.06054312747366323,
                0.06957170849847515,
                0.0,
                0.0,
                0.0,
                0.1792963901504284,
                0.0,
                0.0,
                0.0,
                0.0,
                0.12080700935999836,
                0.0,
                0.0,
                0.0,
                0.3599267424697225,
                0.0,
                0.08170998206643942,
                0.20733826729608223,
                1.0,
                0.0,
                0.12761067582569455,
                0.11053612838693777,
                0.054908950307551115,
                0.0,
                0.10421038501258616,
                0.0,
                0.0,
                0.11091495090648071,
                0.07690064187840952,
                0.11213269979954354,
                0.0,
                0.0,
                0.0,
                0.1218599688619304,
                0.13714963084541515,
                0.0,
                0.0,
                0.07299355109902433,
                0.08318463919904592,
                0.0961770509361069,
                0.15977679308408305,
                0.0
            ],
            [
                0.1036239671022311,
                0.0,
                0.0,
                0.0,
                0.06459342618638117,
                0.0,
                0.0,
                0.12215122766088321,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06644841197060346,
                0.0,
                0.0,
                0.0,
                0.024114807715132684,
                0.0,
                0.10712860467035713,
                0.10543444867319697,
                0.0,
                1.0,
                0.0,
                0.04830736261871309,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1827729531987195,
                0.0,
                0.0,
                0.141788613942896,
                0.0,
                0.0,
                0.11725246958632564,
                0.08338158705312199,
                0.0,
                0.0,
                0.05203767522217858,
                0.034076778024562944,
                0.0,
                0.06748137365205366,
                0.0
            ],
            [
                0.10631250356648787,
                0.0,
                0.0,
                0.0,
                0.13253862101296573,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13634484824621496,
                0.0,
                0.0,
                0.0,
                0.12888138193568247,
                0.0,
                0.0,
                0.0,
                0.12761067582569455,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.19852754837982925,
                0.0,
                0.0,
                0.0,
                0.0,
                0.12655487474280236,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13905745329965813,
                0.06992180837270343,
                0.0,
                0.18032680985752106,
                0.0
            ],
            [
                0.03558030363228244,
                0.0,
                0.0,
                0.024180207921136055,
                0.03518887454508236,
                0.0,
                0.0,
                0.0,
                0.044402033098879495,
                0.05789632796154076,
                0.0,
                0.0,
                0.0,
                0.16534871751800959,
                0.0,
                0.0,
                0.0,
                0.09923339498412995,
                0.07159652935799214,
                0.08829048411772128,
                0.11182873423925337,
                0.11053612838693777,
                0.04830736261871309,
                0.0,
                1.0,
                0.12745403240708386,
                0.0370580552185211,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.259559045580687,
                0.04090083667379304,
                0.0,
                0.0,
                0.2662507931855356,
                0.1704150807547694,
                0.03315639035554597,
                0.0,
                0.17331811991175286,
                0.11216893479212126,
                0.06376161152514,
                0.3775816210694709,
                0.046667008057666534
            ],
            [
                0.0,
                0.0,
                0.0,
                0.028481150396957117,
                0.04144793260202287,
                0.0,
                0.0,
                0.0,
                0.05708653516419446,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06574684619148859,
                0.0,
                0.20995689157753775,
                0.11019052384188976,
                0.054908950307551115,
                0.0,
                0.0,
                0.12745403240708386,
                1.0,
                0.04364958512928506,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.039576674924692155,
                0.0,
                0.0,
                0.0,
                0.0,
                0.19556192557649052,
                0.0,
                0.0,
                0.04348652421552913,
                0.021866187973691522,
                0.16370194144341657,
                0.12165027912925375,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.02614508229770233,
                0.038048308928746215,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09913777529200142,
                0.0,
                0.08086191124963885,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02696066333755999,
                0.0,
                0.0,
                0.0,
                0.0370580552185211,
                0.04364958512928506,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03633053470642247,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06396456543511535,
                0.0,
                0.0,
                0.03991969209844506,
                0.02007268934510007,
                0.17026424536891416,
                0.0,
                0.0
            ],
            [
                0.08681771220652762,
                0.0,
                0.0,
                0.0,
                0.3823756623760705,
                0.0,
                0.4181706810613262,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.111342950253068,
                0.0,
                0.0,
                0.0,
                0.16200862517602016,
                0.0,
                0.0,
                0.0,
                0.10421038501258616,
                0.0,
                0.19852754837982925,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10334818883163978,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11355813809042822,
                0.0571000703832064,
                0.0,
                0.14725982886425487,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.2891473410216564,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.2755390322329701,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1392561358983066,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11531362755421255,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09483662830791573,
                0.09333686040659102,
                0.11091495090648071,
                0.1827729531987195,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05370683235338853,
                0.0,
                0.0
            ],
            [
                0.0,
                0.09542952299555654,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.14861394974524283,
                0.0,
                0.0,
                0.0,
                0.07690064187840952,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.18811489281328794,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1052670547309224,
                0.0,
                0.0
            ],
            [
                0.05534347642641864,
                0.0,
                0.0,
                0.023705504185451504,
                0.10349414887544764,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.233080126337122,
                0.0,
                0.0,
                0.0,
                0.1132492811971638,
                0.03828788841429091,
                0.0,
                0.02444498396730018,
                0.11213269979954354,
                0.0,
                0.12655487474280236,
                0.259559045580687,
                0.039576674924692155,
                0.03633053470642247,
                0.10334818883163978,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.26102378080556055,
                0.1486642206232837,
                0.0,
                0.0,
                0.1085844462847365,
                0.05459916505889383,
                0.03216685091560606,
                0.42714988543201765,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06493605963471129,
                0.0,
                0.09070355583884175,
                0.0892691492808457,
                0.0,
                0.141788613942896,
                0.0,
                0.04090083667379304,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.09927522117076713,
                0.07059745117073112,
                0.0,
                0.0,
                0.04405921457450316,
                0.0,
                0.0,
                0.05713507201895294,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.6821576976823215,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13647311708206558,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.18811489281328794,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.17614389477686285,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.28121593778916465,
                0.0,
                0.0,
                0.0,
                0.12307341127961102,
                0.051045524642274895,
                0.0,
                0.0,
                0.1218599688619304,
                0.11725246958632564,
                0.0,
                0.2662507931855356,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.26102378080556055,
                0.09927522117076713,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.37193024595279445,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.04173658014933267,
                0.06073824044878767,
                0.0,
                0.0,
                0.0,
                0.10435417395559149,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08499085958755526,
                0.0,
                0.5080434508232072,
                0.4567905789547512,
                0.13714963084541515,
                0.08338158705312199,
                0.0,
                0.1704150807547694,
                0.19556192557649052,
                0.06396456543511535,
                0.0,
                0.0,
                0.1392561358983066,
                0.0,
                0.0,
                0.1486642206232837,
                0.07059745117073112,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.1835745970085889,
                0.03204294398943426,
                0.11771943172971426,
                0.09599512842023285,
                0.0
            ],
            [
                0.027306238358139317,
                0.0,
                0.0,
                0.04003175615700063,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03407650794424143,
                0.04443275548510048,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.033256984591801694,
                0.024974422698578212,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03315639035554597,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.07183732869814709,
                0.05432052919517385,
                0.0,
                0.035814771527909676
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.2891473410216564,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.060810955755342296,
                0.0,
                0.0,
                0.02604741261774867,
                0.1137185177835588,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.15155704220942065,
                0.0,
                0.0,
                0.0,
                0.12989965292905936,
                0.07043571688490859,
                0.1539818891534187,
                0.23917009130027997,
                0.07299355109902433,
                0.05203767522217858,
                0.13905745329965813,
                0.17331811991175286,
                0.04348652421552913,
                0.03991969209844506,
                0.11355813809042822,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1085844462847365,
                0.04405921457450316,
                0.0,
                0.0,
                0.0,
                0.1835745970085889,
                0.0,
                0.0,
                1.0,
                0.05999311255933477,
                0.03534467066625858,
                0.1630569501944479,
                0.0
            ],
            [
                0.3030771575070263,
                0.0,
                0.0,
                0.013097336032310593,
                0.13429220439089942,
                0.0,
                0.0,
                0.05452921242590832,
                0.09620237343045629,
                0.12543939486745864,
                0.0,
                0.0,
                0.0,
                0.11117600049851045,
                0.0,
                0.0,
                0.0,
                0.06054077676164524,
                0.07050601377905272,
                0.0,
                0.013505900014590955,
                0.08318463919904592,
                0.034076778024562944,
                0.06992180837270343,
                0.11216893479212126,
                0.021866187973691522,
                0.02007268934510007,
                0.0571000703832064,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05459916505889383,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03204294398943426,
                0.07183732869814709,
                0.0,
                0.05999311255933477,
                1.0,
                0.04027936211780113,
                0.05186521275699031,
                0.10110971554045894
            ],
            [
                0.03422090907064613,
                0.14098119210199575,
                0.0,
                0.07308532994364707,
                0.033687758542280524,
                0.0,
                0.0,
                0.0,
                0.07451511580279034,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1347036565770165,
                0.0,
                0.0,
                0.0238708195509115,
                0.0961770509361069,
                0.0,
                0.0,
                0.06376161152514,
                0.16370194144341657,
                0.17026424536891416,
                0.0,
                0.0,
                0.0,
                0.05370683235338853,
                0.1052670547309224,
                0.03216685091560606,
                0.0,
                0.0,
                0.17614389477686285,
                0.0,
                0.11771943172971426,
                0.05432052919517385,
                0.0,
                0.03534467066625858,
                0.04027936211780113,
                1.0,
                0.0,
                0.0
            ],
            [
                0.078858381162189,
                0.0,
                0.0,
                0.0,
                0.09831187060715597,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.3321136045449061,
                0.0,
                0.0,
                0.0,
                0.24966483075195378,
                0.054556039725545985,
                0.12333447378807406,
                0.12138403450934353,
                0.15977679308408305,
                0.06748137365205366,
                0.18032680985752106,
                0.3775816210694709,
                0.12165027912925375,
                0.0,
                0.14725982886425487,
                0.0,
                0.0,
                0.0,
                0.0,
                0.42714988543201765,
                0.05713507201895294,
                0.0,
                0.0,
                0.37193024595279445,
                0.09599512842023285,
                0.0,
                0.0,
                0.1630569501944479,
                0.05186521275699031,
                0.0,
                1.0,
                0.0
            ],
            [
                0.03843302699175461,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04796205659778813,
                0.18819656383951613,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10578008238368444,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.046667008057666534,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.035814771527909676,
                0.0,
                0.0,
                0.10110971554045894,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "W06-3120": {
        "input_sentences": [
            "Abstract",
            "Mainly we introduce two reordering approaches and add morphological information.",
            "We have studied different techniques to improve the standard Phrase-Based translation system.",
            "TALP Phrase-Based Statistical Translation System For European Language Pairs",
            "This paper reports translation results for the \u201cExploiting Parallel Texts for Statistical Machine Translation\u201d (HLT-NAACL Workshop on Parallel Texts 2006)."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Mainly we introduce two reordering approaches and add morphological information.": 0,
            "We have studied different techniques to improve the standard Phrase-Based translation system.": 0,
            "TALP Phrase-Based Statistical Translation System For European Language Pairs": 0,
            "This paper reports translation results for the \u201cExploiting Parallel Texts for Statistical Machine Translation\u201d (HLT-NAACL Workshop on Parallel Texts 2006).": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.266273569273078,
                0.07829590496171927
            ],
            [
                0.0,
                0.0,
                0.266273569273078,
                1.0,
                0.1387454185427328
            ],
            [
                0.0,
                0.0,
                0.07829590496171927,
                0.1387454185427328,
                1.0
            ]
        ]
    },
    "W05-1518": {
        "input_sentences": [
            "Abstract",
            "Moreover, our experiments show that even parsers far below the state of the art can contribute to the total improvement.",
            "This paper explores the possibilities of improving parsing results by combining outputs of several parsers.",
            "Improving Parsing Accuracy By Combining Diverse Dependency Parsers",
            "All our experiments were conducted on Czech but the method is language-independent.",
            "We were able to significantly improve over the best parsing result for the given setting, known so far.",
            "We differ from them in exploring context features more deeply.",
            "To some extent, we are porting the ideas of Henderson and Brill (1999) to the world of dependency structures."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Moreover, our experiments show that even parsers far below the state of the art can contribute to the total improvement.": 0,
            "This paper explores the possibilities of improving parsing results by combining outputs of several parsers.": 0,
            "Improving Parsing Accuracy By Combining Diverse Dependency Parsers": 0,
            "All our experiments were conducted on Czech but the method is language-independent.": 0,
            "We were able to significantly improve over the best parsing result for the given setting, known so far.": 0,
            "We differ from them in exploring context features more deeply.": 0,
            "To some extent, we are porting the ideas of Henderson and Brill (1999) to the world of dependency structures.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07279661477492956,
                0.08753377505266656,
                0.11174927650196473,
                0.08785781795864608,
                0.0,
                0.0
            ],
            [
                0.0,
                0.07279661477492956,
                1.0,
                0.39551707551248966,
                0.0,
                0.06308343369529341,
                0.0,
                0.0
            ],
            [
                0.0,
                0.08753377505266656,
                0.39551707551248966,
                1.0,
                0.0,
                0.07585422909713817,
                0.0,
                0.10488517001957287
            ],
            [
                0.0,
                0.11174927650196473,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.08785781795864608,
                0.06308343369529341,
                0.07585422909713817,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.10488517001957287,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "W10-1401": {
        "input_sentences": [
            "Abstract",
            "Thereis ample evidence that the application of read ily available statistical parsing models to suchlanguages is susceptible to serious performance degradation.",
            "The term Morphologically Rich Languages(MRLs) refers to languages in which signif icant information concerning syntactic units and relations is expressed at word-level.",
            "We synthesize the contributions of re searchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages.",
            "In this paper we re view the current state-of-affairs with respectto parsing MRLs and point out central challenges.",
            "The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite languagespecific idiosyncrasies, the problems associ ated with parsing MRLs cut across languagesand parsing frameworks.",
            "The overarching analysis suggests itself as a source of directions for future investigations.",
            "Statistical Parsing of Morphologically Rich Languages (SPMRL) What How and Whither"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Thereis ample evidence that the application of read ily available statistical parsing models to suchlanguages is susceptible to serious performance degradation.": 0,
            "The term Morphologically Rich Languages(MRLs) refers to languages in which signif icant information concerning syntactic units and relations is expressed at word-level.": 0,
            "We synthesize the contributions of re searchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages.": 0,
            "In this paper we re view the current state-of-affairs with respectto parsing MRLs and point out central challenges.": 0,
            "The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite languagespecific idiosyncrasies, the problems associ ated with parsing MRLs cut across languagesand parsing frameworks.": 0,
            "The overarching analysis suggests itself as a source of directions for future investigations.": 0,
            "Statistical Parsing of Morphologically Rich Languages (SPMRL) What How and Whither": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.023296892625919413,
                0.028465050396028826,
                0.09616812655274914,
                0.0,
                0.1205267608668863
            ],
            [
                0.0,
                0.0,
                1.0,
                0.06924878322585963,
                0.042305429656200294,
                0.061339427938562355,
                0.0,
                0.31553331783853383
            ],
            [
                0.0,
                0.023296892625919413,
                0.06924878322585963,
                1.0,
                0.08727918262112203,
                0.10245772919668522,
                0.0,
                0.11442878978091929
            ],
            [
                0.0,
                0.028465050396028826,
                0.042305429656200294,
                0.08727918262112203,
                1.0,
                0.15129947842325242,
                0.0,
                0.05255731553153129
            ],
            [
                0.0,
                0.09616812655274914,
                0.061339427938562355,
                0.10245772919668522,
                0.15129947842325242,
                1.0,
                0.0,
                0.17756295882104614
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.1205267608668863,
                0.31553331783853383,
                0.11442878978091929,
                0.05255731553153129,
                0.17756295882104614,
                0.0,
                1.0
            ]
        ]
    },
    "P13-1007": {
        "input_sentences": [
            "Abstract",
            "Finally, we significantly improve the performance of the previous model using a rich set of automatically generated features.",
            "We also present a general model for learning to build partial orders from a set of pairpreferences.",
            "No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation.",
            "Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases.",
            "In this paper we report early, though promising, results for automatic QSD when handling both phenomena.",
            "We give an algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice.",
            "Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Finally, we significantly improve the performance of the previous model using a rich set of automatically generated features.": 0,
            "We also present a general model for learning to build partial orders from a set of pairpreferences.": 0,
            "No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation.": 0,
            "Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases.": 0,
            "In this paper we report early, though promising, results for automatic QSD when handling both phenomena.": 0,
            "We give an algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice.": 0,
            "Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.143480755608925,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.143480755608925,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.042224829039207955,
                0.058981975112227074,
                0.0,
                0.09594169584755255
            ],
            [
                0.0,
                0.0,
                0.0,
                0.042224829039207955,
                1.0,
                0.04393024758751845,
                0.0,
                0.21437443106287143
            ],
            [
                0.0,
                0.0,
                0.0,
                0.058981975112227074,
                0.04393024758751845,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.09594169584755255,
                0.21437443106287143,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P04-1013": {
        "input_sentences": [
            "Abstract",
            "Discriminative Training Of A Neural Network Statistical Parser",
            "Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing.",
            "We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.",
            "One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem.",
            "The latter model outperforms the previous two, achieving state-ofthe-art levels of performance (90.1% F-measure on constituents).",
            "We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Discriminative Training Of A Neural Network Statistical Parser": 0,
            "Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing.": 0,
            "We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.": 0,
            "One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem.": 0,
            "The latter model outperforms the previous two, achieving state-ofthe-art levels of performance (90.1% F-measure on constituents).": 0,
            "We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.04084698877051038,
                0.6380754849050481,
                0.04509334888344975,
                0.0,
                0.15531091947298012
            ],
            [
                0.0,
                0.04084698877051038,
                1.0,
                0.18210153279473693,
                0.1389597322183653,
                0.0,
                0.12637198412546113
            ],
            [
                0.0,
                0.6380754849050481,
                0.18210153279473693,
                1.0,
                0.08408054228891987,
                0.039388235096239226,
                0.3210696593534612
            ],
            [
                0.0,
                0.04509334888344975,
                0.1389597322183653,
                0.08408054228891987,
                1.0,
                0.0,
                0.30756907729396626
            ],
            [
                0.0,
                0.0,
                0.0,
                0.039388235096239226,
                0.0,
                1.0,
                0.054668067247105954
            ],
            [
                0.0,
                0.15531091947298012,
                0.12637198412546113,
                0.3210696593534612,
                0.30756907729396626,
                0.054668067247105954,
                1.0
            ]
        ]
    },
    "W12-3160": {
        "input_sentences": [
            "Abstract",
            "By removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features.",
            "Thismay be partly due to the perceived complex ity of implementation, and partly due to the lack of standard methodology for applying these methods to MT. This papers aims to shedlight on large-margin learning for MT, explic itly presenting the simple passive-aggressivealgorithm which underlies many previous ap proaches, with direct application to MT, andempirically comparing several widespread op timization strategies.",
            "However, these methods have not yet met with wide-spread adoption.",
            "Optimization Strategies for Online Large-Margin Learning in Machine Translation",
            "The introduction of large-margin based dis criminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process.",
            "Large-Margin Learning"
        ],
        "authority_scores": {
            "Abstract": 0,
            "By removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features.": 0,
            "Thismay be partly due to the perceived complex ity of implementation, and partly due to the lack of standard methodology for applying these methods to MT. This papers aims to shedlight on large-margin learning for MT, explic itly presenting the simple passive-aggressivealgorithm which underlies many previous ap proaches, with direct application to MT, andempirically comparing several widespread op timization strategies.": 0,
            "However, these methods have not yet met with wide-spread adoption.": 0,
            "Optimization Strategies for Online Large-Margin Learning in Machine Translation": 0,
            "The introduction of large-margin based dis criminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process.": 0,
            "Large-Margin Learning": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.01824587558308581,
                0.05805060997985028,
                0.0,
                0.12921163167387764,
                0.0
            ],
            [
                0.0,
                0.01824587558308581,
                1.0,
                0.027234798567015744,
                0.12695701341466212,
                0.03926608696227211,
                0.16875002732283834
            ],
            [
                0.0,
                0.05805060997985028,
                0.027234798567015744,
                1.0,
                0.0,
                0.0416426585340159,
                0.0
            ],
            [
                0.0,
                0.0,
                0.12695701341466212,
                0.0,
                1.0,
                0.2811269516216097,
                0.4866908078138695
            ],
            [
                0.0,
                0.12921163167387764,
                0.03926608696227211,
                0.0416426585340159,
                0.2811269516216097,
                1.0,
                0.15512525670944652
            ],
            [
                0.0,
                0.0,
                0.16875002732283834,
                0.0,
                0.4866908078138695,
                0.15512525670944652,
                1.0
            ]
        ]
    },
    "W04-1505": {
        "input_sentences": [
            "Abstract",
            "Fast Deep-Linguistic Statistical Dependency Parsing",
            "We show that DG allows for the expression of the majority of English LDDs in a context-free way and offers simple yet powerful statistical models.",
            "We present and evaluate an implemented statistical minimal parsing strategy exploiting DG charateristics to permit fast, robust, deeplinguistic analysis of unrestricted text, and compare its probability model to (Collins, 1999) and an adaptation, (Dubey and Keller, 2003)."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Fast Deep-Linguistic Statistical Dependency Parsing": 0,
            "We show that DG allows for the expression of the majority of English LDDs in a context-free way and offers simple yet powerful statistical models.": 0,
            "We present and evaluate an implemented statistical minimal parsing strategy exploiting DG charateristics to permit fast, robust, deeplinguistic analysis of unrestricted text, and compare its probability model to (Collins, 1999) and an adaptation, (Dubey and Keller, 2003).": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.052338609028865876,
                0.15535740157354963
            ],
            [
                0.0,
                0.052338609028865876,
                1.0,
                0.05786375218384446
            ],
            [
                0.0,
                0.15535740157354963,
                0.05786375218384446,
                1.0
            ]
        ]
    },
    "N06-1045": {
        "input_sentences": [
            "Abstract",
            "We introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees.",
            "We also demonstrate our algorithm\u2019s effectiveness on two large-scale tasks.",
            "Ranked lists of output trees from syntactic statistical NLP applications frequently contain multiple repeated entries.",
            "A Better N-Best List: Practical Determinization Of Weighted Finite Tree Automata",
            "This redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes. is due to nondeterminism in the weighted automata that produce the results."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees.": 0,
            "We also demonstrate our algorithm\u2019s effectiveness on two large-scale tasks.": 0,
            "Ranked lists of output trees from syntactic statistical NLP applications frequently contain multiple repeated entries.": 0,
            "A Better N-Best List: Practical Determinization Of Weighted Finite Tree Automata": 0,
            "This redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes. is due to nondeterminism in the weighted automata that produce the results.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.08326729380678476,
                0.05570945812052302,
                0.05053610460098273,
                0.09245941680110256
            ],
            [
                0.0,
                0.08326729380678476,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.05570945812052302,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.05053610460098273,
                0.0,
                0.0,
                1.0,
                0.17751249265257416
            ],
            [
                0.0,
                0.09245941680110256,
                0.0,
                0.0,
                0.17751249265257416,
                1.0
            ]
        ]
    },
    "E09-1034": {
        "input_sentences": [
            "Abstract",
            "In particular, algorithms are presented for: all well-nested structures of degree at most with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power; all well-nested structures with degree bounded by any constant and a new class of structures with gap deup to includes some ill-nested structures.",
            "Parsing Mildly Non-Projective Dependency Structures",
            "The third case includes all the degree in a number of dependency treebanks.",
            "We present parsing algorithms for various mildly non-projective dependency formalisms."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In particular, algorithms are presented for: all well-nested structures of degree at most with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power; all well-nested structures with degree bounded by any constant and a new class of structures with gap deup to includes some ill-nested structures.": 0,
            "Parsing Mildly Non-Projective Dependency Structures": 0,
            "The third case includes all the degree in a number of dependency treebanks.": 0,
            "We present parsing algorithms for various mildly non-projective dependency formalisms.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.2113800349448257,
                0.139973230995697,
                0.08068492332362451
            ],
            [
                0.0,
                0.2113800349448257,
                1.0,
                0.10693767167847647,
                0.6292214704307451
            ],
            [
                0.0,
                0.139973230995697,
                0.10693767167847647,
                1.0,
                0.0816373962851974
            ],
            [
                0.0,
                0.08068492332362451,
                0.6292214704307451,
                0.0816373962851974,
                1.0
            ]
        ]
    },
    "P10-1044": {
        "input_sentences": [
            "Abstract",
            "We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences.",
            "We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al.,",
            "A Latent Dirichlet Allocation Method for Selectional Preferences",
            "computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability.",
            "By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation\u2019s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007)."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences.": 0,
            "We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al.,": 0,
            "A Latent Dirichlet Allocation Method for Selectional Preferences": 0,
            "computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability.": 0,
            "By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation\u2019s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007).": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.12629737234531183,
                0.17989332340527617,
                0.0,
                0.024838524342442686
            ],
            [
                0.0,
                0.12629737234531183,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.17989332340527617,
                0.0,
                1.0,
                0.0,
                0.0792100098042098
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.03266017357834997
            ],
            [
                0.0,
                0.024838524342442686,
                0.0,
                0.0792100098042098,
                0.03266017357834997,
                1.0
            ]
        ]
    },
    "P06-1109": {
        "input_sentences": [
            "Abstract",
            "Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.",
            "To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank PCFG).",
            "We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.",
            "We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.",
            "We investigate generalizations of the allsubtrees &quot;DOP&quot; approach to unsupervised parsing.",
            "An All-Subtrees Approach To Unsupervised Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.": 0,
            "To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank PCFG).": 0,
            "We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.": 0,
            "We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.": 0,
            "We investigate generalizations of the allsubtrees &quot;DOP&quot; approach to unsupervised parsing.": 0,
            "An All-Subtrees Approach To Unsupervised Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.027096044186571824,
                0.0,
                0.0397397029074538,
                0.04387747133088672,
                0.12952586974535968
            ],
            [
                0.0,
                0.027096044186571824,
                1.0,
                0.0,
                0.21624841817457072,
                0.0818052041812729,
                0.1118223522749469
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0397397029074538,
                0.21624841817457072,
                0.0,
                1.0,
                0.06853321380136415,
                0.06009776217855158
            ],
            [
                0.0,
                0.04387747133088672,
                0.0818052041812729,
                0.0,
                0.06853321380136415,
                1.0,
                0.3380963769862967
            ],
            [
                0.0,
                0.12952586974535968,
                0.1118223522749469,
                0.0,
                0.06009776217855158,
                0.3380963769862967,
                1.0
            ]
        ]
    },
    "P11-2121": {
        "input_sentences": [
            "Abstract",
            "Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features.",
            "The new addition to the algorithm shows a clear advantage in parsing speed.",
            "First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed.",
            "Getting the Most out of Transition-based Dependency Parsing",
            "This paper suggests two ways of improving transition-based, non-projective dependency parsing.",
            "The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features.": 0,
            "The new addition to the algorithm shows a clear advantage in parsing speed.": 0,
            "First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed.": 0,
            "Getting the Most out of Transition-based Dependency Parsing": 0,
            "This paper suggests two ways of improving transition-based, non-projective dependency parsing.": 0,
            "The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10044691648347362
            ],
            [
                0.0,
                0.0,
                1.0,
                0.12296927058602616,
                0.0618756485254685,
                0.04010867640945497,
                0.0541950497038431
            ],
            [
                0.0,
                0.0,
                0.12296927058602616,
                1.0,
                0.15576902824672947,
                0.4213511766092257,
                0.07317608677480973
            ],
            [
                0.0,
                0.0,
                0.0618756485254685,
                0.15576902824672947,
                1.0,
                0.44390446181839444,
                0.08038586824213408
            ],
            [
                0.0,
                0.0,
                0.04010867640945497,
                0.4213511766092257,
                0.44390446181839444,
                1.0,
                0.0521072643931925
            ],
            [
                0.0,
                0.10044691648347362,
                0.0541950497038431,
                0.07317608677480973,
                0.08038586824213408,
                0.0521072643931925,
                1.0
            ]
        ]
    },
    "N12-1090": {
        "input_sentences": [
            "Abstract",
            "Experimental results on two target languages demonstrate the promise of our approach.",
            "Translation-Based Projection for Multilingual Coreference Resolution",
            "To build a coreference resolver for a new language, the typical approach is to first coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques.",
            "However, the high cost associated with manually coreference-annotating documents needed by a supervised approach makes it difficult to deploy coreference technologies across a large number of natural languages.",
            "To alleviate this corpus annotation bottleneck, we examine a translation-based projection approach to multilingual coreference resolution."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Experimental results on two target languages demonstrate the promise of our approach.": 0,
            "Translation-Based Projection for Multilingual Coreference Resolution": 0,
            "To build a coreference resolver for a new language, the typical approach is to first coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques.": 0,
            "However, the high cost associated with manually coreference-annotating documents needed by a supervised approach makes it difficult to deploy coreference technologies across a large number of natural languages.": 0,
            "To alleviate this corpus annotation bottleneck, we examine a translation-based projection approach to multilingual coreference resolution.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.08989428388126201,
                0.10478221947042009,
                0.04897390032354158
            ],
            [
                0.0,
                0.0,
                1.0,
                0.07650331850165207,
                0.08917349539203004,
                0.6400540952577286
            ],
            [
                0.0,
                0.08989428388126201,
                0.07650331850165207,
                1.0,
                0.19314427420401145,
                0.07344939346168315
            ],
            [
                0.0,
                0.10478221947042009,
                0.08917349539203004,
                0.19314427420401145,
                1.0,
                0.08561379137117253
            ],
            [
                0.0,
                0.04897390032354158,
                0.6400540952577286,
                0.07344939346168315,
                0.08561379137117253,
                1.0
            ]
        ]
    },
    "E09-1018": {
        "input_sentences": [
            "Abstract",
            "We have compared it to several systems available on the web (all we have found so far).",
            "The algorithm is fast and robust, and has been made publically available for downloading.",
            "While EM frequently fails to find good models for the tasks to which it is set, in this case it works quite well.",
            "We present an algorithm for pronounanaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion.",
            "EM Works for Pronoun Anaphora Resolution",
            "Our program significantly outperforms all of them."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We have compared it to several systems available on the web (all we have found so far).": 0,
            "The algorithm is fast and robust, and has been made publically available for downloading.": 0,
            "While EM frequently fails to find good models for the tasks to which it is set, in this case it works quite well.": 0,
            "We present an algorithm for pronounanaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion.": 0,
            "EM Works for Pronoun Anaphora Resolution": 0,
            "Our program significantly outperforms all of them.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.13721137870372932,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.13721137870372932,
                1.0,
                0.0,
                0.08509151865760584,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.04755314207065487,
                0.19208695904323586,
                0.0
            ],
            [
                0.0,
                0.0,
                0.08509151865760584,
                0.04755314207065487,
                1.0,
                0.07041419189977115,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.19208695904323586,
                0.07041419189977115,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P12-1099": {
        "input_sentences": [
            "Abstract",
            "In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain.",
            "Mixing Multiple Translation Models in Statistical Machine Translation",
            "Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain.",
            "Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.",
            "We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain.": 0,
            "Mixing Multiple Translation Models in Statistical Machine Translation": 0,
            "Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain.": 0,
            "Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.": 0,
            "We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.2031925018588875,
                0.14088836287521989,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.2950496791282229,
                0.20946443823555969,
                0.08991503566526544
            ],
            [
                0.0,
                0.2031925018588875,
                0.2950496791282229,
                1.0,
                0.11318239477858229,
                0.054228771051092896
            ],
            [
                0.0,
                0.14088836287521989,
                0.20946443823555969,
                0.11318239477858229,
                1.0,
                0.17873249133676067
            ],
            [
                0.0,
                0.0,
                0.08991503566526544,
                0.054228771051092896,
                0.17873249133676067,
                1.0
            ]
        ]
    },
    "W99-0613": {
        "input_sentences": [
            "Abstract",
            "A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.",
            "We present two algorithms.",
            "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98).",
            "The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.",
            "The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).",
            "Unsupervised Models for Named Entity Classification Collins"
        ],
        "authority_scores": {
            "Abstract": 0,
            "A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.": 0,
            "We present two algorithms.": 0,
            "This paper discusses the use of unlabeled examples for the problem of named entity classification.": 0,
            "The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98).": 0,
            "The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.": 0,
            "The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).": 0,
            "Unsupervised Models for Named Entity Classification Collins": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.09128298762907389,
                0.0,
                0.032921701835538185,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.14644803232993356,
                0.0
            ],
            [
                0.0,
                0.09128298762907389,
                0.0,
                1.0,
                0.0,
                0.10547843178746845,
                0.0,
                0.29999688581755607
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.24403252043743348,
                0.0
            ],
            [
                0.0,
                0.032921701835538185,
                0.0,
                0.10547843178746845,
                0.0,
                1.0,
                0.0,
                0.12946096103744403
            ],
            [
                0.0,
                0.0,
                0.14644803232993356,
                0.0,
                0.24403252043743348,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.29999688581755607,
                0.0,
                0.12946096103744403,
                0.0,
                1.0
            ]
        ]
    },
    "E03-1005": {
        "input_sentences": [
            "Abstract",
            "Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.",
            "Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.",
            "This paper proposes an integration of the two models which outperforms each of them separately.",
            "An Efficient Implementation of a New DOP Model"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.": 0,
            "Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.": 0,
            "This paper proposes an integration of the two models which outperforms each of them separately.": 0,
            "An Efficient Implementation of a New DOP Model": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.08525658323919712,
                0.04046534254315847,
                0.04478316447511461
            ],
            [
                0.0,
                0.08525658323919712,
                1.0,
                0.03537696649082206,
                0.0391518373358478
            ],
            [
                0.0,
                0.04046534254315847,
                0.03537696649082206,
                1.0,
                0.0
            ],
            [
                0.0,
                0.04478316447511461,
                0.0391518373358478,
                0.0,
                1.0
            ]
        ]
    },
    "P13-2083": {
        "input_sentences": [
            "Abstract",
            "In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods.",
            "We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.",
            "A Structured Distributional Semantic Model for Event Co-reference",
            "We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods.": 0,
            "We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.": 0,
            "A Structured Distributional Semantic Model for Event Co-reference": 0,
            "We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0356609107029053,
                0.06135399677050564,
                0.0958418685907325
            ],
            [
                0.0,
                0.0356609107029053,
                1.0,
                0.0968172452833018,
                0.09822673917852051
            ],
            [
                0.0,
                0.06135399677050564,
                0.0968172452833018,
                1.0,
                0.12569305457401644
            ],
            [
                0.0,
                0.0958418685907325,
                0.09822673917852051,
                0.12569305457401644,
                1.0
            ]
        ]
    },
    "N12-1086": {
        "input_sentences": [
            "Abstract",
            "Sparse measures are desirable forhigh-dimensional multi-class learning problems such as the induction of labels on natu ral language types, which typically associate with only a few labels.",
            "Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach producessignificantly smaller lexicons and obtains bet ter predictive performance.",
            "Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties",
            "We present novel methods to construct compact natural language lexicons within a graph based semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data.",
            "To achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in Gaussian and entropic pairwise Markovnetworks constructed from labeled and unla beled data."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Sparse measures are desirable forhigh-dimensional multi-class learning problems such as the induction of labels on natu ral language types, which typically associate with only a few labels.": 0,
            "Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach producessignificantly smaller lexicons and obtains bet ter predictive performance.": 0,
            "Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties": 0,
            "We present novel methods to construct compact natural language lexicons within a graph based semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data.": 0,
            "To achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in Gaussian and entropic pairwise Markovnetworks constructed from labeled and unla beled data.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0739505463791809,
                0.0,
                0.18760537572526315,
                0.07932604995406119
            ],
            [
                0.0,
                0.0739505463791809,
                1.0,
                0.2774231416195876,
                0.13765119512981025,
                0.02203577413780586
            ],
            [
                0.0,
                0.0,
                0.2774231416195876,
                1.0,
                0.08060402657942885,
                0.27747188603023437
            ],
            [
                0.0,
                0.18760537572526315,
                0.13765119512981025,
                0.08060402657942885,
                1.0,
                0.04878206211956928
            ],
            [
                0.0,
                0.07932604995406119,
                0.02203577413780586,
                0.27747188603023437,
                0.04878206211956928,
                1.0
            ]
        ]
    },
    "W08-0406": {
        "input_sentences": [
            "Abstract",
            "In decoding, the alternatives are scored based on the output word order, not the order of the input.",
            "This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules.",
            "Syntactic Reordering Integrated with Phrase-Based SMT",
            "Unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based SMT.",
            "Manual evaluation supports the claim that the present approach is significantly superior to previous approaches.",
            "On an English- Danish task, we achieve an absolute improvement in translation quality of 1.1 % BLEU.",
            "We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In decoding, the alternatives are scored based on the output word order, not the order of the input.": 0,
            "This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules.": 0,
            "Syntactic Reordering Integrated with Phrase-Based SMT": 0,
            "Unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based SMT.": 0,
            "Manual evaluation supports the claim that the present approach is significantly superior to previous approaches.": 0,
            "On an English- Danish task, we achieve an absolute improvement in translation quality of 1.1 % BLEU.": 0,
            "We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.11980566766514912,
                0.055741134848651626,
                0.03487803364934749,
                0.0,
                0.0,
                0.1082050814940026
            ],
            [
                0.0,
                0.11980566766514912,
                1.0,
                0.14522536998578014,
                0.09086961280669914,
                0.0,
                0.0,
                0.08729059382979973
            ],
            [
                0.0,
                0.055741134848651626,
                0.14522536998578014,
                1.0,
                0.4353022831736816,
                0.0,
                0.0,
                0.41815733136806116
            ],
            [
                0.0,
                0.03487803364934749,
                0.09086961280669914,
                0.4353022831736816,
                1.0,
                0.16336490123880237,
                0.0,
                0.34203459672446995
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.16336490123880237,
                1.0,
                0.0,
                0.15693056016884827
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.1082050814940026,
                0.08729059382979973,
                0.41815733136806116,
                0.34203459672446995,
                0.15693056016884827,
                0.0,
                1.0
            ]
        ]
    },
    "P09-2003": {
        "input_sentences": [
            "Abstract",
            "The characteristic property of the Earley parser is that we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible.",
            "We present a CYK and an Earley-style algorithm for parsing Range Concatenation Grammar (RCG), using the deductive parsing framework.",
            "Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart.",
            "An Earley Parsing Algorithm for Range Concatenation Grammars"
        ],
        "authority_scores": {
            "Abstract": 0,
            "The characteristic property of the Earley parser is that we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible.": 0,
            "We present a CYK and an Earley-style algorithm for parsing Range Concatenation Grammar (RCG), using the deductive parsing framework.": 0,
            "Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart.": 0,
            "An Earley Parsing Algorithm for Range Concatenation Grammars": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06653209209897797,
                0.10276703756906368,
                0.12132806386411743
            ],
            [
                0.0,
                0.06653209209897797,
                1.0,
                0.0,
                0.49864143077017503
            ],
            [
                0.0,
                0.10276703756906368,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.12132806386411743,
                0.49864143077017503,
                0.0,
                1.0
            ]
        ]
    },
    "N12-1049": {
        "input_sentences": [
            "Abstract",
            "While most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a comof time This is used to construct a parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity.",
            "In this way, we can employ a loosely supervised EM-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework.",
            "We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference.",
            "Parsing Time: Learning to Interpret Time Expressions",
            "We achieve an accuracy of 72% on an adapted TempEval-2 task \u2013 comparable to state of the art systems."
        ],
        "authority_scores": {
            "Abstract": 0,
            "While most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a comof time This is used to construct a parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity.": 0,
            "In this way, we can employ a loosely supervised EM-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework.": 0,
            "We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference.": 0,
            "Parsing Time: Learning to Interpret Time Expressions": 0,
            "We achieve an accuracy of 72% on an adapted TempEval-2 task \u2013 comparable to state of the art systems.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.35867855172004304,
                0.03443898511151448
            ],
            [
                0.0,
                0.0,
                1.0,
                0.09876543571051094,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.09876543571051094,
                1.0,
                0.17218195895375613,
                0.0
            ],
            [
                0.0,
                0.35867855172004304,
                0.0,
                0.17218195895375613,
                1.0,
                0.0
            ],
            [
                0.0,
                0.03443898511151448,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "D07-1013": {
        "input_sentences": [
            "Abstract",
            "This analysisleads to new directions for parser develop ment.",
            "Characterizing the Errors of Data-Driven Dependency Parsing Models",
            "We present a comparative error analysisof the two dominant approaches in datadriven dependency parsing: global, exhaus tive, graph-based models, and local, greedy, transition-based models.",
            "We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This analysisleads to new directions for parser develop ment.": 0,
            "Characterizing the Errors of Data-Driven Dependency Parsing Models": 0,
            "We present a comparative error analysisof the two dominant approaches in datadriven dependency parsing: global, exhaus tive, graph-based models, and local, greedy, transition-based models.": 0,
            "We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.20599229568910507,
                0.18164671435914215
            ],
            [
                0.0,
                0.0,
                0.20599229568910507,
                1.0,
                0.10652630837470763
            ],
            [
                0.0,
                0.0,
                0.18164671435914215,
                0.10652630837470763,
                1.0
            ]
        ]
    },
    "C10-1045": {
        "input_sentences": [
            "Abstract",
            "Better Arabic Parsing: Baselines Evaluations and Analysis",
            "First, we identify sources of syntactic ambiguity under studied in the existing parsing literature.",
            "Second, we show that although the PennArabic Treebank is similar to other tree banks in gross statistical terms, annotation consistency remains problematic.",
            "Third,we develop a human interpretable grammar that is competitive with a latent vari able PCFG.",
            "In this paper, we offer broad insightinto the underperformance of Arabic constituency parsing by analyzing the inter play of linguistic phenomena, annotationchoices, and model design.",
            "Fourth, we show how to build better models for three different parsers.Finally, we show that in application set tings, the absence of gold segmentation lowers parsing performance by 2?5% F1."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Better Arabic Parsing: Baselines Evaluations and Analysis": 0,
            "First, we identify sources of syntactic ambiguity under studied in the existing parsing literature.": 0,
            "Second, we show that although the PennArabic Treebank is similar to other tree banks in gross statistical terms, annotation consistency remains problematic.": 0,
            "Third,we develop a human interpretable grammar that is competitive with a latent vari able PCFG.": 0,
            "In this paper, we offer broad insightinto the underperformance of Arabic constituency parsing by analyzing the inter play of linguistic phenomena, annotationchoices, and model design.": 0,
            "Fourth, we show how to build better models for three different parsers.Finally, we show that in application set tings, the absence of gold segmentation lowers parsing performance by 2?5% F1.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06404475072342918,
                0.0,
                0.0,
                0.1261990130060454,
                0.12220903679007672
            ],
            [
                0.0,
                0.06404475072342918,
                1.0,
                0.0,
                0.0,
                0.03598657969943479,
                0.03484880854200166
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.1261990130060454,
                0.03598657969943479,
                0.0,
                0.0,
                1.0,
                0.02438739859798149
            ],
            [
                0.0,
                0.12220903679007672,
                0.03484880854200166,
                0.0,
                0.0,
                0.02438739859798149,
                1.0
            ]
        ]
    },
    "H05-1094": {
        "input_sentences": [
            "Abstract",
            "Many learning tasks have subtasks for which much training data exists.",
            "Specifically, we perform joint decoding ofseparately-trained sequence models, preserv ing uncertainty between the tasks and allowinginformation from the new task to affect predic tions on the old task.",
            "On two standard text data sets, we show that joint decoding outperforms cascaded decoding.",
            "Composition Of Conditional Random Fields For Transfer Learning",
            "Therefore, we wantto transfer learning from the old, general purpose subtask to a more specific new task, for which there is often less data.",
            "While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Many learning tasks have subtasks for which much training data exists.": 0,
            "Specifically, we perform joint decoding ofseparately-trained sequence models, preserv ing uncertainty between the tasks and allowinginformation from the new task to affect predic tions on the old task.": 0,
            "On two standard text data sets, we show that joint decoding outperforms cascaded decoding.": 0,
            "Composition Of Conditional Random Fields For Transfer Learning": 0,
            "Therefore, we wantto transfer learning from the old, general purpose subtask to a more specific new task, for which there is often less data.": 0,
            "While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07643085021558825,
                0.07870671042336531,
                0.08031530805255507,
                0.14694183616355166,
                0.08541562943612103
            ],
            [
                0.0,
                0.07643085021558825,
                1.0,
                0.16389370522638824,
                0.0,
                0.16996274817715826,
                0.32665922732624403
            ],
            [
                0.0,
                0.07870671042336531,
                0.16389370522638824,
                1.0,
                0.0,
                0.059888230678179895,
                0.0
            ],
            [
                0.0,
                0.08031530805255507,
                0.0,
                0.0,
                1.0,
                0.14218604311112398,
                0.13747523492032995
            ],
            [
                0.0,
                0.14694183616355166,
                0.16996274817715826,
                0.059888230678179895,
                0.14218604311112398,
                1.0,
                0.4098823301831909
            ],
            [
                0.0,
                0.08541562943612103,
                0.32665922732624403,
                0.0,
                0.13747523492032995,
                0.4098823301831909,
                1.0
            ]
        ]
    },
    "E09-1038": {
        "input_sentences": [
            "Abstract",
            "We present a framework for interfacing a PCFG parser with lexical information from an external resource following a different tagging scheme than the treebank.",
            "This is achieved by defining a stochastic mapping layer between the two resources.",
            "Lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora.",
            "We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens.",
            "Enhancing Unlexicalized Parsing Performance Using a Wide Coverage Lexicon Fuzzy Tag-Set Mapping and EM-HMM-Based Lexical Probabilities"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We present a framework for interfacing a PCFG parser with lexical information from an external resource following a different tagging scheme than the treebank.": 0,
            "This is achieved by defining a stochastic mapping layer between the two resources.": 0,
            "Lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora.": 0,
            "We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens.": 0,
            "Enhancing Unlexicalized Parsing Performance Using a Wide Coverage Lexicon Fuzzy Tag-Set Mapping and EM-HMM-Based Lexical Probabilities": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.04068115708228679,
                0.07130067824641996,
                0.03513168186144273
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.07410839552271431
            ],
            [
                0.0,
                0.04068115708228679,
                0.0,
                1.0,
                0.0,
                0.14553621826234303
            ],
            [
                0.0,
                0.07130067824641996,
                0.0,
                0.0,
                1.0,
                0.1675547438929288
            ],
            [
                0.0,
                0.03513168186144273,
                0.07410839552271431,
                0.14553621826234303,
                0.1675547438929288,
                1.0
            ]
        ]
    },
    "C10-2096": {
        "input_sentences": [
            "Abstract",
            "More importantly,our model takes into account all the probabilities of different steps, such as segmen tation, parsing, and translation.",
            "Machine Translation with Lattices and Forests",
            "The main advantage of our model is that we can make global decision to search for the best segmentation, parse-tree and translation in one step.",
            "In order to alleviate this problem, we use compact structures, lattice and forest, in each module insteadof 1-best results.",
            "We integrate both lat tice and forest into a single tree-to-stringsystem, and explore the algorithms of lattice parsing, lattice-forest-based rule ex traction and decoding.",
            "Medium-scale experiments show an improvement of +0.9 BLEU points over a state-of-the-art forest-based baseline.",
            "Traditional 1-best translation pipelinessuffer a major drawback: the errors of 1 best outputs, inevitably introduced by each module, will propagate and accumulate along the pipeline."
        ],
        "authority_scores": {
            "Abstract": 0,
            "More importantly,our model takes into account all the probabilities of different steps, such as segmen tation, parsing, and translation.": 0,
            "Machine Translation with Lattices and Forests": 0,
            "The main advantage of our model is that we can make global decision to search for the best segmentation, parse-tree and translation in one step.": 0,
            "In order to alleviate this problem, we use compact structures, lattice and forest, in each module insteadof 1-best results.": 0,
            "We integrate both lat tice and forest into a single tree-to-stringsystem, and explore the algorithms of lattice parsing, lattice-forest-based rule ex traction and decoding.": 0,
            "Medium-scale experiments show an improvement of +0.9 BLEU points over a state-of-the-art forest-based baseline.": 0,
            "Traditional 1-best translation pipelinessuffer a major drawback: the errors of 1 best outputs, inevitably introduced by each module, will propagate and accumulate along the pipeline.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06960719161947129,
                0.10477654927162382,
                0.0,
                0.052852370800175096,
                0.0,
                0.03407492410719467
            ],
            [
                0.0,
                0.06960719161947129,
                1.0,
                0.06475995349920069,
                0.0,
                0.0,
                0.0,
                0.057853263790342946
            ],
            [
                0.0,
                0.10477654927162382,
                0.06475995349920069,
                1.0,
                0.048064285053167476,
                0.04917188864698301,
                0.0,
                0.11417972813044201
            ],
            [
                0.0,
                0.0,
                0.0,
                0.048064285053167476,
                1.0,
                0.17864338991728462,
                0.05059342988670058,
                0.14354040450072186
            ],
            [
                0.0,
                0.052852370800175096,
                0.0,
                0.04917188864698301,
                0.17864338991728462,
                1.0,
                0.12884212387253305,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.05059342988670058,
                0.12884212387253305,
                1.0,
                0.0
            ],
            [
                0.0,
                0.03407492410719467,
                0.057853263790342946,
                0.11417972813044201,
                0.14354040450072186,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "E12-1047": {
        "input_sentences": [
            "Abstract",
            "Linear Context-Free Rewriting Systems",
            "Efficient parsing with Linear Context-Free Rewriting Systems",
            "Instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy.",
            "Previous work on treebank parsing with discontinuous constituents using Linear Rewriting systems has been limited to sentences of up to 30 words, for reasons of computational complexity.",
            "The resulting parser has been applied to a discontinuous treebank with favorable results.",
            "There have been some results on an a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Linear Context-Free Rewriting Systems": 0,
            "Efficient parsing with Linear Context-Free Rewriting Systems": 0,
            "Instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy.": 0,
            "Previous work on treebank parsing with discontinuous constituents using Linear Rewriting systems has been limited to sentences of up to 30 words, for reasons of computational complexity.": 0,
            "The resulting parser has been applied to a discontinuous treebank with favorable results.": 0,
            "There have been some results on an a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.8109710326710888,
                0.0,
                0.24223102266746363,
                0.0,
                0.0
            ],
            [
                0.0,
                0.8109710326710888,
                1.0,
                0.0,
                0.2619231234634759,
                0.0,
                0.12501454347317423
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.24223102266746363,
                0.2619231234634759,
                0.0,
                1.0,
                0.1525032133594692,
                0.21802615859352312
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.1525032133594692,
                1.0,
                0.0727889899304163
            ],
            [
                0.0,
                0.0,
                0.12501454347317423,
                0.0,
                0.21802615859352312,
                0.0727889899304163,
                1.0
            ]
        ]
    },
    "P05-1039": {
        "input_sentences": [
            "Abstract",
            "What To Do When Lexicalization Fails: Parsing German With Suffix Analysis And Smoothing",
            "In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results.",
            "In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve labelled bracket of 76.2, higher than previously reported results on the NEGRA corpus."
        ],
        "authority_scores": {
            "Abstract": 0,
            "What To Do When Lexicalization Fails: Parsing German With Suffix Analysis And Smoothing": 0,
            "In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results.": 0,
            "In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve labelled bracket of 76.2, higher than previously reported results on the NEGRA corpus.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.17929008377668199,
                0.25568845451102185
            ],
            [
                0.0,
                0.17929008377668199,
                1.0,
                0.1841865488459004
            ],
            [
                0.0,
                0.25568845451102185,
                0.1841865488459004,
                1.0
            ]
        ]
    },
    "A00-2005": {
        "input_sentences": [
            "Abstract",
            "Bagging and boosting, two effective machine learning techniques, are applied to natural language parsing.",
            "The best resulting system provides roughly as large of a gain in F-measure as doubling the corpus size.",
            "Error analysis of the result of the boosting technique reveals some inconsistent annotations in the Penn Treebank, suggesting a semi-automatic method for finding inconsistent treebank annotations.",
            "Experiments using these techniques with a trainable statistical parser are described.",
            "Bagging And Boosting A Treebank Parser"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Bagging and boosting, two effective machine learning techniques, are applied to natural language parsing.": 0,
            "The best resulting system provides roughly as large of a gain in F-measure as doubling the corpus size.": 0,
            "Error analysis of the result of the boosting technique reveals some inconsistent annotations in the Penn Treebank, suggesting a semi-automatic method for finding inconsistent treebank annotations.": 0,
            "Experiments using these techniques with a trainable statistical parser are described.": 0,
            "Bagging And Boosting A Treebank Parser": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.034268493532312705,
                0.0898661226995404,
                0.24537991376743748
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.034268493532312705,
                0.0,
                1.0,
                0.0,
                0.24519658672093742
            ],
            [
                0.0,
                0.0898661226995404,
                0.0,
                0.0,
                1.0,
                0.16895096685987404
            ],
            [
                0.0,
                0.24537991376743748,
                0.0,
                0.24519658672093742,
                0.16895096685987404,
                1.0
            ]
        ]
    },
    "D07-1091": {
        "input_sentences": [
            "Abstract",
            "In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.",
            "Factored Translation Models",
            "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.": 0,
            "Factored Translation Models": 0,
            "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.16273185231513437,
                0.045896640833401545
            ],
            [
                0.0,
                0.16273185231513437,
                1.0,
                0.12663001672534585
            ],
            [
                0.0,
                0.045896640833401545,
                0.12663001672534585,
                1.0
            ]
        ]
    },
    "E06-1010": {
        "input_sentences": [
            "Abstract",
            "The results indicate that, whereas complete linguistic coverage in principle requires unrestricted non-projective dependency graphs, limiting the degree of non-projectivity to at most 2 can reduce average running time from quadratic to linear, while excluding less than 0.5% of the dependency graphs found in the two treebanks.",
            "The constraints are evaluated experimentally using data from the Prague Dependency Treebank and the Danish Dependency Treebank.",
            "In particular, we define a new measure the non-projectivity in an acyclic dependency graph obeying the single-head constraint.",
            "This is a substantial improvement over the commonly used projective approximation (degree 0), which excludes 15\u201325% of the graphs.",
            "We investigate a series of graph-theoretic constraints on non-projective dependency and their effect on i.e. whether they allow naturally occurring syntactic constructions to be adequately and i.e. whether they reduce the search space for the parser.",
            "Constraints On Non-Projective Dependency Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "The results indicate that, whereas complete linguistic coverage in principle requires unrestricted non-projective dependency graphs, limiting the degree of non-projectivity to at most 2 can reduce average running time from quadratic to linear, while excluding less than 0.5% of the dependency graphs found in the two treebanks.": 0,
            "The constraints are evaluated experimentally using data from the Prague Dependency Treebank and the Danish Dependency Treebank.": 0,
            "In particular, we define a new measure the non-projectivity in an acyclic dependency graph obeying the single-head constraint.": 0,
            "This is a substantial improvement over the commonly used projective approximation (degree 0), which excludes 15\u201325% of the graphs.": 0,
            "We investigate a series of graph-theoretic constraints on non-projective dependency and their effect on i.e. whether they allow naturally occurring syntactic constructions to be adequately and i.e. whether they reduce the search space for the parser.": 0,
            "Constraints On Non-Projective Dependency Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06976879402634242,
                0.12498347321662315,
                0.16026068232914603,
                0.1235359411136715,
                0.22033921561666311
            ],
            [
                0.0,
                0.06976879402634242,
                1.0,
                0.05129100981074695,
                0.0,
                0.07963776422979703,
                0.19891845469741057
            ],
            [
                0.0,
                0.12498347321662315,
                0.05129100981074695,
                1.0,
                0.0,
                0.102484553057344,
                0.1262625585338991
            ],
            [
                0.0,
                0.16026068232914603,
                0.0,
                0.0,
                1.0,
                0.030435994975001514,
                0.07602274054474525
            ],
            [
                0.0,
                0.1235359411136715,
                0.07963776422979703,
                0.102484553057344,
                0.030435994975001514,
                1.0,
                0.24357229018595838
            ],
            [
                0.0,
                0.22033921561666311,
                0.19891845469741057,
                0.1262625585338991,
                0.07602274054474525,
                0.24357229018595838,
                1.0
            ]
        ]
    },
    "W12-3117": {
        "input_sentences": [
            "Abstract",
            "In total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data.",
            "Two sets features are proposed: one i.e. respecting the data limitation suggested by the organisers, and one i.e. using data or tools trained on data that was not provided by the workshop organisers.",
            "DCU-Symantec Submission for the WMT 2012 Quality Estimation Task",
            "In this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the Latent Dirichlet Allocation approach, and features based on source and target language syntax extracted using part-of-speech (POS) taggers and parsers.",
            "We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques.",
            "This paper describes the features and the machine learning methods used by Dublin City (DCU) and the WMT 2012 quality estimation task."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data.": 0,
            "Two sets features are proposed: one i.e. respecting the data limitation suggested by the organisers, and one i.e. using data or tools trained on data that was not provided by the workshop organisers.": 0,
            "DCU-Symantec Submission for the WMT 2012 Quality Estimation Task": 0,
            "In this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the Latent Dirichlet Allocation approach, and features based on source and target language syntax extracted using part-of-speech (POS) taggers and parsers.": 0,
            "We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques.": 0,
            "This paper describes the features and the machine learning methods used by Dublin City (DCU) and the WMT 2012 quality estimation task.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.16497880754310057,
                0.06237700103028172,
                0.0807645462152195,
                0.0,
                0.14268497348725223
            ],
            [
                0.0,
                0.16497880754310057,
                1.0,
                0.0,
                0.07272416348566986,
                0.03495679968365924,
                0.025433517597516324
            ],
            [
                0.0,
                0.06237700103028172,
                0.0,
                1.0,
                0.0,
                0.0,
                0.4862784798342898
            ],
            [
                0.0,
                0.0807645462152195,
                0.07272416348566986,
                0.0,
                1.0,
                0.25570586952686425,
                0.0802739372163283
            ],
            [
                0.0,
                0.0,
                0.03495679968365924,
                0.0,
                0.25570586952686425,
                1.0,
                0.12880732829201122
            ],
            [
                0.0,
                0.14268497348725223,
                0.025433517597516324,
                0.4862784798342898,
                0.0802739372163283,
                0.12880732829201122,
                1.0
            ]
        ]
    },
    "N01-1023": {
        "input_sentences": [
            "Abstract",
            "Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.",
            "The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text.",
            "We propose a novel Co-Training method for statistical parsing.",
            "The algorithm iteratively labels the entire data set with parse trees.",
            "Applying Co-Training Methods To Statistical Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.": 0,
            "The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text.": 0,
            "We propose a novel Co-Training method for statistical parsing.": 0,
            "The algorithm iteratively labels the entire data set with parse trees.": 0,
            "Applying Co-Training Methods To Statistical Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.10328206497924951,
                0.1730932918437717,
                0.11525383770882917,
                0.1975133769142783
            ],
            [
                0.0,
                0.10328206497924951,
                1.0,
                0.03953421072961049,
                0.24868667338512895,
                0.045111716240822254
            ],
            [
                0.0,
                0.1730932918437717,
                0.03953421072961049,
                1.0,
                0.0,
                0.34692615700469637
            ],
            [
                0.0,
                0.11525383770882917,
                0.24868667338512895,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.1975133769142783,
                0.045111716240822254,
                0.34692615700469637,
                0.0,
                1.0
            ]
        ]
    },
    "W09-1207": {
        "input_sentences": [
            "Multilingual Dependency-based Syntactic and Semantic Parsing",
            "Abstract",
            "In Jason Eisner.",
            "2000.",
            "Bilexical grammars and their cubicparsing algorithms.",
            "In EMNLP/CoNLL- Chang and Chih-Jen Lin, 2001. a for support vector Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li.",
            "dependency parser.",
            "In in Probabilistic",
            "2008.",
            "A cascaded syntactic and semantic dependency parsing system."
        ],
        "authority_scores": {
            "Multilingual Dependency-based Syntactic and Semantic Parsing": 0,
            "Abstract": 0,
            "In Jason Eisner.": 0,
            "2000.": 0,
            "Bilexical grammars and their cubicparsing algorithms.": 0,
            "In EMNLP/CoNLL- Chang and Chih-Jen Lin, 2001. a for support vector Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li.": 0,
            "dependency parser.": 0,
            "In in Probabilistic": 0,
            "2008.": 0,
            "A cascaded syntactic and semantic dependency parsing system.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.20426973400040402,
                0.0,
                0.0,
                0.6492129958573976
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.20426973400040402,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.2300858752242662
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.6492129958573976,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.2300858752242662,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "N09-1061": {
        "input_sentences": [
            "Abstract",
            "Our results generalize previous work on Synchronous Context-Free Grammar, and are particularly relevant for machine translation from or to languages that require syntactic analyses with discontinuous constituents.",
            "Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation.",
            "Optimal Reduction of Rule Length in Linear Context-Free Rewriting Systems",
            "The parsing complexity of an is exponential in both the of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which productions have rank at most and has minimal fan-out."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Our results generalize previous work on Synchronous Context-Free Grammar, and are particularly relevant for machine translation from or to languages that require syntactic analyses with discontinuous constituents.": 0,
            "Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation.": 0,
            "Optimal Reduction of Rule Length in Linear Context-Free Rewriting Systems": 0,
            "The parsing complexity of an is exponential in both the of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which productions have rank at most and has minimal fan-out.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.22139586012203316,
                0.08609064791129394,
                0.0
            ],
            [
                0.0,
                0.22139586012203316,
                1.0,
                0.3367773551240035,
                0.041397241428395716
            ],
            [
                0.0,
                0.08609064791129394,
                0.3367773551240035,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.041397241428395716,
                0.0,
                1.0
            ]
        ]
    },
    "E09-1055": {
        "input_sentences": [
            "Abstract",
            "In this paper, we provide two key tools for this approach.",
            "For efficient parsing, the extracted grammars need to be transformed in order to minimize the number of nonterminal symbols per production.",
            "Our second contribution is an algorithm that computes this transformation for a large, empirically relevant class of grammars.",
            "An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures.",
            "Treebank Grammar Techniques for Non-Projective Dependency Parsing",
            "First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks.",
            "We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we provide two key tools for this approach.": 0,
            "For efficient parsing, the extracted grammars need to be transformed in order to minimize the number of nonterminal symbols per production.": 0,
            "Our second contribution is an algorithm that computes this transformation for a large, empirically relevant class of grammars.": 0,
            "An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures.": 0,
            "Treebank Grammar Techniques for Non-Projective Dependency Parsing": 0,
            "First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks.": 0,
            "We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.06887110984006725,
                0.11236681622949254,
                0.04326749888472311,
                0.04491988007591836,
                0.02938725336276652
            ],
            [
                0.0,
                0.0,
                0.06887110984006725,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.11236681622949254,
                0.0,
                1.0,
                0.36470051035126083,
                0.14147733230359855,
                0.11236681622949254
            ],
            [
                0.0,
                0.0,
                0.04326749888472311,
                0.0,
                0.36470051035126083,
                1.0,
                0.17593711701605116,
                0.13973612149588505
            ],
            [
                0.0,
                0.0,
                0.04491988007591836,
                0.0,
                0.14147733230359855,
                0.17593711701605116,
                1.0,
                0.04491988007591836
            ],
            [
                0.0,
                0.0,
                0.02938725336276652,
                0.0,
                0.11236681622949254,
                0.13973612149588505,
                0.04491988007591836,
                1.0
            ]
        ]
    },
    "W11-2138": {
        "input_sentences": [
            "Abstract",
            "We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words.",
            "This method called \u201creverse self-training\u201d improves the decoder\u2019s ability to produce grammatically correct translations into languages with morphology richer than the source language esp. in small-data setting.",
            "Improving Translation Model by Monolingual Data",
            "We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation.",
            "We also provide a description of the systems we"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words.": 0,
            "This method called \u201creverse self-training\u201d improves the decoder\u2019s ability to produce grammatically correct translations into languages with morphology richer than the source language esp. in small-data setting.": 0,
            "Improving Translation Model by Monolingual Data": 0,
            "We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation.": 0,
            "We also provide a description of the systems we": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.037837176350990764,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.037837176350990764,
                1.0,
                0.057098837973667944,
                0.03292816764221489,
                0.0
            ],
            [
                0.0,
                0.0,
                0.057098837973667944,
                1.0,
                0.52266012740354,
                0.0
            ],
            [
                0.0,
                0.0,
                0.03292816764221489,
                0.52266012740354,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "D12-1127": {
        "input_sentences": [
            "Abstract",
            "Wiki-ly Supervised Part-of-Speech Tagging",
            "In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary.",
            "Despite significant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks.",
            "However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps.",
            "We achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised Penn",
            "Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods.",
            "Use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Wiki-ly Supervised Part-of-Speech Tagging": 0,
            "In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary.": 0,
            "Despite significant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks.": 0,
            "However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps.": 0,
            "We achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised Penn": 0,
            "Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods.": 0,
            "Use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.17599399827275003,
                0.0,
                0.09734336694632958,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.04199331357036109,
                0.10218403657558967,
                0.08104408679335674,
                0.05009202240207653,
                0.09317292505037256
            ],
            [
                0.0,
                0.17599399827275003,
                0.04199331357036109,
                1.0,
                0.06315458426570209,
                0.0,
                0.05401235067445396,
                0.0
            ],
            [
                0.0,
                0.0,
                0.10218403657558967,
                0.06315458426570209,
                1.0,
                0.052021442357392166,
                0.1121921071112247,
                0.10434063435502434
            ],
            [
                0.0,
                0.09734336694632958,
                0.08104408679335674,
                0.0,
                0.052021442357392166,
                1.0,
                0.14873080551465812,
                0.0827545251697618
            ],
            [
                0.0,
                0.0,
                0.05009202240207653,
                0.05401235067445396,
                0.1121921071112247,
                0.14873080551465812,
                1.0,
                0.23839276674527998
            ],
            [
                0.0,
                0.0,
                0.09317292505037256,
                0.0,
                0.10434063435502434,
                0.0827545251697618,
                0.23839276674527998,
                1.0
            ]
        ]
    },
    "W11-2133": {
        "input_sentences": [
            "Abstract",
            "We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task.",
            "Our topic modeling approach is simpler to construct than its counterparts.",
            "During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language\u2019s vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM).",
            "This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training.",
            "Bilingual Latent Semantic Models",
            "Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task.": 0,
            "Our topic modeling approach is simpler to construct than its counterparts.": 0,
            "During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language\u2019s vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM).": 0,
            "This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training.": 0,
            "Bilingual Latent Semantic Models": 0,
            "Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.04749882677116277,
                0.056492974052744206,
                0.021572401062125202,
                0.0,
                0.063638561566906
            ],
            [
                0.0,
                0.04749882677116277,
                1.0,
                0.036063155554318056,
                0.1561542436012336,
                0.0,
                0.08124930165289804
            ],
            [
                0.0,
                0.056492974052744206,
                0.036063155554318056,
                1.0,
                0.3652440159532255,
                0.0,
                0.08213446946217977
            ],
            [
                0.0,
                0.021572401062125202,
                0.1561542436012336,
                0.3652440159532255,
                1.0,
                0.2163122070433179,
                0.23271660346612988
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.2163122070433179,
                1.0,
                0.6789348096346638
            ],
            [
                0.0,
                0.063638561566906,
                0.08124930165289804,
                0.08213446946217977,
                0.23271660346612988,
                0.6789348096346638,
                1.0
            ]
        ]
    },
    "D12-1069": {
        "input_sentences": [
            "Abstract",
            "We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation.",
            "Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: supervision a knowledge base, supervision dependencyparsed sentences.",
            "We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences.",
            "This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments.",
            "On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.",
            "We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase.",
            "Weakly Supervised Training of Semantic Parsers"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation.": 0,
            "Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: supervision a knowledge base, supervision dependencyparsed sentences.": 0,
            "We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences.": 0,
            "This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments.": 0,
            "On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.": 0,
            "We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase.": 0,
            "Weakly Supervised Training of Semantic Parsers": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.14124948508242505,
                0.11510576125132296,
                0.18009806961717945,
                0.05477883691435853,
                0.07833837159079275,
                0.04387865081764366
            ],
            [
                0.0,
                0.14124948508242505,
                1.0,
                0.17055002943874842,
                0.07959668160203913,
                0.035420931779491495,
                0.050654929387640844,
                0.028372685232761752
            ],
            [
                0.0,
                0.11510576125132296,
                0.17055002943874842,
                1.0,
                0.05731767584550559,
                0.10658467759327597,
                0.0,
                0.14507899673589514
            ],
            [
                0.0,
                0.18009806961717945,
                0.07959668160203913,
                0.05731767584550559,
                1.0,
                0.05253944824092301,
                0.05209905745109053,
                0.058363131717010026
            ],
            [
                0.0,
                0.05477883691435853,
                0.035420931779491495,
                0.10658467759327597,
                0.05253944824092301,
                1.0,
                0.06227075616125336,
                0.034878906853246454
            ],
            [
                0.0,
                0.07833837159079275,
                0.050654929387640844,
                0.0,
                0.05209905745109053,
                0.06227075616125336,
                1.0,
                0.0
            ],
            [
                0.0,
                0.04387865081764366,
                0.028372685232761752,
                0.14507899673589514,
                0.058363131717010026,
                0.034878906853246454,
                0.0,
                1.0
            ]
        ]
    },
    "P08-1028": {
        "input_sentences": [
            "Abstract",
            "This paper proposes a framework for representing the meaning of phrases and sentences in vector space.",
            "Vector-based Models of Semantic Composition",
            "Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.",
            "Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.",
            "Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This paper proposes a framework for representing the meaning of phrases and sentences in vector space.": 0,
            "Vector-based Models of Semantic Composition": 0,
            "Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.": 0,
            "Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.": 0,
            "Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09053857780810368,
                0.0,
                0.07588958357228515,
                0.06211777527427815
            ],
            [
                0.0,
                0.09053857780810368,
                1.0,
                0.08247283944278769,
                0.1665912655471916,
                0.1913042240967014
            ],
            [
                0.0,
                0.0,
                0.08247283944278769,
                1.0,
                0.04927434272346614,
                0.15876756217170362
            ],
            [
                0.0,
                0.07588958357228515,
                0.1665912655471916,
                0.04927434272346614,
                1.0,
                0.05714845012173264
            ],
            [
                0.0,
                0.06211777527427815,
                0.1913042240967014,
                0.15876756217170362,
                0.05714845012173264,
                1.0
            ]
        ]
    },
    "W05-0636": {
        "input_sentences": [
            "Abstract",
            "Our current results are negative, because a locallytrained SRL model can return inaccurate probability estimates.",
            "A striking feature of human syntactic prois that it is that is, it seems to take into account semantic information from the discourse context and world knowledge.",
            "In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses.",
            "To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser.",
            "Joint Parsing And Semantic Role Labeling"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Our current results are negative, because a locallytrained SRL model can return inaccurate probability estimates.": 0,
            "A striking feature of human syntactic prois that it is that is, it seems to take into account semantic information from the discourse context and world knowledge.": 0,
            "In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses.": 0,
            "To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser.": 0,
            "Joint Parsing And Semantic Role Labeling": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.08572224173087356,
                0.09074854881118093,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.04008408361445201,
                0.07565291959538457
            ],
            [
                0.0,
                0.08572224173087356,
                0.0,
                1.0,
                0.0727007296634865,
                0.0
            ],
            [
                0.0,
                0.09074854881118093,
                0.04008408361445201,
                0.0727007296634865,
                1.0,
                0.3783098344643069
            ],
            [
                0.0,
                0.0,
                0.07565291959538457,
                0.0,
                0.3783098344643069,
                1.0
            ]
        ]
    },
    "A00-2018": {
        "input_sentences": [
            "Abstract",
            "We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.",
            "A Maximum-Entropy-Inspired Parser *",
            "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.",
            "The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.",
            "This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.": 0,
            "A Maximum-Entropy-Inspired Parser *": 0,
            "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.": 0,
            "The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.": 0,
            "This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.023748647662372082,
                0.09491164494923993,
                0.045667976875145996
            ],
            [
                0.0,
                0.0,
                1.0,
                0.05214448684945385,
                0.2923665094141451,
                0.10027237144034327
            ],
            [
                0.0,
                0.023748647662372082,
                0.05214448684945385,
                1.0,
                0.10587994618584393,
                0.02723507895955209
            ],
            [
                0.0,
                0.09491164494923993,
                0.2923665094141451,
                0.10587994618584393,
                1.0,
                0.0
            ],
            [
                0.0,
                0.045667976875145996,
                0.10027237144034327,
                0.02723507895955209,
                0.0,
                1.0
            ]
        ]
    },
    "N06-1039": {
        "input_sentences": [
            "Abstract",
            "surface text patterns for a question answering system. of the 40th Annual Meeting of the As",
            "Preemptive Information Extraction Using Unrestricted Relation Discovery"
        ],
        "authority_scores": {
            "Abstract": 0,
            "surface text patterns for a question answering system. of the 40th Annual Meeting of the As": 0,
            "Preemptive Information Extraction Using Unrestricted Relation Discovery": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "W06-2920": {
        "input_sentences": [
            "Abstract",
            "Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing and helping with the also to Alexander Yeh for additional help with the paper reviews.",
            "In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.",
            "CoNLL-X Shared Task On Multilingual Dependency Parsing",
            "We also give an overview of the parsing approaches that participants took and the results that they achieved.",
            "Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?",
            "The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.",
            "His work was made possible by the MITRE Cor",
            "Each year the Conference on Computational Natural Language Learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing and helping with the also to Alexander Yeh for additional help with the paper reviews.": 0,
            "In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.": 0,
            "CoNLL-X Shared Task On Multilingual Dependency Parsing": 0,
            "We also give an overview of the parsing approaches that participants took and the results that they achieved.": 0,
            "Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?": 0,
            "The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.": 0,
            "His work was made possible by the MITRE Cor": 0,
            "Each year the Conference on Computational Natural Language Learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.11669111607428327,
                0.11326793502266845,
                0.0,
                0.0,
                0.07359669365621223,
                0.0,
                0.043757948171315175
            ],
            [
                0.0,
                0.11669111607428327,
                1.0,
                0.15199805298303262,
                0.047648731630020834,
                0.05991328307306113,
                0.09876187942769671,
                0.0,
                0.0
            ],
            [
                0.0,
                0.11326793502266845,
                0.15199805298303262,
                1.0,
                0.07836886788727618,
                0.09854063277704198,
                0.8029870165477709,
                0.0,
                0.1075345319771749
            ],
            [
                0.0,
                0.0,
                0.047648731630020834,
                0.07836886788727618,
                1.0,
                0.030890765201905864,
                0.05092076200497288,
                0.0,
                0.06442839585029339
            ],
            [
                0.0,
                0.0,
                0.05991328307306113,
                0.09854063277704198,
                0.030890765201905864,
                1.0,
                0.06402751812973247,
                0.0,
                0.03591088317420022
            ],
            [
                0.0,
                0.07359669365621223,
                0.09876187942769671,
                0.8029870165477709,
                0.05092076200497288,
                0.06402751812973247,
                1.0,
                0.0,
                0.0698713718565137
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.043757948171315175,
                0.0,
                0.1075345319771749,
                0.06442839585029339,
                0.03591088317420022,
                0.0698713718565137,
                0.0,
                1.0
            ]
        ]
    },
    "P05-1065": {
        "input_sentences": [
            "Abstract",
            "However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers.",
            "This task can be addressed with natural language processing technology to assess reading level.",
            "Reading proficiency is a fundamental component of language competency.",
            "Reading Level Assessment Using Support Vector Machines And Statistical Language Models",
            "In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level.",
            "Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models."
        ],
        "authority_scores": {
            "Abstract": 0,
            "However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers.": 0,
            "This task can be addressed with natural language processing technology to assess reading level.": 0,
            "Reading proficiency is a fundamental component of language competency.": 0,
            "Reading Level Assessment Using Support Vector Machines And Statistical Language Models": 0,
            "In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level.": 0,
            "Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09614972762704939,
                0.06852861613531791,
                0.101378268138382,
                0.11397904204518183,
                0.0691989788474307
            ],
            [
                0.0,
                0.09614972762704939,
                1.0,
                0.08645918070734643,
                0.12790396916036495,
                0.21052885402508212,
                0.1683276957564277
            ],
            [
                0.0,
                0.06852861613531791,
                0.08645918070734643,
                1.0,
                0.09116075750908677,
                0.1024915497551787,
                0.0622246902262749
            ],
            [
                0.0,
                0.101378268138382,
                0.12790396916036495,
                0.09116075750908677,
                1.0,
                0.4654959976852997,
                0.3023141341170323
            ],
            [
                0.0,
                0.11397904204518183,
                0.21052885402508212,
                0.1024915497551787,
                0.4654959976852997,
                1.0,
                0.22169215100650402
            ],
            [
                0.0,
                0.0691989788474307,
                0.1683276957564277,
                0.0622246902262749,
                0.3023141341170323,
                0.22169215100650402,
                1.0
            ]
        ]
    },
    "C04-1074": {
        "input_sentences": [
            "Abstract",
            "The factors used in the algorithms and the algorithms themselves are evaluated on a Germancorpus annotated with syntactic and coreference in formation (Negra) (Skut et al, 1997).",
            "A commonformat for pronoun resolution algorithms with sev eral open parameters is proposed, and the parameter settings optimal on the evaluation data are given.",
            "Optimizing Algorithms For Pronoun Resolution",
            "The paper aims at a deeper understanding of sev eral well-known algorithms and proposes ways to optimize them.",
            "It describes and discusses factorsand strategies of factor interaction used in the algo rithms."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The factors used in the algorithms and the algorithms themselves are evaluated on a Germancorpus annotated with syntactic and coreference in formation (Negra) (Skut et al, 1997).": 0,
            "A commonformat for pronoun resolution algorithms with sev eral open parameters is proposed, and the parameter settings optimal on the evaluation data are given.": 0,
            "Optimizing Algorithms For Pronoun Resolution": 0,
            "The paper aims at a deeper understanding of sev eral well-known algorithms and proposes ways to optimize them.": 0,
            "It describes and discusses factorsand strategies of factor interaction used in the algo rithms.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.051945440948322166,
                0.11423234007106052,
                0.0602420050389642,
                0.06085096225481543
            ],
            [
                0.0,
                0.051945440948322166,
                1.0,
                0.2861149798843129,
                0.15088669328836155,
                0.0
            ],
            [
                0.0,
                0.11423234007106052,
                0.2861149798843129,
                1.0,
                0.06882559164676821,
                0.0
            ],
            [
                0.0,
                0.0602420050389642,
                0.15088669328836155,
                0.06882559164676821,
                1.0,
                0.0
            ],
            [
                0.0,
                0.06085096225481543,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "I05-6010": {
        "input_sentences": [
            "Abstract",
            "This article is devoted to the problem of quantifying noun groups in German.After a thorough description of the phenom ena, the results of corpus-based in vestigations are described.",
            "The information gained from cor pus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and extraction tool for German text corpora.",
            "We argue that a more sophisticatedand fine-grained annotation in the tree bank would have very positve effects onstochastic parsers trained on the tree bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source ofdata for theoretical linguistic investigations.",
            "Some remarks on the Annotation of Quantifying Noun Groups in Treebanks",
            "Moreover,some examples are given that under line the necessity of integrating somekind of information other than gram mar sensu stricto into the treebank."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This article is devoted to the problem of quantifying noun groups in German.After a thorough description of the phenom ena, the results of corpus-based in vestigations are described.": 0,
            "The information gained from cor pus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and extraction tool for German text corpora.": 0,
            "We argue that a more sophisticatedand fine-grained annotation in the tree bank would have very positve effects onstochastic parsers trained on the tree bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source ofdata for theoretical linguistic investigations.": 0,
            "Some remarks on the Annotation of Quantifying Noun Groups in Treebanks": 0,
            "Moreover,some examples are given that under line the necessity of integrating somekind of information other than gram mar sensu stricto into the treebank.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.04478733333808305,
                0.0,
                0.24304433017845503,
                0.0
            ],
            [
                0.0,
                0.04478733333808305,
                1.0,
                0.0,
                0.0,
                0.05096385333466977
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.05730294295912977,
                0.07368512903032723
            ],
            [
                0.0,
                0.24304433017845503,
                0.0,
                0.05730294295912977,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.05096385333466977,
                0.07368512903032723,
                0.0,
                1.0
            ]
        ]
    },
    "W04-0305": {
        "input_sentences": [
            "Abstract",
            "We simulate the effects of lookahead by summing probabilities over possible parses for the lookahead words and using this sum to choose which parse to pursue.",
            "Deterministic parsing takes the extreme position that there can only be one analysis for any sentence prefix.",
            "Lookahead In Deterministic Left-Corner Parsing",
            "One method which has been extensively used to address the difficulty of deterministic parsing is lookahead, where information about a bounded number of subsequent words is used to decide which analyses to pursue.",
            "This suggests that one word lookahead is sufficient, but that other modifications to our left-corner parsing model could make deterministic parsing more effective.",
            "We find that a large improvement is achieved with one word lookahead, but that more lookahead results in relatively small additional improvements.",
            "To support incremental interpretation, any model of human sentence processing must not only process the sentence incrementally, it must to some degree restrict the number of analyses which it produces for any sentence prefix.",
            "Experiments with an incremental statistical parser show that performance is severely degraded when the search for the most probable parse is pruned to only the most probable analysis after each prefix."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We simulate the effects of lookahead by summing probabilities over possible parses for the lookahead words and using this sum to choose which parse to pursue.": 0,
            "Deterministic parsing takes the extreme position that there can only be one analysis for any sentence prefix.": 0,
            "Lookahead In Deterministic Left-Corner Parsing": 0,
            "One method which has been extensively used to address the difficulty of deterministic parsing is lookahead, where information about a bounded number of subsequent words is used to decide which analyses to pursue.": 0,
            "This suggests that one word lookahead is sufficient, but that other modifications to our left-corner parsing model could make deterministic parsing more effective.": 0,
            "We find that a large improvement is achieved with one word lookahead, but that more lookahead results in relatively small additional improvements.": 0,
            "To support incremental interpretation, any model of human sentence processing must not only process the sentence incrementally, it must to some degree restrict the number of analyses which it produces for any sentence prefix.": 0,
            "Experiments with an incremental statistical parser show that performance is severely degraded when the search for the most probable parse is pruned to only the most probable analysis after each prefix.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.11760316533335397,
                0.14826467533279672,
                0.05914961724092664,
                0.11970005610921715,
                0.0,
                0.05270343158343205
            ],
            [
                0.0,
                0.0,
                1.0,
                0.21651535093427643,
                0.08726344605981,
                0.16334764585092973,
                0.0,
                0.256327068069446,
                0.135664873404954
            ],
            [
                0.0,
                0.11760316533335397,
                0.21651535093427643,
                1.0,
                0.18220948108590512,
                0.5842774817332688,
                0.1310294178321339,
                0.0,
                0.0
            ],
            [
                0.0,
                0.14826467533279672,
                0.08726344605981,
                0.18220948108590512,
                1.0,
                0.12441807299380131,
                0.052809551313124,
                0.08215620416191372,
                0.0
            ],
            [
                0.0,
                0.05914961724092664,
                0.16334764585092973,
                0.5842774817332688,
                0.12441807299380131,
                1.0,
                0.136024675337129,
                0.051262482165017616,
                0.0
            ],
            [
                0.0,
                0.11970005610921715,
                0.0,
                0.1310294178321339,
                0.052809551313124,
                0.136024675337129,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.256327068069446,
                0.0,
                0.08215620416191372,
                0.051262482165017616,
                0.0,
                1.0,
                0.07537934873748957
            ],
            [
                0.0,
                0.05270343158343205,
                0.135664873404954,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07537934873748957,
                1.0
            ]
        ]
    },
    "D11-1006": {
        "input_sentences": [
            "Abstract",
            "Multi-Source Transfer",
            "We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.",
            "Multi-Source Transfer of Delexicalized Dependency Parsers",
            "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.",
            "Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.",
            "We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.",
            "The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Multi-Source Transfer": 0,
            "We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.": 0,
            "Multi-Source Transfer of Delexicalized Dependency Parsers": 0,
            "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.": 0,
            "Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.": 0,
            "We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.": 0,
            "The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.7157790301227551,
                0.06733929748781982,
                0.07591586614177816,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.7157790301227551,
                0.0,
                1.0,
                0.17016928708362059,
                0.09691523025959264,
                0.22700437353621844,
                0.042720182109922705
            ],
            [
                0.0,
                0.06733929748781982,
                0.0,
                0.17016928708362059,
                1.0,
                0.12705360935929175,
                0.10330081403048082,
                0.06416319246089684
            ],
            [
                0.0,
                0.07591586614177816,
                0.0,
                0.09691523025959264,
                0.12705360935929175,
                1.0,
                0.1408439798515656,
                0.046348371507212036
            ],
            [
                0.0,
                0.0,
                0.0,
                0.22700437353621844,
                0.10330081403048082,
                0.1408439798515656,
                1.0,
                0.14132012903003177
            ],
            [
                0.0,
                0.0,
                0.0,
                0.042720182109922705,
                0.06416319246089684,
                0.046348371507212036,
                0.14132012903003177,
                1.0
            ]
        ]
    },
    "P03-1013": {
        "input_sentences": [
            "Abstract",
            "We present a probabilistic parsing model for German trained on the Negra treebank.",
            "Probabilistic Parsing For German Using Sister-Head Dependencies",
            "This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.",
            "We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.",
            "Learning curves show that this effect is not due to lack of training data.",
            "We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.",
            "This model outperforms the baseline, achieving a labeled precision and recall of up to 74%."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We present a probabilistic parsing model for German trained on the Negra treebank.": 0,
            "Probabilistic Parsing For German Using Sister-Head Dependencies": 0,
            "This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.": 0,
            "We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.": 0,
            "Learning curves show that this effect is not due to lack of training data.": 0,
            "We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.": 0,
            "This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.3718185312635713,
                0.10950250885650678,
                0.16075978798961865,
                0.0,
                0.052159587419998855,
                0.062350710967762096
            ],
            [
                0.0,
                0.3718185312635713,
                1.0,
                0.25752812935430786,
                0.4082615448952272,
                0.0,
                0.409062571132213,
                0.0
            ],
            [
                0.0,
                0.10950250885650678,
                0.25752812935430786,
                1.0,
                0.12218489015733434,
                0.0,
                0.2998831970307866,
                0.0
            ],
            [
                0.0,
                0.16075978798961865,
                0.4082615448952272,
                0.12218489015733434,
                1.0,
                0.0,
                0.30502045051065885,
                0.11128695042394539
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.052159587419998855,
                0.409062571132213,
                0.2998831970307866,
                0.30502045051065885,
                0.0,
                1.0,
                0.04734260216917138
            ],
            [
                0.0,
                0.062350710967762096,
                0.0,
                0.0,
                0.11128695042394539,
                0.0,
                0.04734260216917138,
                1.0
            ]
        ]
    },
    "W08-1007": {
        "input_sentences": [
            "Abstract",
            "Constituency representations are automatically transformed into dependency representations with complex arc labels, which makes it possible to recover the constituent structure with both constituent labels and grammatical functions.",
            "A Dependency-Driven Parser for German Dependency and Constituency Representations",
            "We report a labeled attachment score close to 90% for dependency versions of the TIGER and T\u00a8uBa- D/Z treebanks.",
            "We present a dependency-driven parser that parses both dependency structures and constituent structures.",
            "Moreover, the parser is able to recover both constituent labels and grammatical functions with an F-Score over 75% for T\u00a8uBa-D/Z and over 65% for TIGER."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Constituency representations are automatically transformed into dependency representations with complex arc labels, which makes it possible to recover the constituent structure with both constituent labels and grammatical functions.": 0,
            "A Dependency-Driven Parser for German Dependency and Constituency Representations": 0,
            "We report a labeled attachment score close to 90% for dependency versions of the TIGER and T\u00a8uBa- D/Z treebanks.": 0,
            "We present a dependency-driven parser that parses both dependency structures and constituent structures.": 0,
            "Moreover, the parser is able to recover both constituent labels and grammatical functions with an F-Score over 75% for T\u00a8uBa-D/Z and over 65% for TIGER.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.2950956911863551,
                0.027614342441923078,
                0.13280195503447123,
                0.3524963271995955
            ],
            [
                0.0,
                0.2950956911863551,
                1.0,
                0.10384232280314776,
                0.38442698188907554,
                0.07352133607704488
            ],
            [
                0.0,
                0.027614342441923078,
                0.10384232280314776,
                1.0,
                0.07649104698908321,
                0.22387932532515836
            ],
            [
                0.0,
                0.13280195503447123,
                0.38442698188907554,
                0.07649104698908321,
                1.0,
                0.10831275381291736
            ],
            [
                0.0,
                0.3524963271995955,
                0.07352133607704488,
                0.22387932532515836,
                0.10831275381291736,
                1.0
            ]
        ]
    },
    "P11-1086": {
        "input_sentences": [
            "Abstract",
            "Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both trainand decoding Here, we take the approach, where we only use minrules that cannot be formed out other rules), and instead rely on a model the derivation history to capture dependencies between minimal rules.",
            "Most statistical machine translation systems on rules that can be formed out of smaller rules in the grammar).",
            "Rule Markov Models for Fast Tree-to-String Translation",
            "Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both trainand decoding Here, we take the approach, where we only use minrules that cannot be formed out other rules), and instead rely on a model the derivation history to capture dependencies between minimal rules.": 0,
            "Most statistical machine translation systems on rules that can be formed out of smaller rules in the grammar).": 0,
            "Rule Markov Models for Fast Tree-to-String Translation": 0,
            "Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.2090169704736544,
                0.05063591513359127,
                0.18715243666164447
            ],
            [
                0.0,
                0.2090169704736544,
                1.0,
                0.04805860017349097,
                0.13210411877103456
            ],
            [
                0.0,
                0.05063591513359127,
                0.04805860017349097,
                1.0,
                0.19629642189627783
            ],
            [
                0.0,
                0.18715243666164447,
                0.13210411877103456,
                0.19629642189627783,
                1.0
            ]
        ]
    },
    "N06-2033": {
        "input_sentences": [
            "Abstract",
            "We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.",
            "Parser Combination By Reparsing",
            "We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.": 0,
            "Parser Combination By Reparsing": 0,
            "We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.421697442660751,
                0.05200909993820063
            ],
            [
                0.0,
                0.421697442660751,
                1.0,
                0.0
            ],
            [
                0.0,
                0.05200909993820063,
                0.0,
                1.0
            ]
        ]
    },
    "W10-2924": {
        "input_sentences": [
            "Abstract",
            "Experimental results show improved results for our joint extraction method compared to a pipelined approach.",
            "Both entity and relation extraction can benefit from being performed jointly, allowing each task to correct the errors of the other.",
            "We present a new method for joint entity and relation extraction using a graph we call a \u201ccard-pyramid.\u201d This graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes.",
            "Joint Entity and Relation Extraction Using Card-Pyramid Parsing",
            "We give an efficient labeling algorithm that is analogous to parsing using dynamic programming."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Experimental results show improved results for our joint extraction method compared to a pipelined approach.": 0,
            "Both entity and relation extraction can benefit from being performed jointly, allowing each task to correct the errors of the other.": 0,
            "We present a new method for joint entity and relation extraction using a graph we call a \u201ccard-pyramid.\u201d This graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes.": 0,
            "Joint Entity and Relation Extraction Using Card-Pyramid Parsing": 0,
            "We give an efficient labeling algorithm that is analogous to parsing using dynamic programming.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0392495227366451,
                0.1508898425400733,
                0.12388408452469882,
                0.0
            ],
            [
                0.0,
                0.0392495227366451,
                1.0,
                0.2276446623217787,
                0.22878237316672978,
                0.0
            ],
            [
                0.0,
                0.1508898425400733,
                0.2276446623217787,
                1.0,
                0.4496818676792591,
                0.09233785020701715
            ],
            [
                0.0,
                0.12388408452469882,
                0.22878237316672978,
                0.4496818676792591,
                1.0,
                0.21294910693470254
            ],
            [
                0.0,
                0.0,
                0.0,
                0.09233785020701715,
                0.21294910693470254,
                1.0
            ]
        ]
    },
    "P11-1141": {
        "input_sentences": [
            "Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation",
            "Abstract",
            "annotation of a large corpus.",
            "Chinese word segmentation as tagging.",
            "2007.",
            "Linguistics and Language 8(1):29\u201348.",
            "Chinese segmentawith a word-based perceptron algorithm.",
            "In Pro",
            "2003.",
            "Yue Zhang and Stephen Clark.",
            "Nianwen Xue.",
            "Lan- 11(2):207\u2013238."
        ],
        "authority_scores": {
            "Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation": 0,
            "Abstract": 0,
            "annotation of a large corpus.": 0,
            "Chinese word segmentation as tagging.": 0,
            "2007.": 0,
            "Linguistics and Language 8(1):29\u201348.": 0,
            "Chinese segmentawith a word-based perceptron algorithm.": 0,
            "In Pro": 0,
            "2003.": 0,
            "Yue Zhang and Stephen Clark.": 0,
            "Nianwen Xue.": 0,
            "Lan- 11(2):207\u2013238.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.39563845723309354,
                0.0,
                0.0,
                0.18057147447676078,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.39563845723309354,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.2984039655299266,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.18057147447676078,
                0.0,
                0.0,
                0.2984039655299266,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P12-1051": {
        "input_sentences": [
            "Abstract",
            "However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata.",
            "Semantic Parsing with Bayesian Tree Transducers",
            "This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions.",
            "Many semantic parsing models use tree transformations to map between natural language and meaning representation.",
            "In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers."
        ],
        "authority_scores": {
            "Abstract": 0,
            "However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata.": 0,
            "Semantic Parsing with Bayesian Tree Transducers": 0,
            "This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions.": 0,
            "Many semantic parsing models use tree transformations to map between natural language and meaning representation.": 0,
            "In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.11064477150438191,
                0.04298808623367016,
                0.21204318984740228,
                0.1562010489902421
            ],
            [
                0.0,
                0.11064477150438191,
                1.0,
                0.16245218974733183,
                0.22497261755868855,
                0.4346476680398911
            ],
            [
                0.0,
                0.04298808623367016,
                0.16245218974733183,
                1.0,
                0.14822576276796842,
                0.0989091440295846
            ],
            [
                0.0,
                0.21204318984740228,
                0.22497261755868855,
                0.14822576276796842,
                1.0,
                0.0885314027338105
            ],
            [
                0.0,
                0.1562010489902421,
                0.4346476680398911,
                0.0989091440295846,
                0.0885314027338105,
                1.0
            ]
        ]
    },
    "E12-1083": {
        "input_sentences": [
            "Abstract",
            "We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance.",
            "In this paper we analyze patents along the orthogonal dimensions of topic and textual structure.",
            "We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning.",
            "A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs.",
            "Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents.",
            "Structural and Topical Dimensions in Multi-Task Patent Translation",
            "We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance.": 0,
            "In this paper we analyze patents along the orthogonal dimensions of topic and textual structure.": 0,
            "We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning.": 0,
            "A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs.": 0,
            "Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents.": 0,
            "Structural and Topical Dimensions in Multi-Task Patent Translation": 0,
            "We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.17979234701731572,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.17979234701731572,
                1.0,
                0.0,
                0.13833197386044702,
                0.05348671194312779,
                0.1531750648265433,
                0.1586057687388173,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.15596000601430007,
                0.12111708590308116,
                0.0
            ],
            [
                0.0,
                0.13833197386044702,
                0.0,
                1.0,
                0.0,
                0.034738942161562855,
                0.21840121435715656,
                0.17860485219305583
            ],
            [
                0.0,
                0.05348671194312779,
                0.0,
                0.0,
                1.0,
                0.07379389645743116,
                0.05730765165229773,
                0.0
            ],
            [
                0.0,
                0.1531750648265433,
                0.15596000601430007,
                0.034738942161562855,
                0.07379389645743116,
                1.0,
                0.16411745904723757,
                0.0
            ],
            [
                0.0,
                0.1586057687388173,
                0.12111708590308116,
                0.21840121435715656,
                0.05730765165229773,
                0.16411745904723757,
                1.0,
                0.07630725086791272
            ],
            [
                0.0,
                0.0,
                0.0,
                0.17860485219305583,
                0.0,
                0.0,
                0.07630725086791272,
                1.0
            ]
        ]
    },
    "P07-1108": {
        "input_sentences": [
            "Abstract",
            "Pivot Language Approach for Phrase-Based Statistical Machine Translation",
            "Using only bilingual corpora, we can build a model for The advantage of this method lies in that we can perform between even if there is no bilingual corpus available for this language pair.",
            "To conduct transbetween languages with a small bilingual corpus, we bring in a third which is named the lan- For and there exist bilingual corpora.",
            "Moreover, with small bilingual corpus available, our method can further improve the translaquality by using the additional bilingual corpora.",
            "This paper proposes a novel method for phrase-based statistical machine translation by using pivot language.",
            "Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly with 5,000 sentence pairs for French-Spanish translation."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Pivot Language Approach for Phrase-Based Statistical Machine Translation": 0,
            "Using only bilingual corpora, we can build a model for The advantage of this method lies in that we can perform between even if there is no bilingual corpus available for this language pair.": 0,
            "To conduct transbetween languages with a small bilingual corpus, we bring in a third which is named the lan- For and there exist bilingual corpora.": 0,
            "Moreover, with small bilingual corpus available, our method can further improve the translaquality by using the additional bilingual corpora.": 0,
            "This paper proposes a novel method for phrase-based statistical machine translation by using pivot language.": 0,
            "Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly with 5,000 sentence pairs for French-Spanish translation.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.051551344701645026,
                0.0,
                0.0,
                0.6498636478019028,
                0.14086668678521924
            ],
            [
                0.0,
                0.051551344701645026,
                1.0,
                0.28434412269465,
                0.481978926799583,
                0.12476563003651418,
                0.1297229761806556
            ],
            [
                0.0,
                0.0,
                0.28434412269465,
                1.0,
                0.3968824529869927,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.481978926799583,
                0.3968824529869927,
                1.0,
                0.0945327718706238,
                0.061229568020325184
            ],
            [
                0.0,
                0.6498636478019028,
                0.12476563003651418,
                0.0,
                0.0945327718706238,
                1.0,
                0.17585700429068854
            ],
            [
                0.0,
                0.14086668678521924,
                0.1297229761806556,
                0.0,
                0.061229568020325184,
                0.17585700429068854,
                1.0
            ]
        ]
    },
    "W03-1509": {
        "input_sentences": [
            "Abstract",
            "Chinese Named Entity Recognition Combining Statistical Model Wih Human Knowledge",
            "In this paper, we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well.",
            "In order to avoid data sparseness problem, we employ a back-off model and YI CI CI LIN a Chinese thesaurus, to smooth the parameters in the model.",
            "Named Entity Recognition is one of the key techniques in the fields of natural language processing, information retrieval, question answering and so on.",
            "The F-measure of person names, location names, and organization names on the newswire test data for the 1999 IEER evaluation in Mandarin is 86.84%, 84.40% and 76.22% respectively.",
            "Unfortunately, Chinese Named Entity Recognition (NER) is more difficult for the lack of capitalization information and the uncertainty in word segmentation."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Chinese Named Entity Recognition Combining Statistical Model Wih Human Knowledge": 0,
            "In this paper, we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well.": 0,
            "In order to avoid data sparseness problem, we employ a back-off model and YI CI CI LIN a Chinese thesaurus, to smooth the parameters in the model.": 0,
            "Named Entity Recognition is one of the key techniques in the fields of natural language processing, information retrieval, question answering and so on.": 0,
            "The F-measure of person names, location names, and organization names on the newswire test data for the 1999 IEER evaluation in Mandarin is 86.84%, 84.40% and 76.22% respectively.": 0,
            "Unfortunately, Chinese Named Entity Recognition (NER) is more difficult for the lack of capitalization information and the uncertainty in word segmentation.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.294507325376498,
                0.141894506093931,
                0.17587815969144008,
                0.0,
                0.2398825811051145
            ],
            [
                0.0,
                0.294507325376498,
                1.0,
                0.07135947315663532,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.141894506093931,
                0.07135947315663532,
                1.0,
                0.0,
                0.031013120517974693,
                0.037098015096972484
            ],
            [
                0.0,
                0.17587815969144008,
                0.0,
                0.0,
                1.0,
                0.0,
                0.20088527213385274
            ],
            [
                0.0,
                0.0,
                0.0,
                0.031013120517974693,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.2398825811051145,
                0.0,
                0.037098015096972484,
                0.20088527213385274,
                0.0,
                1.0
            ]
        ]
    },
    "P05-1053": {
        "input_sentences": [
            "Abstract",
            "This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.",
            "Extracting semantic relationships between entities is challenging.",
            "Exploring Various Knowledge In Relation Extraction",
            "We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.",
            "Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.",
            "This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.",
            "Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.": 0,
            "Extracting semantic relationships between entities is challenging.": 0,
            "Exploring Various Knowledge In Relation Extraction": 0,
            "We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.": 0,
            "Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.": 0,
            "This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.": 0,
            "Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.11194152654762883,
                0.1333715905241887,
                0.030407139610294884,
                0.06359552574448078,
                0.19830202329854696
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.08415952793235285,
                0.0,
                0.07729434342639083,
                0.0
            ],
            [
                0.0,
                0.11194152654762883,
                0.0,
                1.0,
                0.10692327484989803,
                0.04695325228467944,
                0.22033626247104932,
                0.07475895451529736
            ],
            [
                0.0,
                0.1333715905241887,
                0.08415952793235285,
                0.10692327484989803,
                1.0,
                0.05945966277921798,
                0.24880644527048654,
                0.18941229760181924
            ],
            [
                0.0,
                0.030407139610294884,
                0.0,
                0.04695325228467944,
                0.05945966277921798,
                1.0,
                0.12963891536350372,
                0.04886648426737964
            ],
            [
                0.0,
                0.06359552574448078,
                0.07729434342639083,
                0.22033626247104932,
                0.24880644527048654,
                0.12963891536350372,
                1.0,
                0.09529450552994968
            ],
            [
                0.0,
                0.19830202329854696,
                0.0,
                0.07475895451529736,
                0.18941229760181924,
                0.04886648426737964,
                0.09529450552994968,
                1.0
            ]
        ]
    },
    "E12-3010": {
        "input_sentences": [
            "Abstract",
            "In this study we quantify and compare the occurrence of this phenomenon in these two languages.",
            "Null subjects are non overtly expressed pronouns found in languages such as Italian and Spanish.",
            "Improving machine translation of null subjects in Italian and Spanish",
            "A second evaluation of the improved Its-2 system shows an average increase of 15.46% in correct pro-drop translations for Italian-French and 12.80% for",
            "Next, we evaluate null subjects\u2019 translation into French, a \u201cnon prodrop\u201d language.",
            "We use the Europarl corpus to evaluate two MT systems on their performance regarding null subject translation: Its-2, a rule-based system developed at LATL, and a statistical system built using the Moses toolkit.",
            "Then we add a rule-based preprocessor and a statistical post-editor to the Its-2 translation pipeline."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this study we quantify and compare the occurrence of this phenomenon in these two languages.": 0,
            "Null subjects are non overtly expressed pronouns found in languages such as Italian and Spanish.": 0,
            "Improving machine translation of null subjects in Italian and Spanish": 0,
            "A second evaluation of the improved Its-2 system shows an average increase of 15.46% in correct pro-drop translations for Italian-French and 12.80% for": 0,
            "Next, we evaluate null subjects\u2019 translation into French, a \u201cnon prodrop\u201d language.": 0,
            "We use the Europarl corpus to evaluate two MT systems on their performance regarding null subject translation: Its-2, a rule-based system developed at LATL, and a statistical system built using the Moses toolkit.": 0,
            "Then we add a rule-based preprocessor and a statistical post-editor to the Its-2 translation pipeline.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.11488098614209637,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.11488098614209637,
                1.0,
                0.3936504715828255,
                0.052351717999743946,
                0.2726732116124826,
                0.03741723203110001,
                0.0
            ],
            [
                0.0,
                0.0,
                0.3936504715828255,
                1.0,
                0.06282009167907142,
                0.2668187883990874,
                0.08979854096029044,
                0.06876496230542274
            ],
            [
                0.0,
                0.0,
                0.052351717999743946,
                0.06282009167907142,
                1.0,
                0.07721738524252585,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.2726732116124826,
                0.2668187883990874,
                0.07721738524252585,
                1.0,
                0.15398291999224553,
                0.06293942078944206
            ],
            [
                0.0,
                0.0,
                0.03741723203110001,
                0.08979854096029044,
                0.0,
                0.15398291999224553,
                1.0,
                0.21817835747264783
            ],
            [
                0.0,
                0.0,
                0.0,
                0.06876496230542274,
                0.0,
                0.06293942078944206,
                0.21817835747264783,
                1.0
            ]
        ]
    },
    "W11-0314": {
        "input_sentences": [
            "Abstract",
            "ULISSE: an Unsupervised Algorithm for Detecting Reliable Dependency Parses",
            "In all cases, ULISSE appears to outperform the baseline algorithms.",
            "In this paper we present ULISSE, an unsupervised linguistically\u2013driven algorithm to select reliable parses from the output of a dependency parser.",
            "Different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains."
        ],
        "authority_scores": {
            "Abstract": 0,
            "ULISSE: an Unsupervised Algorithm for Detecting Reliable Dependency Parses": 0,
            "In all cases, ULISSE appears to outperform the baseline algorithms.": 0,
            "In this paper we present ULISSE, an unsupervised linguistically\u2013driven algorithm to select reliable parses from the output of a dependency parser.": 0,
            "Different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09057264623870495,
                0.5179010824951867,
                0.04219919367163024
            ],
            [
                0.0,
                0.09057264623870495,
                1.0,
                0.06030721518669817,
                0.0
            ],
            [
                0.0,
                0.5179010824951867,
                0.06030721518669817,
                1.0,
                0.06887603746201743
            ],
            [
                0.0,
                0.04219919367163024,
                0.0,
                0.06887603746201743,
                1.0
            ]
        ]
    },
    "D07-1015": {
        "input_sentences": [
            "Abstract",
            "The new training methods give improvements in accuracy over perceptron-trained models.",
            "To demonstrate an application of the method, we perform experiments which use the algorithm in training both log-linear and max-margin dependency parsers.",
            "This paper provides an algorithmic framework for learning statistical models involv ing directed spanning trees, or equivalently non-projective dependency structures.",
            "We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff?s Matrix-Tree Theorem.",
            "Structured Prediction Models via the Matrix-Tree Theorem"
        ],
        "authority_scores": {
            "Abstract": 0,
            "The new training methods give improvements in accuracy over perceptron-trained models.": 0,
            "To demonstrate an application of the method, we perform experiments which use the algorithm in training both log-linear and max-margin dependency parsers.": 0,
            "This paper provides an algorithmic framework for learning statistical models involv ing directed spanning trees, or equivalently non-projective dependency structures.": 0,
            "We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff?s Matrix-Tree Theorem.": 0,
            "Structured Prediction Models via the Matrix-Tree Theorem": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06883048177005709,
                0.04601725000037768,
                0.0,
                0.0845198019219617
            ],
            [
                0.0,
                0.06883048177005709,
                1.0,
                0.04726152402073329,
                0.0,
                0.0
            ],
            [
                0.0,
                0.04601725000037768,
                0.04726152402073329,
                1.0,
                0.16350725371004113,
                0.058034384563905965
            ],
            [
                0.0,
                0.0,
                0.0,
                0.16350725371004113,
                1.0,
                0.3003134845359771
            ],
            [
                0.0,
                0.0845198019219617,
                0.0,
                0.058034384563905965,
                0.3003134845359771,
                1.0
            ]
        ]
    },
    "P13-1109": {
        "input_sentences": [
            "Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation",
            "Abstract",
            "Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data.",
            "Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.",
            "Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations.",
            "In this paper, we propose a novel approach to finding translations for oov words.",
            "We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases."
        ],
        "authority_scores": {
            "Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation": 0,
            "Abstract": 0,
            "Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data.": 0,
            "Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.": 0,
            "Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations.": 0,
            "In this paper, we propose a novel approach to finding translations for oov words.": 0,
            "We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.2921966878345477,
                0.09912988426080624,
                0.12074337252373656,
                0.07094755479364064,
                0.13072437929836148
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.2921966878345477,
                0.0,
                1.0,
                0.0,
                0.08208822392113525,
                0.07699143100647735,
                0.06386632992358397
            ],
            [
                0.09912988426080624,
                0.0,
                0.0,
                1.0,
                0.06549493218457127,
                0.0,
                0.08040278761449855
            ],
            [
                0.12074337252373656,
                0.0,
                0.08208822392113525,
                0.06549493218457127,
                1.0,
                0.16696381525185527,
                0.4689029617126491
            ],
            [
                0.07094755479364064,
                0.0,
                0.07699143100647735,
                0.0,
                0.16696381525185527,
                1.0,
                0.050893848160870545
            ],
            [
                0.13072437929836148,
                0.0,
                0.06386632992358397,
                0.08040278761449855,
                0.4689029617126491,
                0.050893848160870545,
                1.0
            ]
        ]
    },
    "P05-1013": {
        "input_sentences": [
            "Abstract",
            "We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.",
            "This leads to the best reported performance for robust non-projective parsing of Czech.",
            "In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures.",
            "Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.",
            "Pseudo-Projective Dependency Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.": 0,
            "This leads to the best reported performance for robust non-projective parsing of Czech.": 0,
            "In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures.": 0,
            "Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.": 0,
            "Pseudo-Projective Dependency Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.11970540867827117,
                0.2735468919984433,
                0.06863792038585721,
                0.19967507121792194
            ],
            [
                0.0,
                0.11970540867827117,
                1.0,
                0.13780197977899936,
                0.032450489372445726,
                0.18317317589198318
            ],
            [
                0.0,
                0.2735468919984433,
                0.13780197977899936,
                1.0,
                0.08144321369095367,
                0.30648150243898925
            ],
            [
                0.0,
                0.06863792038585721,
                0.032450489372445726,
                0.08144321369095367,
                1.0,
                0.12131451390225402
            ],
            [
                0.0,
                0.19967507121792194,
                0.18317317589198318,
                0.30648150243898925,
                0.12131451390225402,
                1.0
            ]
        ]
    },
    "W05-1505": {
        "input_sentences": [
            "Abstract",
            "Analysis of the types of dependency errors made by these parsers on a Czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser.",
            "Corrective Modeling For Non-Projective Dependency Parsing",
            "Corrective Modeling",
            "We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers.",
            "The continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees.",
            "Our model, based on a MaxEnt classifier, improves overall dependency accuracy by .7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Analysis of the types of dependency errors made by these parsers on a Czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser.": 0,
            "Corrective Modeling For Non-Projective Dependency Parsing": 0,
            "Corrective Modeling": 0,
            "We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers.": 0,
            "The continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees.": 0,
            "Our model, based on a MaxEnt classifier, improves overall dependency accuracy by .7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.04068803006005379,
                0.0,
                0.06080328261400608,
                0.06964197987329572,
                0.019500210770886643
            ],
            [
                0.0,
                0.04068803006005379,
                1.0,
                0.6064243627418964,
                0.26236638753320074,
                0.20312743135865047,
                0.15521218761793484
            ],
            [
                0.0,
                0.0,
                0.6064243627418964,
                1.0,
                0.1401974069492356,
                0.0,
                0.0
            ],
            [
                0.0,
                0.06080328261400608,
                0.26236638753320074,
                0.1401974069492356,
                1.0,
                0.23754525591804326,
                0.2523031455935222
            ],
            [
                0.0,
                0.06964197987329572,
                0.20312743135865047,
                0.0,
                0.23754525591804326,
                1.0,
                0.09735117967117567
            ],
            [
                0.0,
                0.019500210770886643,
                0.15521218761793484,
                0.0,
                0.2523031455935222,
                0.09735117967117567,
                1.0
            ]
        ]
    },
    "C10-1151": {
        "input_sentences": [
            "Abstract",
            "In this paper, we focus on the challenge of con stituent syntactic parsing with treebanksof different annotations and propose a collaborative decoding (or co-decoding) ap proach to improve parsing accuracy byleveraging bracket structure consensus be tween multiple parsing decoders trainedon individual treebanks.",
            "However, such corpora are generally used independently due to distinctions in annotation standards.",
            "There often exist multiple corpora for the same natural language processing (NLP)tasks.",
            "For the purpose of full use of readily available human annotations, it is significant to simultaneously utilize multiple corpora of different annotation standards.",
            "Heterogeneous Parsing via Collaborative Decoding",
            "Experimental results show the effectiveness of the proposed approach, which outperforms state of-the-art baselines, especially on long sentences."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we focus on the challenge of con stituent syntactic parsing with treebanksof different annotations and propose a collaborative decoding (or co-decoding) ap proach to improve parsing accuracy byleveraging bracket structure consensus be tween multiple parsing decoders trainedon individual treebanks.": 0,
            "However, such corpora are generally used independently due to distinctions in annotation standards.": 0,
            "There often exist multiple corpora for the same natural language processing (NLP)tasks.": 0,
            "For the purpose of full use of readily available human annotations, it is significant to simultaneously utilize multiple corpora of different annotation standards.": 0,
            "Heterogeneous Parsing via Collaborative Decoding": 0,
            "Experimental results show the effectiveness of the proposed approach, which outperforms state of-the-art baselines, especially on long sentences.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.033871194581715725,
                0.0977011401009634,
                0.4204190427688553,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.07842143773490254,
                0.22620589470418923,
                0.0,
                0.0
            ],
            [
                0.0,
                0.033871194581715725,
                0.07842143773490254,
                1.0,
                0.11090466082549635,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0977011401009634,
                0.22620589470418923,
                0.11090466082549635,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.4204190427688553,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "D07-1111": {
        "input_sentences": [
            "Abstract",
            "We apply this pars ing framework to both tracks of the CoNLL 2007 shared task, in each case taking ad vantage of multiple models trained with different learners.",
            "Parser actions are determined by a classifier, based on features that represent the current state of the parser.",
            "In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme.",
            "In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.",
            "Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles",
            "We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We apply this pars ing framework to both tracks of the CoNLL 2007 shared task, in each case taking ad vantage of multiple models trained with different learners.": 0,
            "Parser actions are determined by a classifier, based on features that represent the current state of the parser.": 0,
            "In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme.": 0,
            "In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.": 0,
            "Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles": 0,
            "We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.024104350904694784,
                0.019260060654471813,
                0.03943512092845111,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.18203713615279954,
                0.0
            ],
            [
                0.0,
                0.024104350904694784,
                0.0,
                1.0,
                0.09847448192609816,
                0.10128742431762353,
                0.06247129230653035
            ],
            [
                0.0,
                0.019260060654471813,
                0.0,
                0.09847448192609816,
                1.0,
                0.28742705179141836,
                0.03415996070482918
            ],
            [
                0.0,
                0.03943512092845111,
                0.18203713615279954,
                0.10128742431762353,
                0.28742705179141836,
                1.0,
                0.38197519747963804
            ],
            [
                0.0,
                0.0,
                0.0,
                0.06247129230653035,
                0.03415996070482918,
                0.38197519747963804,
                1.0
            ]
        ]
    },
    "W99-0623": {
        "input_sentences": [
            "Abstract",
            "The resulting parsers surpass the best previously published performance results for the Penn Treebank.",
            "Exploiting Diversity in Natural Language Processing: Combining Parsers",
            "Two general approaches are presented and two combination techniques are described for each approach.",
            "Both parametric and non-parametric models are explored.",
            "Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The resulting parsers surpass the best previously published performance results for the Penn Treebank.": 0,
            "Exploiting Diversity in Natural Language Processing: Combining Parsers": 0,
            "Two general approaches are presented and two combination techniques are described for each approach.": 0,
            "Both parametric and non-parametric models are explored.": 0,
            "Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06224277704893783,
                0.0,
                0.0,
                0.10497944026737055
            ],
            [
                0.0,
                0.06224277704893783,
                1.0,
                0.0,
                0.0,
                0.05192171700553539
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.10497944026737055,
                0.05192171700553539,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P09-1111": {
        "input_sentences": [
            "Abstract",
            "An Optimal-Time Binarization Algorithm for Linear Context-Free Rewriting Systems with Fan-Out Two"
        ],
        "authority_scores": {
            "Abstract": 0,
            "An Optimal-Time Binarization Algorithm for Linear Context-Free Rewriting Systems with Fan-Out Two": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0
            ],
            [
                0.0,
                1.0
            ]
        ]
    },
    "W11-0115": {
        "input_sentences": [
            "Abstract",
            "In brief, distributional semantic spaces containing representations for complex constructions such as Adjective-Noun and Verb-Noun pairs, as well as for their constituent parts, are built.",
            "This article introduces and evaluates an approach to semantic compositionality in computational linguistics based on the combination of Distributional Semantics and supervised Machine Learning.",
            "In particular, the distributional semantic representations of the constituents are used to predict those of the complex structures.",
            "Computing Semantic Compositionality in Distributional Semantics",
            "In a second experimental setting based on Verb-Noun pairs, a comparatively much lower performance was obtained by all the models; however, the proposed approach gives the best results in combination with a Random Indexing semantic space.",
            "This approach outperforms the rivals in a series of experiments with Adjective-Noun pairs extracted from the BNC.",
            "These representations are then used as feature vectors in a supervised learning model using multivariate multiple regression."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In brief, distributional semantic spaces containing representations for complex constructions such as Adjective-Noun and Verb-Noun pairs, as well as for their constituent parts, are built.": 0,
            "This article introduces and evaluates an approach to semantic compositionality in computational linguistics based on the combination of Distributional Semantics and supervised Machine Learning.": 0,
            "In particular, the distributional semantic representations of the constituents are used to predict those of the complex structures.": 0,
            "Computing Semantic Compositionality in Distributional Semantics": 0,
            "In a second experimental setting based on Verb-Noun pairs, a comparatively much lower performance was obtained by all the models; however, the proposed approach gives the best results in combination with a Random Indexing semantic space.": 0,
            "This approach outperforms the rivals in a series of experiments with Adjective-Noun pairs extracted from the BNC.": 0,
            "These representations are then used as feature vectors in a supervised learning model using multivariate multiple regression.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.058849673523683706,
                0.20930072640699543,
                0.11272604851262649,
                0.16484784926581136,
                0.2193630318473366,
                0.046811394527632604
            ],
            [
                0.0,
                0.058849673523683706,
                1.0,
                0.08219436255268095,
                0.3548307665969746,
                0.15206390379764984,
                0.053732057617199014,
                0.13375122988269428
            ],
            [
                0.0,
                0.20930072640699543,
                0.08219436255268095,
                1.0,
                0.15744260156089224,
                0.028043130806923333,
                0.0,
                0.1531839387811031
            ],
            [
                0.0,
                0.11272604851262649,
                0.3548307665969746,
                0.15744260156089224,
                1.0,
                0.04091324873540788,
                0.0,
                0.0
            ],
            [
                0.0,
                0.16484784926581136,
                0.15206390379764984,
                0.028043130806923333,
                0.04091324873540788,
                1.0,
                0.12518833573055527,
                0.0
            ],
            [
                0.0,
                0.2193630318473366,
                0.053732057617199014,
                0.0,
                0.0,
                0.12518833573055527,
                1.0,
                0.0
            ],
            [
                0.0,
                0.046811394527632604,
                0.13375122988269428,
                0.1531839387811031,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P07-1021": {
        "input_sentences": [
            "Abstract",
            "In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages.",
            "In previous work, several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense; the best-known but also most restrictive of these is projectivity.",
            "Mildly Context-Sensitive Dependency Languages",
            "Most constraints are formulated on fully specified structures, which makes them hard to integrate into models where structures are composed from lexical information.",
            "Dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages.": 0,
            "In previous work, several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense; the best-known but also most restrictive of these is projectivity.": 0,
            "Mildly Context-Sensitive Dependency Languages": 0,
            "Most constraints are formulated on fully specified structures, which makes them hard to integrate into models where structures are composed from lexical information.": 0,
            "Dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06732953170523145,
                0.4031535327849477,
                0.0,
                0.023149245593213477
            ],
            [
                0.0,
                0.06732953170523145,
                1.0,
                0.05738032074855085,
                0.15691165239375132,
                0.028474082175941422
            ],
            [
                0.0,
                0.4031535327849477,
                0.05738032074855085,
                1.0,
                0.0,
                0.05742042103240571
            ],
            [
                0.0,
                0.0,
                0.15691165239375132,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.023149245593213477,
                0.028474082175941422,
                0.05742042103240571,
                0.0,
                1.0
            ]
        ]
    },
    "P10-1074": {
        "input_sentences": [
            "Abstract",
            "Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model.",
            "Experiments on joint parsing and named entity recognition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.",
            "Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data",
            "Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data.",
            "One of the main obstacles to producing high quality joint models is the lack of jointly annotated data.",
            "In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model.": 0,
            "Experiments on joint parsing and named entity recognition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.": 0,
            "Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data": 0,
            "Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data.": 0,
            "One of the main obstacles to producing high quality joint models is the lack of jointly annotated data.": 0,
            "In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.234560984570071,
                0.0950959075658468,
                0.23606500225390253,
                0.07730985870855739,
                0.2803254044019043
            ],
            [
                0.0,
                0.234560984570071,
                1.0,
                0.44515497710369695,
                0.07516682640580102,
                0.16220192512535153,
                0.23986852343246856
            ],
            [
                0.0,
                0.0950959075658468,
                0.44515497710369695,
                1.0,
                0.06430736307180662,
                0.14808350237781195,
                0.07186543956174153
            ],
            [
                0.0,
                0.23606500225390253,
                0.07516682640580102,
                0.06430736307180662,
                1.0,
                0.1453344125881527,
                0.22696817236890393
            ],
            [
                0.0,
                0.07730985870855739,
                0.16220192512535153,
                0.14808350237781195,
                0.1453344125881527,
                1.0,
                0.09235793205459515
            ],
            [
                0.0,
                0.2803254044019043,
                0.23986852343246856,
                0.07186543956174153,
                0.22696817236890393,
                0.09235793205459515,
                1.0
            ]
        ]
    },
    "N06-1022": {
        "input_sentences": [
            "Abstract",
            "We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG.",
            "We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results.",
            "Multilevel Coarse-To-Fine PCFG Parsing",
            "We use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level.",
            "Our approach requires the user to specify a sequence of nested partitions or equivalence classes of the PCFG nonterminals.",
            "We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG.",
            "We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG.": 0,
            "We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results.": 0,
            "Multilevel Coarse-To-Fine PCFG Parsing": 0,
            "We use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level.": 0,
            "Our approach requires the user to specify a sequence of nested partitions or equivalence classes of the PCFG nonterminals.": 0,
            "We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG.": 0,
            "We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.11214221239329034,
                0.05284326055021944,
                0.26261350735121736,
                0.08972044505605271,
                0.05597394609332713
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.05206835143420443,
                0.0,
                0.04660172965171926,
                0.1844529865655483
            ],
            [
                0.0,
                0.11214221239329034,
                0.0,
                1.0,
                0.06017532411358476,
                0.060126824556552064,
                0.1499141447041733,
                0.4991335991929846
            ],
            [
                0.0,
                0.05284326055021944,
                0.05206835143420443,
                0.06017532411358476,
                1.0,
                0.0,
                0.04553054391800718,
                0.030035526107417953
            ],
            [
                0.0,
                0.26261350735121736,
                0.0,
                0.060126824556552064,
                0.0,
                1.0,
                0.02274692381344717,
                0.03001131834895696
            ],
            [
                0.0,
                0.08972044505605271,
                0.04660172965171926,
                0.1499141447041733,
                0.04553054391800718,
                0.02274692381344717,
                1.0,
                0.16875053582416993
            ],
            [
                0.0,
                0.05597394609332713,
                0.1844529865655483,
                0.4991335991929846,
                0.030035526107417953,
                0.03001131834895696,
                0.16875053582416993,
                1.0
            ]
        ]
    },
    "W12-2429": {
        "input_sentences": [
            "Abstract",
            "However, these require manually labeled training examples which are expensive to create and consequently supervised WSD systems are normally limited to disambiguating a small set of ambiguous terms.",
            "An alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones.",
            "The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus.",
            "The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches).",
            "This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus.",
            "The most accurate approaches to Word Sense Disambiguation (WSD) for biomedical documents are based on supervised learning.",
            "Scaling up WSD with Automatically Generated Examples"
        ],
        "authority_scores": {
            "Abstract": 0,
            "However, these require manually labeled training examples which are expensive to create and consequently supervised WSD systems are normally limited to disambiguating a small set of ambiguous terms.": 0,
            "An alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones.": 0,
            "The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus.": 0,
            "The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches).": 0,
            "This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus.": 0,
            "The most accurate approaches to Word Sense Disambiguation (WSD) for biomedical documents are based on supervised learning.": 0,
            "Scaling up WSD with Automatically Generated Examples": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.267766195330611,
                0.0,
                0.13673665672153462,
                0.09226787492992683,
                0.09325721424447787,
                0.11232744384808081
            ],
            [
                0.0,
                0.267766195330611,
                1.0,
                0.06782950941096842,
                0.3398317188921987,
                0.17548766409237776,
                0.0,
                0.1701553966345833
            ],
            [
                0.0,
                0.0,
                0.06782950941096842,
                1.0,
                0.12853558981288749,
                0.19090084647085712,
                0.0,
                0.0
            ],
            [
                0.0,
                0.13673665672153462,
                0.3398317188921987,
                0.12853558981288749,
                1.0,
                0.1662728405739593,
                0.07283752657322835,
                0.16122056945568372
            ],
            [
                0.0,
                0.09226787492992683,
                0.17548766409237776,
                0.19090084647085712,
                0.1662728405739593,
                1.0,
                0.11340149955561682,
                0.33583761630469
            ],
            [
                0.0,
                0.09325721424447787,
                0.0,
                0.0,
                0.07283752657322835,
                0.11340149955561682,
                1.0,
                0.0784366577463478
            ],
            [
                0.0,
                0.11232744384808081,
                0.1701553966345833,
                0.0,
                0.16122056945568372,
                0.33583761630469,
                0.0784366577463478,
                1.0
            ]
        ]
    },
    "E09-1053": {
        "input_sentences": [
            "Abstract",
            "Unlike earlier proposals, our dependency structures are always tree-shaped.",
            "Dependency Trees and the Strong Generative Capacity of CCG",
            "We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees \u2013 but the mechanisms they use to bring the words in these trees into a linear order are incomparable.",
            "We propose a novel algorithm for extracting dependencies from the derivations of a large fragment of CCG."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Unlike earlier proposals, our dependency structures are always tree-shaped.": 0,
            "Dependency Trees and the Strong Generative Capacity of CCG": 0,
            "We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees \u2013 but the mechanisms they use to bring the words in these trees into a linear order are incomparable.": 0,
            "We propose a novel algorithm for extracting dependencies from the derivations of a large fragment of CCG.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09001775755656882,
                0.03334452052572089,
                0.0
            ],
            [
                0.0,
                0.09001775755656882,
                1.0,
                0.3994635764964098,
                0.07864440790235185
            ],
            [
                0.0,
                0.03334452052572089,
                0.3994635764964098,
                1.0,
                0.029131586308236992
            ],
            [
                0.0,
                0.0,
                0.07864440790235185,
                0.029131586308236992,
                1.0
            ]
        ]
    },
    "W10-1407": {
        "input_sentences": [
            "Abstract",
            "Direct Parsing of Discontinuous Constituents in German",
            "In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks.",
            "Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems.",
            "Discontinuities occur especially frequently in languages with a relatively free word order, such as German.",
            "Generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser.",
            "In both treebanks, discontinuities are annotated with crossing branches."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Direct Parsing of Discontinuous Constituents in German": 0,
            "In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks.": 0,
            "Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems.": 0,
            "Discontinuities occur especially frequently in languages with a relatively free word order, such as German.": 0,
            "Generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser.": 0,
            "In both treebanks, discontinuities are annotated with crossing branches.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.05871106288279993,
                0.0,
                0.07960236323907142,
                0.0,
                0.0
            ],
            [
                0.0,
                0.05871106288279993,
                1.0,
                0.03873834017542323,
                0.09902757845247039,
                0.21105603877269422,
                0.08149938604875165
            ],
            [
                0.0,
                0.0,
                0.03873834017542323,
                1.0,
                0.0,
                0.0484319522589724,
                0.0
            ],
            [
                0.0,
                0.07960236323907142,
                0.09902757845247039,
                0.0,
                1.0,
                0.0,
                0.11049951088374246
            ],
            [
                0.0,
                0.0,
                0.21105603877269422,
                0.0484319522589724,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.08149938604875165,
                0.0,
                0.11049951088374246,
                0.0,
                1.0
            ]
        ]
    },
    "P04-1042": {
        "input_sentences": [
            "Abstract",
            "By this new evaluation metric our algorithm achieves 60% error reduction on gold-standard input trees and 5% error reduction on state-ofthe-art machine-parsed input trees, when compared with the best previous work.",
            "Deep Dependencies From Context-Free Statistical Parsers: Correcting The Surface Dependency Approximation",
            "We find that our algorithm compares favorably with prior work on English using an existing evaluation metric, and also introduce and argue for a new dependency-based evaluation metric.",
            "We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks.",
            "We also present the first results on nonlocal dependency reconstruction for a language other than English, comparing performance on English and German.",
            "We use an algorithm based on loglinear classifiers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation.",
            "Our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order, the surface dependencies in context-free parse trees are a poorer approximation to underlying dependency structure."
        ],
        "authority_scores": {
            "Abstract": 0,
            "By this new evaluation metric our algorithm achieves 60% error reduction on gold-standard input trees and 5% error reduction on state-ofthe-art machine-parsed input trees, when compared with the best previous work.": 0,
            "Deep Dependencies From Context-Free Statistical Parsers: Correcting The Surface Dependency Approximation": 0,
            "We find that our algorithm compares favorably with prior work on English using an existing evaluation metric, and also introduce and argue for a new dependency-based evaluation metric.": 0,
            "We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks.": 0,
            "We also present the first results on nonlocal dependency reconstruction for a language other than English, comparing performance on English and German.": 0,
            "We use an algorithm based on loglinear classifiers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation.": 0,
            "Our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order, the surface dependencies in context-free parse trees are a poorer approximation to underlying dependency structure.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.18347487358167153,
                0.06885289899943273,
                0.0,
                0.06062908074373799,
                0.11542817157079112
            ],
            [
                0.0,
                0.0,
                1.0,
                0.03153497378157441,
                0.1296430668013106,
                0.03791508393755931,
                0.27074764048962874,
                0.2830408547013117
            ],
            [
                0.0,
                0.18347487358167153,
                0.03153497378157441,
                1.0,
                0.057106071896767883,
                0.14229423111856104,
                0.07744815235354036,
                0.19882287446759264
            ],
            [
                0.0,
                0.06885289899943273,
                0.1296430668013106,
                0.057106071896767883,
                1.0,
                0.14749184907934007,
                0.23820408696022763,
                0.1746054242121496
            ],
            [
                0.0,
                0.0,
                0.03791508393755931,
                0.14229423111856104,
                0.14749184907934007,
                1.0,
                0.044096001286352736,
                0.08300429610920237
            ],
            [
                0.0,
                0.06062908074373799,
                0.27074764048962874,
                0.07744815235354036,
                0.23820408696022763,
                0.044096001286352736,
                1.0,
                0.2598964354493444
            ],
            [
                0.0,
                0.11542817157079112,
                0.2830408547013117,
                0.19882287446759264,
                0.1746054242121496,
                0.08300429610920237,
                0.2598964354493444,
                1.0
            ]
        ]
    },
    "W12-3131": {
        "input_sentences": [
            "Abstract",
            "This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12).",
            "The CMU-Avenue French-English Translation System",
            "Translation System",
            "We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12).": 0,
            "The CMU-Avenue French-English Translation System": 0,
            "Translation System": 0,
            "We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.34479307212858923,
                0.2715108651291912,
                0.03574034137969838
            ],
            [
                0.0,
                0.34479307212858923,
                1.0,
                0.3115442086992723,
                0.0410101318357254
            ],
            [
                0.0,
                0.2715108651291912,
                0.3115442086992723,
                1.0,
                0.13163503185293263
            ],
            [
                0.0,
                0.03574034137969838,
                0.0410101318357254,
                0.13163503185293263,
                1.0
            ]
        ]
    },
    "P04-1015": {
        "input_sentences": [
            "Abstract",
            "The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.",
            "We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.",
            "Incremental Parsing With The Perceptron Algorithm",
            "A beam-search algorithm is used during both training and decoding phases of the method.",
            "This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.": 0,
            "We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.": 0,
            "Incremental Parsing With The Perceptron Algorithm": 0,
            "A beam-search algorithm is used during both training and decoding phases of the method.": 0,
            "This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.3510228752135376,
                0.13238327957158225,
                0.0,
                0.11462233171550767
            ],
            [
                0.0,
                0.3510228752135376,
                1.0,
                0.0552850908583776,
                0.11387445392016877,
                0.0264620163388264
            ],
            [
                0.0,
                0.13238327957158225,
                0.0552850908583776,
                1.0,
                0.13029321231006596,
                0.4786465198477012
            ],
            [
                0.0,
                0.0,
                0.11387445392016877,
                0.13029321231006596,
                1.0,
                0.06236439263199073
            ],
            [
                0.0,
                0.11462233171550767,
                0.0264620163388264,
                0.4786465198477012,
                0.06236439263199073,
                1.0
            ]
        ]
    },
    "E99-1016": {
        "input_sentences": [
            "Abstract",
            "This paper presents a new approach to partial parsing of context-free structures.",
            "An em- pirical evaluation of the method yields very good results for NP/PP chunking of German ewspaper texts.",
            "The approach is based on Markov Mod- els.",
            "Cascaded Markov Models",
            "Each layer of the resulting structure is represented byits own Markov Model, and output of a lower layer is passed as input to the next higher layer."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This paper presents a new approach to partial parsing of context-free structures.": 0,
            "An em- pirical evaluation of the method yields very good results for NP/PP chunking of German ewspaper texts.": 0,
            "The approach is based on Markov Mod- els.": 0,
            "Cascaded Markov Models": 0,
            "Each layer of the resulting structure is represented byits own Markov Model, and output of a lower layer is passed as input to the next higher layer.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.1120620580524022,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.1120620580524022,
                0.0,
                1.0,
                0.1493915352834391,
                0.053297101285115965
            ],
            [
                0.0,
                0.0,
                0.0,
                0.1493915352834391,
                1.0,
                0.06896891312617925
            ],
            [
                0.0,
                0.0,
                0.0,
                0.053297101285115965,
                0.06896891312617925,
                1.0
            ]
        ]
    },
    "C08-1049": {
        "input_sentences": [
            "Abstract",
            "As a derivation of the forest reranking for parsing (Huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking.",
            "Word Lattice",
            "In this paper, we describe a new rerank ing strategy named word lattice reranking,for the task of joint Chinese word segmen tation and part-of-speech (POS) tagging.",
            "With aperceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local fea tures that can?t be easily incorporated intothe perceptron baseline.",
            "Word Lattice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging",
            "Experimental results show that, this strategy achieves im provement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking."
        ],
        "authority_scores": {
            "Abstract": 0,
            "As a derivation of the forest reranking for parsing (Huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking.": 0,
            "Word Lattice": 0,
            "In this paper, we describe a new rerank ing strategy named word lattice reranking,for the task of joint Chinese word segmen tation and part-of-speech (POS) tagging.": 0,
            "With aperceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local fea tures that can?t be easily incorporated intothe perceptron baseline.": 0,
            "Word Lattice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging": 0,
            "Experimental results show that, this strategy achieves im provement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.18175667304304033,
                0.12552588273825754,
                0.09260151655061258,
                0.16688035282945146,
                0.18168604310254896
            ],
            [
                0.0,
                0.18175667304304033,
                1.0,
                0.30790697131566214,
                0.1698268991544983,
                0.5508915299850384,
                0.0
            ],
            [
                0.0,
                0.12552588273825754,
                0.30790697131566214,
                1.0,
                0.08715147694431992,
                0.46972793425351816,
                0.16548425737122036
            ],
            [
                0.0,
                0.09260151655061258,
                0.1698268991544983,
                0.08715147694431992,
                1.0,
                0.1559270005130607,
                0.1825493526033387
            ],
            [
                0.0,
                0.16688035282945146,
                0.5508915299850384,
                0.46972793425351816,
                0.1559270005130607,
                1.0,
                0.22106478015073647
            ],
            [
                0.0,
                0.18168604310254896,
                0.0,
                0.16548425737122036,
                0.1825493526033387,
                0.22106478015073647,
                1.0
            ]
        ]
    },
    "P04-1054": {
        "input_sentences": [
            "Abstract",
            "We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a \u201cbag-of-words\u201d kernel.",
            "Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.",
            "We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.",
            "Dependency Tree Kernels For Relation Extraction"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a \u201cbag-of-words\u201d kernel.": 0,
            "Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.": 0,
            "We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.": 0,
            "Dependency Tree Kernels For Relation Extraction": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07536301602683898,
                0.06947841138762685,
                0.11357509635691121
            ],
            [
                0.0,
                0.07536301602683898,
                1.0,
                0.0,
                0.09303699966763025
            ],
            [
                0.0,
                0.06947841138762685,
                0.0,
                1.0,
                0.2960238424768766
            ],
            [
                0.0,
                0.11357509635691121,
                0.09303699966763025,
                0.2960238424768766,
                1.0
            ]
        ]
    },
    "P08-1013": {
        "input_sentences": [
            "Abstract",
            "Applying a Grammar-Based Language Model to a Simplified Broadcast-News Transcription Task",
            "We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree.",
            "To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task.",
            "The language model is applied by means of an N-best rescoring step, which allows to directly measure the performance gains relative to the baseline system without rescoring.",
            "Language Model",
            "We report a significant reduction in word error rate compared to a state-of-the-art baseline system."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Applying a Grammar-Based Language Model to a Simplified Broadcast-News Transcription Task": 0,
            "We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree.": 0,
            "To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task.": 0,
            "The language model is applied by means of an N-best rescoring step, which allows to directly measure the performance gains relative to the baseline system without rescoring.": 0,
            "Language Model": 0,
            "We report a significant reduction in word error rate compared to a state-of-the-art baseline system.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.27702190417638217,
                0.3343136810551131,
                0.07603451606036324,
                0.33956423901297395,
                0.0
            ],
            [
                0.0,
                0.27702190417638217,
                1.0,
                0.0,
                0.06487630795959635,
                0.28973254889637867,
                0.0
            ],
            [
                0.0,
                0.3343136810551131,
                0.0,
                1.0,
                0.04409111059093177,
                0.0,
                0.0
            ],
            [
                0.0,
                0.07603451606036324,
                0.06487630795959635,
                0.04409111059093177,
                1.0,
                0.22391791397520558,
                0.05689643691732357
            ],
            [
                0.0,
                0.33956423901297395,
                0.28973254889637867,
                0.0,
                0.22391791397520558,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.05689643691732357,
                0.0,
                1.0
            ]
        ]
    },
    "P10-1097": {
        "input_sentences": [
            "Abstract",
            "We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.",
            "It employs a systematic combination of firstand second-order context vectors.",
            "Contextualizing Semantic Representations Using Syntactically Enriched Vector Models",
            "We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.": 0,
            "It employs a systematic combination of firstand second-order context vectors.": 0,
            "Contextualizing Semantic Representations Using Syntactically Enriched Vector Models": 0,
            "We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.03619788713249855
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.39406983670324713
            ],
            [
                0.0,
                0.03619788713249855,
                0.0,
                0.39406983670324713,
                1.0
            ]
        ]
    },
    "P07-1083": {
        "input_sentences": [
            "Abstract",
            "We also show strong improvements over other recent discriminative and heuristic similarity functions.",
            "A character-based measure of similarity is an important component of many natural language processing systems, including approaches to transliteration, coreference, word alignment, spelling correction, and the identification of cognates in related vocabu- We propose an alignment-based disfor string similarity.",
            "We gather features from substring pairs consistent with a character-based alignment of the two strings.",
            "Alignment-Based Discriminative String Similarity",
            "This approach achieves exceptional performance; on nine separate cognate identification experiments using six language pairs, we more than double the precision of traditional orthographic measures like Longest Common Subsequence Ratio and Dice\u2019s Coefficient."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We also show strong improvements over other recent discriminative and heuristic similarity functions.": 0,
            "A character-based measure of similarity is an important component of many natural language processing systems, including approaches to transliteration, coreference, word alignment, spelling correction, and the identification of cognates in related vocabu- We propose an alignment-based disfor string similarity.": 0,
            "We gather features from substring pairs consistent with a character-based alignment of the two strings.": 0,
            "Alignment-Based Discriminative String Similarity": 0,
            "This approach achieves exceptional performance; on nine separate cognate identification experiments using six language pairs, we more than double the precision of traditional orthographic measures like Longest Common Subsequence Ratio and Dice\u2019s Coefficient.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07516141728962629,
                0.0,
                0.2783636855638486,
                0.0
            ],
            [
                0.0,
                0.07516141728962629,
                1.0,
                0.18635005752899697,
                0.4136483976843177,
                0.05573783683258382
            ],
            [
                0.0,
                0.0,
                0.18635005752899697,
                1.0,
                0.2126348673906855,
                0.05302704053741744
            ],
            [
                0.0,
                0.2783636855638486,
                0.4136483976843177,
                0.2126348673906855,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.05573783683258382,
                0.05302704053741744,
                0.0,
                1.0
            ]
        ]
    },
    "W10-3504": {
        "input_sentences": [
            "Abstract",
            "In this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones.",
            "We report empirical evidence that our method successfully expands existing textual entailment corpora.",
            "Expanding textual entailment corpora fromWikipedia using co-training",
            "The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones.": 0,
            "We report empirical evidence that our method successfully expands existing textual entailment corpora.": 0,
            "Expanding textual entailment corpora fromWikipedia using co-training": 0,
            "The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.20883466568052533,
                0.09035671070062311,
                0.27192963037853823
            ],
            [
                0.0,
                0.20883466568052533,
                1.0,
                0.2057849314445246,
                0.08984317689613555
            ],
            [
                0.0,
                0.09035671070062311,
                0.2057849314445246,
                1.0,
                0.06222936284553251
            ],
            [
                0.0,
                0.27192963037853823,
                0.08984317689613555,
                0.06222936284553251,
                1.0
            ]
        ]
    },
    "P11-2074": {
        "input_sentences": [
            "Abstract",
            "We model the feature space with a log-linear combination of multiple mixture components.",
            "The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task.",
            "All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance.",
            "It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications.",
            "Each component contains a large set of features trained in a maximumentropy framework.",
            "This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT.",
            "Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation",
            "In this paper we present a novel discriminative mixture model for statistical machine translation (SMT)."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We model the feature space with a log-linear combination of multiple mixture components.": 0,
            "The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task.": 0,
            "All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance.": 0,
            "It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications.": 0,
            "Each component contains a large set of features trained in a maximumentropy framework.": 0,
            "This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT.": 0,
            "Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation": 0,
            "In this paper we present a novel discriminative mixture model for statistical machine translation (SMT).": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.12088921333872393,
                0.17703055472465407,
                0.0,
                0.0,
                0.15706107523130716,
                0.1551715153983802
            ],
            [
                0.0,
                0.0,
                1.0,
                0.09080574206408071,
                0.0,
                0.08385071328379404,
                0.06628846906494912,
                0.05758808161740652,
                0.04816437750564215
            ],
            [
                0.0,
                0.12088921333872393,
                0.09080574206408071,
                1.0,
                0.0,
                0.21266212759802347,
                0.0,
                0.27723294099991347,
                0.16287261958233684
            ],
            [
                0.0,
                0.17703055472465407,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.12873726471627112,
                0.0
            ],
            [
                0.0,
                0.0,
                0.08385071328379404,
                0.21266212759802347,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.06628846906494912,
                0.0,
                0.0,
                0.0,
                1.0,
                0.07239295348653435,
                0.14063676814676937
            ],
            [
                0.0,
                0.15706107523130716,
                0.05758808161740652,
                0.27723294099991347,
                0.12873726471627112,
                0.0,
                0.07239295348653435,
                1.0,
                0.46407408096488423
            ],
            [
                0.0,
                0.1551715153983802,
                0.04816437750564215,
                0.16287261958233684,
                0.0,
                0.0,
                0.14063676814676937,
                0.46407408096488423,
                1.0
            ]
        ]
    },
    "W10-1404": {
        "input_sentences": [
            "Abstract",
            "The present work has examined several directions that try to explore the rich set of morphosyntactic features in the BDT: i) experimenting the impact of morphological features, ii) application of dependency tree transformations, iii) application of a two-stage parsing scheme (stacking), and iv) combinations of the individual experiments.",
            "All the tests were conducted using MaltParser (Vivre et al., 2007a), a freely available and state of the art dependency parser generator.",
            "Application of Different Techniques to Dependency Parsing of Basque",
            "We present a set of experiments on dependency parsing of the Basque Dependency Treebank (BDT)."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The present work has examined several directions that try to explore the rich set of morphosyntactic features in the BDT: i) experimenting the impact of morphological features, ii) application of dependency tree transformations, iii) application of a two-stage parsing scheme (stacking), and iv) combinations of the individual experiments.": 0,
            "All the tests were conducted using MaltParser (Vivre et al., 2007a), a freely available and state of the art dependency parser generator.": 0,
            "Application of Different Techniques to Dependency Parsing of Basque": 0,
            "We present a set of experiments on dependency parsing of the Basque Dependency Treebank (BDT).": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.01532173170026992,
                0.18726360402015377,
                0.2755615011987317
            ],
            [
                0.0,
                0.01532173170026992,
                1.0,
                0.04159085171668782,
                0.06864668793598691
            ],
            [
                0.0,
                0.18726360402015377,
                0.04159085171668782,
                1.0,
                0.35183947906595825
            ],
            [
                0.0,
                0.2755615011987317,
                0.06864668793598691,
                0.35183947906595825,
                1.0
            ]
        ]
    },
    "E06-2025": {
        "input_sentences": [
            "Abstract",
            "We show that all current estimation methods are inconsistent in the \u201cweight-distribution test\u201d, and argue that these results force us to rethink both the methods proposed and the criteria used.",
            "Estimation Methods",
            "We analyze estimation methods for Data- Oriented Parsing, as well as the theoretical criteria used to evaluate them.",
            "Theoretical Evaluation Of Estimation Methods For Data-Oriented Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We show that all current estimation methods are inconsistent in the \u201cweight-distribution test\u201d, and argue that these results force us to rethink both the methods proposed and the criteria used.": 0,
            "Estimation Methods": 0,
            "We analyze estimation methods for Data- Oriented Parsing, as well as the theoretical criteria used to evaluate them.": 0,
            "Theoretical Evaluation Of Estimation Methods For Data-Oriented Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.33289163108465586,
                0.24550173492892394,
                0.1288300025089341
            ],
            [
                0.0,
                0.33289163108465586,
                1.0,
                0.3115442086992723,
                0.387002827584368
            ],
            [
                0.0,
                0.24550173492892394,
                0.3115442086992723,
                1.0,
                0.6150863983548802
            ],
            [
                0.0,
                0.1288300025089341,
                0.387002827584368,
                0.6150863983548802,
                1.0
            ]
        ]
    },
    "P08-1006": {
        "input_sentences": [
            "Abstract",
            "Syntactic Parsers and Their Representations",
            "This paper presents a comparative evaluation of several state-of-the-art English parsers based on different frameworks.",
            "Task-oriented Evaluation of Syntactic Parsers and Their Representations",
            "We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy.",
            "Our approach is to measure the impact of each parser when it is used as a component of an information extraction system that performs protein-protein interaction (PPI) identification in biomedical papers.",
            "We evaluate eight parsers (based on dependency parsing, phrase structure parsing, or deep parsing) using five different parse representations.",
            "Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Syntactic Parsers and Their Representations": 0,
            "This paper presents a comparative evaluation of several state-of-the-art English parsers based on different frameworks.": 0,
            "Task-oriented Evaluation of Syntactic Parsers and Their Representations": 0,
            "We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy.": 0,
            "Our approach is to measure the impact of each parser when it is used as a component of an information extraction system that performs protein-protein interaction (PPI) identification in biomedical papers.": 0,
            "We evaluate eight parsers (based on dependency parsing, phrase structure parsing, or deep parsing) using five different parse representations.": 0,
            "Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.08348828488197338,
                0.6025490382567462,
                0.0,
                0.0,
                0.16019565245640235,
                0.13288943762383904
            ],
            [
                0.0,
                0.08348828488197338,
                1.0,
                0.1624668763457072,
                0.0,
                0.0,
                0.12021035857532351,
                0.09928485148381049
            ],
            [
                0.0,
                0.6025490382567462,
                0.1624668763457072,
                1.0,
                0.0,
                0.0,
                0.0965257363205172,
                0.08007240283472407
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.26662004723806987,
                0.05186900654724611,
                0.11446263964175288
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.26662004723806987,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.16019565245640235,
                0.12021035857532351,
                0.0965257363205172,
                0.05186900654724611,
                0.0,
                1.0,
                0.07161303630194604
            ],
            [
                0.0,
                0.13288943762383904,
                0.09928485148381049,
                0.08007240283472407,
                0.11446263964175288,
                0.0,
                0.07161303630194604,
                1.0
            ]
        ]
    },
    "D08-1008": {
        "input_sentences": [
            "Abstract",
            "Dependency-based Semantic Role Labeling of PropBank",
            "The complete syntactic?semanticoutput is selected from a candidate pool gen erated by the subsystems.We evaluate the system on the CoNLL 2005 test sets using segment-based and dependency-based metrics.",
            "The syntactic model is a projective parser using pseudo-projective transfor mations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers.",
            "To tackle the problemof joint syntactic?semantic analysis, the system relies on a syntactic and a semantic subcomponent.",
            "Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently.",
            "We present a PropBank semantic role label ing system for English that is integrated with a dependency parser.",
            "Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance.",
            "Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Dependency-based Semantic Role Labeling of PropBank": 0,
            "The complete syntactic?semanticoutput is selected from a candidate pool gen erated by the subsystems.We evaluate the system on the CoNLL 2005 test sets using segment-based and dependency-based metrics.": 0,
            "The syntactic model is a projective parser using pseudo-projective transfor mations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers.": 0,
            "To tackle the problemof joint syntactic?semantic analysis, the system relies on a syntactic and a semantic subcomponent.": 0,
            "Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently.": 0,
            "We present a PropBank semantic role label ing system for English that is integrated with a dependency parser.": 0,
            "Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance.": 0,
            "Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.14252935297131286,
                0.043780440412945036,
                0.12387129076153648,
                0.0439897286077728,
                0.3645732437144538,
                0.45694879820970014,
                0.1372474274871565
            ],
            [
                0.0,
                0.14252935297131286,
                1.0,
                0.05482234704302216,
                0.08710941533850224,
                0.20631660958207398,
                0.03054353016080625,
                0.13837212049289754,
                0.22417961144476906
            ],
            [
                0.0,
                0.043780440412945036,
                0.05482234704302216,
                1.0,
                0.13016791215768073,
                0.022254068004618842,
                0.0880422861948853,
                0.02550208325412712,
                0.034716213095919046
            ],
            [
                0.0,
                0.12387129076153648,
                0.08710941533850224,
                0.13016791215768073,
                1.0,
                0.0,
                0.07963552264622632,
                0.07215496098259538,
                0.0
            ],
            [
                0.0,
                0.0439897286077728,
                0.20631660958207398,
                0.022254068004618842,
                0.0,
                1.0,
                0.0,
                0.05124798703259166,
                0.44754790509721454
            ],
            [
                0.0,
                0.3645732437144538,
                0.03054353016080625,
                0.0880422861948853,
                0.07963552264622632,
                0.0,
                1.0,
                0.21236372055051603,
                0.04411744865414602
            ],
            [
                0.0,
                0.45694879820970014,
                0.13837212049289754,
                0.02550208325412712,
                0.07215496098259538,
                0.05124798703259166,
                0.21236372055051603,
                1.0,
                0.1199198302659375
            ],
            [
                0.0,
                0.1372474274871565,
                0.22417961144476906,
                0.034716213095919046,
                0.0,
                0.44754790509721454,
                0.04411744865414602,
                0.1199198302659375,
                1.0
            ]
        ]
    },
    "W08-2107": {
        "input_sentences": [
            "Abstract",
            "Given the limited coverage provided by lexical resources, such as dictionaries, and the constantly growing number of VPCs, possible ways of automatically identifying them are crucial for any NLP task that requires some degree of semantic interpretation.",
            "Picking them up and Figuring them out: Verb-Particle Constructions Noise and Idiomaticity",
            "This paper investigates, in a first stage, some methods for the automatic acquisition of verb-particle constructions (VPCs) taking into account their statistical properties and some regular patterns found in productive combinations of verbs and particles.",
            "The results obtained show that such combination can successfully be used to detect VPCs and distinguish idiomatic from compositional cases.",
            "In a second stage we also study whether the combination of statistical and linguistic properties can provide some indication of the degree of idiomaticity of a given VPC."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Given the limited coverage provided by lexical resources, such as dictionaries, and the constantly growing number of VPCs, possible ways of automatically identifying them are crucial for any NLP task that requires some degree of semantic interpretation.": 0,
            "Picking them up and Figuring them out: Verb-Particle Constructions Noise and Idiomaticity": 0,
            "This paper investigates, in a first stage, some methods for the automatic acquisition of verb-particle constructions (VPCs) taking into account their statistical properties and some regular patterns found in productive combinations of verbs and particles.": 0,
            "The results obtained show that such combination can successfully be used to detect VPCs and distinguish idiomatic from compositional cases.": 0,
            "In a second stage we also study whether the combination of statistical and linguistic properties can provide some indication of the degree of idiomaticity of a given VPC.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.025097474459120035,
                0.03296483159915582,
                0.09006501057624192
            ],
            [
                0.0,
                0.0,
                1.0,
                0.20208264067387743,
                0.0,
                0.08615197395747062
            ],
            [
                0.0,
                0.025097474459120035,
                0.20208264067387743,
                1.0,
                0.03594543797533339,
                0.14731273122136723
            ],
            [
                0.0,
                0.03296483159915582,
                0.0,
                0.03594543797533339,
                1.0,
                0.06449705241635614
            ],
            [
                0.0,
                0.09006501057624192,
                0.08615197395747062,
                0.14731273122136723,
                0.06449705241635614,
                1.0
            ]
        ]
    },
    "P07-1055": {
        "input_sentences": [
            "Abstract",
            "In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.",
            "Structured Models for Fine-to-Coarse Sentiment Analysis",
            "Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions.",
            "The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another.",
            "Experiments show that this method can significantly reduce classification error relative to models trained in isolation."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.": 0,
            "Structured Models for Fine-to-Coarse Sentiment Analysis": 0,
            "Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions.": 0,
            "The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another.": 0,
            "Experiments show that this method can significantly reduce classification error relative to models trained in isolation.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.1948299794864631,
                0.04497600651017465,
                0.1146241636303756,
                0.0
            ],
            [
                0.0,
                0.1948299794864631,
                1.0,
                0.0,
                0.0,
                0.09923337072177646
            ],
            [
                0.0,
                0.04497600651017465,
                0.0,
                1.0,
                0.08501720189677947,
                0.04581554378205222
            ],
            [
                0.0,
                0.1146241636303756,
                0.0,
                0.08501720189677947,
                1.0,
                0.04859208037710932
            ],
            [
                0.0,
                0.0,
                0.09923337072177646,
                0.04581554378205222,
                0.04859208037710932,
                1.0
            ]
        ]
    },
    "P08-1102": {
        "input_sentences": [
            "Abstract",
            "With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.",
            "On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.",
            "Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.",
            "We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.",
            "A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging",
            "Cascaded Linear Model"
        ],
        "authority_scores": {
            "Abstract": 0,
            "With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.": 0,
            "On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.": 0,
            "Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.": 0,
            "We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.": 0,
            "A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging": 0,
            "Cascaded Linear Model": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09985545105671101,
                0.04922165767715351,
                0.05500672816916665,
                0.06796933082472051,
                0.12709014970295665
            ],
            [
                0.0,
                0.09985545105671101,
                1.0,
                0.3146032628102605,
                0.3177589435778502,
                0.3926403819571843,
                0.0
            ],
            [
                0.0,
                0.04922165767715351,
                0.3146032628102605,
                1.0,
                0.3828323243500233,
                0.4730486209635918,
                0.20772681383199545
            ],
            [
                0.0,
                0.05500672816916665,
                0.3177589435778502,
                0.3828323243500233,
                1.0,
                0.8092874757148654,
                0.4328166132287353
            ],
            [
                0.0,
                0.06796933082472051,
                0.3926403819571843,
                0.4730486209635918,
                0.8092874757148654,
                1.0,
                0.5348119502855482
            ],
            [
                0.0,
                0.12709014970295665,
                0.0,
                0.20772681383199545,
                0.4328166132287353,
                0.5348119502855482,
                1.0
            ]
        ]
    },
    "P11-2124": {
        "input_sentences": [
            "Abstract",
            "We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser.",
            "We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task.",
            "This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.",
            "Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser.": 0,
            "We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task.": 0,
            "This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.": 0,
            "Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.05886492310232546,
                0.28922517126187297,
                0.20426471284871925
            ],
            [
                0.0,
                0.05886492310232546,
                1.0,
                0.1592852602518591,
                0.1335922476623724
            ],
            [
                0.0,
                0.28922517126187297,
                0.1592852602518591,
                1.0,
                0.2556712016743063
            ],
            [
                0.0,
                0.20426471284871925,
                0.1335922476623724,
                0.2556712016743063,
                1.0
            ]
        ]
    },
    "S12-1097": {
        "input_sentences": [
            "University_Of_Sheffield: Two Approaches to Semantic Text Similarity",
            "Abstract",
            "This approach also makes use of information from WordNet.",
            "Two approaches were developed.",
            "The first is an unsupervised technique based on the widely used vector space model and information from WordNet.The second method relies on supervised ma chine learning and represents each sentence as a set of n-grams.",
            "This paper describes the University of Sheffield?s submission to SemEval-2012 Task 6: Semantic Text Similarity.",
            "Resultsfrom the formal evaluation show that both approaches are useful for determining the simi larity in meaning between pairs of sentences with the best performance being obtained bythe supervised approach.",
            "Incorporating information from WordNet alo improves perfor mance for both approaches."
        ],
        "authority_scores": {
            "University_Of_Sheffield: Two Approaches to Semantic Text Similarity": 0,
            "Abstract": 0,
            "This approach also makes use of information from WordNet.": 0,
            "Two approaches were developed.": 0,
            "The first is an unsupervised technique based on the widely used vector space model and information from WordNet.The second method relies on supervised ma chine learning and represents each sentence as a set of n-grams.": 0,
            "This paper describes the University of Sheffield?s submission to SemEval-2012 Task 6: Semantic Text Similarity.": 0,
            "Resultsfrom the formal evaluation show that both approaches are useful for determining the simi larity in meaning between pairs of sentences with the best performance being obtained bythe supervised approach.": 0,
            "Incorporating information from WordNet alo improves perfor mance for both approaches.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.1812603532706148,
                0.0,
                0.35381268327851667,
                0.05398383590193536,
                0.08452221088839335
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.12157651979442317,
                0.0,
                0.09124823345672212,
                0.21276519024653573
            ],
            [
                0.1812603532706148,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.08540508230095656,
                0.1337182928292661
            ],
            [
                0.0,
                0.0,
                0.12157651979442317,
                0.0,
                1.0,
                0.0,
                0.03975401489100061,
                0.09269517031646436
            ],
            [
                0.35381268327851667,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.05398383590193536,
                0.0,
                0.09124823345672212,
                0.08540508230095656,
                0.03975401489100061,
                0.0,
                1.0,
                0.039824629307683775
            ],
            [
                0.08452221088839335,
                0.0,
                0.21276519024653573,
                0.1337182928292661,
                0.09269517031646436,
                0.0,
                0.039824629307683775,
                1.0
            ]
        ]
    },
    "W09-1218": {
        "input_sentences": [
            "Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage Approximate Max-Margin Linear Models",
            "Abstract",
            "Dependency Parsing",
            "In the closed challenge of the CoNLL-2009 Shared Task (Haji\u02c7c et al., 2009), our system achieved the 3rd best performances for English and Czech, and the 4th best performance for Japanese.",
            "The system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classifier, and an argument classifier for semantic dependency parsing.",
            "This paper describes a system for syntacticsemantic dependency parsing for multiple languages.",
            "All components are trained with an approximate max-margin learning algorithm.",
            "For semantic dependency parsing, we explore use of global features."
        ],
        "authority_scores": {
            "Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage Approximate Max-Margin Linear Models": 0,
            "Abstract": 0,
            "Dependency Parsing": 0,
            "In the closed challenge of the CoNLL-2009 Shared Task (Haji\u02c7c et al., 2009), our system achieved the 3rd best performances for English and Czech, and the 4th best performance for Japanese.": 0,
            "The system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classifier, and an argument classifier for semantic dependency parsing.": 0,
            "This paper describes a system for syntacticsemantic dependency parsing for multiple languages.": 0,
            "All components are trained with an approximate max-margin learning algorithm.": 0,
            "For semantic dependency parsing, we explore use of global features.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.2812943846447426,
                0.0,
                0.22578691724725558,
                0.09410057476232842,
                0.30216596281051555,
                0.1800080959066055
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.2812943846447426,
                0.0,
                1.0,
                0.0,
                0.4514564436438089,
                0.33452702897418884,
                0.0,
                0.349667159976535
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.22578691724725558,
                0.0,
                0.4514564436438089,
                0.0,
                1.0,
                0.15102438280341668,
                0.0,
                0.21027550649101298
            ],
            [
                0.09410057476232842,
                0.0,
                0.33452702897418884,
                0.0,
                0.15102438280341668,
                1.0,
                0.0,
                0.11697311615679262
            ],
            [
                0.30216596281051555,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.1800080959066055,
                0.0,
                0.349667159976535,
                0.0,
                0.21027550649101298,
                0.11697311615679262,
                0.0,
                1.0
            ]
        ]
    },
    "W09-2208": {
        "input_sentences": [
            "Abstract",
            "We show that our algorithm achieves an averimprovement of recall and precision compared to the supervised algorithm.",
            "We also show that our algorithm achieves high accuracy when the training and test sets are from different domains.",
            "We present a simple semi-supervised learning algorithm for named entity recognition (NER) using conditional random fields (CRFs).",
            "A Simple Semi-supervised Algorithm For Named Entity Recognition",
            "Named Entity Recognition",
            "The algorithm is based on exploiting evidence that is independent from the features used for a classifier, which provides high-precision labels to unlabeled data.",
            "Such independent evidence is used to automatically extract highaccuracy and non-redundant data, leading to a much improved classifier at the next iteration."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We show that our algorithm achieves an averimprovement of recall and precision compared to the supervised algorithm.": 0,
            "We also show that our algorithm achieves high accuracy when the training and test sets are from different domains.": 0,
            "We present a simple semi-supervised learning algorithm for named entity recognition (NER) using conditional random fields (CRFs).": 0,
            "A Simple Semi-supervised Algorithm For Named Entity Recognition": 0,
            "Named Entity Recognition": 0,
            "The algorithm is based on exploiting evidence that is independent from the features used for a classifier, which provides high-precision labels to unlabeled data.": 0,
            "Such independent evidence is used to automatically extract highaccuracy and non-redundant data, leading to a much improved classifier at the next iteration.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.19278361846597789,
                0.13487223088626116,
                0.23741893899340685,
                0.0,
                0.15982719616690835,
                0.0
            ],
            [
                0.0,
                0.19278361846597789,
                1.0,
                0.03299014420935342,
                0.05807337050744992,
                0.0,
                0.10926158135945875,
                0.0
            ],
            [
                0.0,
                0.13487223088626116,
                0.03299014420935342,
                1.0,
                0.5680769674823902,
                0.36446553866706705,
                0.027350468323393592,
                0.0
            ],
            [
                0.0,
                0.23741893899340685,
                0.05807337050744992,
                0.5680769674823902,
                1.0,
                0.6415777430342045,
                0.048145708924981954,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.36446553866706705,
                0.6415777430342045,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.15982719616690835,
                0.10926158135945875,
                0.027350468323393592,
                0.048145708924981954,
                0.0,
                1.0,
                0.3088475547253388
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.3088475547253388,
                1.0
            ]
        ]
    },
    "D11-1086": {
        "input_sentences": [
            "Abstract",
            "Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations.",
            "Mapping documents into an interlingual representation can help bridge the language bar rier of cross-lingual corpora.",
            "Later, we explore different selection strategies to remove the noisy pairsbased on the association scores.",
            "In theory, sucha covariance matrix should represent seman tic equivalence, and should be highly sparse.",
            "Our experimental results on the task of aligning compa rable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.",
            "Many existing approaches are based on word co-occurrencesextracted from aligned training data, repre sented as a covariance matrix.",
            "In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways.",
            "First, we explore word association measures and bilingual dictionaries to weigh the word pairs.",
            "Improving Bilingual Projections via Sparse Covariance Matrices"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations.": 0,
            "Mapping documents into an interlingual representation can help bridge the language bar rier of cross-lingual corpora.": 0,
            "Later, we explore different selection strategies to remove the noisy pairsbased on the association scores.": 0,
            "In theory, sucha covariance matrix should represent seman tic equivalence, and should be highly sparse.": 0,
            "Our experimental results on the task of aligning compa rable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.": 0,
            "Many existing approaches are based on word co-occurrencesextracted from aligned training data, repre sented as a covariance matrix.": 0,
            "In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways.": 0,
            "First, we explore word association measures and bilingual dictionaries to weigh the word pairs.": 0,
            "Improving Bilingual Projections via Sparse Covariance Matrices": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.027605598862702387,
                0.05455907337225703,
                0.024983248931152863,
                0.07537448304009195,
                0.0,
                0.10166034214767561
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.11462196823100847,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.06463502848211351,
                0.0,
                0.06834773112169429,
                0.1449272745451155,
                0.0
            ],
            [
                0.0,
                0.027605598862702387,
                0.0,
                0.0,
                1.0,
                0.07713349831752664,
                0.10682152981292312,
                0.036501511351232994,
                0.0,
                0.1437234422312281
            ],
            [
                0.0,
                0.05455907337225703,
                0.11462196823100847,
                0.06463502848211351,
                0.07713349831752664,
                1.0,
                0.08387198129854358,
                0.07214075108150875,
                0.06607036223653279,
                0.17149061740743282
            ],
            [
                0.0,
                0.024983248931152863,
                0.0,
                0.0,
                0.10682152981292312,
                0.08387198129854358,
                1.0,
                0.0330341083700687,
                0.152310665850639,
                0.044554318968374335
            ],
            [
                0.0,
                0.07537448304009195,
                0.0,
                0.06834773112169429,
                0.036501511351232994,
                0.07214075108150875,
                0.0330341083700687,
                1.0,
                0.06986551192601602,
                0.13442041780470743
            ],
            [
                0.0,
                0.0,
                0.0,
                0.1449272745451155,
                0.0,
                0.06607036223653279,
                0.152310665850639,
                0.06986551192601602,
                1.0,
                0.12310941545796504
            ],
            [
                0.0,
                0.10166034214767561,
                0.0,
                0.0,
                0.1437234422312281,
                0.17149061740743282,
                0.044554318968374335,
                0.13442041780470743,
                0.12310941545796504,
                1.0
            ]
        ]
    },
    "P12-1065": {
        "input_sentences": [
            "Abstract",
            "The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets.",
            "Bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them.",
            "This paper introduces a novel variant of the Yarowsky algorithm based on this view.",
            "It is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function.",
            "Bootstrapping via Graph Propagation"
        ],
        "authority_scores": {
            "Abstract": 0,
            "The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets.": 0,
            "Bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them.": 0,
            "This paper introduces a novel variant of the Yarowsky algorithm based on this view.": 0,
            "It is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function.": 0,
            "Bootstrapping via Graph Propagation": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.02875535781961868,
                0.047124104756539664,
                0.0791079424249923,
                0.07717576400681966
            ],
            [
                0.0,
                0.02875535781961868,
                1.0,
                0.0,
                0.08939464452404132,
                0.205976010020595
            ],
            [
                0.0,
                0.047124104756539664,
                0.0,
                1.0,
                0.062028538378707726,
                0.0
            ],
            [
                0.0,
                0.0791079424249923,
                0.08939464452404132,
                0.062028538378707726,
                1.0,
                0.4340051276607553
            ],
            [
                0.0,
                0.07717576400681966,
                0.205976010020595,
                0.0,
                0.4340051276607553,
                1.0
            ]
        ]
    },
    "N03-2024": {
        "input_sentences": [
            "Abstract",
            "The interpretation of the probabilistic data helps us gain insight on how extractive summaries can be rewritten in an efficient manner to produce more fluent and easy-to-read text.",
            "References To Named Entities: A Corpus Study",
            "In this paper, we present a corpus study performed to derive a statistical model for the syntactic realization of referential expressions.",
            "References included in multi-document summaries are often problematic."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The interpretation of the probabilistic data helps us gain insight on how extractive summaries can be rewritten in an efficient manner to produce more fluent and easy-to-read text.": 0,
            "References To Named Entities: A Corpus Study": 0,
            "In this paper, we present a corpus study performed to derive a statistical model for the syntactic realization of referential expressions.": 0,
            "References included in multi-document summaries are often problematic.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.07145656434696325
            ],
            [
                0.0,
                0.0,
                1.0,
                0.19477400403431264,
                0.14218783310364055
            ],
            [
                0.0,
                0.0,
                0.19477400403431264,
                1.0,
                0.0
            ],
            [
                0.0,
                0.07145656434696325,
                0.14218783310364055,
                0.0,
                1.0
            ]
        ]
    },
    "W07-0738": {
        "input_sentences": [
            "Abstract",
            "The reason is that, while MT quality aspects are its scope to the lexical dimension.",
            "Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, may not be a reliable MT quality indicator.",
            "We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature.",
            "Linguistic Features for Automatic Evaluation of Heterogenous MT Systems",
            "This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon.",
            "In this work, we suggest using metrics which take into account linguistic features at more abstract levels."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The reason is that, while MT quality aspects are its scope to the lexical dimension.": 0,
            "Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, may not be a reliable MT quality indicator.": 0,
            "We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature.": 0,
            "Linguistic Features for Automatic Evaluation of Heterogenous MT Systems": 0,
            "This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon.": 0,
            "In this work, we suggest using metrics which take into account linguistic features at more abstract levels.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.3016886784613908
            ],
            [
                0.0,
                1.0,
                0.11295347379310382,
                0.058417260878013307,
                0.09701075341553611,
                0.0,
                0.0
            ],
            [
                0.0,
                0.11295347379310382,
                1.0,
                0.08301301392464876,
                0.09478417291776962,
                0.03235084027724377,
                0.0
            ],
            [
                0.0,
                0.058417260878013307,
                0.08301301392464876,
                1.0,
                0.13320926534069458,
                0.22509696558712625,
                0.14059962537777906
            ],
            [
                0.0,
                0.09701075341553611,
                0.09478417291776962,
                0.13320926534069458,
                1.0,
                0.15312381600554087,
                0.20253894656394172
            ],
            [
                0.0,
                0.0,
                0.03235084027724377,
                0.22509696558712625,
                0.15312381600554087,
                1.0,
                0.0
            ],
            [
                0.3016886784613908,
                0.0,
                0.0,
                0.14059962537777906,
                0.20253894656394172,
                0.0,
                1.0
            ]
        ]
    },
    "P12-1045": {
        "input_sentences": [
            "Abstract",
            "Fast Online Lexicon Learning for Grounded Language Acquisition",
            "We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon\u2019s Mechanical Turk we can further improve the results.",
            "It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context.",
            "In this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results.",
            "Recent work by Chen and Mooney (2011) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs.",
            "We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach.",
            "Learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language.",
            "While the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Fast Online Lexicon Learning for Grounded Language Acquisition": 0,
            "We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon\u2019s Mechanical Turk we can further improve the results.": 0,
            "It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context.": 0,
            "In this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results.": 0,
            "Recent work by Chen and Mooney (2011) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs.": 0,
            "We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach.": 0,
            "Learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language.": 0,
            "While the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.05247661000028495,
                0.11211674720707095,
                0.09544080659259756,
                0.11332473028308848,
                0.0,
                0.20964255772223236,
                0.04840671733749123
            ],
            [
                0.0,
                0.05247661000028495,
                1.0,
                0.15239471782726993,
                0.045817706267162875,
                0.03154984018244059,
                0.14471321421626757,
                0.10932885751468788,
                0.0
            ],
            [
                0.0,
                0.11211674720707095,
                0.15239471782726993,
                1.0,
                0.0,
                0.09081099159419102,
                0.09899579911530638,
                0.1601364399912852,
                0.0
            ],
            [
                0.0,
                0.09544080659259756,
                0.045817706267162875,
                0.0,
                1.0,
                0.0,
                0.05586042960402971,
                0.0,
                0.05590657263004891
            ],
            [
                0.0,
                0.11332473028308848,
                0.03154984018244059,
                0.09081099159419102,
                0.0,
                1.0,
                0.03846520854302776,
                0.08762483227557673,
                0.029102950738360035
            ],
            [
                0.0,
                0.0,
                0.14471321421626757,
                0.09899579911530638,
                0.05586042960402971,
                0.03846520854302776,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.20964255772223236,
                0.10932885751468788,
                0.1601364399912852,
                0.0,
                0.08762483227557673,
                0.0,
                1.0,
                0.1008497138537648
            ],
            [
                0.0,
                0.04840671733749123,
                0.0,
                0.0,
                0.05590657263004891,
                0.029102950738360035,
                0.0,
                0.1008497138537648,
                1.0
            ]
        ]
    },
    "P12-2002": {
        "input_sentences": [
            "Abstract",
            "Joint Evaluation of Morphological Segmentation and Syntactic Parsing",
            "The protocol uses distance-based metrics defined for the space of trees over lattices.",
            "Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios.",
            "We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance.",
            "Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags)."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Joint Evaluation of Morphological Segmentation and Syntactic Parsing": 0,
            "The protocol uses distance-based metrics defined for the space of trees over lattices.": 0,
            "Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios.": 0,
            "We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance.": 0,
            "Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags).": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.22210472056362057,
                0.29182724387508596,
                0.07470932013190602
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.05082834483312084,
                0.034302681678313705
            ],
            [
                0.0,
                0.22210472056362057,
                0.0,
                1.0,
                0.1767222002895234,
                0.13069902491299487
            ],
            [
                0.0,
                0.29182724387508596,
                0.05082834483312084,
                0.1767222002895234,
                1.0,
                0.1935395531078669
            ],
            [
                0.0,
                0.07470932013190602,
                0.034302681678313705,
                0.13069902491299487,
                0.1935395531078669,
                1.0
            ]
        ]
    },
    "W07-0718": {
        "input_sentences": [
            "Abstract",
            "(Meta-) Evaluation of Machine Translation",
            "We measured the correlation of automatic evaluation metrics with human judgments.",
            "This meta-evaluation reveals surprising facts about the most commonly used methodologies.",
            "We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process.",
            "We measured timing and intraand inter-annotator agreement for three types of subjective evaluation.",
            "j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back."
        ],
        "authority_scores": {
            "Abstract": 0,
            "(Meta-) Evaluation of Machine Translation": 0,
            "We measured the correlation of automatic evaluation metrics with human judgments.": 0,
            "This meta-evaluation reveals surprising facts about the most commonly used methodologies.": 0,
            "We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process.": 0,
            "We measured timing and intraand inter-annotator agreement for three types of subjective evaluation.": 0,
            "j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1864382730940362
            ],
            [
                0.0,
                1.0,
                0.07963518157001334,
                0.2415969704624687,
                0.10304902942897998,
                0.06712138575512071,
                0.30232818773998626
            ],
            [
                0.0,
                0.07963518157001334,
                1.0,
                0.04628826363199768,
                0.14510032787430022,
                0.1457322824055158,
                0.0
            ],
            [
                0.0,
                0.2415969704624687,
                0.04628826363199768,
                1.0,
                0.05989765512164341,
                0.03901457041881044,
                0.0
            ],
            [
                0.0,
                0.10304902942897998,
                0.14510032787430022,
                0.05989765512164341,
                1.0,
                0.056019209281393846,
                0.04205362625273956
            ],
            [
                0.0,
                0.06712138575512071,
                0.1457322824055158,
                0.03901457041881044,
                0.056019209281393846,
                1.0,
                0.0
            ],
            [
                0.1864382730940362,
                0.30232818773998626,
                0.0,
                0.0,
                0.04205362625273956,
                0.0,
                1.0
            ]
        ]
    },
    "W10-2009": {
        "input_sentences": [
            "Abstract",
            "Surprisal is estimated using a Probabilistic Context-Free Grammar (PCFG), which is induced from an automatically annotated corpus.",
            "This paper investigates whether surprisal theory can account for differential processing difficulty in the NP-/S-coordination ambiguity in Dutch.",
            "Surprisal Theory",
            "Modeling the Noun Phrase versus Sentence Coordination Ambiguity in Dutch: Evidence from Surprisal Theory",
            "We find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987).",
            "We argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Surprisal is estimated using a Probabilistic Context-Free Grammar (PCFG), which is induced from an automatically annotated corpus.": 0,
            "This paper investigates whether surprisal theory can account for differential processing difficulty in the NP-/S-coordination ambiguity in Dutch.": 0,
            "Surprisal Theory": 0,
            "Modeling the Noun Phrase versus Sentence Coordination Ambiguity in Dutch: Evidence from Surprisal Theory": 0,
            "We find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987).": 0,
            "We argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.029821444041626025,
                0.09857900644296301,
                0.030157567104265315,
                0.027383318995465773,
                0.0616275361755965
            ],
            [
                0.0,
                0.029821444041626025,
                1.0,
                0.30251313253882783,
                0.2900606525139063,
                0.1372711146329025,
                0.1705696530065279
            ],
            [
                0.0,
                0.09857900644296301,
                0.30251313253882783,
                1.0,
                0.30592281452658215,
                0.10179175492608217,
                0.0
            ],
            [
                0.0,
                0.030157567104265315,
                0.2900606525139063,
                0.30592281452658215,
                1.0,
                0.08497937157663563,
                0.051204547077405405
            ],
            [
                0.0,
                0.027383318995465773,
                0.1372711146329025,
                0.10179175492608217,
                0.08497937157663563,
                1.0,
                0.04649414993561018
            ],
            [
                0.0,
                0.0616275361755965,
                0.1705696530065279,
                0.0,
                0.051204547077405405,
                0.04649414993561018,
                1.0
            ]
        ]
    },
    "P06-1012": {
        "input_sentences": [
            "Abstract",
            "This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains.",
            "Estimating Class Priors In Domain Adaptation For Word Sense Disambiguation",
            "This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations.",
            "Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word).",
            "By using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains.": 0,
            "Estimating Class Priors In Domain Adaptation For Word Sense Disambiguation": 0,
            "This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations.": 0,
            "Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word).": 0,
            "By using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.2009806056637076,
                0.023546720785366973,
                0.36047724524475894,
                0.1714943006580119
            ],
            [
                0.0,
                0.2009806056637076,
                1.0,
                0.14936129809255014,
                0.1875959301142697,
                0.08481891254557651
            ],
            [
                0.0,
                0.023546720785366973,
                0.14936129809255014,
                1.0,
                0.09687471108506998,
                0.2879163602518612
            ],
            [
                0.0,
                0.36047724524475894,
                0.1875959301142697,
                0.09687471108506998,
                1.0,
                0.05501296354769951
            ],
            [
                0.0,
                0.1714943006580119,
                0.08481891254557651,
                0.2879163602518612,
                0.05501296354769951,
                1.0
            ]
        ]
    },
    "J01-2004": {
        "input_sentences": [
            " A small recognition experiment also demonstrates the utility of the model",
            " A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers",
            " The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling",
            " A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity",
            " Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model",
            "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition"
        ],
        "authority_scores": {
            " A small recognition experiment also demonstrates the utility of the model": 0,
            " A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers": 0,
            " The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling": 0,
            " A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity": 0,
            " Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model": 0,
            "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.05981767912992207,
                0.1231431460008623,
                0.09431933278829903
            ],
            [
                0.0,
                1.0,
                0.022842658362148653,
                0.02570647782038806,
                0.033702023615509155,
                0.19448803060610512
            ],
            [
                0.0,
                0.022842658362148653,
                1.0,
                0.17556684056262958,
                0.0234514994017299,
                0.266685428162317
            ],
            [
                0.05981767912992207,
                0.02570647782038806,
                0.17556684056262958,
                1.0,
                0.10556660081636961,
                0.07496673478971269
            ],
            [
                0.1231431460008623,
                0.033702023615509155,
                0.0234514994017299,
                0.10556660081636961,
                1.0,
                0.0
            ],
            [
                0.09431933278829903,
                0.19448803060610512,
                0.266685428162317,
                0.07496673478971269,
                0.0,
                1.0
            ]
        ]
    },
    "N07-1050": {
        "input_sentences": [
            "Abstract",
            "An open issue in data-driven dependency parsing is how to handle non-projective dependencies, which seem to be required by linguistically adequate representations, but which pose problems in parsing with respect to both accuracy and efficiency.",
            "Incremental Non-Projective Dependency Parsing",
            "Moreover, by restricting the class of permissible structures to limited degrees of non-projectivity, the parsing time can be reduced by up to 50% without a significant decrease in accuracy.",
            "Using data from five different languages, we evaluate an incremental deterministic parser that derives non-projective depenstructures in supported by SVM classifiers for predicting the next parser action.",
            "The experiments show that unrestricted non-projective parsing gives a significant improvement in accuracy, compared to a strictly projective baseline, with up to 35% error reduction, leading to state-of-the-art results for the given data sets."
        ],
        "authority_scores": {
            "Abstract": 0,
            "An open issue in data-driven dependency parsing is how to handle non-projective dependencies, which seem to be required by linguistically adequate representations, but which pose problems in parsing with respect to both accuracy and efficiency.": 0,
            "Incremental Non-Projective Dependency Parsing": 0,
            "Moreover, by restricting the class of permissible structures to limited degrees of non-projectivity, the parsing time can be reduced by up to 50% without a significant decrease in accuracy.": 0,
            "Using data from five different languages, we evaluate an incremental deterministic parser that derives non-projective depenstructures in supported by SVM classifiers for predicting the next parser action.": 0,
            "The experiments show that unrestricted non-projective parsing gives a significant improvement in accuracy, compared to a strictly projective baseline, with up to 35% error reduction, leading to state-of-the-art results for the given data sets.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.3208848098088128,
                0.09915044379040928,
                0.06358639054604527,
                0.14531360531602286
            ],
            [
                0.0,
                0.3208848098088128,
                1.0,
                0.11311588004763545,
                0.2008221971370781,
                0.1956096041616785
            ],
            [
                0.0,
                0.09915044379040928,
                0.11311588004763545,
                1.0,
                0.01742879032958039,
                0.11150164598030597
            ],
            [
                0.0,
                0.06358639054604527,
                0.2008221971370781,
                0.01742879032958039,
                1.0,
                0.07736782028398499
            ],
            [
                0.0,
                0.14531360531602286,
                0.1956096041616785,
                0.11150164598030597,
                0.07736782028398499,
                1.0
            ]
        ]
    },
    "P12-1053": {
        "input_sentences": [
            "Abstract",
            "Strong Lexicalization of Tree Adjoining Grammars",
            "It can be effectively constructed and the maximal rank of the nononly increases by Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.",
            "it was shown Tree-adjoining grammars are not closed unstrong Comput.",
            "A more powerful model, the simple context-free tree grammar, admits such a normal form.",
            "Linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Strong Lexicalization of Tree Adjoining Grammars": 0,
            "It can be effectively constructed and the maximal rank of the nononly increases by Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.": 0,
            "it was shown Tree-adjoining grammars are not closed unstrong Comput.": 0,
            "A more powerful model, the simple context-free tree grammar, admits such a normal form.": 0,
            "Linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.2562760422343445,
                0.2517783560447331,
                0.05519139536088405,
                0.18362100553732805
            ],
            [
                0.0,
                0.2562760422343445,
                1.0,
                0.19806227657571074,
                0.2570656466468799,
                0.15131514869205756
            ],
            [
                0.0,
                0.2517783560447331,
                0.19806227657571074,
                1.0,
                0.04265452719365379,
                0.14191101933199168
            ],
            [
                0.0,
                0.05519139536088405,
                0.2570656466468799,
                0.04265452719365379,
                1.0,
                0.17426826981755877
            ],
            [
                0.0,
                0.18362100553732805,
                0.15131514869205756,
                0.14191101933199168,
                0.17426826981755877,
                1.0
            ]
        ]
    },
    "W05-0602": {
        "input_sentences": [
            "Abstract",
            "We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer.",
            "A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation.",
            "It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label.",
            "A Statistical Semantic Parser That Integrates Syntax And Semantics",
            "We introduce a learning semantic parser,SCISSOR, that maps natural-language sentences to a detailed, formal, meaning representation language.",
            "We present experimentalresults demonstrating that SCISSOR produces more accurate semantic representa tions than several previous approaches."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer.": 0,
            "A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation.": 0,
            "It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label.": 0,
            "A Statistical Semantic Parser That Integrates Syntax And Semantics": 0,
            "We introduce a learning semantic parser,SCISSOR, that maps natural-language sentences to a detailed, formal, meaning representation language.": 0,
            "We present experimentalresults demonstrating that SCISSOR produces more accurate semantic representa tions than several previous approaches.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.18225564670997887,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.18519879129409944,
                0.11044697523775737,
                0.12951252565990962,
                0.0
            ],
            [
                0.0,
                0.0,
                0.18519879129409944,
                1.0,
                0.20620154470123164,
                0.06790432215118136,
                0.03238272028101808
            ],
            [
                0.0,
                0.0,
                0.11044697523775737,
                0.20620154470123164,
                1.0,
                0.12148827108582502,
                0.05793623403290721
            ],
            [
                0.0,
                0.18225564670997887,
                0.12951252565990962,
                0.06790432215118136,
                0.12148827108582502,
                1.0,
                0.09564738235256108
            ],
            [
                0.0,
                0.0,
                0.0,
                0.03238272028101808,
                0.05793623403290721,
                0.09564738235256108,
                1.0
            ]
        ]
    },
    "P13-1126": {
        "input_sentences": [
            "Abstract",
            "An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.",
            "Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set.",
            "This is a simple, computationally cheap form of instance weighting for phrase pairs.",
            "Thus, we obtain a decoding feature whose value represents the phrase pair\u2019s closeness to the dev.",
            "Vector Space Model for Adaptation in Statistical Machine Translation",
            "This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM).",
            "The general idea is first to create a vector profile for the in-domain development (\u201cdev\u201d) set.",
            "This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set.",
            "Experiments on large scale NIST evaluation data show improvements over strong baselines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation."
        ],
        "authority_scores": {
            "Abstract": 0,
            "An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.": 0,
            "Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set.": 0,
            "This is a simple, computationally cheap form of instance weighting for phrase pairs.": 0,
            "Thus, we obtain a decoding feature whose value represents the phrase pair\u2019s closeness to the dev.": 0,
            "Vector Space Model for Adaptation in Statistical Machine Translation": 0,
            "This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM).": 0,
            "The general idea is first to create a vector profile for the in-domain development (\u201cdev\u201d) set.": 0,
            "This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set.": 0,
            "Experiments on large scale NIST evaluation data show improvements over strong baselines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.05857638918075466,
                0.09355898246472097,
                0.0,
                0.0,
                0.021072560990227845
            ],
            [
                0.0,
                0.0,
                1.0,
                0.04544084608653521,
                0.15798863050399087,
                0.0934550873160405,
                0.05626738634331536,
                0.25840313958025823,
                0.2971124592169028,
                0.034449902594650764
            ],
            [
                0.0,
                0.0,
                0.04544084608653521,
                1.0,
                0.060465180825283425,
                0.0,
                0.0,
                0.0,
                0.1825515253287597,
                0.0
            ],
            [
                0.0,
                0.0,
                0.15798863050399087,
                0.060465180825283425,
                1.0,
                0.0,
                0.0,
                0.06216665349451435,
                0.08070980994936608,
                0.0
            ],
            [
                0.0,
                0.05857638918075466,
                0.0934550873160405,
                0.0,
                0.0,
                1.0,
                0.6020794368639761,
                0.06716357948527811,
                0.08719722602173322,
                0.08415434681393986
            ],
            [
                0.0,
                0.09355898246472097,
                0.05626738634331536,
                0.0,
                0.0,
                0.6020794368639761,
                1.0,
                0.12330983538159683,
                0.05249965673926595,
                0.05066760173939264
            ],
            [
                0.0,
                0.0,
                0.25840313958025823,
                0.0,
                0.06216665349451435,
                0.06716357948527811,
                0.12330983538159683,
                1.0,
                0.241100164943486,
                0.0
            ],
            [
                0.0,
                0.0,
                0.2971124592169028,
                0.1825515253287597,
                0.08070980994936608,
                0.08719722602173322,
                0.05249965673926595,
                0.241100164943486,
                1.0,
                0.0
            ],
            [
                0.0,
                0.021072560990227845,
                0.034449902594650764,
                0.0,
                0.0,
                0.08415434681393986,
                0.05066760173939264,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P13-2073": {
        "input_sentences": [
            "Abstract",
            "An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs.",
            "Although pivoting is a robust technique, it introduces some low quality translations.",
            "In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT.",
            "The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table.",
            "We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.",
            "Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation",
            "One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages."
        ],
        "authority_scores": {
            "Abstract": 0,
            "An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs.": 0,
            "Although pivoting is a robust technique, it introduces some low quality translations.": 0,
            "In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT.": 0,
            "The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table.": 0,
            "We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.": 0,
            "Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation": 0,
            "One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.11355005877486496,
                0.0,
                0.0606165007200394,
                0.3447612208851409,
                0.1328606587689343
            ],
            [
                0.0,
                0.0,
                1.0,
                0.07428813551794945,
                0.047988814491602215,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.11355005877486496,
                0.07428813551794945,
                1.0,
                0.16756985182876785,
                0.06499006365859616,
                0.3760183457789695,
                0.10371270638964288
            ],
            [
                0.0,
                0.0,
                0.047988814491602215,
                0.16756985182876785,
                1.0,
                0.0,
                0.40515841097516675,
                0.2675773871890108
            ],
            [
                0.0,
                0.0606165007200394,
                0.0,
                0.06499006365859616,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.3447612208851409,
                0.0,
                0.3760183457789695,
                0.40515841097516675,
                0.0,
                1.0,
                0.11609226842955357
            ],
            [
                0.0,
                0.1328606587689343,
                0.0,
                0.10371270638964288,
                0.2675773871890108,
                0.0,
                0.11609226842955357,
                1.0
            ]
        ]
    },
    "N06-1037": {
        "input_sentences": [
            "Abstract",
            "Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes.",
            "This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.",
            "Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.",
            "It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types.",
            "Exploring Syntactic Features For Relation Extraction Using A Convolution Tree Kernel"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes.": 0,
            "This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.": 0,
            "Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.": 0,
            "It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types.": 0,
            "Exploring Syntactic Features For Relation Extraction Using A Convolution Tree Kernel": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.16362586876355228,
                0.09275538126828375,
                0.21593527758551823,
                0.09800488039611685
            ],
            [
                0.0,
                0.16362586876355228,
                1.0,
                0.2920669372368839,
                0.028405861902009347,
                0.28883772217176423
            ],
            [
                0.0,
                0.09275538126828375,
                0.2920669372368839,
                1.0,
                0.10975301996478114,
                0.5269466286923327
            ],
            [
                0.0,
                0.21593527758551823,
                0.028405861902009347,
                0.10975301996478114,
                1.0,
                0.1053847275547428
            ],
            [
                0.0,
                0.09800488039611685,
                0.28883772217176423,
                0.5269466286923327,
                0.1053847275547428,
                1.0
            ]
        ]
    },
    "W12-3407": {
        "input_sentences": [
            "To learn arc scores, it uses large-margin structured learning algorithms, which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set.",
            "The MST Parser can be considered a representative of global, exhaustive graph-based parsing (McDonald et al., 2005, 2006).",
            "For example, this analyzer assigns tags of the form &NCSUBJ> (see figure 1), meaning that the corresponding wordform is a non-clausal syntactic subject and that its head is situated to its right (the \u201c>\u201d or \u201c<\u201d symbols mark the direction of the head).",
            "If only categorial (POS) ambiguity is taken into account, there is an average of 1.55 interpretations per wordform, which rises to 2.65 when the full morphosyntactic information is taken into account, giving an overall 64% of ambiguous word-forms. can pose an important problem, as determining the correct interpretation for each word-form requires in many cases the inspection of local contexts, and in some others, as the agreement of verbs with subject, object or indirect object, it could also suppose the examination of elements which can be far from each other, added to the free constituent order of the main sentence elements in Basque.",
            "In the rest of this paper, section 2 will first present the corpus and the different parsers we will combine, followed by the experimental results in section 3, and the main conclusions of the work.",
            "The results show a modest improvement over the baseline, although they also present interesting lines for further research.",
            "The first step consisted in applying the complete set of text processing tools for Basque, including: properties, such as case, number, tense, or different types of subordination for verbs.",
            "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al., 2007b) and MST Parser (McDonald et al., 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (McDonald and Nivre, 2007).",
            "This is in contrast to the local but richer contexts used by transition-based parsers.",
            "Several variants of the parser have been implemented, and we will use one of its standard versions (MaltParser version 1.4).",
            "As both the chunker and the partial dependency analyzer are based on a set of local rules in the CG formalism, we could expect that the stacked parsers could benefit mostly on the local dependency relations.",
            "The analyzers are a rulebased chunker, a rule-based shallow dependency parser and two state of the art data-driven dependency parsers, MaltParser and MST.",
            "However, when examining the scores for the output dependency relations, we noticed that the gold partial dependency tags are beneficial for some relations, although negative for some others.",
            "The experiments were performed on the development set, leaving the best system for the final test.",
            "Combining Rule-Based and Statistical Syntactic Analyzers",
            "As could be expected, the gold tags gave a noticeable improvement to the parser\u2019s results, reaching 95% LAS.",
            "This subsection will present the four types of analyzers that have been used.",
            "MaltParser (Nivre, 2006) is a representative of local, greedy, transition-based dependency parsing models, where the parser obtains deterministically a dependency tree in a single pass over the input using two data structures: a stack of partially analyzed items and the remaining input sequence.",
            "This paper presents the results of a set of preliminary experiments combining two knowledge-based partial dependency analyzers with two statistical parsers, applied to the Basque Dependency Treebank.",
            "In our work we will study the use of two partial rule-based syntactic analyzers together with two data-driven parsers: set of predefined tags to each word, where each tag gives both the name of a dependency relation (e.g. subject) together with the direction of its head (left or right).",
            "For example the non-clausal modifier (ncmod) relation\u2019s f-score increases 3.25 points, while the dependency relation for clausal subordinate sentences functioning as indirect object decreases 0.46 points, which is surprising in principle.",
            "In this paper we will experiment the use of the stacking technique, giving the tags obtained by the rulebased syntactic partial parsers as input to the statistical parsers.",
            "For all those reasons, the relation between the input dependency tags and the obtained results seems to be intricate, and we think that it deserves new experiments in order to determine their nature.",
            "The rule-based dependency analyzer (RBDA, Aranzabe et al., 2004) uses a set of 505 CG rules that try to assign dependency relations to wordforms.",
            "This means that the result of this analysis is on the one hand a partial analysis and, on the other hand, it does not define a dependency tree, and can also be seen as a set of constraints on the shape of the tree.",
            "For evaluation, we divided the treebank in three sets, corresponding to training, development, and test (80%, 10%, and 10%, respectively).",
            "The last two lines of the sentence in figure 1 do not properly correspond to the treebank, but are the result of the rule-based partial syntactic analyzers (see subsection 2.2).",
            "The table shows the differences in f-score2 corresponding to five local dependency relations, (determination of verbal modifiers, such as subject, object and indirect object).",
            "We will experiment the effect of using the output of the knowledge-based analyzers as input to the data-driven parsers in a stacked learning scheme.",
            "We use the freely available version of MSTParser1.",
            "Although the potential gain is in theory high, the experiments have shown very modest improvements, which seem to happen in the set of local dependency relations.",
            "The evaluation of the chunker on the BDT gave a result of 87% precision and 85% recall over all chunks.",
            "The two most successful approaches have been stacking (Martins et al., 2008) and voting (Sagae and Lavie, 2006, Nivre and McDonald, 2008, McDonald and Nivre, 2011).",
            "In the last years, many attempts have been performed trying to combine different parsers (Surdeanu and Manning, 2010), with significant improvements over the best individual parser\u2019s baseline.",
            "Table 1 shows the results of using the output of the knowledge-based analyzers as input to the statistical parsers.",
            "Although not shown in Table 2, we also inspected the results on the long distance relations, where we did not observe noticeable improvements with respect to the baseline on any parser.",
            "We can point out some avenues for further research: schemes, such as voting, trying to get the best from each type of parser.",
            "The rule-based chunker (RBC henceforth, Aranzabe et al., 2009) uses 560 rules, where 479 of the rules deal with noun phrases and the rest with verb phrases.",
            "The information in figure 1 has been simplified due to space reasons, as typically each word contains many morphosyntactic features (case, number, type of subordinated sentence, ...), which are relevant for parsing.",
            "As table 1 shows, the parser type is relevant, as MaltParser seems to be sensitive when using the stacked features, while the partial parsers do not seem to give any significant improvement to MST.",
            "For that reason, MaltParser, seems to mostly benefit of the local nature of the stacked features, while MST does not get a significant improvement, except for some local dependency relations, such as ncobj and ncsubj.",
            "Each word contains its form, lemma, category or coarse part of speech (CPOS), POS, morphosyntactic features such as case, number of subordinate relations, and the dependency relation (headword + dependency).",
            "We have performed three experiments for each statistical parser, trying with the chunks provided by the chunker, the partial dependency parser, and both.",
            "McDonald and Nivre (2007) examined the types of errors made by the two data-driven parsers used in this work, showing how the greedy algorithm of MaltParser performed better with local dependency relations, while the graph-based algorithm of MST was more accurate for global relations.",
            "Yeh (2000) used the output of several baseline diverse parsers to increase the performance of a second transformation-based parser.",
            "The rule-based analyzers are based on the Contraint Grammar (CG) formalism (Karlsson et al., 1995), based on the assignment of morphosyntactic tags to words using a formalism that has the capabilities of finite state automata or regular expressions, by means of a set of rules that examine mainly local contexts of words to determine the correct tag assignment.",
            "Abstract",
            "We performed an additional test using the partial dependency analyzer\u2019s gold dependency relations as input to MaltParser.",
            "The table shows modest gains, suggesting that the rule-based analyzers help the statistical ones, giving slight increases over the baseline, which are statistically significant when applying MaltParser to the output of the rule-based dependency parser and a combination of the chunker and rule-based parsers.",
            "The first one is to mark the beginning of the phrase (B-VP if it is a verb phrase and B-NP whether it's a noun phrase) and the other one to mark the continuation of the phrase (I-NP or I-VP, meaning that the word is inside an NP or VP).",
            "Figure 1 shows how the two last lines of the example sentence contain the tags assigned by the rule-based chunker (B-NP, I-NP, B-VP and I-VP) and the rule-based partial dependency analyzer (&NCSUBJ, &<NCMOD, &<AUXMOD, &CCOMP_OBJ and &MAINV) .",
            "We have presented a preliminary effort to integrate different syntactic analyzers, with the objective of getting the best from each system.",
            "This means that there is room for improvement in the first-stage knowledge-based parsers, which will have, at least in theory, a positive effect on the second-phase statistical parsers, allowing us to test whether knowledge-based and machine learningbased systems can be successfully combined.",
            "As each type of syntactic information can have an important influence on the results on specific relations, their study can shed light on novel schemes of parser combination.",
            "Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish (Nivre et al., 2007a).",
            "First, subsection 2.1 will describe the Basque Dependency Treebank, and then subsection 2.2 will explain the main details of the analyzers that have been employed.",
            "The general idea will be to apply a stacked scheme where the output of the rule-based partial parsers will be given as input to MaltParser and MST, two state of the art statistical parsers.",
            "Regarding the data-driven parsers, they are trained using two kinds of tags as input: syntactic analyzers (two last lines of the example in figure 1).",
            "2 f-score = 2 * precision * recall / (precision + recall) (ncmod = non-clausal modifier, ncobj = non-clausal object, ncpred = non-clausal predicate, ncsubj = non-clausal subject, nciobj = non-clausal indirect object) Table 2 shows how the addition of the rule-based parsers\u2019 tags performs in accord with this behavior, as MaltParser gets f-score improvements for the local relations.",
            "Our work will make use the second version of the Basque dependency Treebank (BDT II, Aduriz et al., 2003), containing 150,000 tokens (11,225 sentences).",
            "The learning configuration can include any kind of information (such as word-form, lemma, category, subcategory or morphological features).",
            "As the analyzers are applied after morphological processing, the errors can be propagated and augmented.",
            "This section will describe the main resources that have been used in the experiments.",
            "The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, and not just over single arc attachments.",
            "Finally, we must also take into account that the rule-based analyzers were developed mainly having linguistic principles in mind, such as coverage of diverse linguistic phenomena or the treatment of specific syntactic constructions (Aranzabe et al., 2004), instead of performanceoriented measures, such as precision and recall.",
            "In this paper we present a set of preliminary experiments on the combination of two knowledge-based partial syntactic analyzers with two state of the art data-driven statistical parsers.",
            "These tags contain errors of the CG-based syntactic taggers.",
            "Most of the experiments on combined parsers have relied on different types of statistical parsers (Sagae and Lavie, 2006, Martins et al., 2008, McDonald and Nivre, 2011), trained on an automatically annotated treebank.",
            "Looking with more detail at the errors made by the different versions of the parsers, we observe significant differences in the results for different dependency relations, seeing that the statistical parsers behave in a different manner regarding to each relation, as shown in table 2.",
            "As the CG formalism only allows the assignment of tags, the rules only aim at marking the name of the dependency relation together with the direction of the head (left or right).",
            "We must take into account that this evaluation was performed on the gold POS tags, rather than on automatically assigned POS tasks, as in the present experiment.",
            "For that reason, the results can serve as an upper bound on the real results.",
            "The chunker delimits the chunks with three tags, using a standard IOB marking style (see figure 1).",
            "The system was evaluated on the BDT, obtaining f-scores between 90% for the auxmod dependency relation between the auxiliary and the main verb and 52% for the subject dependency relation, giving a (macro) average of 65%.",
            "Consequently, the morphological analyzer for Basque (Aduriz et al. 2000) gives a high ambiguity.",
            "For that reason, we performed a matching process trying to link the multiword units given by the morphological analysis module and the treebank, obtaining a correct match for 99% of the sentences.",
            "In our experiments, we will use the StackLazy algorithm with the liblinear classifier.",
            "The last tag marks words that are outside a chunk.",
            "When performing this task, we found the problem of matching the treebank tokens with those obtained from the analyzers, as there were divergences on the treatment of multiword units, mostly coming from Named Entities, verb compounds and complex postpositions (formed with morphemes appearing at two different words).",
            "To determine the best action at each step, the parser uses history-based feature models and discriminative machine learning.",
            "In the following experiments we will make use of the second order non-projective algorithm.",
            "This algorithm finds the highest scoring directed spanning tree in a dependency graph forming a valid dependency tree.",
            "Figure 1 presents an example of a syntactically annotated sentence.",
            "The erroneous assignment of incorrect part of speech or morphological features can difficult the work of the parser.",
            "The experiments have been performed on the Basque Dependency Treebank (Aduriz et al., 2003).",
            "As it was successfully done on part of speech (POS) tagging, where the use of rule-based POS taggers (Tapanainen and Voutilainen, 1994) or a combination of a rulebased POS tagger with a statistical one (Aduriz et al., 1997, Ezeiza et al., 1998) outperformed purely statistical taggers, we think that exploring the combination of knowledge-based and data-driven systems in syntactic processing can be an interesting line of research."
        ],
        "authority_scores": {
            "To learn arc scores, it uses large-margin structured learning algorithms, which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set.": 0,
            "The MST Parser can be considered a representative of global, exhaustive graph-based parsing (McDonald et al., 2005, 2006).": 0,
            "For example, this analyzer assigns tags of the form &NCSUBJ> (see figure 1), meaning that the corresponding wordform is a non-clausal syntactic subject and that its head is situated to its right (the \u201c>\u201d or \u201c<\u201d symbols mark the direction of the head).": 0,
            "If only categorial (POS) ambiguity is taken into account, there is an average of 1.55 interpretations per wordform, which rises to 2.65 when the full morphosyntactic information is taken into account, giving an overall 64% of ambiguous word-forms. can pose an important problem, as determining the correct interpretation for each word-form requires in many cases the inspection of local contexts, and in some others, as the agreement of verbs with subject, object or indirect object, it could also suppose the examination of elements which can be far from each other, added to the free constituent order of the main sentence elements in Basque.": 0,
            "In the rest of this paper, section 2 will first present the corpus and the different parsers we will combine, followed by the experimental results in section 3, and the main conclusions of the work.": 0,
            "The results show a modest improvement over the baseline, although they also present interesting lines for further research.": 0,
            "The first step consisted in applying the complete set of text processing tools for Basque, including: properties, such as case, number, tense, or different types of subordination for verbs.": 0,
            "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al., 2007b) and MST Parser (McDonald et al., 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (McDonald and Nivre, 2007).": 0,
            "This is in contrast to the local but richer contexts used by transition-based parsers.": 0,
            "Several variants of the parser have been implemented, and we will use one of its standard versions (MaltParser version 1.4).": 0,
            "As both the chunker and the partial dependency analyzer are based on a set of local rules in the CG formalism, we could expect that the stacked parsers could benefit mostly on the local dependency relations.": 0,
            "The analyzers are a rulebased chunker, a rule-based shallow dependency parser and two state of the art data-driven dependency parsers, MaltParser and MST.": 0,
            "However, when examining the scores for the output dependency relations, we noticed that the gold partial dependency tags are beneficial for some relations, although negative for some others.": 0,
            "The experiments were performed on the development set, leaving the best system for the final test.": 0,
            "Combining Rule-Based and Statistical Syntactic Analyzers": 0,
            "As could be expected, the gold tags gave a noticeable improvement to the parser\u2019s results, reaching 95% LAS.": 0,
            "This subsection will present the four types of analyzers that have been used.": 0,
            "MaltParser (Nivre, 2006) is a representative of local, greedy, transition-based dependency parsing models, where the parser obtains deterministically a dependency tree in a single pass over the input using two data structures: a stack of partially analyzed items and the remaining input sequence.": 0,
            "This paper presents the results of a set of preliminary experiments combining two knowledge-based partial dependency analyzers with two statistical parsers, applied to the Basque Dependency Treebank.": 0,
            "In our work we will study the use of two partial rule-based syntactic analyzers together with two data-driven parsers: set of predefined tags to each word, where each tag gives both the name of a dependency relation (e.g. subject) together with the direction of its head (left or right).": 0,
            "For example the non-clausal modifier (ncmod) relation\u2019s f-score increases 3.25 points, while the dependency relation for clausal subordinate sentences functioning as indirect object decreases 0.46 points, which is surprising in principle.": 0,
            "In this paper we will experiment the use of the stacking technique, giving the tags obtained by the rulebased syntactic partial parsers as input to the statistical parsers.": 0,
            "For all those reasons, the relation between the input dependency tags and the obtained results seems to be intricate, and we think that it deserves new experiments in order to determine their nature.": 0,
            "The rule-based dependency analyzer (RBDA, Aranzabe et al., 2004) uses a set of 505 CG rules that try to assign dependency relations to wordforms.": 0,
            "This means that the result of this analysis is on the one hand a partial analysis and, on the other hand, it does not define a dependency tree, and can also be seen as a set of constraints on the shape of the tree.": 0,
            "For evaluation, we divided the treebank in three sets, corresponding to training, development, and test (80%, 10%, and 10%, respectively).": 0,
            "The last two lines of the sentence in figure 1 do not properly correspond to the treebank, but are the result of the rule-based partial syntactic analyzers (see subsection 2.2).": 0,
            "The table shows the differences in f-score2 corresponding to five local dependency relations, (determination of verbal modifiers, such as subject, object and indirect object).": 0,
            "We will experiment the effect of using the output of the knowledge-based analyzers as input to the data-driven parsers in a stacked learning scheme.": 0,
            "We use the freely available version of MSTParser1.": 0,
            "Although the potential gain is in theory high, the experiments have shown very modest improvements, which seem to happen in the set of local dependency relations.": 0,
            "The evaluation of the chunker on the BDT gave a result of 87% precision and 85% recall over all chunks.": 0,
            "The two most successful approaches have been stacking (Martins et al., 2008) and voting (Sagae and Lavie, 2006, Nivre and McDonald, 2008, McDonald and Nivre, 2011).": 0,
            "In the last years, many attempts have been performed trying to combine different parsers (Surdeanu and Manning, 2010), with significant improvements over the best individual parser\u2019s baseline.": 0,
            "Table 1 shows the results of using the output of the knowledge-based analyzers as input to the statistical parsers.": 0,
            "Although not shown in Table 2, we also inspected the results on the long distance relations, where we did not observe noticeable improvements with respect to the baseline on any parser.": 0,
            "We can point out some avenues for further research: schemes, such as voting, trying to get the best from each type of parser.": 0,
            "The rule-based chunker (RBC henceforth, Aranzabe et al., 2009) uses 560 rules, where 479 of the rules deal with noun phrases and the rest with verb phrases.": 0,
            "The information in figure 1 has been simplified due to space reasons, as typically each word contains many morphosyntactic features (case, number, type of subordinated sentence, ...), which are relevant for parsing.": 0,
            "As table 1 shows, the parser type is relevant, as MaltParser seems to be sensitive when using the stacked features, while the partial parsers do not seem to give any significant improvement to MST.": 0,
            "For that reason, MaltParser, seems to mostly benefit of the local nature of the stacked features, while MST does not get a significant improvement, except for some local dependency relations, such as ncobj and ncsubj.": 0,
            "Each word contains its form, lemma, category or coarse part of speech (CPOS), POS, morphosyntactic features such as case, number of subordinate relations, and the dependency relation (headword + dependency).": 0,
            "We have performed three experiments for each statistical parser, trying with the chunks provided by the chunker, the partial dependency parser, and both.": 0,
            "McDonald and Nivre (2007) examined the types of errors made by the two data-driven parsers used in this work, showing how the greedy algorithm of MaltParser performed better with local dependency relations, while the graph-based algorithm of MST was more accurate for global relations.": 0,
            "Yeh (2000) used the output of several baseline diverse parsers to increase the performance of a second transformation-based parser.": 0,
            "The rule-based analyzers are based on the Contraint Grammar (CG) formalism (Karlsson et al., 1995), based on the assignment of morphosyntactic tags to words using a formalism that has the capabilities of finite state automata or regular expressions, by means of a set of rules that examine mainly local contexts of words to determine the correct tag assignment.": 0,
            "Abstract": 0,
            "We performed an additional test using the partial dependency analyzer\u2019s gold dependency relations as input to MaltParser.": 0,
            "The table shows modest gains, suggesting that the rule-based analyzers help the statistical ones, giving slight increases over the baseline, which are statistically significant when applying MaltParser to the output of the rule-based dependency parser and a combination of the chunker and rule-based parsers.": 0,
            "The first one is to mark the beginning of the phrase (B-VP if it is a verb phrase and B-NP whether it's a noun phrase) and the other one to mark the continuation of the phrase (I-NP or I-VP, meaning that the word is inside an NP or VP).": 0,
            "Figure 1 shows how the two last lines of the example sentence contain the tags assigned by the rule-based chunker (B-NP, I-NP, B-VP and I-VP) and the rule-based partial dependency analyzer (&NCSUBJ, &<NCMOD, &<AUXMOD, &CCOMP_OBJ and &MAINV) .": 0,
            "We have presented a preliminary effort to integrate different syntactic analyzers, with the objective of getting the best from each system.": 0,
            "This means that there is room for improvement in the first-stage knowledge-based parsers, which will have, at least in theory, a positive effect on the second-phase statistical parsers, allowing us to test whether knowledge-based and machine learningbased systems can be successfully combined.": 0,
            "As each type of syntactic information can have an important influence on the results on specific relations, their study can shed light on novel schemes of parser combination.": 0,
            "Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish (Nivre et al., 2007a).": 0,
            "First, subsection 2.1 will describe the Basque Dependency Treebank, and then subsection 2.2 will explain the main details of the analyzers that have been employed.": 0,
            "The general idea will be to apply a stacked scheme where the output of the rule-based partial parsers will be given as input to MaltParser and MST, two state of the art statistical parsers.": 0,
            "Regarding the data-driven parsers, they are trained using two kinds of tags as input: syntactic analyzers (two last lines of the example in figure 1).": 0,
            "2 f-score = 2 * precision * recall / (precision + recall) (ncmod = non-clausal modifier, ncobj = non-clausal object, ncpred = non-clausal predicate, ncsubj = non-clausal subject, nciobj = non-clausal indirect object) Table 2 shows how the addition of the rule-based parsers\u2019 tags performs in accord with this behavior, as MaltParser gets f-score improvements for the local relations.": 0,
            "Our work will make use the second version of the Basque dependency Treebank (BDT II, Aduriz et al., 2003), containing 150,000 tokens (11,225 sentences).": 0,
            "The learning configuration can include any kind of information (such as word-form, lemma, category, subcategory or morphological features).": 0,
            "As the analyzers are applied after morphological processing, the errors can be propagated and augmented.": 0,
            "This section will describe the main resources that have been used in the experiments.": 0,
            "The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, and not just over single arc attachments.": 0,
            "Finally, we must also take into account that the rule-based analyzers were developed mainly having linguistic principles in mind, such as coverage of diverse linguistic phenomena or the treatment of specific syntactic constructions (Aranzabe et al., 2004), instead of performanceoriented measures, such as precision and recall.": 0,
            "In this paper we present a set of preliminary experiments on the combination of two knowledge-based partial syntactic analyzers with two state of the art data-driven statistical parsers.": 0,
            "These tags contain errors of the CG-based syntactic taggers.": 0,
            "Most of the experiments on combined parsers have relied on different types of statistical parsers (Sagae and Lavie, 2006, Martins et al., 2008, McDonald and Nivre, 2011), trained on an automatically annotated treebank.": 0,
            "Looking with more detail at the errors made by the different versions of the parsers, we observe significant differences in the results for different dependency relations, seeing that the statistical parsers behave in a different manner regarding to each relation, as shown in table 2.": 0,
            "As the CG formalism only allows the assignment of tags, the rules only aim at marking the name of the dependency relation together with the direction of the head (left or right).": 0,
            "We must take into account that this evaluation was performed on the gold POS tags, rather than on automatically assigned POS tasks, as in the present experiment.": 0,
            "For that reason, the results can serve as an upper bound on the real results.": 0,
            "The chunker delimits the chunks with three tags, using a standard IOB marking style (see figure 1).": 0,
            "The system was evaluated on the BDT, obtaining f-scores between 90% for the auxmod dependency relation between the auxiliary and the main verb and 52% for the subject dependency relation, giving a (macro) average of 65%.": 0,
            "Consequently, the morphological analyzer for Basque (Aduriz et al. 2000) gives a high ambiguity.": 0,
            "For that reason, we performed a matching process trying to link the multiword units given by the morphological analysis module and the treebank, obtaining a correct match for 99% of the sentences.": 0,
            "In our experiments, we will use the StackLazy algorithm with the liblinear classifier.": 0,
            "The last tag marks words that are outside a chunk.": 0,
            "When performing this task, we found the problem of matching the treebank tokens with those obtained from the analyzers, as there were divergences on the treatment of multiword units, mostly coming from Named Entities, verb compounds and complex postpositions (formed with morphemes appearing at two different words).": 0,
            "To determine the best action at each step, the parser uses history-based feature models and discriminative machine learning.": 0,
            "In the following experiments we will make use of the second order non-projective algorithm.": 0,
            "This algorithm finds the highest scoring directed spanning tree in a dependency graph forming a valid dependency tree.": 0,
            "Figure 1 presents an example of a syntactically annotated sentence.": 0,
            "The erroneous assignment of incorrect part of speech or morphological features can difficult the work of the parser.": 0,
            "The experiments have been performed on the Basque Dependency Treebank (Aduriz et al., 2003).": 0,
            "As it was successfully done on part of speech (POS) tagging, where the use of rule-based POS taggers (Tapanainen and Voutilainen, 1994) or a combination of a rulebased POS tagger with a statistical one (Aduriz et al., 1997, Ezeiza et al., 1998) outperformed purely statistical taggers, we think that exploring the combination of knowledge-based and data-driven systems in syntactic processing can be an interesting line of research.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.04401562861963607,
                0.0,
                0.03668622499886485,
                0.0,
                0.0,
                0.021153115846837614,
                0.03169370781646804,
                0.0,
                0.0,
                0.08052352277863091,
                0.060876232413639084,
                0.10955721074532628,
                0.03514789972099554,
                0.0,
                0.0,
                0.0,
                0.035353740319489714,
                0.08354110505450917,
                0.04588687200029596,
                0.05128159616625582,
                0.0,
                0.02616473833409722,
                0.11362488152176413,
                0.03912575825519299,
                0.052500963098769476,
                0.044540170495286024,
                0.02433301662331941,
                0.04824935197092149,
                0.0,
                0.055934465192306444,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03392131370428358,
                0.03343967605423698,
                0.0,
                0.02591018626989217,
                0.04519272372254479,
                0.03397072874267086,
                0.050042279092978655,
                0.0,
                0.04068710380257355,
                0.0,
                0.066471691525233,
                0.018468226567212954,
                0.0,
                0.04553895111808621,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02919481904473383,
                0.0,
                0.0,
                0.04112699205954334,
                0.021323320999006815,
                0.04271508269315827,
                0.0,
                0.0,
                0.2831983131345958,
                0.0,
                0.029104642633135093,
                0.0,
                0.0,
                0.020090215396220818,
                0.026573701994707634,
                0.0,
                0.0,
                0.0,
                0.08369999517131749,
                0.0,
                0.0378666341594286,
                0.0,
                0.0,
                0.0,
                0.08817777126616948,
                0.0,
                0.08518766597049135,
                0.057416217417583425,
                0.07295837934154607,
                0.04030254102076003,
                0.0
            ],
            [
                0.04401562861963607,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.3036140948926674,
                0.03128599310732875,
                0.04389817171363615,
                0.023003491601221535,
                0.12745373011797295,
                0.0,
                0.0,
                0.04497090861157849,
                0.036505995678613075,
                0.0,
                0.18925562623870315,
                0.02386553695323614,
                0.019705791739814005,
                0.0,
                0.0,
                0.0,
                0.09004072060847834,
                0.0,
                0.0,
                0.026153359683941627,
                0.0,
                0.02600389932794601,
                0.0,
                0.0,
                0.0,
                0.20964347260809882,
                0.030536125909376453,
                0.0327094756603783,
                0.032237087638829084,
                0.040096076536903495,
                0.0729112609156155,
                0.06201113015526176,
                0.09609657972964741,
                0.05384286293945442,
                0.0,
                0.08532963810913831,
                0.20997463835345137,
                0.058907645685214734,
                0.0812730011581006,
                0.0,
                0.0,
                0.07047270947155744,
                0.0,
                0.031680027595898975,
                0.0,
                0.03316232538139743,
                0.031442577808975844,
                0.042984232225177124,
                0.0,
                0.07421591653027436,
                0.0,
                0.008989163790726812,
                0.061752512636435435,
                0.0,
                0.0,
                0.0,
                0.13036904349838263,
                0.06277057283558696,
                0.025161677887197573,
                0.0357825381917103,
                0.16957663902878078,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08790840635929519,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05589199042178537,
                0.0,
                0.05815274497981286,
                0.0,
                0.040960328312163084,
                0.11671648960219948,
                0.09937124423406445
            ],
            [
                0.0,
                0.0,
                1.0,
                0.07077902997399702,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04447035790816026,
                0.0,
                0.02807158838369203,
                0.0,
                0.059048220032726896,
                0.02943384352642868,
                0.0,
                0.0,
                0.0,
                0.29203821137633906,
                0.13942917561219328,
                0.05983244157554917,
                0.026495018017331555,
                0.03961634060496744,
                0.0,
                0.05200350542504922,
                0.08113665728350765,
                0.09405922580852698,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03513365226126543,
                0.0,
                0.051304391810690464,
                0.04474273516798173,
                0.0,
                0.0,
                0.0,
                0.015084198879633807,
                0.0,
                0.05482714205860479,
                0.0,
                0.09849383471629984,
                0.14788037021381117,
                0.03368161759583641,
                0.0,
                0.02805047742018701,
                0.026676379008080005,
                0.0,
                0.0,
                0.1647734670410053,
                0.26028328221255825,
                0.0,
                0.05341955203859745,
                0.0,
                0.0,
                0.0,
                0.01875813992326458,
                0.03303807590610499,
                0.0894462367965095,
                0.0,
                0.0,
                0.2624434623500735,
                0.026993713926450034,
                0.0,
                0.07777345751273111,
                0.03493755454661981,
                0.050074830972771205,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.058906698034624054,
                0.0,
                0.13133416173019807,
                0.0,
                0.0,
                0.014847878707223601
            ],
            [
                0.03668622499886485,
                0.0,
                0.07077902997399702,
                1.0,
                0.02341639934842727,
                0.0,
                0.04769150292941624,
                0.0,
                0.07144121325912926,
                0.0,
                0.03795159625289616,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.012442959055622429,
                0.022346468824926285,
                0.06419362674530697,
                0.05796206289158036,
                0.031197349603567816,
                0.032565551990231846,
                0.0,
                0.0,
                0.0,
                0.02839907900942417,
                0.1749913144692058,
                0.0,
                0.0,
                0.01981486945931084,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11514864273278817,
                0.0,
                0.03647697629346304,
                0.11727043724876425,
                0.0,
                0.013635762544877842,
                0.0,
                0.06216097824890855,
                0.0,
                0.0,
                0.02054040997146022,
                0.023959474948896633,
                0.017200153586177422,
                0.0,
                0.0,
                0.0635537225485412,
                0.01185930096383193,
                0.05279973902601382,
                0.0,
                0.0,
                0.07575788210893404,
                0.01703744826223436,
                0.1100068714029169,
                0.0,
                0.04484775376834672,
                0.0,
                0.04167490871008084,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.12565347262372736,
                0.0,
                0.0,
                0.12497762377142682,
                0.06755102786273973,
                0.02414399234121448,
                0.0,
                0.0,
                0.025956687082853667,
                0.0,
                0.040793337061553483,
                0.0,
                0.03660892351179485,
                0.0,
                0.032201947225282565,
                0.04421630241559504
            ],
            [
                0.0,
                0.0,
                0.0,
                0.02341639934842727,
                1.0,
                0.11986413314473537,
                0.03461866103855976,
                0.05068001350520505,
                0.02901479499449704,
                0.0,
                0.02133355941994148,
                0.024087922276649253,
                0.0,
                0.0,
                0.0,
                0.041365506747934065,
                0.09169109882549814,
                0.0,
                0.1248218828055438,
                0.06540772234875038,
                0.0,
                0.10746844368212062,
                0.03723536294532794,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.024116153368363157,
                0.0,
                0.0,
                0.0,
                0.0,
                0.12862949023571235,
                0.08488569557724056,
                0.03652834120722134,
                0.0,
                0.056077830100099174,
                0.0,
                0.02281001916957183,
                0.0,
                0.0,
                0.0,
                0.05486662882006975,
                0.022101684571526427,
                0.0,
                0.0,
                0.0,
                0.014615267354781473,
                0.0,
                0.0,
                0.04855974347437785,
                0.03075491544031112,
                0.03562806986501691,
                0.039550780108699064,
                0.05958597940179076,
                0.0400716118569038,
                0.023894552282583692,
                0.008336597903897047,
                0.04352042613717719,
                0.0,
                0.0,
                0.3468838759853325,
                0.0,
                0.0,
                0.1448758666961397,
                0.0,
                0.06963379904829718,
                0.15774744434154933,
                0.0,
                0.049937234977327986,
                0.09245053401025695,
                0.0,
                0.04322568897973829,
                0.0,
                0.0,
                0.0,
                0.0,
                0.029396165758163742,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06656358109736413,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.11986413314473537,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.15784543117356964,
                0.1437377501415755,
                0.0,
                0.06239388982344334,
                0.0,
                0.0,
                0.0,
                0.05837128537037522,
                0.0,
                0.0,
                0.0,
                0.10803486956565231,
                0.0,
                0.0,
                0.0,
                0.11104001822836909,
                0.0,
                0.0,
                0.07779126841640799,
                0.08551542018665982,
                0.13938743221602357,
                0.1259341289646825,
                0.0,
                0.0,
                0.09222019573068417,
                0.08289961144328567,
                0.0,
                0.0,
                0.0,
                0.08935642105390958,
                0.0,
                0.0,
                0.0,
                0.13193944672072444,
                0.0,
                0.06543227505987852,
                0.0,
                0.06217058171896686,
                0.05585164394234836,
                0.043587576739912114,
                0.0,
                0.0,
                0.10643042629061492,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08659262965670726,
                0.0,
                0.0,
                0.04481954610327172,
                0.0,
                0.07828312558008348,
                0.14492826379266974,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11217986677308436
            ],
            [
                0.021153115846837614,
                0.0,
                0.0,
                0.04769150292941624,
                0.03461866103855976,
                0.0,
                1.0,
                0.02365105965660192,
                0.0,
                0.0,
                0.03169032246796871,
                0.0,
                0.0,
                0.04186100046331344,
                0.0,
                0.0,
                0.10168555446099041,
                0.0,
                0.07461927774942952,
                0.027147309006263144,
                0.0,
                0.0,
                0.0,
                0.028231268373532164,
                0.02314734047357215,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03309160429732102,
                0.0,
                0.0,
                0.03628744932997851,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10699304597654437,
                0.0,
                0.0,
                0.1050675024534704,
                0.0,
                0.040244161902271564,
                0.0,
                0.017511338109759536,
                0.0,
                0.0,
                0.049205337229164696,
                0.0,
                0.0,
                0.04486558475338539,
                0.0,
                0.0,
                0.02215220362265793,
                0.04357258613534137,
                0.0,
                0.0,
                0.0,
                0.03182455899100191,
                0.0,
                0.08534382454363935,
                0.0,
                0.028059143709315917,
                0.0,
                0.03466350673643413,
                0.0,
                0.07773063605983337,
                0.08995253296361445,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04530416892438609,
                0.0,
                0.0,
                0.0,
                0.027159866833797317,
                0.07060065574300627,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06015060197528294,
                0.030808967612943425
            ],
            [
                0.03169370781646804,
                0.3036140948926674,
                0.0,
                0.0,
                0.05068001350520505,
                0.0,
                0.02365105965660192,
                1.0,
                0.03964512934654353,
                0.09108911469629588,
                0.07663127248613097,
                0.383813120745145,
                0.04882730709056917,
                0.0,
                0.0,
                0.020809678068982772,
                0.0,
                0.19360698813017949,
                0.12758277551441444,
                0.16646956063829071,
                0.015298620750852915,
                0.08793362431872584,
                0.023042521916108682,
                0.12158853914368116,
                0.017340827020700535,
                0.0,
                0.0,
                0.021429378030400254,
                0.16320875801555726,
                0.039196472140423884,
                0.024790570934595083,
                0.0,
                0.3556665944875053,
                0.07052968347818019,
                0.041448944547313925,
                0.01837625308323336,
                0.02285614812174752,
                0.06420547718009689,
                0.035348485399092466,
                0.11240201556319775,
                0.07729327848794544,
                0.0397999136673313,
                0.0785578701034523,
                0.3033045376877189,
                0.05019369425331708,
                0.06824636501121473,
                0.0,
                0.08904645506448439,
                0.06640796749295927,
                0.0,
                0.016347718832915945,
                0.03317544224320401,
                0.07542738691110881,
                0.01792335504622476,
                0.24489347863960953,
                0.025711025621014218,
                0.17568691122677277,
                0.21361541191122013,
                0.021060270526122634,
                0.10991263014737164,
                0.0,
                0.06310670277729978,
                0.0,
                0.021020503757991187,
                0.05527561217795003,
                0.23677615768012714,
                0.0,
                0.28981796398908016,
                0.1621923659513138,
                0.023402684283955755,
                0.0,
                0.0,
                0.0,
                0.03730329879618098,
                0.100221654108504,
                0.0,
                0.0377977384946736,
                0.0,
                0.02008311266710429,
                0.01897089953829853,
                0.031865938849925994,
                0.041873220170108895,
                0.0,
                0.023348801475789178,
                0.1685581308518529,
                0.1798153991943378
            ],
            [
                0.0,
                0.03128599310732875,
                0.0,
                0.07144121325912926,
                0.02901479499449704,
                0.0,
                0.0,
                0.03964512934654353,
                1.0,
                0.0,
                0.18533820657395997,
                0.0723296769891213,
                0.0,
                0.0,
                0.05930997044904688,
                0.0,
                0.1579032309421129,
                0.14022987000137502,
                0.06645963558115293,
                0.054875770883862175,
                0.0,
                0.07017470101091114,
                0.0,
                0.027026732188015667,
                0.0,
                0.0,
                0.03449240937948276,
                0.05473566101540061,
                0.07241444751109764,
                0.0,
                0.06332093657264654,
                0.0,
                0.0,
                0.030413449613484196,
                0.0910878241163797,
                0.0,
                0.0,
                0.021885132737057687,
                0.0,
                0.03605461555386118,
                0.1165668190236282,
                0.0,
                0.0,
                0.15210028887466367,
                0.156463978980435,
                0.14304843410927126,
                0.0,
                0.0,
                0.08545418667562267,
                0.0,
                0.04178126612398474,
                0.0,
                0.09234889892441485,
                0.0,
                0.018566334673425448,
                0.0,
                0.09183180082230767,
                0.03776888085782185,
                0.04872896783928347,
                0.0,
                0.0,
                0.0,
                0.14331670353027498,
                0.0,
                0.01884129147180545,
                0.07006906847602905,
                0.04719187021709005,
                0.054471985684799246,
                0.05026102926141349,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02982151966155441,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.029827393505456563
            ],
            [
                0.0,
                0.04389817171363615,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09108911469629588,
                0.0,
                1.0,
                0.0,
                0.10968766782148393,
                0.0,
                0.0,
                0.0,
                0.04589904989591514,
                0.0,
                0.06370087586994574,
                0.0,
                0.049522179059997735,
                0.0,
                0.06015221897840899,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.23931939696676083,
                0.0,
                0.0,
                0.0,
                0.038393122573110026,
                0.0,
                0.04053174462244443,
                0.05041286460996164,
                0.0,
                0.0,
                0.10386856022443597,
                0.05245639424361137,
                0.0,
                0.10728509781384266,
                0.03921832391543736,
                0.04410099097308936,
                0.0,
                0.0,
                0.06728753742575574,
                0.06655263049762918,
                0.0,
                0.0,
                0.0,
                0.0,
                0.039532806074259776,
                0.031829312068708863,
                0.0,
                0.05125699545432685,
                0.0,
                0.021327265915358086,
                0.12658005992932592,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08729981722108124,
                0.0,
                0.0,
                0.0,
                0.13308091415185197,
                0.0,
                0.0,
                0.0,
                0.08336891514461477,
                0.0,
                0.0,
                0.04184333181860327,
                0.070285388962023,
                0.0,
                0.0,
                0.05149948982365257,
                0.0,
                0.028418105374505443
            ],
            [
                0.08052352277863091,
                0.023003491601221535,
                0.04447035790816026,
                0.03795159625289616,
                0.02133355941994148,
                0.0,
                0.03169032246796871,
                0.07663127248613097,
                0.18533820657395997,
                0.0,
                1.0,
                0.20152614534433974,
                0.19983687792333626,
                0.052656463676328626,
                0.04360853761019819,
                0.0,
                0.0,
                0.12606466886477646,
                0.21329239976359432,
                0.14151903296007398,
                0.026025014244587048,
                0.09098313404030212,
                0.0391984330394297,
                0.32727356682625797,
                0.08626398674403454,
                0.0,
                0.06839638617404942,
                0.15111157755396218,
                0.1255280685311244,
                0.0,
                0.2164389355925262,
                0.05555866802701218,
                0.0,
                0.022361941024147097,
                0.06697367700128566,
                0.03604129679163543,
                0.0,
                0.14485428836034162,
                0.0,
                0.1353509485099125,
                0.3949088195432375,
                0.09943341339464491,
                0.16236806489997538,
                0.18134508227402676,
                0.04879624198686287,
                0.26607376114944936,
                0.0,
                0.26527875857413225,
                0.1251711200341275,
                0.0,
                0.15985796506210614,
                0.0,
                0.067900901003913,
                0.03515302906402837,
                0.013651173618189044,
                0.04373791722319365,
                0.16312439955011043,
                0.027770131209193468,
                0.06804346228845805,
                0.03194531356228346,
                0.0,
                0.0,
                0.0,
                0.07105397706472756,
                0.013853339692333945,
                0.13652566261478782,
                0.13416493867896476,
                0.04005133737974944,
                0.09526254399516909,
                0.24359059253753804,
                0.0,
                0.0,
                0.05749587253054675,
                0.06345793508781324,
                0.06608750054789847,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02192671572600757,
                0.0,
                0.07123198680070938,
                0.0,
                0.0,
                0.06037883640756232,
                0.021931034557070614
            ],
            [
                0.060876232413639084,
                0.12745373011797295,
                0.0,
                0.0,
                0.024087922276649253,
                0.0,
                0.0,
                0.383813120745145,
                0.0723296769891213,
                0.10968766782148393,
                0.20152614534433974,
                1.0,
                0.09378588683250076,
                0.0,
                0.20215017875868577,
                0.03997054575994314,
                0.06441585506914754,
                0.1668856339871395,
                0.18660058492802764,
                0.2464236283935039,
                0.029385087974879636,
                0.15088014279204495,
                0.04425931884285688,
                0.1417580165299906,
                0.03330769066433956,
                0.0,
                0.11756280890192654,
                0.04116084508044019,
                0.2253200694111392,
                0.0,
                0.04761691395088394,
                0.06273181380027294,
                0.0,
                0.05868319151517582,
                0.1260677820188321,
                0.03529650301772207,
                0.043901338183508255,
                0.09022164289219038,
                0.0,
                0.18596575523480294,
                0.14846238920645985,
                0.076446366215199,
                0.2229009950081997,
                0.22873219951008641,
                0.09350102634553882,
                0.12159196083733374,
                0.0,
                0.17103750452635802,
                0.3114335440188437,
                0.0,
                0.16429518806215498,
                0.03956225665093407,
                0.07666754495585239,
                0.034426591351965796,
                0.09165267184084747,
                0.08780698975745406,
                0.34866372697335535,
                0.19503948591364836,
                0.056055884182811495,
                0.03606975352983943,
                0.0,
                0.04831116182216549,
                0.0,
                0.040375492814959946,
                0.06421804648076974,
                0.3694814560273769,
                0.039178429559629184,
                0.04522234114282219,
                0.07571030540091968,
                0.044951106882888996,
                0.0,
                0.0,
                0.06491912959678729,
                0.07165095041772884,
                0.0,
                0.0,
                0.0,
                0.0,
                0.023949439825428857,
                0.06119634101849954,
                0.0,
                0.08042870521001415,
                0.0,
                0.04484761055573021,
                0.06817431118317673,
                0.14392987913399155
            ],
            [
                0.10955721074532628,
                0.0,
                0.02807158838369203,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04882730709056917,
                0.0,
                0.0,
                0.19983687792333626,
                0.09378588683250076,
                1.0,
                0.0,
                0.0,
                0.11805948194697392,
                0.0,
                0.054465950950775596,
                0.12655824404647895,
                0.10065912905222482,
                0.02676260525387232,
                0.07905191824939693,
                0.0762678717437782,
                0.1433480495048222,
                0.05876686205182084,
                0.0,
                0.044255020194566265,
                0.10775792595393896,
                0.06822642064432727,
                0.0,
                0.12465973129674042,
                0.0,
                0.0,
                0.0,
                0.08581983868327134,
                0.07412553090708206,
                0.0,
                0.0,
                0.0,
                0.04161895254313242,
                0.11474236740742133,
                0.13487915157508734,
                0.10138674312548984,
                0.14172762754979343,
                0.06252733615898161,
                0.020471963436987817,
                0.0,
                0.33569299707878325,
                0.06979983468684052,
                0.0,
                0.08091235379573074,
                0.0,
                0.0,
                0.07229864556282935,
                0.0,
                0.04497752056040993,
                0.09323995774655582,
                0.041495633366904955,
                0.0448992200974276,
                0.032850695432621535,
                0.0,
                0.0,
                0.0,
                0.03677216747014885,
                0.0,
                0.04257696053141566,
                0.057629414187285656,
                0.0,
                0.08896882686505654,
                0.07745996423165355,
                0.10827209430260162,
                0.0,
                0.042089338950433444,
                0.12894816193091654,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07325081655211513,
                0.0,
                0.0,
                0.06209007031762972,
                0.0
            ],
            [
                0.03514789972099554,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04186100046331344,
                0.0,
                0.0,
                0.0,
                0.052656463676328626,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1123157383418093,
                0.045107817758661996,
                0.0,
                0.0,
                0.053966916376008665,
                0.04690891861861408,
                0.03846149227677423,
                0.18491036054767848,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11304577385286503,
                0.0,
                0.0,
                0.136476462254703,
                0.0,
                0.0,
                0.10003157516571347,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.15431111318902097,
                0.0480389725060703,
                0.0,
                0.029096742074255267,
                0.0,
                0.19715035574719522,
                0.0,
                0.0,
                0.0,
                0.09419027328770119,
                0.06707640215070428,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09162932868651535,
                0.04662291723323013,
                0.0,
                0.1184156231371179,
                0.0,
                0.04490943898384221,
                0.0,
                0.0,
                0.06610699222641317,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05383413646689862,
                0.08018580454819928,
                0.0,
                0.0,
                0.08302750546671799,
                0.06760182080007539,
                0.0,
                0.0,
                0.0,
                0.18307319858722282,
                0.0
            ],
            [
                0.0,
                0.04497090861157849,
                0.059048220032726896,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05930997044904688,
                0.0,
                0.04360853761019819,
                0.20215017875868577,
                0.0,
                0.0,
                1.0,
                0.0,
                0.11153075915404126,
                0.02859533202995954,
                0.37306489506742657,
                0.22012776168713014,
                0.0,
                0.16217678098536437,
                0.0,
                0.10477099383271479,
                0.0,
                0.0,
                0.2921516985380887,
                0.0,
                0.11873554351645323,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.26016557033349974,
                0.0,
                0.0,
                0.08483922847465655,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09820413118066786,
                0.03133652981653753,
                0.04517868415268845,
                0.14712476787322482,
                0.0,
                0.0,
                0.33718549461155817,
                0.0,
                0.16196796360531143,
                0.15540097729019334,
                0.11904003223716861,
                0.07237323640964403,
                0.0,
                0.06652470277669102,
                0.18364357095442282,
                0.15608648103611256,
                0.0459581845151896,
                0.0,
                0.0,
                0.08364680633138968,
                0.0,
                0.0,
                0.15958628014584095,
                0.2853734536023934,
                0.18905695206906303,
                0.06294356203147926,
                0.058077710465484246,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.041466486817209894,
                0.042865854721549544,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.19417847690270026
            ],
            [
                0.0,
                0.036505995678613075,
                0.02943384352642868,
                0.0,
                0.041365506747934065,
                0.15784543117356964,
                0.0,
                0.020809678068982772,
                0.0,
                0.04589904989591514,
                0.0,
                0.03997054575994314,
                0.11805948194697392,
                0.0,
                0.0,
                1.0,
                0.0,
                0.02321280800729562,
                0.04987636937860063,
                0.03327721247757157,
                0.0,
                0.040420236143421856,
                0.08436426525305805,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10694209725471276,
                0.0,
                0.03192796219135118,
                0.06835923672759246,
                0.17223623202267607,
                0.04192365526298419,
                0.0,
                0.0,
                0.11156893781442442,
                0.06626821397716791,
                0.0,
                0.08921896207251652,
                0.0,
                0.03667466145033218,
                0.021465424764960075,
                0.0,
                0.09365004284627114,
                0.024251996742965177,
                0.0,
                0.026749065034369728,
                0.0,
                0.049697861554587386,
                0.0775223651343809,
                0.0,
                0.0,
                0.0,
                0.04350932917857552,
                0.015180020037222429,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.060426048448820276,
                0.0,
                0.0358278069079142,
                0.03829279976542728,
                0.11352631131599557,
                0.11585262016484336,
                0.04413184604536232,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03479717789873706,
                0.0,
                0.0,
                0.0,
                0.042827299624622846,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.09169109882549814,
                0.1437377501415755,
                0.10168555446099041,
                0.0,
                0.1579032309421129,
                0.0,
                0.0,
                0.06441585506914754,
                0.0,
                0.0,
                0.11153075915404126,
                0.0,
                1.0,
                0.0,
                0.0591880737167926,
                0.04887163680537758,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.22760160795157844,
                0.0,
                0.06449135057645619,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08112161316201115,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.17534225747008292,
                0.12028096024009033,
                0.03152458889678134,
                0.0,
                0.0,
                0.03908410750042522,
                0.0,
                0.0,
                0.06361811377070682,
                0.0,
                0.0,
                0.05867239894995628,
                0.3718216696063581,
                0.0,
                0.06389874556632737,
                0.0,
                0.0,
                0.0,
                0.07768679669401486,
                0.19132705096819275,
                0.0,
                0.03543052753833895,
                0.17896326076557528,
                0.0,
                0.10331086965789618,
                0.0,
                0.0,
                0.1053754101195176,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.038511913033728765,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.035353740319489714,
                0.18925562623870315,
                0.0,
                0.012442959055622429,
                0.0,
                0.0,
                0.0,
                0.19360698813017949,
                0.14022987000137502,
                0.06370087586994574,
                0.12606466886477646,
                0.1668856339871395,
                0.054465950950775596,
                0.0,
                0.02859533202995954,
                0.02321280800729562,
                0.0,
                1.0,
                0.07012488428097471,
                0.06184159562245125,
                0.01706532629139019,
                0.06906166384086909,
                0.09012334447421859,
                0.06021413340263207,
                0.09486159659314346,
                0.0,
                0.01662995093829878,
                0.05029397532881875,
                0.1642136484481718,
                0.0,
                0.05818257353074322,
                0.0,
                0.09034862191474659,
                0.019416789347735185,
                0.1623638037343891,
                0.020498367793126634,
                0.02549560720630458,
                0.010551558741266556,
                0.03943057659026706,
                0.08801283922827742,
                0.10818331782040431,
                0.044396061851804676,
                0.08762983983358885,
                0.16294421285454128,
                0.03745718597818659,
                0.05874090020350567,
                0.0,
                0.22207393588222002,
                0.08186301845066338,
                0.0,
                0.0383797296303462,
                0.0,
                0.02108669214040434,
                0.01999316847456382,
                0.021189549056388382,
                0.028680169843705607,
                0.10199429666266605,
                0.146321727682257,
                0.027926658590219384,
                0.02094743135575908,
                0.0,
                0.0,
                0.0,
                0.07590241728503368,
                0.009084020467897231,
                0.049996501115745356,
                0.022752788237043117,
                0.06495040284637851,
                0.019736063062332145,
                0.026105258108174784,
                0.0,
                0.0,
                0.03770164411518182,
                0.0416111344982887,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09377335023771356,
                0.0,
                0.1378864363416293,
                0.0,
                0.026045152840004922,
                0.039592083781619075,
                0.029659681163616378
            ],
            [
                0.08354110505450917,
                0.02386553695323614,
                0.0,
                0.022346468824926285,
                0.1248218828055438,
                0.06239388982344334,
                0.07461927774942952,
                0.12758277551441444,
                0.06645963558115293,
                0.0,
                0.21329239976359432,
                0.18660058492802764,
                0.12655824404647895,
                0.1123157383418093,
                0.37306489506742657,
                0.04987636937860063,
                0.0591880737167926,
                0.07012488428097471,
                1.0,
                0.17474774139258692,
                0.027000289779039767,
                0.213474624138537,
                0.1279498021192038,
                0.13211171602859423,
                0.08949669028783026,
                0.04281134709008092,
                0.16393107290879175,
                0.037820364726221335,
                0.1609219223622877,
                0.0,
                0.13253936827438503,
                0.0,
                0.0,
                0.023199944561004385,
                0.3269996006320691,
                0.044043967596986394,
                0.0,
                0.01669438595958711,
                0.0,
                0.06949171029806189,
                0.040271730793646904,
                0.07024222769485934,
                0.20943437023995104,
                0.06522269098715057,
                0.05062485889119061,
                0.07923008053325162,
                0.0,
                0.1517322897050802,
                0.1445564132738006,
                0.0,
                0.08776484431619765,
                0.12755789073722315,
                0.18803654663837716,
                0.04295846739319645,
                0.043071971485155645,
                0.19080096046583864,
                0.14577376072733106,
                0.06532267028920259,
                0.019095335975480163,
                0.11357200280610233,
                0.0,
                0.15576639005890977,
                0.07196627249228975,
                0.07371669243562143,
                0.0346175588390564,
                0.5063145404114047,
                0.03599883560335056,
                0.14994650686922326,
                0.13486016177150906,
                0.0413030212046375,
                0.0,
                0.11147202938048739,
                0.0,
                0.06583599225144784,
                0.05912316882722407,
                0.0395998542843512,
                0.0629784539824795,
                0.0,
                0.05520199120078586,
                0.022748409393396157,
                0.05309491105536313,
                0.07390137300523915,
                0.13737405998109198,
                0.0,
                0.2799476025518569,
                0.09334582317622438
            ],
            [
                0.04588687200029596,
                0.019705791739814005,
                0.29203821137633906,
                0.06419362674530697,
                0.06540772234875038,
                0.0,
                0.027147309006263144,
                0.16646956063829071,
                0.054875770883862175,
                0.049522179059997735,
                0.14151903296007398,
                0.2464236283935039,
                0.10065912905222482,
                0.045107817758661996,
                0.22012776168713014,
                0.03327721247757157,
                0.04887163680537758,
                0.06184159562245125,
                0.17474774139258692,
                1.0,
                0.07112152764399013,
                0.18532748676335162,
                0.09191043272956714,
                0.10715089030972987,
                0.061262385607661195,
                0.0,
                0.16488381872591645,
                0.06099627271428401,
                0.17094798455136592,
                0.05711968522014201,
                0.05372147863645952,
                0.0,
                0.0,
                0.019156211602957083,
                0.09564631019909874,
                0.0,
                0.0,
                0.03717568133052224,
                0.04291553194748022,
                0.05737935724892242,
                0.01662619914181003,
                0.1101491376067381,
                0.06266006691265881,
                0.14111776714189708,
                0.04180098390922647,
                0.13770980120999185,
                0.0,
                0.08263152333966478,
                0.1507583773665076,
                0.02411278415722953,
                0.12646395609116162,
                0.06809511724776841,
                0.05816686602732014,
                0.10358895173432867,
                0.03272363096623739,
                0.04788434509417075,
                0.11874786016094618,
                0.22078931724742626,
                0.06014542332240632,
                0.08722280133637006,
                0.05031587965393618,
                0.03665317415540348,
                0.0,
                0.04555169570486478,
                0.06992907410983204,
                0.2755834749795815,
                0.13085008093202452,
                0.03430971815011161,
                0.07922925971644147,
                0.3693566044425419,
                0.030518459238349147,
                0.0,
                0.03506184942585879,
                0.13979915800980006,
                0.08714848685846988,
                0.0,
                0.05508135826903661,
                0.0973379935205781,
                0.018170189987918046,
                0.01878337867682075,
                0.04643714847170894,
                0.030510209524744942,
                0.0,
                0.06626982763242599,
                0.025861569112328602,
                0.11156186853304693
            ],
            [
                0.05128159616625582,
                0.0,
                0.13942917561219328,
                0.05796206289158036,
                0.0,
                0.0,
                0.0,
                0.015298620750852915,
                0.0,
                0.0,
                0.026025014244587048,
                0.029385087974879636,
                0.02676260525387232,
                0.0,
                0.0,
                0.0,
                0.0,
                0.01706532629139019,
                0.027000289779039767,
                0.07112152764399013,
                1.0,
                0.0,
                0.08058154556533521,
                0.023184338446115803,
                0.009504634516703211,
                0.0,
                0.0,
                0.1346756229057693,
                0.0,
                0.0,
                0.013587893813395127,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.012506902493783719,
                0.12929958606690042,
                0.01639774363648867,
                0.009350618933167309,
                0.0,
                0.0,
                0.0,
                0.0320860280911604,
                0.04879980097152703,
                0.0,
                0.07231594629042616,
                0.0,
                0.0,
                0.0,
                0.02268734253157944,
                0.014092401780238746,
                0.0,
                0.046152110910846075,
                0.41455739125180197,
                0.050476865656133144,
                0.0,
                0.0,
                0.0,
                0.011521492333565843,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06187337273150777,
                0.08184106222593558,
                0.0,
                0.0,
                0.0,
                0.13045262504797064,
                0.0,
                0.04090948974590364,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05009811996264711,
                0.022951019191831613,
                0.060391028018345574,
                0.0,
                0.019454123005826697,
                0.0
            ],
            [
                0.0,
                0.0,
                0.05983244157554917,
                0.031197349603567816,
                0.10746844368212062,
                0.0,
                0.0,
                0.08793362431872584,
                0.07017470101091114,
                0.06015221897840899,
                0.09098313404030212,
                0.15088014279204495,
                0.07905191824939693,
                0.0,
                0.16217678098536437,
                0.040420236143421856,
                0.0,
                0.06906166384086909,
                0.213474624138537,
                0.18532748676335162,
                0.0,
                1.0,
                0.16711375008158272,
                0.0,
                0.028768543892974122,
                0.0,
                0.09193719124410715,
                0.0,
                0.2105859128192589,
                0.06938054581921967,
                0.0,
                0.0,
                0.07143345475298839,
                0.04653626544751912,
                0.20722656919158586,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0972799236505798,
                0.0,
                0.0,
                0.10190150958267497,
                0.03707692979689154,
                0.05345476702277214,
                0.02071448399872349,
                0.0,
                0.11348332208141836,
                0.11398251820118399,
                0.0,
                0.05293425379149795,
                0.04625352226574559,
                0.10428131782944162,
                0.03852051874372558,
                0.05395219877723875,
                0.0,
                0.22231875450357289,
                0.20521766517505732,
                0.034811723368721355,
                0.03669653926524782,
                0.0,
                0.0,
                0.0,
                0.0,
                0.025759749814152112,
                0.27043823741166984,
                0.12283268443039175,
                0.11685033498083647,
                0.1078172207575075,
                0.03695317454430729,
                0.11818574097007659,
                0.0,
                0.042587949167121296,
                0.05758899610868466,
                0.0,
                0.0,
                0.06690468769989219,
                0.0,
                0.055375045914809434,
                0.0,
                0.056404980084154926,
                0.0,
                0.0,
                0.0,
                0.0,
                0.12430049398423144
            ],
            [
                0.02616473833409722,
                0.0,
                0.026495018017331555,
                0.032565551990231846,
                0.03723536294532794,
                0.05837128537037522,
                0.0,
                0.023042521916108682,
                0.0,
                0.0,
                0.0391984330394297,
                0.04425931884285688,
                0.0762678717437782,
                0.053966916376008665,
                0.0,
                0.08436426525305805,
                0.0,
                0.09012334447421859,
                0.1279498021192038,
                0.09191043272956714,
                0.08058154556533521,
                0.16711375008158272,
                1.0,
                0.034919855551378436,
                0.014315718568520415,
                0.0,
                0.0,
                0.017691021576735112,
                0.055527835668240974,
                0.0,
                0.06312732478165518,
                0.0,
                0.0,
                0.0,
                0.13138059445422404,
                0.04120440332734766,
                0.0,
                0.0,
                0.07484958773097639,
                0.0,
                0.10311957694401043,
                0.07705156130963071,
                0.0761814673004347,
                0.014083742920725468,
                0.0,
                0.061603771452359596,
                0.0,
                0.10888814666569034,
                0.013427097829353597,
                0.0,
                0.0375741597157234,
                0.0,
                0.0,
                0.04018888654606796,
                0.04828065288500973,
                0.021225735454201422,
                0.04613276927220659,
                0.09418273073882419,
                0.013664369181976898,
                0.01550285925171978,
                0.0,
                0.0,
                0.0673265257315512,
                0.017353475448933398,
                0.0,
                0.04468795653806222,
                0.05439280265691814,
                0.03299812999481409,
                0.08615005775529203,
                0.10576329971265451,
                0.03457778109054249,
                0.10428530191330504,
                0.039725496775848676,
                0.11364062848686347,
                0.0,
                0.0,
                0.05891816201872889,
                0.0,
                0.05165313782354007,
                0.07521393982588966,
                0.14270199084993532,
                0.034568434065883155,
                0.0,
                0.0,
                0.09038094926238144,
                0.04310632533541221
            ],
            [
                0.11362488152176413,
                0.09004072060847834,
                0.03961634060496744,
                0.0,
                0.0,
                0.0,
                0.028231268373532164,
                0.12158853914368116,
                0.027026732188015667,
                0.0,
                0.32727356682625797,
                0.1417580165299906,
                0.1433480495048222,
                0.04690891861861408,
                0.10477099383271479,
                0.0,
                0.0,
                0.06021413340263207,
                0.13211171602859423,
                0.10715089030972987,
                0.023184338446115803,
                0.0,
                0.034919855551378436,
                1.0,
                0.052217828787877035,
                0.0,
                0.06093080106114496,
                0.06291273727345664,
                0.02246374026132117,
                0.0,
                0.10986262721353653,
                0.0,
                0.051246463093183925,
                0.0,
                0.02825642247157489,
                0.03210732624394159,
                0.0,
                0.2654296022027721,
                0.0,
                0.0,
                0.06699049142726805,
                0.08858008250556373,
                0.04533784842492507,
                0.08859534791153502,
                0.020587300717855615,
                0.17141904805597158,
                0.0,
                0.1947492802419804,
                0.13479375360226417,
                0.0,
                0.13458353101418888,
                0.0,
                0.028647621436856726,
                0.031316014491561364,
                0.037132378335313285,
                0.038963847101126124,
                0.05033228609003731,
                0.0,
                0.034119633369343136,
                0.08180398030343006,
                0.0,
                0.0,
                0.0,
                0.06329831126043761,
                0.16876767277812874,
                0.06057967476391526,
                0.11952060109179194,
                0.05447155878372831,
                0.05194302303555382,
                0.1477717053870621,
                0.0,
                0.0,
                0.0,
                0.056531390543753925,
                0.1348145301046768,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08122259816289232,
                0.0,
                0.06345689092256268,
                0.0,
                0.0,
                0.1546151404836984,
                0.10241930416326182
            ],
            [
                0.03912575825519299,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02314734047357215,
                0.017340827020700535,
                0.0,
                0.0,
                0.08626398674403454,
                0.03330769066433956,
                0.05876686205182084,
                0.03846149227677423,
                0.0,
                0.0,
                0.0,
                0.09486159659314346,
                0.08949669028783026,
                0.061262385607661195,
                0.009504634516703211,
                0.028768543892974122,
                0.014315718568520415,
                0.052217828787877035,
                1.0,
                0.0,
                0.09690241995274572,
                0.013313514297546625,
                0.0,
                0.0,
                0.045805989700672414,
                0.06321477688957719,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.029561616228169958,
                0.06952253468447696,
                0.024726649504219867,
                0.05342753871440818,
                0.010598829011922644,
                0.0,
                0.04790856867145094,
                0.0,
                0.07045633893775129,
                0.010104665700071288,
                0.0,
                0.029194688810233318,
                0.0,
                0.04150681317891563,
                0.0,
                0.031704857425377424,
                0.015973590401194056,
                0.0259662563751412,
                0.0,
                0.0,
                0.011666795917091128,
                0.0,
                0.0,
                0.0,
                0.038839967872535686,
                0.0,
                0.062090584750866845,
                0.0,
                0.0,
                0.010992117173916181,
                0.01453947806480481,
                0.0,
                0.0,
                0.0,
                0.023175567725953878,
                0.0,
                0.10628175380464995,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.22914244595652672,
                0.0,
                0.0,
                0.022051045475106883,
                0.0
            ],
            [
                0.052500963098769476,
                0.0,
                0.05200350542504922,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.18491036054767848,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04281134709008092,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.046915372622660774,
                0.06195076244926812,
                0.0,
                0.0,
                0.0,
                0.07534473215322043,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07561349343828991,
                0.0,
                0.0,
                0.0,
                0.0,
                0.044207457673741915,
                0.0,
                0.0,
                0.04468949843753353,
                0.0,
                0.0,
                0.0,
                0.03264032974508299,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03332929404236285,
                0.0,
                0.0,
                0.06786807336201589,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03322958088352345,
                0.0,
                0.0,
                0.027856065799319926,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.061692464721776356,
                0.0
            ],
            [
                0.044540170495286024,
                0.026153359683941627,
                0.08113665728350765,
                0.02839907900942417,
                0.0,
                0.10803486956565231,
                0.0,
                0.0,
                0.03449240937948276,
                0.0,
                0.06839638617404942,
                0.11756280890192654,
                0.044255020194566265,
                0.0,
                0.2921516985380887,
                0.0,
                0.22760160795157844,
                0.01662995093829878,
                0.16393107290879175,
                0.16488381872591645,
                0.0,
                0.09193719124410715,
                0.0,
                0.06093080106114496,
                0.09690241995274572,
                0.046915372622660774,
                1.0,
                0.0,
                0.06905204881839068,
                0.0,
                0.0,
                0.0983961020181901,
                0.0,
                0.0,
                0.08685837003287654,
                0.0,
                0.0,
                0.049339248996950906,
                0.10967487923431232,
                0.04601373206284602,
                0.0,
                0.0,
                0.0542310786931423,
                0.018224126681920305,
                0.02627419398833847,
                0.08556213542045683,
                0.0,
                0.05305790701868485,
                0.16504530350914517,
                0.0,
                0.26745603421429365,
                0.09037526213691091,
                0.03656104184088774,
                0.04208950501002392,
                0.0,
                0.29118868234038625,
                0.10465315150940796,
                0.249657590963697,
                0.026727521572407046,
                0.0426264865999298,
                0.0,
                0.04864578190520694,
                0.0,
                0.0,
                0.09280927413154065,
                0.16346182809658408,
                0.10994828925779992,
                0.04352623631493549,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07016462844381442,
                0.0,
                0.0,
                0.04339601637958865,
                0.0,
                0.0,
                0.06049382144524374,
                0.024929140889175423,
                0.0,
                0.0,
                0.18831273069604407,
                0.0,
                0.08056698695501355,
                0.06836853835332737
            ],
            [
                0.02433301662331941,
                0.0,
                0.09405922580852698,
                0.1749913144692058,
                0.0,
                0.0,
                0.0,
                0.021429378030400254,
                0.05473566101540061,
                0.0,
                0.15111157755396218,
                0.04116084508044019,
                0.10775792595393896,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05029397532881875,
                0.037820364726221335,
                0.06099627271428401,
                0.1346756229057693,
                0.0,
                0.017691021576735112,
                0.06291273727345664,
                0.013313514297546625,
                0.06195076244926812,
                0.0,
                1.0,
                0.0,
                0.0,
                0.09673555937601726,
                0.0,
                0.0,
                0.0,
                0.1450519426669525,
                0.07921998243631617,
                0.0,
                0.0,
                0.0,
                0.10907018667976667,
                0.12772118586685,
                0.05919589876543631,
                0.022968962559080063,
                0.09112125002353051,
                0.0,
                0.02223857001065956,
                0.0,
                0.08706825343063064,
                0.08237261854877763,
                0.0,
                0.04903034672114464,
                0.0,
                0.0,
                0.031730502851160414,
                0.0,
                0.019739779815653336,
                0.0,
                0.0,
                0.21388711704992108,
                0.014417546511041551,
                0.0,
                0.0,
                0.0,
                0.016138606133927202,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13636447157381157,
                0.017967538194313168,
                0.0,
                0.0,
                0.0,
                0.07026023840828007,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.032148392620119766,
                0.0,
                0.0,
                0.027250152999480342,
                0.0
            ],
            [
                0.04824935197092149,
                0.02600389932794601,
                0.0,
                0.0,
                0.024116153368363157,
                0.0,
                0.0,
                0.16320875801555726,
                0.07241444751109764,
                0.0,
                0.1255280685311244,
                0.2253200694111392,
                0.06822642064432727,
                0.0,
                0.11873554351645323,
                0.0,
                0.06449135057645619,
                0.1642136484481718,
                0.1609219223622877,
                0.17094798455136592,
                0.0,
                0.2105859128192589,
                0.055527835668240974,
                0.02246374026132117,
                0.0,
                0.0,
                0.06905204881839068,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.025278669571123197,
                0.47759454134137413,
                0.0,
                0.0,
                0.01819021012121466,
                0.0,
                0.16842435798925898,
                0.0694756436805499,
                0.0,
                0.0,
                0.11787426035619741,
                0.12389575608912659,
                0.09304196380890922,
                0.0,
                0.14106883533373962,
                0.140813083913229,
                0.0,
                0.03472722871078408,
                0.03960862369260309,
                0.24602008211218246,
                0.0,
                0.015431733174665096,
                0.03846712746607064,
                0.3561729505167161,
                0.32342487793342967,
                0.02080628629109306,
                0.0,
                0.0723399621752234,
                0.04836778259769535,
                0.0,
                0.06400170597259076,
                0.03771930699878954,
                0.2908485136168356,
                0.03922434675520138,
                0.04527534181451392,
                0.04177533187292635,
                0.0,
                0.08812548310105732,
                0.0,
                0.06499521494282025,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.023977508615440846,
                0.09583989598474482,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1118695280968525
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.039196472140423884,
                0.0,
                0.23931939696676083,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05711968522014201,
                0.0,
                0.06938054581921967,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.036712445224505445,
                0.0,
                0.0,
                0.0,
                0.0,
                0.14599949589355038,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09615906005339142,
                0.0,
                0.0,
                0.0,
                0.08106830856982448,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03277790404937506
            ],
            [
                0.055934465192306444,
                0.0,
                0.0,
                0.01981486945931084,
                0.0,
                0.11104001822836909,
                0.03309160429732102,
                0.024790570934595083,
                0.06332093657264654,
                0.0,
                0.2164389355925262,
                0.04761691395088394,
                0.12465973129674042,
                0.11304577385286503,
                0.0,
                0.0,
                0.0,
                0.05818257353074322,
                0.13253936827438503,
                0.05372147863645952,
                0.013587893813395127,
                0.0,
                0.06312732478165518,
                0.10986262721353653,
                0.045805989700672414,
                0.0,
                0.0,
                0.09673555937601726,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.06634739878425135,
                0.0,
                0.1860614988059235,
                0.0,
                0.0,
                0.0,
                0.0,
                0.14775422383192746,
                0.06848076156479324,
                0.08196073658468271,
                0.10541359665934806,
                0.0,
                0.04872799511959161,
                0.0,
                0.10072488850405682,
                0.07084300567697188,
                0.0,
                0.014519679336486431,
                0.0,
                0.06800224328837066,
                0.036707424760818945,
                0.0,
                0.022835959637233675,
                0.0,
                0.0,
                0.0623855581394659,
                0.01667893529050426,
                0.0,
                0.0,
                0.07243404250647777,
                0.05552589720732894,
                0.0,
                0.09360891761075765,
                0.0,
                0.03550143015264086,
                0.1065216466686011,
                0.020785742334391375,
                0.0,
                0.0,
                0.0,
                0.03313195818019077,
                0.10623090640642972,
                0.0,
                0.06338780451978959,
                0.0,
                0.0,
                0.0,
                0.0534400200409694,
                0.03719086044175806,
                0.0,
                0.0,
                0.09723741793465496,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05555866802701218,
                0.06273181380027294,
                0.0,
                0.0,
                0.0,
                0.10694209725471276,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06321477688957719,
                0.07534473215322043,
                0.0983961020181901,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0400784764965221,
                0.0,
                0.0,
                0.0,
                0.0,
                0.17907284423696418,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03806232101762229,
                0.0,
                0.03825723865749299,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13527878453851802,
                0.06845690518832764,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10749667368363035,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08558097588832127,
                0.0,
                0.16144028002007085,
                0.06799328855050626,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.20964347260809882,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.3556665944875053,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09034862191474659,
                0.0,
                0.0,
                0.0,
                0.07143345475298839,
                0.0,
                0.051246463093183925,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0813026501661613,
                0.04149727163820328,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1339250125749039,
                0.0,
                0.03178724137649432,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0807825494053484,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0455022904268985,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03572572301086211,
                0.0,
                0.0,
                0.5689333770841705,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06477524017000641,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08600245367755001,
                0.05655690853817135
            ],
            [
                0.0,
                0.030536125909376453,
                0.0,
                0.0,
                0.12862949023571235,
                0.07779126841640799,
                0.03628744932997851,
                0.07052968347818019,
                0.030413449613484196,
                0.038393122573110026,
                0.022361941024147097,
                0.05868319151517582,
                0.0,
                0.136476462254703,
                0.0,
                0.03192796219135118,
                0.0,
                0.019416789347735185,
                0.023199944561004385,
                0.019156211602957083,
                0.0,
                0.04653626544751912,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.025278669571123197,
                0.0,
                0.06634739878425135,
                0.0,
                0.0,
                1.0,
                0.03179723227796049,
                0.1436055041054798,
                0.17861468592462063,
                0.0,
                0.0,
                0.11723350737144916,
                0.0554312925364697,
                0.0,
                0.21221666546309453,
                0.04886930776363939,
                0.11359297779959404,
                0.0,
                0.0,
                0.05627603765082311,
                0.11462631215056308,
                0.0,
                0.0,
                0.11521229897775749,
                0.03223745235105989,
                0.027499522114711926,
                0.012312241681167464,
                0.0,
                0.04200325897088646,
                0.02504638623229811,
                0.033567407755783894,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.024459937071127907,
                0.0,
                0.07299048775469665,
                0.17836306077375688,
                0.0,
                0.04513688606121917,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08792253426443754,
                0.0,
                0.0,
                0.030813204307840396,
                0.08579671785120531,
                0.0,
                0.0,
                0.0,
                0.03582369935090506,
                0.06824160071350566,
                0.0
            ],
            [
                0.0,
                0.0327094756603783,
                0.0,
                0.0,
                0.08488569557724056,
                0.08551542018665982,
                0.0,
                0.041448944547313925,
                0.0910878241163797,
                0.0,
                0.06697367700128566,
                0.1260677820188321,
                0.08581983868327134,
                0.0,
                0.26016557033349974,
                0.06835923672759246,
                0.08112161316201115,
                0.1623638037343891,
                0.3269996006320691,
                0.09564631019909874,
                0.0,
                0.20722656919158586,
                0.13138059445422404,
                0.02825642247157489,
                0.0,
                0.0,
                0.08685837003287654,
                0.1450519426669525,
                0.47759454134137413,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03179723227796049,
                1.0,
                0.1339130229498377,
                0.0,
                0.022880885197767573,
                0.0,
                0.28645804279181225,
                0.0,
                0.0,
                0.0714285243010549,
                0.04812641609387834,
                0.15584452035876867,
                0.11703451902425523,
                0.0,
                0.1774459890647232,
                0.3260477355240834,
                0.0,
                0.10114842006633724,
                0.04982242456314796,
                0.25771793266505894,
                0.05887774267799902,
                0.01941108501876478,
                0.04838657286385875,
                0.29520803201573936,
                0.2507338878161528,
                0.08896798336671315,
                0.0,
                0.0,
                0.06084029120684765,
                0.0,
                0.0,
                0.047445913347207866,
                0.2754123239084038,
                0.04933905485877145,
                0.10273224784648194,
                0.19960389423418115,
                0.0,
                0.0,
                0.15278062416859253,
                0.08175540807945282,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.030160543407826784,
                0.031178370083339575,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.12793732389778328
            ],
            [
                0.0,
                0.032237087638829084,
                0.0,
                0.0,
                0.03652834120722134,
                0.13938743221602357,
                0.0,
                0.01837625308323336,
                0.0,
                0.04053174462244443,
                0.03604129679163543,
                0.03529650301772207,
                0.07412553090708206,
                0.0,
                0.0,
                0.17223623202267607,
                0.0,
                0.020498367793126634,
                0.044043967596986394,
                0.0,
                0.0,
                0.0,
                0.04120440332734766,
                0.03210732624394159,
                0.0,
                0.0,
                0.0,
                0.07921998243631617,
                0.0,
                0.0,
                0.1860614988059235,
                0.0,
                0.0,
                0.1436055041054798,
                0.1339130229498377,
                1.0,
                0.03702122141116258,
                0.0,
                0.0,
                0.0887272038138787,
                0.034640902055702735,
                0.030210448890563363,
                0.07878594860684959,
                0.051797617321321994,
                0.09546290069320655,
                0.0,
                0.0,
                0.044435021261853426,
                0.0985620627247724,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10192835523790449,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06050820388746659,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.21709631166928822,
                0.0,
                0.0,
                0.10230514193672645,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03072809418433949,
                0.0,
                0.0,
                0.0,
                0.037819196153090956,
                0.0,
                0.0
            ],
            [
                0.0,
                0.040096076536903495,
                0.0,
                0.0,
                0.0,
                0.1259341289646825,
                0.0,
                0.02285614812174752,
                0.0,
                0.05041286460996164,
                0.0,
                0.043901338183508255,
                0.0,
                0.10003157516571347,
                0.0,
                0.04192365526298419,
                0.0,
                0.02549560720630458,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0813026501661613,
                0.17861468592462063,
                0.0,
                0.03702122141116258,
                1.0,
                0.0,
                0.07121387034569454,
                0.13077602959499074,
                0.0,
                0.0,
                0.2031270988690062,
                0.0,
                0.04028132927597526,
                0.0,
                0.0,
                0.0,
                0.026636992074930567,
                0.0,
                0.0,
                0.08444582717640786,
                0.0,
                0.21295511396874642,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06718372764644816,
                0.0,
                0.0,
                0.0,
                0.11265711229336783,
                0.0,
                0.0,
                0.0,
                0.047039031580880734,
                0.0,
                0.04589594470116847
            ],
            [
                0.03392131370428358,
                0.0729112609156155,
                0.0,
                0.0,
                0.056077830100099174,
                0.0,
                0.0,
                0.06420547718009689,
                0.021885132737057687,
                0.0,
                0.14485428836034162,
                0.09022164289219038,
                0.0,
                0.0,
                0.08483922847465655,
                0.0,
                0.0,
                0.010551558741266556,
                0.01669438595958711,
                0.03717568133052224,
                0.0,
                0.0,
                0.0,
                0.2654296022027721,
                0.0,
                0.0,
                0.049339248996950906,
                0.0,
                0.01819021012121466,
                0.0,
                0.0,
                0.0400784764965221,
                0.04149727163820328,
                0.0,
                0.022880885197767573,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.046005936689142074,
                0.011563049338270421,
                0.016670746791496344,
                0.12291818660688243,
                0.0,
                0.0,
                0.11420268932376848,
                0.05562957281777155,
                0.08490476987964879,
                0.0,
                0.02319766198092558,
                0.0,
                0.030068268078348163,
                0.0,
                0.04075700881545991,
                0.0,
                0.0169583791874116,
                0.04319702849958888,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09618570379816628,
                0.01760106059473031,
                0.025030549463714274,
                0.04410882107696873,
                0.0,
                0.09094079860819126,
                0.0,
                0.0,
                0.041475921898311056,
                0.03992517719912814,
                0.06149356152052946,
                0.0,
                0.0,
                0.0,
                0.034305464914723374,
                0.06577070915113496,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08164535032609178,
                0.08293492719937447
            ],
            [
                0.03343967605423698,
                0.06201113015526176,
                0.03513365226126543,
                0.11514864273278817,
                0.0,
                0.0,
                0.10699304597654437,
                0.035348485399092466,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03943057659026706,
                0.0,
                0.04291553194748022,
                0.0,
                0.0,
                0.07484958773097639,
                0.0,
                0.0,
                0.0,
                0.10967487923431232,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07121387034569454,
                0.0,
                1.0,
                0.2003136810844977,
                0.048150761065917265,
                0.31184231891320163,
                0.0,
                0.0,
                0.0,
                0.03322784548611779,
                0.0,
                0.0,
                0.0,
                0.024026534986732077,
                0.06642556143280656,
                0.0,
                0.0,
                0.11168911538573899,
                0.0,
                0.0,
                0.0,
                0.05193482937790414,
                0.0,
                0.0,
                0.16045067347317063,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0526778954713561,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.14138061532635907,
                0.06060814683806853,
                0.0,
                0.0
            ],
            [
                0.0,
                0.09609657972964741,
                0.0,
                0.0,
                0.02281001916957183,
                0.09222019573068417,
                0.0,
                0.11240201556319775,
                0.03605461555386118,
                0.10386856022443597,
                0.1353509485099125,
                0.18596575523480294,
                0.04161895254313242,
                0.0,
                0.0,
                0.11156893781442442,
                0.0,
                0.08801283922827742,
                0.06949171029806189,
                0.05737935724892242,
                0.0,
                0.0972799236505798,
                0.0,
                0.0,
                0.029561616228169958,
                0.0,
                0.04601373206284602,
                0.10907018667976667,
                0.16842435798925898,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11723350737144916,
                0.28645804279181225,
                0.0887272038138787,
                0.13077602959499074,
                0.0,
                0.2003136810844977,
                1.0,
                0.3565356544084206,
                0.052600461897553935,
                0.13947201884690044,
                0.09312736379396522,
                0.06383148486328492,
                0.029901046602691512,
                0.0,
                0.1720995327867762,
                0.20266367995591175,
                0.0,
                0.07107965060467264,
                0.0,
                0.08749829270886476,
                0.10255206596847186,
                0.014595948373574292,
                0.0,
                0.24883159000308736,
                0.09029996279967142,
                0.07516560843037699,
                0.0,
                0.06280110281683064,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07326581563098605,
                0.0,
                0.04282322304571056,
                0.13375082239266092,
                0.0,
                0.0,
                0.0,
                0.061475065120503346,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03450554965203611,
                0.0,
                0.0,
                0.0,
                0.11838718762354714,
                0.0,
                0.0
            ],
            [
                0.02591018626989217,
                0.05384286293945442,
                0.051304391810690464,
                0.03647697629346304,
                0.0,
                0.08289961144328567,
                0.0,
                0.07729327848794544,
                0.1165668190236282,
                0.05245639424361137,
                0.3949088195432375,
                0.14846238920645985,
                0.11474236740742133,
                0.0,
                0.0,
                0.06626821397716791,
                0.0,
                0.10818331782040431,
                0.040271730793646904,
                0.01662619914181003,
                0.012506902493783719,
                0.0,
                0.10311957694401043,
                0.06699049142726805,
                0.06952253468447696,
                0.0,
                0.0,
                0.12772118586685,
                0.0694756436805499,
                0.0,
                0.14775422383192746,
                0.0,
                0.0,
                0.0554312925364697,
                0.0,
                0.034640902055702735,
                0.0,
                0.0,
                0.048150761065917265,
                0.3565356544084206,
                1.0,
                0.11031693865183652,
                0.02445771963027404,
                0.1944123967935903,
                0.0,
                0.047359971866375486,
                0.0,
                0.1425913664300189,
                0.08311798454983371,
                0.0,
                0.05998927258901275,
                0.0,
                0.044300546482664946,
                0.0337871482207847,
                0.041719601074787116,
                0.021019234066526695,
                0.14475265607159396,
                0.0,
                0.12391152271464786,
                0.015352034704070362,
                0.05645387087367515,
                0.0,
                0.0,
                0.017184646586964576,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08738024357690324,
                0.019132122770996934,
                0.0,
                0.09130004559525352,
                0.0,
                0.030496129575230138,
                0.0,
                0.061017713936960594,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.034232123947454056,
                0.0,
                0.06824579175336305,
                0.029016399858246562,
                0.0
            ],
            [
                0.04519272372254479,
                0.0,
                0.04474273516798173,
                0.11727043724876425,
                0.0,
                0.0,
                0.1050675024534704,
                0.0397999136673313,
                0.0,
                0.0,
                0.09943341339464491,
                0.076446366215199,
                0.13487915157508734,
                0.0,
                0.0,
                0.0,
                0.0,
                0.044396061851804676,
                0.07024222769485934,
                0.1101491376067381,
                0.12929958606690042,
                0.0,
                0.07705156130963071,
                0.08858008250556373,
                0.024726649504219867,
                0.0,
                0.0,
                0.05919589876543631,
                0.0,
                0.0,
                0.06848076156479324,
                0.0,
                0.0,
                0.0,
                0.0,
                0.030210448890563363,
                0.0,
                0.0,
                0.31184231891320163,
                0.052600461897553935,
                0.11031693865183652,
                1.0,
                0.042659321496998054,
                0.06992520653624074,
                0.0,
                0.03262984715755798,
                0.0,
                0.1225906454996127,
                0.023191790219693593,
                0.023594131755257066,
                0.02331055567837139,
                0.0,
                0.0,
                0.029465887257802192,
                0.0,
                0.03666189150989827,
                0.0,
                0.0,
                0.012398630215202346,
                0.026777123704671033,
                0.3091396378766852,
                0.0,
                0.0,
                0.029973577857936966,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08280856233511526,
                0.07825590312901248,
                0.11678455934761472,
                0.0,
                0.0,
                0.12473797029291521,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0597079042149101,
                0.0,
                0.13946346013391914,
                0.05061060266249032,
                0.11956863692690237
            ],
            [
                0.03397072874267086,
                0.08532963810913831,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0785578701034523,
                0.0,
                0.10728509781384266,
                0.16236806489997538,
                0.2229009950081997,
                0.10138674312548984,
                0.15431111318902097,
                0.09820413118066786,
                0.08921896207251652,
                0.0,
                0.08762983983358885,
                0.20943437023995104,
                0.06266006691265881,
                0.01639774363648867,
                0.10190150958267497,
                0.0761814673004347,
                0.04533784842492507,
                0.05342753871440818,
                0.0,
                0.0542310786931423,
                0.022968962559080063,
                0.0,
                0.0,
                0.08196073658468271,
                0.17907284423696418,
                0.0,
                0.21221666546309453,
                0.0714285243010549,
                0.07878594860684959,
                0.2031270988690062,
                0.046005936689142074,
                0.0,
                0.13947201884690044,
                0.02445771963027404,
                0.042659321496998054,
                1.0,
                0.06411380170896841,
                0.08572387990407233,
                0.0,
                0.0,
                0.20018226474444842,
                0.15222551519448393,
                0.0,
                0.09428309100845281,
                0.0,
                0.03620871194908724,
                0.07684420339326963,
                0.0,
                0.02755822329546134,
                0.09197548310687544,
                0.0,
                0.0,
                0.02012798368748331,
                0.0,
                0.0,
                0.08741272752697264,
                0.02253071288888375,
                0.0,
                0.16514111682482627,
                0.0,
                0.08341573958948426,
                0.05640045654919198,
                0.025084040159149272,
                0.06306487870150207,
                0.0,
                0.18531670739343745,
                0.03998333839480332,
                0.0,
                0.1228446275845207,
                0.07649581182115923,
                0.0,
                0.0,
                0.08133542291131245,
                0.06449091820961751,
                0.04488158382156195,
                0.0,
                0.10010514465440656,
                0.21269180213082556,
                0.049387535390929015
            ],
            [
                0.050042279092978655,
                0.20997463835345137,
                0.0,
                0.013635762544877842,
                0.05486662882006975,
                0.0,
                0.040244161902271564,
                0.3033045376877189,
                0.15210028887466367,
                0.03921832391543736,
                0.18134508227402676,
                0.22873219951008641,
                0.14172762754979343,
                0.0480389725060703,
                0.03133652981653753,
                0.0,
                0.17534225747008292,
                0.16294421285454128,
                0.06522269098715057,
                0.14111776714189708,
                0.009350618933167309,
                0.03707692979689154,
                0.014083742920725468,
                0.08859534791153502,
                0.010598829011922644,
                0.0,
                0.018224126681920305,
                0.09112125002353051,
                0.11787426035619741,
                0.0,
                0.10541359665934806,
                0.0,
                0.1339250125749039,
                0.04886930776363939,
                0.04812641609387834,
                0.051797617321321994,
                0.0,
                0.011563049338270421,
                0.0,
                0.09312736379396522,
                0.1944123967935903,
                0.06992520653624074,
                0.06411380170896841,
                1.0,
                0.08266802538279285,
                0.044276190157042906,
                0.0,
                0.18497825549186278,
                0.07581285624799938,
                0.0,
                0.03206704195469646,
                0.0,
                0.04879270724229203,
                0.050521021972950705,
                0.03303037212785088,
                0.01571475016462526,
                0.11358780230063295,
                0.09883764752633056,
                0.05882415991945388,
                0.047823319755149,
                0.0,
                0.06893430965770411,
                0.07572163869551385,
                0.10369134264886025,
                0.009954830318041303,
                0.11405651640432546,
                0.10367879521868426,
                0.14084444620727654,
                0.11563797167390595,
                0.014303876559610362,
                0.03596199952119993,
                0.0,
                0.0,
                0.022800024765220788,
                0.0,
                0.029285603907317,
                0.1460089577166157,
                0.0,
                0.0,
                0.015756253678318555,
                0.12309499730505728,
                0.15540159418629768,
                0.0,
                0.05558979741400191,
                0.07606398858694066,
                0.050380387885975805
            ],
            [
                0.0,
                0.058907645685214734,
                0.0,
                0.0,
                0.022101684571526427,
                0.08935642105390958,
                0.0,
                0.05019369425331708,
                0.156463978980435,
                0.04410099097308936,
                0.04879624198686287,
                0.09350102634553882,
                0.06252733615898161,
                0.0,
                0.04517868415268845,
                0.03667466145033218,
                0.12028096024009033,
                0.03745718597818659,
                0.05062485889119061,
                0.04180098390922647,
                0.0,
                0.05345476702277214,
                0.0,
                0.020587300717855615,
                0.0,
                0.0,
                0.02627419398833847,
                0.0,
                0.12389575608912659,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11359297779959404,
                0.15584452035876867,
                0.09546290069320655,
                0.04028132927597526,
                0.016670746791496344,
                0.0,
                0.06383148486328492,
                0.0,
                0.0,
                0.08572387990407233,
                0.08266802538279285,
                1.0,
                0.038309775409417005,
                0.0,
                0.0,
                0.17543571710254138,
                0.0,
                0.03182639632802687,
                0.0,
                0.12295335189324877,
                0.03158784947058665,
                0.06567748476078213,
                0.0,
                0.12705705866848893,
                0.028770008249152237,
                0.01906829707402151,
                0.05772989172125203,
                0.0,
                0.0,
                0.10916984165691547,
                0.0,
                0.07248125914055037,
                0.05337430266081826,
                0.035947861429879345,
                0.04149340520305149,
                0.03828575784136009,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1053954294299423,
                0.0,
                0.0,
                0.0,
                0.0,
                0.056150223775571974,
                0.08873461797748766,
                0.0,
                0.0,
                0.04114957408552755,
                0.0,
                0.022720672090684346
            ],
            [
                0.04068710380257355,
                0.0812730011581006,
                0.015084198879633807,
                0.06216097824890855,
                0.0,
                0.0,
                0.017511338109759536,
                0.06824636501121473,
                0.14304843410927126,
                0.0,
                0.26607376114944936,
                0.12159196083733374,
                0.020471963436987817,
                0.029096742074255267,
                0.14712476787322482,
                0.021465424764960075,
                0.03152458889678134,
                0.05874090020350567,
                0.07923008053325162,
                0.13770980120999185,
                0.0,
                0.02071448399872349,
                0.061603771452359596,
                0.17141904805597158,
                0.04790856867145094,
                0.0,
                0.08556213542045683,
                0.02223857001065956,
                0.09304196380890922,
                0.0,
                0.04872799511959161,
                0.0,
                0.03178724137649432,
                0.0,
                0.11703451902425523,
                0.0,
                0.0,
                0.12291818660688243,
                0.03322784548611779,
                0.029901046602691512,
                0.047359971866375486,
                0.03262984715755798,
                0.0,
                0.044276190157042906,
                0.038309775409417005,
                1.0,
                0.0,
                0.03447855410293885,
                0.13088271636030097,
                0.0,
                0.09343981256957905,
                0.019361442542536363,
                0.08470935951956454,
                0.0,
                0.040888804421297345,
                0.018803457650810436,
                0.08483116309724127,
                0.07306715817142,
                0.04003073384212867,
                0.03308926870265894,
                0.0,
                0.023643084671231693,
                0.0,
                0.019503348183231746,
                0.1037220762249517,
                0.11900698109347708,
                0.14345066249644362,
                0.0337877090964073,
                0.0,
                0.2610553556342015,
                0.01968589439890159,
                0.0,
                0.054387489927227965,
                0.0,
                0.04710455907066972,
                0.03134741182554705,
                0.0,
                0.18836325711743773,
                0.07053520275280464,
                0.07916954256122481,
                0.0,
                0.0,
                0.0,
                0.10540544180766163,
                0.06254099018150024,
                0.08776597971773803
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.066471691525233,
                0.0,
                0.05482714205860479,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08904645506448439,
                0.0,
                0.06728753742575574,
                0.26527875857413225,
                0.17103750452635802,
                0.33569299707878325,
                0.19715035574719522,
                0.0,
                0.09365004284627114,
                0.0,
                0.22207393588222002,
                0.1517322897050802,
                0.08263152333966478,
                0.0320860280911604,
                0.11348332208141836,
                0.10888814666569034,
                0.1947492802419804,
                0.07045633893775129,
                0.07561349343828991,
                0.05305790701868485,
                0.08706825343063064,
                0.14106883533373962,
                0.0,
                0.10072488850405682,
                0.0,
                0.0,
                0.05627603765082311,
                0.1774459890647232,
                0.044435021261853426,
                0.0,
                0.0,
                0.0,
                0.1720995327867762,
                0.1425913664300189,
                0.1225906454996127,
                0.20018226474444842,
                0.18497825549186278,
                0.0,
                0.03447855410293885,
                0.0,
                1.0,
                0.06966481025403112,
                0.0,
                0.11624744770135552,
                0.0,
                0.0626054004962954,
                0.043339883215333455,
                0.0,
                0.05392412190376144,
                0.15116834464974357,
                0.1397725669528704,
                0.03851617157831186,
                0.039385116899735964,
                0.0,
                0.0,
                0.0,
                0.04408661964063955,
                0.0,
                0.05104605992907076,
                0.0,
                0.0,
                0.07188668765046405,
                0.04908280279460479,
                0.14758685769328508,
                0.0,
                0.0708861929451049,
                0.07823677131158922,
                0.0814787411497955,
                0.0502458027833716,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08782133634958918,
                0.0,
                0.0,
                0.16772455132675945,
                0.0
            ],
            [
                0.018468226567212954,
                0.07047270947155744,
                0.0,
                0.02054040997146022,
                0.014615267354781473,
                0.13193944672072444,
                0.049205337229164696,
                0.06640796749295927,
                0.08545418667562267,
                0.06655263049762918,
                0.1251711200341275,
                0.3114335440188437,
                0.06979983468684052,
                0.0,
                0.33718549461155817,
                0.024251996742965177,
                0.03908410750042522,
                0.08186301845066338,
                0.1445564132738006,
                0.1507583773665076,
                0.04879980097152703,
                0.11398251820118399,
                0.013427097829353597,
                0.13479375360226417,
                0.010104665700071288,
                0.0,
                0.16504530350914517,
                0.08237261854877763,
                0.140813083913229,
                0.0,
                0.07084300567697188,
                0.03806232101762229,
                0.0,
                0.11462631215056308,
                0.3260477355240834,
                0.0985620627247724,
                0.026636992074930567,
                0.11420268932376848,
                0.0,
                0.20266367995591175,
                0.08311798454983371,
                0.023191790219693593,
                0.15222551519448393,
                0.07581285624799938,
                0.17543571710254138,
                0.13088271636030097,
                0.0,
                0.06966481025403112,
                1.0,
                0.0,
                0.23136455919369053,
                0.024004268673367503,
                0.11026408693014153,
                0.06570910137216415,
                0.00935219239363666,
                0.03829454083359239,
                0.2385185478212493,
                0.04313501196227776,
                0.09647738574720398,
                0.01094260196469315,
                0.0,
                0.029312638015921044,
                0.0,
                0.01224884851616566,
                0.09015510152555864,
                0.17493389265702247,
                0.07131414855687244,
                0.049496035175894716,
                0.11636153371327856,
                0.01363696788459931,
                0.0,
                0.0,
                0.03938946765933924,
                0.059653722335124726,
                0.0,
                0.0,
                0.0,
                0.0,
                0.014531243584475075,
                0.06717393575774842,
                0.0,
                0.02439992574168124,
                0.0,
                0.027211139714217576,
                0.020682269172631985,
                0.15761614005252092
            ],
            [
                0.0,
                0.0,
                0.09849383471629984,
                0.023959474948896633,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02411278415722953,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05562957281777155,
                0.024026534986732077,
                0.0,
                0.0,
                0.023594131755257066,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.3580392737965248,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02816966697215738,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.026543870156487823,
                0.0,
                0.0,
                0.0,
                0.0,
                0.022807658481081232,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.04553895111808621,
                0.031680027595898975,
                0.14788037021381117,
                0.017200153586177422,
                0.0,
                0.06543227505987852,
                0.0,
                0.016347718832915945,
                0.04178126612398474,
                0.0,
                0.15985796506210614,
                0.16429518806215498,
                0.08091235379573074,
                0.0,
                0.16196796360531143,
                0.026749065034369728,
                0.0,
                0.0383797296303462,
                0.08776484431619765,
                0.12646395609116162,
                0.07231594629042616,
                0.05293425379149795,
                0.0375741597157234,
                0.13458353101418888,
                0.029194688810233318,
                0.0,
                0.26745603421429365,
                0.04903034672114464,
                0.03472722871078408,
                0.0,
                0.014519679336486431,
                0.03825723865749299,
                0.0,
                0.0,
                0.10114842006633724,
                0.0,
                0.0,
                0.08490476987964879,
                0.06642556143280656,
                0.07107965060467264,
                0.05998927258901275,
                0.02331055567837139,
                0.09428309100845281,
                0.03206704195469646,
                0.03182639632802687,
                0.09343981256957905,
                0.0,
                0.11624744770135552,
                0.23136455919369053,
                0.3580392737965248,
                1.0,
                0.0,
                0.04428703724689407,
                0.0,
                0.0,
                0.015058783777680305,
                0.10228904483582789,
                0.17333232401129195,
                0.09803228685343102,
                0.010998639171357849,
                0.0,
                0.0,
                0.0,
                0.012311575028371775,
                0.051453163579033456,
                0.062112593160000416,
                0.18314775324641594,
                0.0,
                0.010362599241843368,
                0.0381614564700608,
                0.08605000648241065,
                0.0,
                0.11027061128779957,
                0.07072417743153053,
                0.045507305529875176,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03019710969661949,
                0.0,
                0.024524878078045258,
                0.1785855959352988,
                0.0,
                0.02078818334146141,
                0.055828932172190746
            ],
            [
                0.0,
                0.0,
                0.03368161759583641,
                0.0,
                0.04855974347437785,
                0.0,
                0.04486558475338539,
                0.03317544224320401,
                0.0,
                0.0,
                0.0,
                0.03956225665093407,
                0.0,
                0.09419027328770119,
                0.15540097729019334,
                0.0,
                0.06361811377070682,
                0.0,
                0.12755789073722315,
                0.06809511724776841,
                0.0,
                0.04625352226574559,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09037526213691091,
                0.0,
                0.03960862369260309,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11521229897775749,
                0.04982242456314796,
                0.0,
                0.08444582717640786,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.019361442542536363,
                0.0,
                0.0,
                0.024004268673367503,
                0.0,
                0.0,
                1.0,
                0.0,
                0.041282322677493974,
                0.0,
                0.037946268293257965,
                0.0,
                0.08903308454047115,
                0.0,
                0.0,
                0.0,
                0.04771286488238531,
                0.0,
                0.0,
                0.049366996576795705,
                0.18310822786843803,
                0.0691464930447564,
                0.045582704477069594,
                0.12617680160183803,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06175010529026467,
                0.07009113238410652,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.021851853381531866
            ],
            [
                0.0,
                0.03316232538139743,
                0.0,
                0.0,
                0.03075491544031112,
                0.06217058171896686,
                0.0,
                0.07542738691110881,
                0.09234889892441485,
                0.0,
                0.067900901003913,
                0.07666754495585239,
                0.0,
                0.06707640215070428,
                0.11904003223716861,
                0.049697861554587386,
                0.0,
                0.02108669214040434,
                0.18803654663837716,
                0.05816686602732014,
                0.0,
                0.10428131782944162,
                0.0,
                0.028647621436856726,
                0.04150681317891563,
                0.044207457673741915,
                0.03656104184088774,
                0.0,
                0.24602008211218246,
                0.0,
                0.06800224328837066,
                0.0,
                0.0,
                0.03223745235105989,
                0.25771793266505894,
                0.0,
                0.0,
                0.02319766198092558,
                0.0,
                0.08749829270886476,
                0.044300546482664946,
                0.0,
                0.03620871194908724,
                0.04879270724229203,
                0.12295335189324877,
                0.08470935951956454,
                0.0,
                0.0626054004962954,
                0.11026408693014153,
                0.0,
                0.04428703724689407,
                0.0,
                1.0,
                0.0,
                0.019679823794240603,
                0.0,
                0.12432496624662186,
                0.040034014966426675,
                0.026533898907315803,
                0.040166122462736834,
                0.0,
                0.0,
                0.0,
                0.0,
                0.019971270729692653,
                0.1982488400242882,
                0.050022134510066714,
                0.13354560996783033,
                0.07468911575454211,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.095623591078691,
                0.061737956301343315,
                0.0,
                0.0,
                0.0,
                0.0,
                0.16140631960365945
            ],
            [
                0.0,
                0.031442577808975844,
                0.02805047742018701,
                0.0635537225485412,
                0.03562806986501691,
                0.05585164394234836,
                0.0,
                0.01792335504622476,
                0.0,
                0.039532806074259776,
                0.03515302906402837,
                0.034426591351965796,
                0.07229864556282935,
                0.0,
                0.07237323640964403,
                0.0775223651343809,
                0.0,
                0.01999316847456382,
                0.04295846739319645,
                0.10358895173432867,
                0.0,
                0.03852051874372558,
                0.04018888654606796,
                0.031316014491561364,
                0.0,
                0.0,
                0.04208950501002392,
                0.031730502851160414,
                0.0,
                0.0,
                0.036707424760818945,
                0.0,
                0.0,
                0.027499522114711926,
                0.05887774267799902,
                0.10192835523790449,
                0.21295511396874642,
                0.0,
                0.11168911538573899,
                0.10255206596847186,
                0.0337871482207847,
                0.029465887257802192,
                0.07684420339326963,
                0.050521021972950705,
                0.03158784947058665,
                0.0,
                0.0,
                0.043339883215333455,
                0.06570910137216415,
                0.0,
                0.0,
                0.041282322677493974,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.04146442698165689,
                0.013736885750854922,
                0.0,
                0.06547438460370053,
                0.0,
                0.0,
                0.0,
                0.07509900316428414,
                0.11205560851466372,
                0.05758607454995932,
                0.0,
                0.05705630109855534,
                0.0,
                0.0,
                0.09978374664742752,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.029970774752893458,
                0.0,
                0.0,
                0.0,
                0.03688711061740545,
                0.0,
                0.08252092975207975
            ],
            [
                0.0,
                0.042984232225177124,
                0.026676379008080005,
                0.01185930096383193,
                0.039550780108699064,
                0.043587576739912114,
                0.02215220362265793,
                0.24489347863960953,
                0.018566334673425448,
                0.031829312068708863,
                0.013651173618189044,
                0.09165267184084747,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05867239894995628,
                0.021189549056388382,
                0.043071971485155645,
                0.03272363096623739,
                0.02268734253157944,
                0.05395219877723875,
                0.04828065288500973,
                0.037132378335313285,
                0.031704857425377424,
                0.0,
                0.0,
                0.0,
                0.015431733174665096,
                0.036712445224505445,
                0.0,
                0.0,
                0.0807825494053484,
                0.012312241681167464,
                0.01941108501876478,
                0.0,
                0.0,
                0.030068268078348163,
                0.0,
                0.014595948373574292,
                0.041719601074787116,
                0.0,
                0.0,
                0.03303037212785088,
                0.06567748476078213,
                0.040888804421297345,
                0.0,
                0.0,
                0.00935219239363666,
                0.0,
                0.0,
                0.0,
                0.019679823794240603,
                0.0,
                1.0,
                0.030177485976211275,
                0.08905543568283608,
                0.015289932416694676,
                0.07412403778514752,
                0.07442911368860272,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02588629503522522,
                0.1241344357041107,
                0.0,
                0.08181813618366787,
                0.02034711442717725,
                0.0,
                0.03195443626020457,
                0.0,
                0.0,
                0.0,
                0.07831186163989133,
                0.0,
                0.03540235455692413,
                0.0,
                0.0,
                0.0,
                0.07265124331481516,
                0.0,
                0.0,
                0.0,
                0.10397510276165747,
                0.05304790472671509
            ],
            [
                0.02919481904473383,
                0.0,
                0.0,
                0.05279973902601382,
                0.05958597940179076,
                0.0,
                0.04357258613534137,
                0.025711025621014218,
                0.0,
                0.0,
                0.04373791722319365,
                0.08780698975745406,
                0.04497752056040993,
                0.0,
                0.06652470277669102,
                0.0,
                0.3718216696063581,
                0.028680169843705607,
                0.19080096046583864,
                0.04788434509417075,
                0.014092401780238746,
                0.0,
                0.021225735454201422,
                0.038963847101126124,
                0.015973590401194056,
                0.04468949843753353,
                0.29118868234038625,
                0.019739779815653336,
                0.03846712746607064,
                0.0,
                0.022835959637233675,
                0.0,
                0.0,
                0.0,
                0.04838657286385875,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.021019234066526695,
                0.03666189150989827,
                0.02755822329546134,
                0.01571475016462526,
                0.0,
                0.018803457650810436,
                0.0,
                0.05392412190376144,
                0.03829454083359239,
                0.0,
                0.015058783777680305,
                0.037946268293257965,
                0.0,
                0.0,
                0.030177485976211275,
                1.0,
                0.0,
                0.03811365661675679,
                0.0,
                0.10125624189643684,
                0.0,
                0.04633780939843355,
                0.11412076180007465,
                0.019363143214339445,
                0.02113323115154247,
                0.03722124356583917,
                0.0,
                0.0414611578476195,
                0.016297873564529226,
                0.021557500838577014,
                0.0,
                0.0,
                0.0,
                0.08876789633248985,
                0.06171692648135421,
                0.04133711612585755,
                0.0,
                0.0,
                0.05762372518494832,
                0.0,
                0.0,
                0.0385717282675089,
                0.0,
                0.0,
                0.19138125073618797,
                0.0
            ],
            [
                0.0,
                0.07421591653027436,
                0.0,
                0.0,
                0.0400716118569038,
                0.0,
                0.0,
                0.17568691122677277,
                0.09183180082230767,
                0.05125699545432685,
                0.16312439955011043,
                0.34866372697335535,
                0.09323995774655582,
                0.0,
                0.18364357095442282,
                0.0,
                0.0,
                0.10199429666266605,
                0.14577376072733106,
                0.11874786016094618,
                0.0,
                0.22231875450357289,
                0.04613276927220659,
                0.05033228609003731,
                0.0259662563751412,
                0.0,
                0.10465315150940796,
                0.0,
                0.3561729505167161,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04200325897088646,
                0.29520803201573936,
                0.0,
                0.0,
                0.04075700881545991,
                0.0,
                0.24883159000308736,
                0.14475265607159396,
                0.0,
                0.09197548310687544,
                0.11358780230063295,
                0.12705705866848893,
                0.08483116309724127,
                0.0,
                0.15116834464974357,
                0.2385185478212493,
                0.0,
                0.10228904483582789,
                0.0,
                0.12432496624662186,
                0.0,
                0.08905543568283608,
                0.0,
                1.0,
                0.10539827998240892,
                0.05572542738526803,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.035088417869667696,
                0.27967112881573963,
                0.032587759219034104,
                0.10546817270006272,
                0.09731495644200325,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0683278522695935,
                0.0,
                0.0,
                0.0,
                0.020592879616889884,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07488008753517159
            ],
            [
                0.0,
                0.0,
                0.1647734670410053,
                0.0,
                0.023894552282583692,
                0.10643042629061492,
                0.0,
                0.21361541191122013,
                0.03776888085782185,
                0.0,
                0.027770131209193468,
                0.19503948591364836,
                0.041495633366904955,
                0.0,
                0.15608648103611256,
                0.04350932917857552,
                0.06389874556632737,
                0.146321727682257,
                0.06532267028920259,
                0.22078931724742626,
                0.046152110910846075,
                0.20521766517505732,
                0.09418273073882419,
                0.0,
                0.0,
                0.0,
                0.249657590963697,
                0.0,
                0.32342487793342967,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02504638623229811,
                0.2507338878161528,
                0.0,
                0.0,
                0.0,
                0.05193482937790414,
                0.09029996279967142,
                0.0,
                0.0,
                0.0,
                0.09883764752633056,
                0.028770008249152237,
                0.07306715817142,
                0.0,
                0.1397725669528704,
                0.04313501196227776,
                0.0,
                0.17333232401129195,
                0.08903308454047115,
                0.040034014966426675,
                0.04146442698165689,
                0.015289932416694676,
                0.03811365661675679,
                0.10539827998240892,
                1.0,
                0.02662034700686726,
                0.0,
                0.0,
                0.04792333555107574,
                0.0,
                0.0,
                0.04958476394009215,
                0.23778080917729869,
                0.13222010088725733,
                0.1265911181741167,
                0.10719695514792806,
                0.03977729941350885,
                0.039902311226314956,
                0.0,
                0.17936329060768852,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.023757181523374884,
                0.0,
                0.0,
                0.0,
                0.1941391469986114,
                0.0,
                0.0,
                0.07591125922857209
            ],
            [
                0.04112699205954334,
                0.008989163790726812,
                0.26028328221255825,
                0.07575788210893404,
                0.008336597903897047,
                0.0,
                0.0,
                0.021060270526122634,
                0.04872896783928347,
                0.021327265915358086,
                0.06804346228845805,
                0.056055884182811495,
                0.0448992200974276,
                0.0,
                0.0459581845151896,
                0.015180020037222429,
                0.0,
                0.027926658590219384,
                0.019095335975480163,
                0.06014542332240632,
                0.41455739125180197,
                0.034811723368721355,
                0.013664369181976898,
                0.034119633369343136,
                0.0,
                0.0,
                0.026727521572407046,
                0.21388711704992108,
                0.02080628629109306,
                0.0,
                0.0623855581394659,
                0.13527878453851802,
                0.0,
                0.033567407755783894,
                0.08896798336671315,
                0.06050820388746659,
                0.0,
                0.0169583791874116,
                0.0,
                0.07516560843037699,
                0.12391152271464786,
                0.012398630215202346,
                0.0,
                0.05882415991945388,
                0.01906829707402151,
                0.04003073384212867,
                0.0,
                0.03851617157831186,
                0.09647738574720398,
                0.0,
                0.09803228685343102,
                0.0,
                0.026533898907315803,
                0.013736885750854922,
                0.07412403778514752,
                0.0,
                0.05572542738526803,
                0.02662034700686726,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0911294820889565,
                0.020132406570374534,
                0.0354586826466981,
                0.015651016723265986,
                0.04128466260914045,
                0.013877947868269779,
                0.013921563383818124,
                0.0,
                0.01599411540811404,
                0.018018468352358947,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.008568387898266748,
                0.1519007967289927,
                0.0,
                0.0,
                0.0,
                0.0,
                0.015841381893940208
            ],
            [
                0.021323320999006815,
                0.061752512636435435,
                0.0,
                0.01703744826223436,
                0.04352042613717719,
                0.0,
                0.03182455899100191,
                0.10991263014737164,
                0.0,
                0.12658005992932592,
                0.03194531356228346,
                0.03606975352983943,
                0.032850695432621535,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02094743135575908,
                0.11357200280610233,
                0.08722280133637006,
                0.050476865656133144,
                0.03669653926524782,
                0.01550285925171978,
                0.08180398030343006,
                0.011666795917091128,
                0.03264032974508299,
                0.0426264865999298,
                0.014417546511041551,
                0.0,
                0.14599949589355038,
                0.01667893529050426,
                0.06845690518832764,
                0.0455022904268985,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04319702849958888,
                0.0,
                0.0,
                0.015352034704070362,
                0.026777123704671033,
                0.02012798368748331,
                0.047823319755149,
                0.05772989172125203,
                0.03308926870265894,
                0.0,
                0.039385116899735964,
                0.01094260196469315,
                0.0,
                0.010998639171357849,
                0.0,
                0.040166122462736834,
                0.0,
                0.07442911368860272,
                0.10125624189643684,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.014142458553226663,
                0.0371890732606113,
                0.0,
                0.0,
                0.07864830121424841,
                0.011903645954619154,
                0.01574517415617457,
                0.0,
                0.0,
                0.0,
                0.07408868778654391,
                0.17525152480958048,
                0.08040759908513956,
                0.05086014647611904,
                0.0,
                0.07355119851262912,
                0.0,
                0.197513502147482,
                0.028172030868707027,
                0.0,
                0.06119117694719292,
                0.41945496433797064,
                0.10360296299061457
            ],
            [
                0.04271508269315827,
                0.0,
                0.05341955203859745,
                0.1100068714029169,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05031587965393618,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0723399621752234,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.16045067347317063,
                0.06280110281683064,
                0.05645387087367515,
                0.3091396378766852,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02816966697215738,
                0.0,
                0.0,
                0.0,
                0.06547438460370053,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.08714129699759991,
                0.0,
                0.05666061929225541,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07205803738799564,
                0.05153199803788982,
                0.0,
                0.0,
                0.0,
                0.06290331403773002,
                0.0,
                0.0,
                0.0,
                0.14847887653462624,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08534382454363935,
                0.06310670277729978,
                0.0,
                0.0,
                0.0,
                0.04831116182216549,
                0.0,
                0.0,
                0.08364680633138968,
                0.0,
                0.07768679669401486,
                0.0,
                0.15576639005890977,
                0.03665317415540348,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04864578190520694,
                0.0,
                0.04836778259769535,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06084029120684765,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06893430965770411,
                0.0,
                0.023643084671231693,
                0.0,
                0.0,
                0.029312638015921044,
                0.0,
                0.0,
                0.04771286488238531,
                0.0,
                0.0,
                0.0,
                0.04633780939843355,
                0.0,
                0.04792333555107574,
                0.0,
                0.0,
                0.08714129699759991,
                1.0,
                0.0,
                0.0,
                0.026572494419455594,
                0.04680123355704976,
                0.14922219884709503,
                0.0,
                0.07149223826595646,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09804792450060812,
                0.07011855493341863,
                0.0,
                0.0,
                0.028883498645735764,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10534311847851174,
                0.0,
                0.05277274549342628
            ],
            [
                0.0,
                0.0,
                0.0,
                0.04484775376834672,
                0.3468838759853325,
                0.0,
                0.0,
                0.0,
                0.14331670353027498,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09162932868651535,
                0.0,
                0.0,
                0.19132705096819275,
                0.0,
                0.07196627249228975,
                0.0,
                0.0,
                0.0,
                0.0673265257315512,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07243404250647777,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08741272752697264,
                0.07572163869551385,
                0.10916984165691547,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11412076180007465,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.07587477167354245,
                0.0,
                0.05602685316071424,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08278706845510997,
                0.0,
                0.0,
                0.10003594786859957,
                0.0,
                0.0,
                0.0,
                0.08433677581065835,
                0.0,
                0.0,
                0.0,
                0.10370560677631642,
                0.0
            ],
            [
                0.2831983131345958,
                0.13036904349838263,
                0.0,
                0.0,
                0.0,
                0.0,
                0.028059143709315917,
                0.021020503757991187,
                0.0,
                0.0,
                0.07105397706472756,
                0.040375492814959946,
                0.03677216747014885,
                0.04662291723323013,
                0.0,
                0.0,
                0.0,
                0.07590241728503368,
                0.07371669243562143,
                0.04555169570486478,
                0.011521492333565843,
                0.0,
                0.017353475448933398,
                0.06329831126043761,
                0.038839967872535686,
                0.0,
                0.0,
                0.016138606133927202,
                0.06400170597259076,
                0.0,
                0.05552589720732894,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.017184646586964576,
                0.029973577857936966,
                0.02253071288888375,
                0.10369134264886025,
                0.0,
                0.019503348183231746,
                0.0,
                0.04408661964063955,
                0.01224884851616566,
                0.0,
                0.012311575028371775,
                0.0,
                0.0,
                0.0,
                0.0,
                0.019363143214339445,
                0.0,
                0.0,
                0.0,
                0.014142458553226663,
                0.05666061929225541,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0386066693987075,
                0.0,
                0.0,
                0.01332461479990302,
                0.017624716107001063,
                0.0,
                0.0,
                0.0,
                0.028093360708543208,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.055652771838512576,
                0.0,
                0.08146456975481443,
                0.0,
                0.0,
                0.026730217868143714,
                0.0
            ],
            [
                0.0,
                0.06277057283558696,
                0.01875813992326458,
                0.04167490871008084,
                0.0,
                0.0,
                0.0,
                0.05527561217795003,
                0.01884129147180545,
                0.0,
                0.013853339692333945,
                0.06421804648076974,
                0.0,
                0.0,
                0.15958628014584095,
                0.0,
                0.03543052753833895,
                0.009084020467897231,
                0.0346175588390564,
                0.06992907410983204,
                0.0,
                0.025759749814152112,
                0.0,
                0.16876767277812874,
                0.0,
                0.0,
                0.09280927413154065,
                0.0,
                0.03771930699878954,
                0.0,
                0.0,
                0.10749667368363035,
                0.03572572301086211,
                0.0,
                0.047445913347207866,
                0.0,
                0.0,
                0.09618570379816628,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.009954830318041303,
                0.07248125914055037,
                0.1037220762249517,
                0.0,
                0.0,
                0.09015510152555864,
                0.0,
                0.051453163579033456,
                0.049366996576795705,
                0.019971270729692653,
                0.07509900316428414,
                0.02588629503522522,
                0.02113323115154247,
                0.035088417869667696,
                0.04958476394009215,
                0.0911294820889565,
                0.0371890732606113,
                0.0,
                0.026572494419455594,
                0.0,
                0.0,
                1.0,
                0.0635768196362703,
                0.06005865644064929,
                0.037974051351383986,
                0.0,
                0.0,
                0.04841474597651399,
                0.0,
                0.0,
                0.0,
                0.052940876812039114,
                0.0,
                0.0,
                0.0,
                0.051049382353182,
                0.013617407948139716,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0702899023411807,
                0.08356996822071526
            ],
            [
                0.029104642633135093,
                0.025161677887197573,
                0.03303807590610499,
                0.0,
                0.1448758666961397,
                0.08659262965670726,
                0.03466350673643413,
                0.23677615768012714,
                0.07006906847602905,
                0.0,
                0.13652566261478782,
                0.3694814560273769,
                0.04257696053141566,
                0.1184156231371179,
                0.2853734536023934,
                0.0,
                0.17896326076557528,
                0.049996501115745356,
                0.5063145404114047,
                0.2755834749795815,
                0.0,
                0.27043823741166984,
                0.04468795653806222,
                0.06057967476391526,
                0.062090584750866845,
                0.0,
                0.16346182809658408,
                0.0,
                0.2908485136168356,
                0.0,
                0.09360891761075765,
                0.0,
                0.0,
                0.024459937071127907,
                0.2754123239084038,
                0.0,
                0.0,
                0.01760106059473031,
                0.0,
                0.07326581563098605,
                0.0,
                0.0,
                0.16514111682482627,
                0.11405651640432546,
                0.05337430266081826,
                0.11900698109347708,
                0.0,
                0.05104605992907076,
                0.17493389265702247,
                0.0,
                0.062112593160000416,
                0.18310822786843803,
                0.1982488400242882,
                0.11205560851466372,
                0.1241344357041107,
                0.03722124356583917,
                0.27967112881573963,
                0.23778080917729869,
                0.020132406570374534,
                0.0,
                0.0,
                0.04680123355704976,
                0.07587477167354245,
                0.0386066693987075,
                0.0635768196362703,
                1.0,
                0.10577927547304103,
                0.11621431377158661,
                0.07291736693316771,
                0.0,
                0.0634818191770468,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06639882337639876,
                0.0,
                0.023200918474209523,
                0.023983878960038167,
                0.05597850373290252,
                0.0,
                0.0,
                0.0,
                0.06883455811832549,
                0.2483087528079542
            ],
            [
                0.0,
                0.0357825381917103,
                0.0894462367965095,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04719187021709005,
                0.0,
                0.13416493867896476,
                0.039178429559629184,
                0.057629414187285656,
                0.0,
                0.18905695206906303,
                0.060426048448820276,
                0.0,
                0.022752788237043117,
                0.03599883560335056,
                0.13085008093202452,
                0.0,
                0.12283268443039175,
                0.05439280265691814,
                0.11952060109179194,
                0.0,
                0.0,
                0.10994828925779992,
                0.0,
                0.03922434675520138,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04933905485877145,
                0.0,
                0.0,
                0.025030549463714274,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10367879521868426,
                0.035947861429879345,
                0.14345066249644362,
                0.0,
                0.0,
                0.07131414855687244,
                0.0,
                0.18314775324641594,
                0.0691464930447564,
                0.050022134510066714,
                0.05758607454995932,
                0.0,
                0.0,
                0.032587759219034104,
                0.13222010088725733,
                0.0354586826466981,
                0.0,
                0.0,
                0.14922219884709503,
                0.0,
                0.0,
                0.06005865644064929,
                0.10577927547304103,
                1.0,
                0.0,
                0.08166685275988227,
                0.15329279392932277,
                0.05541659770218853,
                0.0,
                0.06366666118864758,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03410758490432718,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.20276616775440431
            ],
            [
                0.0,
                0.16957663902878078,
                0.0,
                0.0,
                0.06963379904829718,
                0.0,
                0.07773063605983337,
                0.28981796398908016,
                0.054471985684799246,
                0.0,
                0.04005133737974944,
                0.04522234114282219,
                0.0,
                0.04490943898384221,
                0.06294356203147926,
                0.0,
                0.10331086965789618,
                0.06495040284637851,
                0.14994650686922326,
                0.03430971815011161,
                0.0,
                0.11685033498083647,
                0.03299812999481409,
                0.05447155878372831,
                0.0,
                0.03332929404236285,
                0.04352623631493549,
                0.0,
                0.04527534181451392,
                0.0,
                0.03550143015264086,
                0.0,
                0.5689333770841705,
                0.07299048775469665,
                0.10273224784648194,
                0.0,
                0.0,
                0.04410882107696873,
                0.0,
                0.04282322304571056,
                0.0,
                0.0,
                0.08341573958948426,
                0.14084444620727654,
                0.04149340520305149,
                0.0337877090964073,
                0.0,
                0.0,
                0.049496035175894716,
                0.0,
                0.0,
                0.045582704477069594,
                0.13354560996783033,
                0.0,
                0.08181813618366787,
                0.0414611578476195,
                0.10546817270006272,
                0.1265911181741167,
                0.015651016723265986,
                0.07864830121424841,
                0.0,
                0.0,
                0.05602685316071424,
                0.0,
                0.037974051351383986,
                0.11621431377158661,
                0.0,
                1.0,
                0.17508169827848918,
                0.0,
                0.0721586821192029,
                0.0,
                0.0,
                0.0,
                0.0688517429238921,
                0.03082909735824827,
                0.04902969782050625,
                0.0,
                0.05343774455581727,
                0.0,
                0.04133520720555199,
                0.0,
                0.10694782458652954,
                0.0,
                0.1994789642399768,
                0.09177095849400593
            ],
            [
                0.020090215396220818,
                0.0,
                0.0,
                0.0,
                0.15774744434154933,
                0.04481954610327172,
                0.08995253296361445,
                0.1621923659513138,
                0.05026102926141349,
                0.08729981722108124,
                0.09526254399516909,
                0.07571030540091968,
                0.08896882686505654,
                0.0,
                0.058077710465484246,
                0.0358278069079142,
                0.0,
                0.019736063062332145,
                0.13486016177150906,
                0.07922925971644147,
                0.06187337273150777,
                0.1078172207575075,
                0.08615005775529203,
                0.05194302303555382,
                0.010992117173916181,
                0.0,
                0.0,
                0.13636447157381157,
                0.04177533187292635,
                0.0,
                0.1065216466686011,
                0.0,
                0.0,
                0.17836306077375688,
                0.19960389423418115,
                0.21709631166928822,
                0.0,
                0.0,
                0.0,
                0.13375082239266092,
                0.08738024357690324,
                0.08280856233511526,
                0.05640045654919198,
                0.11563797167390595,
                0.03828575784136009,
                0.0,
                0.0,
                0.07188668765046405,
                0.11636153371327856,
                0.0,
                0.010362599241843368,
                0.12617680160183803,
                0.07468911575454211,
                0.05705630109855534,
                0.02034711442717725,
                0.016297873564529226,
                0.09731495644200325,
                0.10719695514792806,
                0.04128466260914045,
                0.011903645954619154,
                0.0,
                0.07149223826595646,
                0.0,
                0.01332461479990302,
                0.0,
                0.07291736693316771,
                0.08166685275988227,
                0.17508169827848918,
                1.0,
                0.05474194940776308,
                0.0,
                0.08007395875111498,
                0.0,
                0.08725731077110613,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07638249112002939,
                0.0,
                0.0,
                0.02654287145645221,
                0.0,
                0.0,
                0.022498708311195426,
                0.029207681454473056
            ],
            [
                0.026573701994707634,
                0.0,
                0.2624434623500735,
                0.0,
                0.0,
                0.0,
                0.0,
                0.023402684283955755,
                0.0,
                0.0,
                0.24359059253753804,
                0.044951106882888996,
                0.07745996423165355,
                0.0,
                0.0,
                0.03829279976542728,
                0.0,
                0.026105258108174784,
                0.0413030212046375,
                0.3693566044425419,
                0.08184106222593558,
                0.03695317454430729,
                0.10576329971265451,
                0.1477717053870621,
                0.01453947806480481,
                0.0,
                0.0,
                0.017967538194313168,
                0.0,
                0.0,
                0.020785742334391375,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09094079860819126,
                0.0,
                0.0,
                0.019132122770996934,
                0.07825590312901248,
                0.025084040159149272,
                0.014303876559610362,
                0.0,
                0.2610553556342015,
                0.0,
                0.04908280279460479,
                0.01363696788459931,
                0.0,
                0.0381614564700608,
                0.0,
                0.0,
                0.0,
                0.0,
                0.021557500838577014,
                0.0,
                0.03977729941350885,
                0.013877947868269779,
                0.01574517415617457,
                0.0,
                0.0,
                0.0,
                0.017624716107001063,
                0.0,
                0.0,
                0.15329279392932277,
                0.0,
                0.05474194940776308,
                1.0,
                0.03511824343914338,
                0.0,
                0.14152450283718773,
                0.11541686973287248,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03510875031734477,
                0.0,
                0.09401792700408024,
                0.02975946042071931,
                0.0
            ],
            [
                0.0,
                0.0,
                0.026993713926450034,
                0.12565347262372736,
                0.049937234977327986,
                0.07828312558008348,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10827209430260162,
                0.06610699222641317,
                0.0,
                0.11352631131599557,
                0.1053754101195176,
                0.0,
                0.0,
                0.030518459238349147,
                0.0,
                0.11818574097007659,
                0.03457778109054249,
                0.0,
                0.0,
                0.06786807336201589,
                0.0,
                0.0,
                0.08812548310105732,
                0.0,
                0.0,
                0.08558097588832127,
                0.0,
                0.04513688606121917,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11678455934761472,
                0.06306487870150207,
                0.03596199952119993,
                0.0,
                0.01968589439890159,
                0.0,
                0.14758685769328508,
                0.0,
                0.0,
                0.08605000648241065,
                0.0,
                0.0,
                0.0,
                0.03195443626020457,
                0.0,
                0.0,
                0.039902311226314956,
                0.013921563383818124,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04841474597651399,
                0.0634818191770468,
                0.05541659770218853,
                0.0721586821192029,
                0.0,
                0.03511824343914338,
                1.0,
                0.0,
                0.04047322009186438,
                0.0,
                0.0,
                0.04030026224233221,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07481955656853269,
                0.2054685772068824
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.09245053401025695,
                0.14492826379266974,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.11585262016484336,
                0.0,
                0.0,
                0.11147202938048739,
                0.0,
                0.0,
                0.0,
                0.10428530191330504,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.15278062416859253,
                0.10230514193672645,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09130004559525352,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09978374664742752,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08007395875111498,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0764936932958258,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.07777345751273111,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13308091415185197,
                0.05749587253054675,
                0.06491912959678729,
                0.042089338950433444,
                0.0,
                0.0,
                0.04413184604536232,
                0.0,
                0.03770164411518182,
                0.0,
                0.03506184942585879,
                0.0,
                0.042587949167121296,
                0.039725496775848676,
                0.0,
                0.0,
                0.0,
                0.07016462844381442,
                0.0,
                0.06499521494282025,
                0.0,
                0.0,
                0.16144028002007085,
                0.0,
                0.0,
                0.08175540807945282,
                0.0,
                0.0,
                0.041475921898311056,
                0.0526778954713561,
                0.061475065120503346,
                0.0,
                0.0,
                0.18531670739343745,
                0.0,
                0.0,
                0.054387489927227965,
                0.0,
                0.0708861929451049,
                0.03938946765933924,
                0.0,
                0.11027061128779957,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.17936329060768852,
                0.01599411540811404,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06366666118864758,
                0.0,
                0.0,
                0.14152450283718773,
                0.04047322009186438,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09044840908681252,
                0.0,
                0.0,
                0.0
            ],
            [
                0.08369999517131749,
                0.0,
                0.03493755454661981,
                0.12497762377142682,
                0.04322568897973829,
                0.0,
                0.0,
                0.03730329879618098,
                0.0,
                0.0,
                0.06345793508781324,
                0.07165095041772884,
                0.12894816193091654,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0416111344982887,
                0.06583599225144784,
                0.13979915800980006,
                0.13045262504797064,
                0.05758899610868466,
                0.11364062848686347,
                0.056531390543753925,
                0.023175567725953878,
                0.0,
                0.0,
                0.07026023840828007,
                0.0,
                0.0,
                0.03313195818019077,
                0.06799328855050626,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03992517719912814,
                0.0,
                0.0,
                0.030496129575230138,
                0.12473797029291521,
                0.03998333839480332,
                0.022800024765220788,
                0.0,
                0.0,
                0.0,
                0.07823677131158922,
                0.059653722335124726,
                0.026543870156487823,
                0.07072417743153053,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08876789633248985,
                0.0,
                0.0,
                0.018018468352358947,
                0.07408868778654391,
                0.0,
                0.0,
                0.08278706845510997,
                0.028093360708543208,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08725731077110613,
                0.11541686973287248,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.05715788546336641,
                0.0,
                0.0,
                0.03736162945357738,
                0.0,
                0.0,
                0.05596247796011594,
                0.0,
                0.0,
                0.04743584242805371,
                0.0
            ],
            [
                0.0,
                0.08790840635929519,
                0.050074830972771205,
                0.06755102786273973,
                0.0,
                0.0,
                0.04530416892438609,
                0.100221654108504,
                0.0,
                0.0,
                0.06608750054789847,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05912316882722407,
                0.08714848685846988,
                0.0,
                0.0,
                0.0,
                0.1348145301046768,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10623090640642972,
                0.0,
                0.06477524017000641,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06149356152052946,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1053954294299423,
                0.04710455907066972,
                0.0,
                0.0814787411497955,
                0.0,
                0.0,
                0.045507305529875176,
                0.0,
                0.0,
                0.0,
                0.07831186163989133,
                0.06171692648135421,
                0.0,
                0.0,
                0.0,
                0.17525152480958048,
                0.07205803738799564,
                0.09804792450060812,
                0.0,
                0.0,
                0.052940876812039114,
                0.0,
                0.0,
                0.0688517429238921,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.05798175637807718,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08710931133032968,
                0.3312374169069556,
                0.12280507884299088
            ],
            [
                0.0378666341594286,
                0.0,
                0.0,
                0.02414399234121448,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05383413646689862,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0395998542843512,
                0.0,
                0.04090948974590364,
                0.0,
                0.0,
                0.0,
                0.10628175380464995,
                0.03322958088352345,
                0.04339601637958865,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08792253426443754,
                0.0,
                0.0,
                0.06718372764644816,
                0.0,
                0.0,
                0.0,
                0.061017713936960594,
                0.0,
                0.1228446275845207,
                0.029285603907317,
                0.0,
                0.03134741182554705,
                0.0,
                0.0502458027833716,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04133711612585755,
                0.0683278522695935,
                0.0,
                0.0,
                0.08040759908513956,
                0.05153199803788982,
                0.07011855493341863,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03082909735824827,
                0.0,
                0.0,
                0.04030026224233221,
                0.0764936932958258,
                0.0,
                0.05715788546336641,
                0.05798175637807718,
                1.0,
                0.0,
                0.0,
                0.17310413633014576,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06229585238890095,
                0.11799380607940874,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0377977384946736,
                0.0,
                0.08336891514461477,
                0.0,
                0.0,
                0.0,
                0.08018580454819928,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0629784539824795,
                0.05508135826903661,
                0.0,
                0.06690468769989219,
                0.05891816201872889,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09615906005339142,
                0.06338780451978959,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07649581182115923,
                0.1460089577166157,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03540235455692413,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05086014647611904,
                0.0,
                0.0,
                0.10003594786859957,
                0.0,
                0.0,
                0.06639882337639876,
                0.0,
                0.04902969782050625,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.2754988942974337,
                0.08959445397905559,
                0.0,
                0.0,
                0.0907538845337181,
                0.03160821824023429
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0973379935205781,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.18836325711743773,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.07670445653929218,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.025956687082853667,
                0.029396165758163742,
                0.0,
                0.027159866833797317,
                0.02008311266710429,
                0.0,
                0.0,
                0.0,
                0.023949439825428857,
                0.0,
                0.0,
                0.041466486817209894,
                0.0,
                0.038511913033728765,
                0.0,
                0.05520199120078586,
                0.018170189987918046,
                0.0,
                0.055375045914809434,
                0.05165313782354007,
                0.0,
                0.0,
                0.027856065799319926,
                0.06049382144524374,
                0.0,
                0.023977508615440846,
                0.0,
                0.0,
                0.0,
                0.0,
                0.030813204307840396,
                0.030160543407826784,
                0.0,
                0.0,
                0.034305464914723374,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07053520275280464,
                0.0,
                0.0,
                0.014531243584475075,
                0.022807658481081232,
                0.0,
                0.06175010529026467,
                0.0,
                0.0,
                0.0,
                0.05762372518494832,
                0.0,
                0.023757181523374884,
                0.0,
                0.07355119851262912,
                0.0,
                0.028883498645735764,
                0.0,
                0.0,
                0.051049382353182,
                0.023200918474209523,
                0.0,
                0.05343774455581727,
                0.07638249112002939,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03736162945357738,
                0.0,
                0.17310413633014576,
                0.0,
                0.07670445653929218,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04783675721649899,
                0.0
            ],
            [
                0.08817777126616948,
                0.05589199042178537,
                0.0,
                0.0,
                0.0,
                0.0,
                0.07060065574300627,
                0.01897089953829853,
                0.02982151966155441,
                0.04184333181860327,
                0.02192671572600757,
                0.06119634101849954,
                0.0,
                0.08302750546671799,
                0.042865854721549544,
                0.03479717789873706,
                0.0,
                0.09377335023771356,
                0.022748409393396157,
                0.01878337867682075,
                0.0,
                0.0,
                0.07521393982588966,
                0.08122259816289232,
                0.0,
                0.0,
                0.024929140889175423,
                0.0,
                0.09583989598474482,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08579671785120531,
                0.031178370083339575,
                0.03072809418433949,
                0.11265711229336783,
                0.06577070915113496,
                0.0,
                0.03450554965203611,
                0.0,
                0.0,
                0.08133542291131245,
                0.015756253678318555,
                0.056150223775571974,
                0.07916954256122481,
                0.0,
                0.0,
                0.06717393575774842,
                0.0,
                0.03019710969661949,
                0.07009113238410652,
                0.095623591078691,
                0.029970774752893458,
                0.0,
                0.0,
                0.020592879616889884,
                0.0,
                0.008568387898266748,
                0.0,
                0.06290331403773002,
                0.0,
                0.0,
                0.055652771838512576,
                0.013617407948139716,
                0.023983878960038167,
                0.03410758490432718,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.03904300662326613,
                0.0,
                0.02155753420625652
            ],
            [
                0.0,
                0.0,
                0.058906698034624054,
                0.040793337061553483,
                0.0,
                0.0,
                0.0,
                0.031865938849925994,
                0.0,
                0.070285388962023,
                0.0,
                0.0,
                0.0,
                0.06760182080007539,
                0.0,
                0.0,
                0.0,
                0.0,
                0.05309491105536313,
                0.04643714847170894,
                0.05009811996264711,
                0.056404980084154926,
                0.14270199084993532,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08106830856982448,
                0.0534400200409694,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06449091820961751,
                0.12309499730505728,
                0.08873461797748766,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.061737956301343315,
                0.0,
                0.07265124331481516,
                0.0,
                0.0,
                0.0,
                0.1519007967289927,
                0.197513502147482,
                0.0,
                0.0,
                0.08433677581065835,
                0.0,
                0.0,
                0.05597850373290252,
                0.0,
                0.04133520720555199,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.2754988942974337,
                0.0,
                0.0,
                0.0,
                1.0,
                0.07553392095644607,
                0.0,
                0.0,
                0.07651139592259563,
                0.026647772848641678
            ],
            [
                0.08518766597049135,
                0.05815274497981286,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.041873220170108895,
                0.0,
                0.0,
                0.07123198680070938,
                0.08042870521001415,
                0.07325081655211513,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1378864363416293,
                0.07390137300523915,
                0.030510209524744942,
                0.022951019191831613,
                0.0,
                0.034568434065883155,
                0.06345689092256268,
                0.22914244595652672,
                0.0,
                0.0,
                0.032148392620119766,
                0.0,
                0.0,
                0.03719086044175806,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.034232123947454056,
                0.0597079042149101,
                0.04488158382156195,
                0.15540159418629768,
                0.0,
                0.0,
                0.0,
                0.08782133634958918,
                0.02439992574168124,
                0.0,
                0.024524878078045258,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0385717282675089,
                0.0,
                0.0,
                0.0,
                0.028172030868707027,
                0.0,
                0.0,
                0.0,
                0.08146456975481443,
                0.0,
                0.0,
                0.0,
                0.0,
                0.02654287145645221,
                0.03510875031734477,
                0.0,
                0.0,
                0.0,
                0.05596247796011594,
                0.0,
                0.0,
                0.08959445397905559,
                0.0,
                0.0,
                0.0,
                0.07553392095644607,
                1.0,
                0.0,
                0.0,
                0.05324707299473037,
                0.0
            ],
            [
                0.057416217417583425,
                0.0,
                0.13133416173019807,
                0.03660892351179485,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13737405998109198,
                0.0,
                0.060391028018345574,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.18831273069604407,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.14138061532635907,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1785855959352988,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1941391469986114,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.10694782458652954,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09044840908681252,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.07295837934154607,
                0.040960328312163084,
                0.0,
                0.0,
                0.06656358109736413,
                0.0,
                0.0,
                0.023348801475789178,
                0.0,
                0.05149948982365257,
                0.0,
                0.04484761055573021,
                0.0,
                0.0,
                0.0,
                0.042827299624622846,
                0.0,
                0.026045152840004922,
                0.0,
                0.06626982763242599,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03582369935090506,
                0.0,
                0.037819196153090956,
                0.047039031580880734,
                0.0,
                0.06060814683806853,
                0.11838718762354714,
                0.06824579175336305,
                0.13946346013391914,
                0.10010514465440656,
                0.05558979741400191,
                0.04114957408552755,
                0.10540544180766163,
                0.0,
                0.0,
                0.027211139714217576,
                0.0,
                0.0,
                0.0,
                0.0,
                0.03688711061740545,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.06119117694719292,
                0.14847887653462624,
                0.10534311847851174,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.09401792700408024,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08710931133032968,
                0.06229585238890095,
                0.0,
                0.0,
                0.0,
                0.03904300662326613,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.04688520986402533
            ],
            [
                0.04030254102076003,
                0.11671648960219948,
                0.0,
                0.032201947225282565,
                0.0,
                0.0,
                0.06015060197528294,
                0.1685581308518529,
                0.0,
                0.0,
                0.06037883640756232,
                0.06817431118317673,
                0.06209007031762972,
                0.18307319858722282,
                0.0,
                0.0,
                0.0,
                0.039592083781619075,
                0.2799476025518569,
                0.025861569112328602,
                0.019454123005826697,
                0.0,
                0.09038094926238144,
                0.1546151404836984,
                0.022051045475106883,
                0.061692464721776356,
                0.08056698695501355,
                0.027250152999480342,
                0.0,
                0.0,
                0.09723741793465496,
                0.0,
                0.08600245367755001,
                0.06824160071350566,
                0.0,
                0.0,
                0.0,
                0.08164535032609178,
                0.0,
                0.0,
                0.029016399858246562,
                0.05061060266249032,
                0.21269180213082556,
                0.07606398858694066,
                0.0,
                0.06254099018150024,
                0.0,
                0.16772455132675945,
                0.020682269172631985,
                0.0,
                0.02078818334146141,
                0.0,
                0.0,
                0.0,
                0.10397510276165747,
                0.19138125073618797,
                0.0,
                0.0,
                0.0,
                0.41945496433797064,
                0.0,
                0.0,
                0.10370560677631642,
                0.026730217868143714,
                0.0702899023411807,
                0.06883455811832549,
                0.0,
                0.1994789642399768,
                0.022498708311195426,
                0.02975946042071931,
                0.07481955656853269,
                0.0,
                0.0,
                0.04743584242805371,
                0.3312374169069556,
                0.11799380607940874,
                0.0907538845337181,
                0.0,
                0.04783675721649899,
                0.0,
                0.07651139592259563,
                0.05324707299473037,
                0.0,
                0.0,
                1.0,
                0.16304899953813873
            ],
            [
                0.0,
                0.09937124423406445,
                0.014847878707223601,
                0.04421630241559504,
                0.0,
                0.11217986677308436,
                0.030808967612943425,
                0.1798153991943378,
                0.029827393505456563,
                0.028418105374505443,
                0.021931034557070614,
                0.14392987913399155,
                0.0,
                0.0,
                0.19417847690270026,
                0.0,
                0.0,
                0.029659681163616378,
                0.09334582317622438,
                0.11156186853304693,
                0.0,
                0.12430049398423144,
                0.04310632533541221,
                0.10241930416326182,
                0.0,
                0.0,
                0.06836853835332737,
                0.0,
                0.1118695280968525,
                0.03277790404937506,
                0.0,
                0.0,
                0.05655690853817135,
                0.0,
                0.12793732389778328,
                0.0,
                0.04589594470116847,
                0.08293492719937447,
                0.0,
                0.0,
                0.0,
                0.11956863692690237,
                0.049387535390929015,
                0.050380387885975805,
                0.022720672090684346,
                0.08776597971773803,
                0.0,
                0.0,
                0.15761614005252092,
                0.0,
                0.055828932172190746,
                0.021851853381531866,
                0.16140631960365945,
                0.08252092975207975,
                0.05304790472671509,
                0.0,
                0.07488008753517159,
                0.07591125922857209,
                0.015841381893940208,
                0.10360296299061457,
                0.0,
                0.05277274549342628,
                0.0,
                0.0,
                0.08356996822071526,
                0.2483087528079542,
                0.20276616775440431,
                0.09177095849400593,
                0.029207681454473056,
                0.0,
                0.2054685772068824,
                0.0,
                0.0,
                0.0,
                0.12280507884299088,
                0.0,
                0.03160821824023429,
                0.0,
                0.0,
                0.02155753420625652,
                0.026647772848641678,
                0.0,
                0.0,
                0.04688520986402533,
                0.16304899953813873,
                1.0
            ]
        ]
    },
    "P87-1015": {
        "input_sentences": [
            "Abstract",
            "CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*",
            "We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.",
            "In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages."
        ],
        "authority_scores": {
            "Abstract": 0,
            "CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*": 0,
            "We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.": 0,
            "In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.11432201079877596
            ],
            [
                0.0,
                1.0,
                0.5495946540701895,
                0.11393957122021557
            ],
            [
                0.0,
                0.5495946540701895,
                1.0,
                0.1270909128320098
            ],
            [
                0.11432201079877596,
                0.11393957122021557,
                0.1270909128320098,
                1.0
            ]
        ]
    },
    "D09-1034": {
        "input_sentences": [
            "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing",
            "Abstract",
            "Empirical results demonstrate the utility of our methods in predicting human reading times.",
            "In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG.",
            "A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006).",
            "We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size."
        ],
        "authority_scores": {
            "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing": 0,
            "Abstract": 0,
            "Empirical results demonstrate the utility of our methods in predicting human reading times.": 0,
            "In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG.": 0,
            "A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006).": 0,
            "We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.21579382168927752,
                0.18781981674304835,
                0.048422658993676065
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.06330384934181253,
                0.05509758013196887,
                0.0
            ],
            [
                0.21579382168927752,
                0.0,
                0.06330384934181253,
                1.0,
                0.053509202714855576,
                0.1051122885769276
            ],
            [
                0.18781981674304835,
                0.0,
                0.05509758013196887,
                0.053509202714855576,
                1.0,
                0.031432848950392236
            ],
            [
                0.048422658993676065,
                0.0,
                0.0,
                0.1051122885769276,
                0.031432848950392236,
                1.0
            ]
        ]
    },
    "W05-0638": {
        "input_sentences": [
            "Abstract",
            "In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs.",
            "Exploiting Full Parsing Information To Label Semantic Roles Using An Ensemble Of ME And SVM Via Integer Linear Programming",
            "The experimental results show that full parsing information not only increases the F-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies, which increases the F-score by 0.64%.",
            "Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ.",
            "The ensemble of SVM and ME also boosts the F-score by 0.77%.",
            "In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs.": 0,
            "Exploiting Full Parsing Information To Label Semantic Roles Using An Ensemble Of ME And SVM Via Integer Linear Programming": 0,
            "The experimental results show that full parsing information not only increases the F-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies, which increases the F-score by 0.64%.": 0,
            "Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ.": 0,
            "The ensemble of SVM and ME also boosts the F-score by 0.77%.": 0,
            "In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.18991033400291965,
                0.18276823787870933,
                0.0,
                0.0,
                0.21210273347579547
            ],
            [
                0.0,
                0.18991033400291965,
                1.0,
                0.08420401873412922,
                0.0,
                0.20443180164813776,
                0.11507728502249547
            ],
            [
                0.0,
                0.18276823787870933,
                0.08420401873412922,
                1.0,
                0.07532851983153278,
                0.1328962085466801,
                0.08859959186045825
            ],
            [
                0.0,
                0.0,
                0.0,
                0.07532851983153278,
                1.0,
                0.07720885213126388,
                0.0
            ],
            [
                0.0,
                0.0,
                0.20443180164813776,
                0.1328962085466801,
                0.07720885213126388,
                1.0,
                0.060540795634403906
            ],
            [
                0.0,
                0.21210273347579547,
                0.11507728502249547,
                0.08859959186045825,
                0.0,
                0.060540795634403906,
                1.0
            ]
        ]
    },
    "W12-3145": {
        "input_sentences": [
            "Abstract",
            "Kenneth Heafield.",
            "In of the Sixth on Statistical Machine pages 187\u2013197.",
            "models for morpheme segmentation and morphology Transactions on Speech and Language 4(1):3:1\u20133:34, February.",
            "2011.",
            "Kriya - The SFU System for Translation Task at WMT-12",
            "KenLM: Faster and smaller model queries."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Kenneth Heafield.": 0,
            "In of the Sixth on Statistical Machine pages 187\u2013197.": 0,
            "models for morpheme segmentation and morphology Transactions on Speech and Language 4(1):3:1\u20133:34, February.": 0,
            "2011.": 0,
            "Kriya - The SFU System for Translation Task at WMT-12": 0,
            "KenLM: Faster and smaller model queries.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P04-1036": {
        "input_sentences": [
            "Abstract",
            "Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.",
            "Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration.",
            "Finding Predominant Word Senses in Untagged Text",
            "We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.",
            "The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.",
            "This is a very promising result given that our method does not require any hand-tagged text, such as SemCor.",
            "The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data.",
            "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.": 0,
            "Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration.": 0,
            "Finding Predominant Word Senses in Untagged Text": 0,
            "We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.": 0,
            "The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.": 0,
            "This is a very promising result given that our method does not require any hand-tagged text, such as SemCor.": 0,
            "The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data.": 0,
            "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.17015623956699327,
                0.11334467476973316,
                0.11738372283221297,
                0.1739064811577693,
                0.09053623944518177,
                0.03357547080922753,
                0.02714543376969908
            ],
            [
                0.0,
                0.17015623956699327,
                1.0,
                0.10102359633544657,
                0.055338235223308795,
                0.13659685139695746,
                0.166334844301741,
                0.0,
                0.06585353943284725
            ],
            [
                0.0,
                0.11334467476973316,
                0.10102359633544657,
                1.0,
                0.08597130631579723,
                0.12736831820616012,
                0.09477258186856681,
                0.046491337042759866,
                0.23598590670624442
            ],
            [
                0.0,
                0.11738372283221297,
                0.055338235223308795,
                0.08597130631579723,
                1.0,
                0.15196435058548835,
                0.0,
                0.02546680813634491,
                0.020589660753194645
            ],
            [
                0.0,
                0.1739064811577693,
                0.13659685139695746,
                0.12736831820616012,
                0.15196435058548835,
                1.0,
                0.0,
                0.03772961772257243,
                0.03050402017781022
            ],
            [
                0.0,
                0.09053623944518177,
                0.166334844301741,
                0.09477258186856681,
                0.0,
                0.0,
                1.0,
                0.06708029646431188,
                0.0
            ],
            [
                0.0,
                0.03357547080922753,
                0.0,
                0.046491337042759866,
                0.02546680813634491,
                0.03772961772257243,
                0.06708029646431188,
                1.0,
                0.15923972414038476
            ],
            [
                0.0,
                0.02714543376969908,
                0.06585353943284725,
                0.23598590670624442,
                0.020589660753194645,
                0.03050402017781022,
                0.0,
                0.15923972414038476,
                1.0
            ]
        ]
    },
    "N10-1091": {
        "input_sentences": [
            "Abstract",
            "In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models.",
            "This study proves that fast and accurate ensemble parsers can be built with minimal effort.",
            "Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking.",
            "Ensemble Models for Dependency Parsing: Cheap and Good?"
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models.": 0,
            "This study proves that fast and accurate ensemble parsers can be built with minimal effort.": 0,
            "Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking.": 0,
            "Ensemble Models for Dependency Parsing: Cheap and Good?": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06965186617093432,
                0.1348202309282242,
                0.23695864045898832
            ],
            [
                0.0,
                0.06965186617093432,
                1.0,
                0.0,
                0.1154592683573037
            ],
            [
                0.0,
                0.1348202309282242,
                0.0,
                1.0,
                0.19155977633761764
            ],
            [
                0.0,
                0.23695864045898832,
                0.1154592683573037,
                0.19155977633761764,
                1.0
            ]
        ]
    },
    "E12-1055": {
        "input_sentences": [
            "Abstract",
            "We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT).",
            "We also explore adapting multiple (4\u201310) data sets with no priori between in-domain and out-of-domain data except for an in-domain development set.",
            "Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation",
            "While techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT).": 0,
            "We also explore adapting multiple (4\u201310) data sets with no priori between in-domain and out-of-domain data except for an in-domain development set.": 0,
            "Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation": 0,
            "While techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.20498516549659912,
                0.4516716487170141,
                0.2945289385797102
            ],
            [
                0.0,
                0.20498516549659912,
                1.0,
                0.1063816407227266,
                0.23622621602994864
            ],
            [
                0.0,
                0.4516716487170141,
                0.1063816407227266,
                1.0,
                0.3454358556531734
            ],
            [
                0.0,
                0.2945289385797102,
                0.23622621602994864,
                0.3454358556531734,
                1.0
            ]
        ]
    },
    "D11-1033": {
        "input_sentences": [
            "Abstract",
            "Domain Adaptation via Pseudo In-Domain Data Selection",
            "As these sentences are not themselves identical the in-domain data, we call them These subcorpora \u2013 1% the size of the original \u2013 can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus.",
            "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain.",
            "These sentences may be selected with simple cross-entropy based methods, of which we present three.",
            "Performance is further improved when we use these domain-adapted models in combination with a true in-domain model.",
            "The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Domain Adaptation via Pseudo In-Domain Data Selection": 0,
            "As these sentences are not themselves identical the in-domain data, we call them These subcorpora \u2013 1% the size of the original \u2013 can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus.": 0,
            "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain.": 0,
            "These sentences may be selected with simple cross-entropy based methods, of which we present three.": 0,
            "Performance is further improved when we use these domain-adapted models in combination with a true in-domain model.": 0,
            "The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.1884152530649151,
                0.24208947464787134,
                0.0,
                0.19459783421153595,
                0.33293437582417496
            ],
            [
                0.0,
                0.1884152530649151,
                1.0,
                0.2640549383396075,
                0.04264851247910935,
                0.1415455438710849,
                0.18880486957267414
            ],
            [
                0.0,
                0.24208947464787134,
                0.2640549383396075,
                1.0,
                0.11680298438476507,
                0.10282733369436077,
                0.11400493666980072
            ],
            [
                0.0,
                0.0,
                0.04264851247910935,
                0.11680298438476507,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.19459783421153595,
                0.1415455438710849,
                0.10282733369436077,
                0.0,
                1.0,
                0.09164014171882383
            ],
            [
                0.0,
                0.33293437582417496,
                0.18880486957267414,
                0.11400493666980072,
                0.0,
                0.09164014171882383,
                1.0
            ]
        ]
    },
    "P10-1021": {
        "input_sentences": [
            "Abstract",
            "Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure",
            "There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated.",
            "The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load.",
            "Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors.",
            "In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure": 0,
            "There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated.": 0,
            "The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load.": 0,
            "Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors.": 0,
            "In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.23649199891508643,
                0.12418459724075503
            ],
            [
                0.0,
                0.0,
                1.0,
                0.03495598015310731,
                0.0,
                0.08718515898893452
            ],
            [
                0.0,
                0.0,
                0.03495598015310731,
                1.0,
                0.0,
                0.19510826997764252
            ],
            [
                0.0,
                0.23649199891508643,
                0.0,
                0.0,
                1.0,
                0.03682765627129264
            ],
            [
                0.0,
                0.12418459724075503,
                0.08718515898893452,
                0.19510826997764252,
                0.03682765627129264,
                1.0
            ]
        ]
    },
    "E12-1014": {
        "input_sentences": [
            "Abstract",
            "We extend existing research on bilingual lexicon induction estimate and phrasal translation probabilities for MT-scale phrasetables.",
            "We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features.",
            "Toward Statistical Machine Translation without Parallel Corpora",
            "We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone.",
            "We estimate the parameters of a phrasebased statistical machine translation sysfrom instead of a corpus.",
            "We report translation results for an end-to-end translation system using these monolingual features alone.",
            "Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights.",
            "In this paper, we examine an idealization where a phrase-table is given.",
            "We propose a novel algorithm to estimate reordering probabilities from monolingual data."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We extend existing research on bilingual lexicon induction estimate and phrasal translation probabilities for MT-scale phrasetables.": 0,
            "We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features.": 0,
            "Toward Statistical Machine Translation without Parallel Corpora": 0,
            "We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone.": 0,
            "We estimate the parameters of a phrasebased statistical machine translation sysfrom instead of a corpus.": 0,
            "We report translation results for an end-to-end translation system using these monolingual features alone.": 0,
            "Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights.": 0,
            "In this paper, we examine an idealization where a phrase-table is given.": 0,
            "We propose a novel algorithm to estimate reordering probabilities from monolingual data.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.05620206680091344,
                0.10110992372980473,
                0.0999045933893193,
                0.06878700447932598,
                0.05422760014740448,
                0.0,
                0.12933150584544226
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.2709657435148865,
                0.0,
                0.15505733912927439,
                0.03379212641212303,
                0.1958273125327976,
                0.05264664227575045
            ],
            [
                0.0,
                0.05620206680091344,
                0.0,
                1.0,
                0.10098715005979599,
                0.35341976249772633,
                0.12258847532784999,
                0.0966414931581528,
                0.0,
                0.0
            ],
            [
                0.0,
                0.10110992372980473,
                0.2709657435148865,
                0.10098715005979599,
                1.0,
                0.06988735237217339,
                0.17207120694140698,
                0.0,
                0.08540912957362436,
                0.05809759734699287
            ],
            [
                0.0,
                0.0999045933893193,
                0.0,
                0.35341976249772633,
                0.06988735237217339,
                1.0,
                0.08483637737011154,
                0.0,
                0.0,
                0.07975355605902551
            ],
            [
                0.0,
                0.06878700447932598,
                0.15505733912927439,
                0.12258847532784999,
                0.17207120694140698,
                0.08483637737011154,
                1.0,
                0.035781626348217306,
                0.0,
                0.055746195413241315
            ],
            [
                0.0,
                0.05422760014740448,
                0.03379212641212303,
                0.0966414931581528,
                0.0,
                0.0,
                0.035781626348217306,
                1.0,
                0.0,
                0.04288830595653825
            ],
            [
                0.0,
                0.0,
                0.1958273125327976,
                0.0,
                0.08540912957362436,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.12933150584544226,
                0.05264664227575045,
                0.0,
                0.05809759734699287,
                0.07975355605902551,
                0.055746195413241315,
                0.04288830595653825,
                0.0,
                1.0
            ]
        ]
    },
    "P14-2022": {
        "input_sentences": [
            "Faster Phrase-Based Decoding by Refining Feature State",
            "Abstract",
            "Most features depend upon only part of the state, but traditional algorithms, including cube pruning, handle state atomically.",
            "Moreover, we exploit shared words in hypotheses to iteratively refine language model scores rather than handling language model state atomically.",
            "Our key contribution avoids this behavior by placing hypotheses into equivalence classes, masking the parts of state that matter least to the score.",
            "Since our algorithm and cube pruning are both approximate, improvement can be used to increase speed or accuracy.",
            "We contribute a faster decoding algorithm for phrase-based machine translation.",
            "When tuned to attain the same accuracy, our algorithm is 4.0\u20137.7 times as fast as the Moses decoder with cube pruning.",
            "Translation hypotheses keep track of state, such as context for the language model and coverage of words in the source sentence.",
            "For example, cube pruning will repeatedly query the language model with hypotheses that differ only in source coverage, despite the fact that source coverage is irrelevant to the language model."
        ],
        "authority_scores": {
            "Faster Phrase-Based Decoding by Refining Feature State": 0,
            "Abstract": 0,
            "Most features depend upon only part of the state, but traditional algorithms, including cube pruning, handle state atomically.": 0,
            "Moreover, we exploit shared words in hypotheses to iteratively refine language model scores rather than handling language model state atomically.": 0,
            "Our key contribution avoids this behavior by placing hypotheses into equivalence classes, masking the parts of state that matter least to the score.": 0,
            "Since our algorithm and cube pruning are both approximate, improvement can be used to increase speed or accuracy.": 0,
            "We contribute a faster decoding algorithm for phrase-based machine translation.": 0,
            "When tuned to attain the same accuracy, our algorithm is 4.0\u20137.7 times as fast as the Moses decoder with cube pruning.": 0,
            "Translation hypotheses keep track of state, such as context for the language model and coverage of words in the source sentence.": 0,
            "For example, cube pruning will repeatedly query the language model with hypotheses that differ only in source coverage, despite the fact that source coverage is irrelevant to the language model.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.10262103531705508,
                0.043280283904648297,
                0.044849411280149194,
                0.0,
                0.5083641748235931,
                0.0,
                0.05518653483924493,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.10262103531705508,
                0.0,
                1.0,
                0.13371285454060985,
                0.06843564522550515,
                0.10896045631127202,
                0.0,
                0.10205728460857362,
                0.08420904559688532,
                0.06770679807129609
            ],
            [
                0.043280283904648297,
                0.0,
                0.13371285454060985,
                1.0,
                0.06464992465058737,
                0.0,
                0.0,
                0.0,
                0.37517343373561246,
                0.317558309773409
            ],
            [
                0.044849411280149194,
                0.0,
                0.06843564522550515,
                0.06464992465058737,
                1.0,
                0.0,
                0.0,
                0.0,
                0.08243488714040108,
                0.02959052229184539
            ],
            [
                0.0,
                0.0,
                0.10896045631127202,
                0.0,
                0.0,
                1.0,
                0.08330165133408067,
                0.28167016008259727,
                0.0,
                0.07599351270006177
            ],
            [
                0.5083641748235931,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08330165133408067,
                1.0,
                0.07802408898031524,
                0.10428871099692637,
                0.0
            ],
            [
                0.0,
                0.0,
                0.10205728460857362,
                0.0,
                0.0,
                0.28167016008259727,
                0.07802408898031524,
                1.0,
                0.0,
                0.07117895626170508
            ],
            [
                0.05518653483924493,
                0.0,
                0.08420904559688532,
                0.37517343373561246,
                0.08243488714040108,
                0.0,
                0.10428871099692637,
                0.0,
                1.0,
                0.46138657260303184
            ],
            [
                0.0,
                0.0,
                0.06770679807129609,
                0.317558309773409,
                0.02959052229184539,
                0.07599351270006177,
                0.0,
                0.07117895626170508,
                0.46138657260303184,
                1.0
            ]
        ]
    },
    "N07-2041": {
        "input_sentences": [
            "Abstract",
            "Simultaneous Identification of Biomedical Named-Entity and Functional Relation Using Statistical Parsing Techniques",
            "We build a parser that derives both syntactic and domain-dependent semantic information achieves an F-score of the relation extraction task.",
            "In this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles.",
            "We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Simultaneous Identification of Biomedical Named-Entity and Functional Relation Using Statistical Parsing Techniques": 0,
            "We build a parser that derives both syntactic and domain-dependent semantic information achieves an F-score of the relation extraction task.": 0,
            "In this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles.": 0,
            "We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.04414918680480012,
                0.20254546380526303,
                0.02920634101925007
            ],
            [
                0.0,
                0.04414918680480012,
                1.0,
                0.0,
                0.1393288514500273
            ],
            [
                0.0,
                0.20254546380526303,
                0.0,
                1.0,
                0.029848124972362357
            ],
            [
                0.0,
                0.02920634101925007,
                0.1393288514500273,
                0.029848124972362357,
                1.0
            ]
        ]
    },
    "P13-2009": {
        "input_sentences": [
            "Abstract",
            "Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance.",
            "Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser.",
            "These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.",
            "In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems.",
            "Semantic Parsing as Machine Translation"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance.": 0,
            "Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser.": 0,
            "These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.": 0,
            "In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems.": 0,
            "Semantic Parsing as Machine Translation": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.03439206357473867,
                0.1312808742618138,
                0.0,
                0.2090808208031968
            ],
            [
                0.0,
                0.03439206357473867,
                1.0,
                0.30925854022203475,
                0.04782158888714563,
                0.492531983760786
            ],
            [
                0.0,
                0.1312808742618138,
                0.30925854022203475,
                1.0,
                0.0,
                0.6278953457208092
            ],
            [
                0.0,
                0.0,
                0.04782158888714563,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.2090808208031968,
                0.492531983760786,
                0.6278953457208092,
                0.0,
                1.0
            ]
        ]
    },
    "W11-2147": {
        "input_sentences": [
            "Abstract",
            "Experiments with word alignment normalization and clause reordering for SMT between English and German",
            "For English?German we attempted to improve the trans lation tables with a combination of standard statistical word alignments and phrase-basedword alignments.",
            "For German?English trans lation we tried to make the German text moresimilar to the English text by normalizing Ger man morphology and performing rule-basedclause reordering of the German text.",
            "This resulted in small improvements for both transla tion directions.",
            "This paper presents the LIU system for theWMT 2011 shared task for translation be tween German and English."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Experiments with word alignment normalization and clause reordering for SMT between English and German": 0,
            "For English?German we attempted to improve the trans lation tables with a combination of standard statistical word alignments and phrase-basedword alignments.": 0,
            "For German?English trans lation we tried to make the German text moresimilar to the English text by normalizing Ger man morphology and performing rule-basedclause reordering of the German text.": 0,
            "This resulted in small improvements for both transla tion directions.": 0,
            "This paper presents the LIU system for theWMT 2011 shared task for translation be tween German and English.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.13511291021442176,
                0.18108639939966426,
                0.0,
                0.08511166447052676
            ],
            [
                0.0,
                0.13511291021442176,
                1.0,
                0.15994853463670436,
                0.0,
                0.05889441050648904
            ],
            [
                0.0,
                0.18108639939966426,
                0.15994853463670436,
                1.0,
                0.0,
                0.11166764614270029
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.08511166447052676,
                0.05889441050648904,
                0.11166764614270029,
                0.0,
                1.0
            ]
        ]
    },
    "P14-1008": {
        "input_sentences": [
            "Logical Inference on Dependency-based Compositional Semantics",
            "Abstract",
            "Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics.",
            "Experiments on FraCaS and PASCAL RTE datasets show promising results.",
            "In this paper, we equip the DCS framework logical inference, by defining abdenotations an abstraction of the computing process of denotations in original DCS.",
            "An inference engine is built to achieve inference on abstract denotations.",
            "Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation."
        ],
        "authority_scores": {
            "Logical Inference on Dependency-based Compositional Semantics": 0,
            "Abstract": 0,
            "Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics.": 0,
            "Experiments on FraCaS and PASCAL RTE datasets show promising results.": 0,
            "In this paper, we equip the DCS framework logical inference, by defining abdenotations an abstraction of the computing process of denotations in original DCS.": 0,
            "An inference engine is built to achieve inference on abstract denotations.": 0,
            "Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.556794074824975,
                0.0,
                0.13080042308717488,
                0.1638490380195514,
                0.1371606569752501
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.34185671375630083,
                0.0
            ],
            [
                0.556794074824975,
                0.0,
                1.0,
                0.0,
                0.18664162173463486,
                0.0,
                0.03833032425807289
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.13080042308717488,
                0.0,
                0.18664162173463486,
                0.0,
                1.0,
                0.16853021732190604,
                0.11610920203212544
            ],
            [
                0.1638490380195514,
                0.34185671375630083,
                0.0,
                0.0,
                0.16853021732190604,
                1.0,
                0.09262915304899486
            ],
            [
                0.1371606569752501,
                0.0,
                0.03833032425807289,
                0.0,
                0.11610920203212544,
                0.09262915304899486,
                1.0
            ]
        ]
    },
    "C10-1132": {
        "input_sentences": [
            "Abstract",
            "A simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches.",
            "A Character-Based Joint Model for Chinese Word Segmentation",
            "Experiments on the Sec ond SIGHAN Bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one.",
            "However, generative and discriminative charac ter-based approaches are significantly different and complement each other.",
            "The character-based tagging approach is a dominant technique for Chinese word segmentation, and both discrimi native and generative models can be adopted in that framework.",
            "In addition, closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best F score in four out of five corpora."
        ],
        "authority_scores": {
            "Abstract": 0,
            "A simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches.": 0,
            "A Character-Based Joint Model for Chinese Word Segmentation": 0,
            "Experiments on the Sec ond SIGHAN Bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one.": 0,
            "However, generative and discriminative charac ter-based approaches are significantly different and complement each other.": 0,
            "The character-based tagging approach is a dominant technique for Chinese word segmentation, and both discrimi native and generative models can be adopted in that framework.": 0,
            "In addition, closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best F score in four out of five corpora.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.3526973069317199,
                0.18824601079309192,
                0.22812656788067628,
                0.12236485086530557,
                0.22030950774615324
            ],
            [
                0.0,
                0.3526973069317199,
                1.0,
                0.1092158259028051,
                0.07575302454249241,
                0.4418483685751145,
                0.11084308790608627
            ],
            [
                0.0,
                0.18824601079309192,
                0.1092158259028051,
                1.0,
                0.09407030086744737,
                0.08541863149823394,
                0.11287109795264282
            ],
            [
                0.0,
                0.22812656788067628,
                0.07575302454249241,
                0.09407030086744737,
                1.0,
                0.08416511614504105,
                0.05443763717064354
            ],
            [
                0.0,
                0.12236485086530557,
                0.4418483685751145,
                0.08541863149823394,
                0.08416511614504105,
                1.0,
                0.0
            ],
            [
                0.0,
                0.22030950774615324,
                0.11084308790608627,
                0.11287109795264282,
                0.05443763717064354,
                0.0,
                1.0
            ]
        ]
    },
    "P11-1089": {
        "input_sentences": [
            "Abstract",
            "Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the \u201cpipeline\u201d approach, assuming that morphological information has been separately obtained.",
            "In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.",
            "In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures.",
            "However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other.",
            "Most previous studies of morphological disambiguation and dependency parsing have been pursued independently.",
            "A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the \u201cpipeline\u201d approach, assuming that morphological information has been separately obtained.": 0,
            "In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.": 0,
            "In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures.": 0,
            "However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other.": 0,
            "Most previous studies of morphological disambiguation and dependency parsing have been pursued independently.": 0,
            "A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0911958014419536,
                0.11662169883760831,
                0.0,
                0.06146631431547778,
                0.07584398866170822
            ],
            [
                0.0,
                0.0911958014419536,
                1.0,
                0.0760305410166498,
                0.0679708251928666,
                0.08748493000304669,
                0.2699461492378048
            ],
            [
                0.0,
                0.11662169883760831,
                0.0760305410166498,
                1.0,
                0.0,
                0.04099668308569163,
                0.25774957268890963
            ],
            [
                0.0,
                0.0,
                0.0679708251928666,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.06146631431547778,
                0.08748493000304669,
                0.04099668308569163,
                0.0,
                1.0,
                0.4343191438837163
            ],
            [
                0.0,
                0.07584398866170822,
                0.2699461492378048,
                0.25774957268890963,
                0.0,
                0.4343191438837163,
                1.0
            ]
        ]
    },
    "W06-0508": {
        "input_sentences": [
            "Abstract",
            "The relations extracted can be used for various tasks, including semantic web annotation and ontology learning.",
            "We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases.",
            "We suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text.",
            "A Hybrid Approach For Extracting Semantic Relations From Texts"
        ],
        "authority_scores": {
            "Abstract": 0,
            "The relations extracted can be used for various tasks, including semantic web annotation and ontology learning.": 0,
            "We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases.": 0,
            "We suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text.": 0,
            "A Hybrid Approach For Extracting Semantic Relations From Texts": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.05679091410242427,
                0.02105734948027468,
                0.15766849504641986
            ],
            [
                0.0,
                0.05679091410242427,
                1.0,
                0.06060495983206641,
                0.20853750836104765
            ],
            [
                0.0,
                0.02105734948027468,
                0.06060495983206641,
                1.0,
                0.03298161578994914
            ],
            [
                0.0,
                0.15766849504641986,
                0.20853750836104765,
                0.03298161578994914,
                1.0
            ]
        ]
    },
    "C10-1061": {
        "input_sentences": [
            "Abstract",
            "Our experiments show that data driven LCFRS parsing is feasible with a reasonable speed and yields output of competitive quality.",
            "We evaluate our parser with a gram mar extracted from the German NeGratreebank.",
            "This paper presents a first efficient imple mentation of a weighted deductive CYKparser for Probabilistic Linear ContextFree Rewriting Systems (PLCFRS), to gether with context-summary estimatesfor parse items used to speed up parsing.",
            "LCFRS, an extension of CFG, can de scribe discontinuities both in constituencyand dependency structures in a straight forward way and is therefore a naturalcandidate to be used for data-driven parsing.",
            "Data-Driven Parsing with Probabilistic Linear Context-Free Rewriting Systems"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Our experiments show that data driven LCFRS parsing is feasible with a reasonable speed and yields output of competitive quality.": 0,
            "We evaluate our parser with a gram mar extracted from the German NeGratreebank.": 0,
            "This paper presents a first efficient imple mentation of a weighted deductive CYKparser for Probabilistic Linear ContextFree Rewriting Systems (PLCFRS), to gether with context-summary estimatesfor parse items used to speed up parsing.": 0,
            "LCFRS, an extension of CFG, can de scribe discontinuities both in constituencyand dependency structures in a straight forward way and is therefore a naturalcandidate to be used for data-driven parsing.": 0,
            "Data-Driven Parsing with Probabilistic Linear Context-Free Rewriting Systems": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.07360752340794291,
                0.17269492005076345,
                0.1770821183870285
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.07360752340794291,
                0.0,
                1.0,
                0.06189493390543049,
                0.34817942678338043
            ],
            [
                0.0,
                0.17269492005076345,
                0.0,
                0.06189493390543049,
                1.0,
                0.14890442587851024
            ],
            [
                0.0,
                0.1770821183870285,
                0.0,
                0.34817942678338043,
                0.14890442587851024,
                1.0
            ]
        ]
    },
    "P12-1025": {
        "input_sentences": [
            "Abstract",
            "Penn Chinese Treebank (CTB) and PKU\u2019s People\u2019s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible.",
            "We empirically analyze the diversity between two representative corpora, i.e.",
            "Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.",
            "The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error.",
            "Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations",
            "We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Penn Chinese Treebank (CTB) and PKU\u2019s People\u2019s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible.": 0,
            "We empirically analyze the diversity between two representative corpora, i.e.": 0,
            "Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.": 0,
            "The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error.": 0,
            "Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations": 0,
            "We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.12034280190422095,
                0.04669981487672907,
                0.10217274596606582,
                0.07499997031336665
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.12034280190422095,
                0.0,
                1.0,
                0.0939797471782157,
                0.0,
                0.03258799068615554
            ],
            [
                0.0,
                0.04669981487672907,
                0.0,
                0.0939797471782157,
                1.0,
                0.2436019212803023,
                0.0585700028326884
            ],
            [
                0.0,
                0.10217274596606582,
                0.0,
                0.0,
                0.2436019212803023,
                1.0,
                0.1281430780068903
            ],
            [
                0.0,
                0.07499997031336665,
                0.0,
                0.03258799068615554,
                0.0585700028326884,
                0.1281430780068903,
                1.0
            ]
        ]
    },
    "P14-1012": {
        "input_sentences": [
            "Abstract",
            "Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation",
            "On two Chinese- English tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the baseline features, respectively.",
            "In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep autoencoder (DAE) paradigm for phrase-based translation model.",
            "Using the unsupervised pre-trained deep belief net (DBN) to initialize DAE\u2019s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features.",
            "Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation": 0,
            "On two Chinese- English tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the baseline features, respectively.": 0,
            "In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep autoencoder (DAE) paradigm for phrase-based translation model.": 0,
            "Using the unsupervised pre-trained deep belief net (DBN) to initialize DAE\u2019s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features.": 0,
            "Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.15239067623315636,
                0.22019775509791184,
                0.25278609931877666,
                0.06063555241201858
            ],
            [
                0.0,
                0.15239067623315636,
                1.0,
                0.11677201091876027,
                0.3234130452737731,
                0.0
            ],
            [
                0.0,
                0.22019775509791184,
                0.11677201091876027,
                1.0,
                0.29406832405189415,
                0.02574867849607202
            ],
            [
                0.0,
                0.25278609931877666,
                0.3234130452737731,
                0.29406832405189415,
                1.0,
                0.021158239610106842
            ],
            [
                0.0,
                0.06063555241201858,
                0.0,
                0.02574867849607202,
                0.021158239610106842,
                1.0
            ]
        ]
    },
    "P14-2110": {
        "input_sentences": [
            "Abstract",
            "Code-switched documents are in social media, providing evidence polylingual topic models to infer aligned topics across languages.",
            "Learning Polylingual Topic Models from Code-Switched Social Media Documents",
            "We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators.",
            "We Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Code-switched documents are in social media, providing evidence polylingual topic models to infer aligned topics across languages.": 0,
            "Learning Polylingual Topic Models from Code-Switched Social Media Documents": 0,
            "We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators.": 0,
            "We Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.6010305842810572,
                0.10281634697550308,
                0.2009870009558809
            ],
            [
                0.0,
                0.6010305842810572,
                1.0,
                0.027165330410330025,
                0.2709087889152935
            ],
            [
                0.0,
                0.10281634697550308,
                0.027165330410330025,
                1.0,
                0.09736607104978136
            ],
            [
                0.0,
                0.2009870009558809,
                0.2709087889152935,
                0.09736607104978136,
                1.0
            ]
        ]
    },
    "D11-1140": {
        "input_sentences": [
            "Abstract",
            "In this paper, we introduce factored lexicons, which both model word meaning model systematic variation in word usage.",
            "Lexical Generalization in CCG Grammar Induction for Semantic Parsing",
            "We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations.",
            "We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model.",
            "Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content.",
            "Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring.",
            "Such lexicons can be inefficient when words appear repeatedly with closely related lexical content."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we introduce factored lexicons, which both model word meaning model systematic variation in word usage.": 0,
            "Lexical Generalization in CCG Grammar Induction for Semantic Parsing": 0,
            "We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations.": 0,
            "We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model.": 0,
            "Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content.": 0,
            "Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring.": 0,
            "Such lexicons can be inefficient when words appear repeatedly with closely related lexical content.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.08853166620263712,
                0.2331056339823224,
                0.036291903543245733,
                0.0,
                0.04085123124431552
            ],
            [
                0.0,
                0.0,
                1.0,
                0.19813707668932254,
                0.06778814092685465,
                0.20186730136910475,
                0.13519012376406056,
                0.0688396699440976
            ],
            [
                0.0,
                0.08853166620263712,
                0.19813707668932254,
                1.0,
                0.2262977111022281,
                0.08106266378629477,
                0.0,
                0.0
            ],
            [
                0.0,
                0.2331056339823224,
                0.06778814092685465,
                0.2262977111022281,
                1.0,
                0.0975823788215377,
                0.0,
                0.054920794080956015
            ],
            [
                0.0,
                0.036291903543245733,
                0.20186730136910475,
                0.08106266378629477,
                0.0975823788215377,
                1.0,
                0.03542273717527521,
                0.2722118249564969
            ],
            [
                0.0,
                0.0,
                0.13519012376406056,
                0.0,
                0.0,
                0.03542273717527521,
                1.0,
                0.03987287208369899
            ],
            [
                0.0,
                0.04085123124431552,
                0.0688396699440976,
                0.0,
                0.054920794080956015,
                0.2722118249564969,
                0.03987287208369899,
                1.0
            ]
        ]
    },
    "P01-1018": {
        "input_sentences": [
            "Abstract",
            "We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar.",
            "Finally, we generalize this result to formalisms beyond CFG.",
            "We consider the question \u201cHow much strong generative power can be squeezed out of a formal system without increasing its weak generative power?\u201d and propose some theoretical and practical constraints on this problem.",
            "Constraints On Strong Generative Power"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar.": 0,
            "Finally, we generalize this result to formalisms beyond CFG.": 0,
            "We consider the question \u201cHow much strong generative power can be squeezed out of a formal system without increasing its weak generative power?\u201d and propose some theoretical and practical constraints on this problem.": 0,
            "Constraints On Strong Generative Power": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.238435587667789,
                0.45167210392589197
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.238435587667789,
                0.0,
                1.0,
                0.5278953151973058
            ],
            [
                0.0,
                0.45167210392589197,
                0.0,
                0.5278953151973058,
                1.0
            ]
        ]
    },
    "D12-1046": {
        "input_sentences": [
            "Abstract",
            "Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system.",
            "We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features.",
            "As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing.",
            "In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing.",
            "Joint Chinese Word Segmentation, POS Tagging and Parsing",
            "Previous work often used a pipeline method \u2013 Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components.",
            "In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system.": 0,
            "We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features.": 0,
            "As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing.": 0,
            "In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing.": 0,
            "Joint Chinese Word Segmentation, POS Tagging and Parsing": 0,
            "Previous work often used a pipeline method \u2013 Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components.": 0,
            "In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.04467568320231189,
                0.03402779671301544,
                0.06504648937122004,
                0.07847014245258696,
                0.07346275597559679
            ],
            [
                0.0,
                0.0,
                1.0,
                0.2779559606353811,
                0.30611320216026844,
                0.4046957571361989,
                0.151168859443678,
                0.0
            ],
            [
                0.0,
                0.04467568320231189,
                0.2779559606353811,
                1.0,
                0.35930007502677624,
                0.6868269699744741,
                0.27562480238442166,
                0.0
            ],
            [
                0.0,
                0.03402779671301544,
                0.30611320216026844,
                0.35930007502677624,
                1.0,
                0.523130410909941,
                0.15305734270048255,
                0.07712809208227743
            ],
            [
                0.0,
                0.06504648937122004,
                0.4046957571361989,
                0.6868269699744741,
                0.523130410909941,
                1.0,
                0.29257970767605024,
                0.0
            ],
            [
                0.0,
                0.07847014245258696,
                0.151168859443678,
                0.27562480238442166,
                0.15305734270048255,
                0.29257970767605024,
                1.0,
                0.0
            ],
            [
                0.0,
                0.07346275597559679,
                0.0,
                0.0,
                0.07712809208227743,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P12-2058": {
        "input_sentences": [
            "Abstract",
            "We propose a novel heuristic algorithm for Cube Pruning running in linear time in the beam size.",
            "Heuristic Cube Pruning in Linear Time",
            "Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We propose a novel heuristic algorithm for Cube Pruning running in linear time in the beam size.": 0,
            "Heuristic Cube Pruning in Linear Time": 0,
            "Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.5829495577403239,
                0.11735318906753682
            ],
            [
                0.0,
                0.5829495577403239,
                1.0,
                0.07970385532520921
            ],
            [
                0.0,
                0.11735318906753682,
                0.07970385532520921,
                1.0
            ]
        ]
    },
    "W10-2803": {
        "input_sentences": [
            "Abstract",
            "In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation.",
            "What Is Word Meaning Really? (And How Can Distributional Models Help Us Describe It?)",
            "More specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees.",
            "Or move away from dictionary senses completely, and only model similarities between individual word usages.",
            "We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation.": 0,
            "What Is Word Meaning Really? (And How Can Distributional Models Help Us Describe It?)": 0,
            "More specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees.": 0,
            "Or move away from dictionary senses completely, and only model similarities between individual word usages.": 0,
            "We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.17592902027816007,
                0.10183003747836954,
                0.027054790482824227,
                0.16927729964015933
            ],
            [
                0.0,
                0.17592902027816007,
                1.0,
                0.07111284201925301,
                0.04822899442360328,
                0.32849795758542605
            ],
            [
                0.0,
                0.10183003747836954,
                0.07111284201925301,
                1.0,
                0.20247494703184624,
                0.07308005537275573
            ],
            [
                0.0,
                0.027054790482824227,
                0.04822899442360328,
                0.20247494703184624,
                1.0,
                0.02599686132090279
            ],
            [
                0.0,
                0.16927729964015933,
                0.32849795758542605,
                0.07308005537275573,
                0.02599686132090279,
                1.0
            ]
        ]
    },
    "P12-1048": {
        "input_sentences": [
            "Abstract",
            "Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.",
            "In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation.",
            "To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily.",
            "Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora.",
            "Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.": 0,
            "In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation.": 0,
            "To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily.": 0,
            "Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora.": 0,
            "Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09610318849859112,
                0.026294011852850914,
                0.0,
                0.08609740509151238
            ],
            [
                0.0,
                0.09610318849859112,
                1.0,
                0.22610492540165666,
                0.4598706143748725,
                0.4910074286227582
            ],
            [
                0.0,
                0.026294011852850914,
                0.22610492540165666,
                1.0,
                0.30259601976145983,
                0.15464952414259264
            ],
            [
                0.0,
                0.0,
                0.4598706143748725,
                0.30259601976145983,
                1.0,
                0.15806395375981344
            ],
            [
                0.0,
                0.08609740509151238,
                0.4910074286227582,
                0.15464952414259264,
                0.15806395375981344,
                1.0
            ]
        ]
    },
    "P04-1040": {
        "input_sentences": [
            "Abstract",
            "Enriching The Output Of A Parser Using Memory-Based Learning",
            "Our method is largely independent of the choice of parser and corpus, and shows state of the art performance.",
            "It also facilitates dependency-based evaluation of phrase structure parsers.",
            "We describe a method for enriching the output of a parser with information available in a corpus.",
            "The method is based on graph rewriting using memorybased learning, applied to dependency structures.",
            "This general framework allows us to accurately recover both grammatical and semantic information as well as non-local dependencies."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Enriching The Output Of A Parser Using Memory-Based Learning": 0,
            "Our method is largely independent of the choice of parser and corpus, and shows state of the art performance.": 0,
            "It also facilitates dependency-based evaluation of phrase structure parsers.": 0,
            "We describe a method for enriching the output of a parser with information available in a corpus.": 0,
            "The method is based on graph rewriting using memorybased learning, applied to dependency structures.": 0,
            "This general framework allows us to accurately recover both grammatical and semantic information as well as non-local dependencies.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07822435913783136,
                0.092697354594617,
                0.39502492913917886,
                0.30340464999334765,
                0.0
            ],
            [
                0.0,
                0.07822435913783136,
                1.0,
                0.0,
                0.2635135090062682,
                0.06008135830031526,
                0.0
            ],
            [
                0.0,
                0.092697354594617,
                0.0,
                1.0,
                0.0,
                0.16864483466736604,
                0.0
            ],
            [
                0.0,
                0.39502492913917886,
                0.2635135090062682,
                0.0,
                1.0,
                0.08118116778770522,
                0.09656833309698316
            ],
            [
                0.0,
                0.30340464999334765,
                0.06008135830031526,
                0.16864483466736604,
                0.08118116778770522,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.09656833309698316,
                0.0,
                1.0
            ]
        ]
    },
    "P14-1004": {
        "input_sentences": [
            "Abstract",
            "Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states).",
            "Discovering Latent Structure in Task-Oriented Dialogues",
            "We propose three new unsupervised models to discover latent structures in task-oriented dialogues.",
            "We show that our models extract meaningful state representations and dialogue structures consistent with human annotations.",
            "We apply them to two real, non-trivial datasets: human-computer spoken dialogues in bus query service, and humanhuman text-based chats from a live technical support service.",
            "A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue, since it provides a basis for analysing, evaluating, and building conversational systems.",
            "Quantitatively, we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states).": 0,
            "Discovering Latent Structure in Task-Oriented Dialogues": 0,
            "We propose three new unsupervised models to discover latent structures in task-oriented dialogues.": 0,
            "We show that our models extract meaningful state representations and dialogue structures consistent with human annotations.": 0,
            "We apply them to two real, non-trivial datasets: human-computer spoken dialogues in bus query service, and humanhuman text-based chats from a live technical support service.": 0,
            "A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue, since it provides a basis for analysing, evaluating, and building conversational systems.": 0,
            "Quantitatively, we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.07356640689484631,
                0.14118523681561343,
                0.0,
                0.049347098659916426,
                0.0610453238236123
            ],
            [
                0.0,
                0.0,
                1.0,
                0.39758152644076244,
                0.0,
                0.05923014338976621,
                0.2909596822379524,
                0.06729510311490569
            ],
            [
                0.0,
                0.07356640689484631,
                0.39758152644076244,
                1.0,
                0.1379913181979644,
                0.04388735063308135,
                0.2471727859356697,
                0.08893251063531543
            ],
            [
                0.0,
                0.14118523681561343,
                0.0,
                0.1379913181979644,
                1.0,
                0.05348625668600657,
                0.09256223704197121,
                0.03545501934274752
            ],
            [
                0.0,
                0.0,
                0.05923014338976621,
                0.04388735063308135,
                0.05348625668600657,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.049347098659916426,
                0.2909596822379524,
                0.2471727859356697,
                0.09256223704197121,
                0.0,
                1.0,
                0.05965442056546081
            ],
            [
                0.0,
                0.0610453238236123,
                0.06729510311490569,
                0.08893251063531543,
                0.03545501934274752,
                0.0,
                0.05965442056546081,
                1.0
            ]
        ]
    },
    "D10-1004": {
        "input_sentences": [
            "Abstract",
            "We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation.",
            "We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009).",
            "Experiments show state-of-the-art performance for 14 languages.",
            "The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs.",
            "By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method.",
            "Turbo Parsers: Dependency Parsing by Approximate Variational Inference",
            "Dependency Parsing."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation.": 0,
            "We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009).": 0,
            "Experiments show state-of-the-art performance for 14 languages.": 0,
            "The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs.": 0,
            "By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method.": 0,
            "Turbo Parsers: Dependency Parsing by Approximate Variational Inference": 0,
            "Dependency Parsing.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0479769525272371,
                0.06170329354045934,
                0.08741434685889668,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.06182975195968488,
                0.0,
                0.0,
                0.17548624043009434,
                0.0993181415487934
            ],
            [
                0.0,
                0.0,
                0.06182975195968488,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0479769525272371,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.06170329354045934,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.08741434685889668,
                0.17548624043009434,
                0.0,
                0.0,
                0.0,
                1.0,
                0.47936917005257945
            ],
            [
                0.0,
                0.0,
                0.0993181415487934,
                0.0,
                0.0,
                0.0,
                0.47936917005257945,
                1.0
            ]
        ]
    },
    "P12-3029": {
        "input_sentences": [
            "Abstract",
            "Syntactic Annotations for the Google Books NGram Corpus",
            "Syntactic Annotations",
            "The annotations are produced automatically with statistical models that are specifically adapted to historical text.",
            "We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published.",
            "This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodifier relationships are recorded.",
            "The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Syntactic Annotations for the Google Books NGram Corpus": 0,
            "Syntactic Annotations": 0,
            "The annotations are produced automatically with statistical models that are specifically adapted to historical text.": 0,
            "We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published.": 0,
            "This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodifier relationships are recorded.": 0,
            "The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.5056276579754371,
                0.0705427235592171,
                0.44260900789491836,
                0.15880975580234385,
                0.09290010954000999
            ],
            [
                0.0,
                0.5056276579754371,
                1.0,
                0.1395151599136691,
                0.0,
                0.3140843925315073,
                0.0
            ],
            [
                0.0,
                0.0705427235592171,
                0.1395151599136691,
                1.0,
                0.0,
                0.04381953425042085,
                0.0
            ],
            [
                0.0,
                0.44260900789491836,
                0.0,
                0.0,
                1.0,
                0.17435659896525577,
                0.04356400827207836
            ],
            [
                0.0,
                0.15880975580234385,
                0.3140843925315073,
                0.04381953425042085,
                0.17435659896525577,
                1.0,
                0.0
            ],
            [
                0.0,
                0.09290010954000999,
                0.0,
                0.0,
                0.04356400827207836,
                0.0,
                1.0
            ]
        ]
    },
    "P01-1005": {
        "input_sentences": [
            "Abstract",
            "In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.",
            "The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.",
            "Scaling To Very Very Large Corpora For Natural Language Disambiguation",
            "Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.",
            "We are fortunate that for this particular application, correctly labeled training data is free.",
            "Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.": 0,
            "The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.": 0,
            "Scaling To Very Very Large Corpora For Natural Language Disambiguation": 0,
            "Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.": 0,
            "We are fortunate that for this particular application, correctly labeled training data is free.": 0,
            "Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.27417964510203574,
                0.06619883766617657,
                0.08820780697614163,
                0.12895294113936098
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.06420135936680478,
                0.0,
                0.0
            ],
            [
                0.0,
                0.27417964510203574,
                0.0,
                1.0,
                0.22213486395330623,
                0.0,
                0.20283993350103185
            ],
            [
                0.0,
                0.06619883766617657,
                0.06420135936680478,
                0.22213486395330623,
                1.0,
                0.07722891258559583,
                0.048974342442728476
            ],
            [
                0.0,
                0.08820780697614163,
                0.0,
                0.0,
                0.07722891258559583,
                1.0,
                0.13051345000212472
            ],
            [
                0.0,
                0.12895294113936098,
                0.0,
                0.20283993350103185,
                0.048974342442728476,
                0.13051345000212472,
                1.0
            ]
        ]
    },
    "D12-1108": {
        "input_sentences": [
            "Abstract",
            "This setup gives us complete freedom to define scoring functions over the entire document.",
            "In this paper, we present a method for decoding complete documents in phrase-based SMT.",
            "Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time.",
            "Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems.",
            "The initial state is improved by the application of a series of operations using a hill climbing strategy to find a (local) maximum of the score function.",
            "any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required.",
            "Document-Wide Decoding for Phrase-Based Statistical Machine Translation",
            "Moreover, by optionally initialising the state with the output of a traditional DP decoder, we can ensure that the final hypothesis is no worse than what would have been found by DP search alone.",
            "We start by describing the decoding algorithm and the state operations used by our decoder, then we present empirical results demonstrating the effectiveness of our approach and its usability with a document-level semantic language model, and finally we discuss some related work.",
            "Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This setup gives us complete freedom to define scoring functions over the entire document.": 0,
            "In this paper, we present a method for decoding complete documents in phrase-based SMT.": 0,
            "Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time.": 0,
            "Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems.": 0,
            "The initial state is improved by the application of a series of operations using a hill climbing strategy to find a (local) maximum of the score function.": 0,
            "any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required.": 0,
            "Document-Wide Decoding for Phrase-Based Statistical Machine Translation": 0,
            "Moreover, by optionally initialising the state with the output of a traditional DP decoder, we can ensure that the final hypothesis is no worse than what would have been found by DP search alone.": 0,
            "We start by describing the decoding algorithm and the state operations used by our decoder, then we present empirical results demonstrating the effectiveness of our approach and its usability with a document-level semantic language model, and finally we discuss some related work.": 0,
            "Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07822693043136779,
                0.1915373979875412,
                0.0,
                0.0,
                0.10809945230517369,
                0.05595984018539598,
                0.0,
                0.030019826323731166,
                0.0
            ],
            [
                0.0,
                0.07822693043136779,
                1.0,
                0.0749979550771083,
                0.028709532234520854,
                0.0,
                0.0,
                0.2627012606866291,
                0.0,
                0.09305561477744734,
                0.050880301855556344
            ],
            [
                0.0,
                0.1915373979875412,
                0.0749979550771083,
                1.0,
                0.04073825580318462,
                0.1007418316939907,
                0.1036374279693397,
                0.160218325128874,
                0.15486952111172586,
                0.15657510791408627,
                0.022734281403781503
            ],
            [
                0.0,
                0.0,
                0.028709532234520854,
                0.04073825580318462,
                1.0,
                0.03293938111348858,
                0.0,
                0.031551593108568345,
                0.0,
                0.05054702404657115,
                0.054187262639238135
            ],
            [
                0.0,
                0.0,
                0.0,
                0.1007418316939907,
                0.03293938111348858,
                1.0,
                0.0,
                0.0,
                0.03460179608402873,
                0.07477710090092136,
                0.029759292854723635
            ],
            [
                0.0,
                0.10809945230517369,
                0.0,
                0.1036374279693397,
                0.0,
                0.0,
                1.0,
                0.04861950024379003,
                0.0,
                0.026082078655508472,
                0.0
            ],
            [
                0.0,
                0.05595984018539598,
                0.2627012606866291,
                0.160218325128874,
                0.031551593108568345,
                0.0,
                0.04861950024379003,
                1.0,
                0.0,
                0.0684896844287762,
                0.055917127742587926
            ],
            [
                0.0,
                0.0,
                0.0,
                0.15486952111172586,
                0.0,
                0.03460179608402873,
                0.0,
                0.0,
                1.0,
                0.054631258490876515,
                0.017585750118296037
            ],
            [
                0.0,
                0.030019826323731166,
                0.09305561477744734,
                0.15657510791408627,
                0.05054702404657115,
                0.07477710090092136,
                0.026082078655508472,
                0.0684896844287762,
                0.054631258490876515,
                1.0,
                0.0733240122733401
            ],
            [
                0.0,
                0.0,
                0.050880301855556344,
                0.022734281403781503,
                0.054187262639238135,
                0.029759292854723635,
                0.0,
                0.055917127742587926,
                0.017585750118296037,
                0.0733240122733401,
                1.0
            ]
        ]
    },
    "P08-1108": {
        "input_sentences": [
            "Abstract",
            "In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.",
            "Integrating Graph-Based and Transition-Based Dependency Parsers",
            "By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.",
            "Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.": 0,
            "Integrating Graph-Based and Transition-Based Dependency Parsers": 0,
            "By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.": 0,
            "Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.5737493065249883,
                0.1474842203425601,
                0.10037449631626544
            ],
            [
                0.0,
                0.5737493065249883,
                1.0,
                0.0,
                0.06420805446928936
            ],
            [
                0.0,
                0.1474842203425601,
                0.0,
                1.0,
                0.0770228660473026
            ],
            [
                0.0,
                0.10037449631626544,
                0.06420805446928936,
                0.0770228660473026,
                1.0
            ]
        ]
    },
    "P12-1002": {
        "input_sentences": [
            "Abstract",
            "With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data.",
            "Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT",
            "Evidence from machine learning indicates that increasing the training sample size results in better prediction.",
            "We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learnalgorithm that applies regularization for joint feature selection over distributed stochastic learning processes.",
            "Joint Feature Selection in Distributed",
            "The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT.",
            "We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets."
        ],
        "authority_scores": {
            "Abstract": 0,
            "With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data.": 0,
            "Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT": 0,
            "Evidence from machine learning indicates that increasing the training sample size results in better prediction.": 0,
            "We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learnalgorithm that applies regularization for joint feature selection over distributed stochastic learning processes.": 0,
            "Joint Feature Selection in Distributed": 0,
            "The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT.": 0,
            "We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.28475841948144387,
                0.10364889306734455,
                0.05784498516458603,
                0.08286517669969072,
                0.045979522110779145,
                0.3282319825873785
            ],
            [
                0.0,
                0.28475841948144387,
                1.0,
                0.10555174473565836,
                0.3499002739010843,
                0.5682100808558768,
                0.06431107008922929,
                0.163144928797828
            ],
            [
                0.0,
                0.10364889306734455,
                0.10555174473565836,
                1.0,
                0.03241829305784823,
                0.0,
                0.0,
                0.07921622026249882
            ],
            [
                0.0,
                0.05784498516458603,
                0.3499002739010843,
                0.03241829305784823,
                1.0,
                0.3490307235705317,
                0.03950394419794238,
                0.08339811445297106
            ],
            [
                0.0,
                0.08286517669969072,
                0.5682100808558768,
                0.0,
                0.3490307235705317,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.045979522110779145,
                0.06431107008922929,
                0.0,
                0.03950394419794238,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.3282319825873785,
                0.163144928797828,
                0.07921622026249882,
                0.08339811445297106,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "N03-1004": {
        "input_sentences": [
            "Abstract",
            "The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques.",
            "We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels.",
            "In Question Answering Two Heads Are Better Than One",
            "Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric.",
            "Motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multistrategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques.": 0,
            "We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels.": 0,
            "In Question Answering Two Heads Are Better Than One": 0,
            "Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric.": 0,
            "Motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multistrategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06967436021150795,
                0.05994396322152722,
                0.0,
                0.1477134047558649
            ],
            [
                0.0,
                0.06967436021150795,
                1.0,
                0.14448998865541002,
                0.17159657145966087,
                0.17933520755468638
            ],
            [
                0.0,
                0.05994396322152722,
                0.14448998865541002,
                1.0,
                0.0,
                0.14337040984827396
            ],
            [
                0.0,
                0.0,
                0.17159657145966087,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.1477134047558649,
                0.17933520755468638,
                0.14337040984827396,
                0.0,
                1.0
            ]
        ]
    },
    "P12-1110": {
        "input_sentences": [
            "Abstract",
            "Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models.",
            "We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese.",
            "We also perform comparison experiments with the partially joint models.",
            "Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese",
            "We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework.",
            "In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models.": 0,
            "We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese.": 0,
            "We also perform comparison experiments with the partially joint models.": 0,
            "Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese": 0,
            "We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework.": 0,
            "In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.4713068031027406,
                0.08192054908030298,
                0.38861248215739824,
                0.16484859775437616,
                0.2009158371971305
            ],
            [
                0.0,
                0.4713068031027406,
                1.0,
                0.07959738028994993,
                0.713853515039751,
                0.0,
                0.25940425183925747
            ],
            [
                0.0,
                0.08192054908030298,
                0.07959738028994993,
                1.0,
                0.07861249131360082,
                0.0,
                0.15957780855687148
            ],
            [
                0.0,
                0.38861248215739824,
                0.713853515039751,
                0.07861249131360082,
                1.0,
                0.07208193990263688,
                0.25619454333975755
            ],
            [
                0.0,
                0.16484859775437616,
                0.0,
                0.0,
                0.07208193990263688,
                1.0,
                0.0
            ],
            [
                0.0,
                0.2009158371971305,
                0.25940425183925747,
                0.15957780855687148,
                0.25619454333975755,
                0.0,
                1.0
            ]
        ]
    },
    "D10-1069": {
        "input_sentences": [
            "Abstract",
            "We show that dependency parsers have more difficulty parsing questions than constituency parsers.",
            "It is well known that parsing accuracies drop significantly on out-of-domain data.",
            "In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set.",
            "Uptraining for Accurate Deterministic Question Parsing",
            "With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.",
            "Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training.",
            "We propose an in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies).",
            "What is less known is that some parsers suffer more from domain shifts than others."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We show that dependency parsers have more difficulty parsing questions than constituency parsers.": 0,
            "It is well known that parsing accuracies drop significantly on out-of-domain data.": 0,
            "In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set.": 0,
            "Uptraining for Accurate Deterministic Question Parsing": 0,
            "With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.": 0,
            "Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training.": 0,
            "We propose an in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies).": 0,
            "What is less known is that some parsers suffer more from domain shifts than others.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07703672993080614,
                0.18751862651045043,
                0.10452252834828252,
                0.1128994041125147,
                0.14220799333631212,
                0.081069988809532,
                0.23523647988396765
            ],
            [
                0.0,
                0.07703672993080614,
                1.0,
                0.07573162177362662,
                0.10603652525928609,
                0.17885594357363918,
                0.0,
                0.0,
                0.27715932799702686
            ],
            [
                0.0,
                0.18751862651045043,
                0.07573162177362662,
                1.0,
                0.18043006358932037,
                0.08544255131593806,
                0.041252511235869996,
                0.03555714047215734,
                0.06823874876393454
            ],
            [
                0.0,
                0.10452252834828252,
                0.10603652525928609,
                0.18043006358932037,
                1.0,
                0.15539938580840199,
                0.09787046705527558,
                0.19594642272910429,
                0.0
            ],
            [
                0.0,
                0.1128994041125147,
                0.17885594357363918,
                0.08544255131593806,
                0.15539938580840199,
                1.0,
                0.36930555596816056,
                0.0,
                0.153330173092272
            ],
            [
                0.0,
                0.14220799333631212,
                0.0,
                0.041252511235869996,
                0.09787046705527558,
                0.36930555596816056,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.081069988809532,
                0.0,
                0.03555714047215734,
                0.19594642272910429,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.23523647988396765,
                0.27715932799702686,
                0.06823874876393454,
                0.0,
                0.153330173092272,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P11-2084": {
        "input_sentences": [
            "Abstract",
            "A topic model outputs a set of multinomial distributions over words for each topic.",
            "In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources.",
            "Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space.",
            "Identifying Word Translations from Comparable Corpora Using Latent Topic Models",
            "The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported."
        ],
        "authority_scores": {
            "Abstract": 0,
            "A topic model outputs a set of multinomial distributions over words for each topic.": 0,
            "In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources.": 0,
            "Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space.": 0,
            "Identifying Word Translations from Comparable Corpora Using Latent Topic Models": 0,
            "The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.12645668377664235,
                0.09080648405217641,
                0.11165659277290771,
                0.05965387012604221
            ],
            [
                0.0,
                0.12645668377664235,
                1.0,
                0.016279278290296052,
                0.41934776010745384,
                0.0
            ],
            [
                0.0,
                0.09080648405217641,
                0.016279278290296052,
                1.0,
                0.135495862538359,
                0.2406880352044895
            ],
            [
                0.0,
                0.11165659277290771,
                0.41934776010745384,
                0.135495862538359,
                1.0,
                0.0
            ],
            [
                0.0,
                0.05965387012604221,
                0.0,
                0.2406880352044895,
                0.0,
                1.0
            ]
        ]
    },
    "N09-2064": {
        "input_sentences": [
            "Abstract",
            "First, we propose a method of parse hybridization that recomproductions of conthereby preserving the structure of the output of the individual parsers to a greater extent.",
            "We propose three ways to improve upon existing methods for parser combination.",
            "Combining Constituent Parsers",
            "the output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (Henderson and Brill, 1999; Sagae and Lavie, 2006).",
            "Second, we propose an efficient lineartime algorithm for computing expected f-score using Minimum Bayes Risk parse selection.",
            "We present results on WSJ section 23 and also on the English side of a Chinese-English parallel corpus.",
            "Third, we extend these parser combination from multiple outputs to muloutputs."
        ],
        "authority_scores": {
            "Abstract": 0,
            "First, we propose a method of parse hybridization that recomproductions of conthereby preserving the structure of the output of the individual parsers to a greater extent.": 0,
            "We propose three ways to improve upon existing methods for parser combination.": 0,
            "Combining Constituent Parsers": 0,
            "the output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (Henderson and Brill, 1999; Sagae and Lavie, 2006).": 0,
            "Second, we propose an efficient lineartime algorithm for computing expected f-score using Minimum Bayes Risk parse selection.": 0,
            "We present results on WSJ section 23 and also on the English side of a Chinese-English parallel corpus.": 0,
            "Third, we extend these parser combination from multiple outputs to muloutputs.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.06676172584383447,
                0.100772350706149,
                0.2871434783593357,
                0.09072618419485903,
                0.0,
                0.0
            ],
            [
                0.0,
                0.06676172584383447,
                1.0,
                0.0,
                0.05567345686149226,
                0.06182111235853505,
                0.0,
                0.2302363325336128
            ],
            [
                0.0,
                0.100772350706149,
                0.0,
                1.0,
                0.08403535182708399,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.2871434783593357,
                0.05567345686149226,
                0.08403535182708399,
                1.0,
                0.17726253592461563,
                0.0,
                0.14088356302044502
            ],
            [
                0.0,
                0.09072618419485903,
                0.06182111235853505,
                0.0,
                0.17726253592461563,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.2302363325336128,
                0.0,
                0.14088356302044502,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "W03-1022": {
        "input_sentences": [
            "Abstract",
            "We used a fixed set 26 semantic labels, which we called su- These are the labels used by lexicographers developing WordNet.",
            "This framework has a number of practical advantages.",
            "We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns.",
            "We present a new framework for classifying common nouns that extends namedentity classification.",
            "We also define a more realistic evaluation procedure than cross-validation.",
            "Supersense Tagging Of Unknown Nouns In WordNet"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We used a fixed set 26 semantic labels, which we called su- These are the labels used by lexicographers developing WordNet.": 0,
            "This framework has a number of practical advantages.": 0,
            "We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns.": 0,
            "We present a new framework for classifying common nouns that extends namedentity classification.": 0,
            "We also define a more realistic evaluation procedure than cross-validation.": 0,
            "Supersense Tagging Of Unknown Nouns In WordNet": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.10630026831943039,
                0.0,
                0.0,
                0.08562761556760151
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.12778629794308544,
                0.0,
                0.0
            ],
            [
                0.0,
                0.10630026831943039,
                0.0,
                1.0,
                0.12876555366634165,
                0.0,
                0.07453519673902247
            ],
            [
                0.0,
                0.0,
                0.12778629794308544,
                0.12876555366634165,
                1.0,
                0.0,
                0.08757924767229369
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.08562761556760151,
                0.0,
                0.07453519673902247,
                0.08757924767229369,
                0.0,
                1.0
            ]
        ]
    },
    "D11-1022": {
        "input_sentences": [
            "Abstract",
            "However, in cases where lightweight decomare not readily available due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient.",
            "We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes.",
            "We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.",
            "Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power.",
            "Dual Decomposition with Many Overlapping Components"
        ],
        "authority_scores": {
            "Abstract": 0,
            "However, in cases where lightweight decomare not readily available due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient.": 0,
            "We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes.": 0,
            "We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.": 0,
            "Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power.": 0,
            "Dual Decomposition with Many Overlapping Components": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.09397985590619753,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.09397985590619753,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.2286244614433553
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.2286244614433553,
                1.0
            ]
        ]
    },
    "D07-1092": {
        "input_sentences": [
            "Abstract",
            "An easy way out commercial translation systems usually offer their users is the possibility to add unknown wordsand their translations into a dedicated lex icon.",
            "In this study we show that ana logical learning offers as well an elegant andeffective solution to the problem of identify ing potential translations of unknown words.",
            "In particular, they drastically impact machine transla tion quality.",
            "Recently, Stroppa and Yvon (2005) have shown how analogical learning alone deals nicely with morphology in differentlanguages.",
            "Translating Unknown Words by Analogical Learning",
            "Unknown words are a well-known hindranceto natural language applications."
        ],
        "authority_scores": {
            "Abstract": 0,
            "An easy way out commercial translation systems usually offer their users is the possibility to add unknown wordsand their translations into a dedicated lex icon.": 0,
            "In this study we show that ana logical learning offers as well an elegant andeffective solution to the problem of identify ing potential translations of unknown words.": 0,
            "In particular, they drastically impact machine transla tion quality.": 0,
            "Recently, Stroppa and Yvon (2005) have shown how analogical learning alone deals nicely with morphology in differentlanguages.": 0,
            "Translating Unknown Words by Analogical Learning": 0,
            "Unknown words are a well-known hindranceto natural language applications.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07612390105321662,
                0.0,
                0.0,
                0.05574469000438112,
                0.04030484466348048
            ],
            [
                0.0,
                0.07612390105321662,
                1.0,
                0.0,
                0.043608813612345323,
                0.2186221507627906,
                0.10066864315790312
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.043608813612345323,
                0.0,
                1.0,
                0.2129901855019968,
                0.0
            ],
            [
                0.0,
                0.05574469000438112,
                0.2186221507627906,
                0.0,
                0.2129901855019968,
                1.0,
                0.20757333367366154
            ],
            [
                0.0,
                0.04030484466348048,
                0.10066864315790312,
                0.0,
                0.0,
                0.20757333367366154,
                1.0
            ]
        ]
    },
    "W06-3114": {
        "input_sentences": [
            " Evaluation was done automatically using the BLEU score and manually on fluency and adequacy",
            "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back"
        ],
        "authority_scores": {
            " Evaluation was done automatically using the BLEU score and manually on fluency and adequacy": 0,
            "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0
            ],
            [
                0.0,
                1.0
            ]
        ]
    },
    "D09-1161": {
        "input_sentences": [
            "Abstract",
            "As a result, it is able to fully utilize both the logarithm of the probability of each k-best parse tree from each individual parser and any additional useful features.",
            "For feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm.",
            "In this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers.",
            "Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively.",
            "The proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model.",
            "Our experiments are carried out on both the Chinese and English Penn Treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model.",
            "K-Best Combination of Syntactic Parsers"
        ],
        "authority_scores": {
            "Abstract": 0,
            "As a result, it is able to fully utilize both the logarithm of the probability of each k-best parse tree from each individual parser and any additional useful features.": 0,
            "For feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm.": 0,
            "In this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers.": 0,
            "Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively.": 0,
            "The proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model.": 0,
            "Our experiments are carried out on both the Chinese and English Penn Treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model.": 0,
            "K-Best Combination of Syntactic Parsers": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.09443731540768514,
                0.027935574270317457,
                0.0,
                0.0,
                0.07011542360467947
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.09443731540768514,
                0.0,
                1.0,
                0.03130257991692563,
                0.18563629198413467,
                0.10886951279464328,
                0.21581748242594956
            ],
            [
                0.0,
                0.027935574270317457,
                0.0,
                0.03130257991692563,
                1.0,
                0.0,
                0.07107738255780964,
                0.06384113401695045
            ],
            [
                0.0,
                0.0,
                0.0,
                0.18563629198413467,
                0.0,
                1.0,
                0.15337456676110306,
                0.137943090475529
            ],
            [
                0.0,
                0.0,
                0.0,
                0.10886951279464328,
                0.07107738255780964,
                0.15337456676110306,
                1.0,
                0.08919846677445883
            ],
            [
                0.0,
                0.07011542360467947,
                0.0,
                0.21581748242594956,
                0.06384113401695045,
                0.137943090475529,
                0.08919846677445883,
                1.0
            ]
        ]
    },
    "W12-3401": {
        "input_sentences": [
            "Abstract",
            "We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.",
            "This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis.",
            "We use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets.",
            "A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking.",
            "Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features.",
            "The standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class.",
            "Probabilistic Lexical Generalization for French Dependency Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.": 0,
            "This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis.": 0,
            "We use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets.": 0,
            "A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking.": 0,
            "Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features.": 0,
            "The standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class.": 0,
            "Probabilistic Lexical Generalization for French Dependency Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.1529041610319658,
                0.05847378571868957,
                0.0,
                0.11564072190110067,
                0.21102351400982255,
                0.33776268755276084
            ],
            [
                0.0,
                0.1529041610319658,
                1.0,
                0.027551959470116648,
                0.0,
                0.0676682553107678,
                0.07518980893318497,
                0.45496215691221087
            ],
            [
                0.0,
                0.05847378571868957,
                0.027551959470116648,
                1.0,
                0.0,
                0.03496231996318873,
                0.14286834363125483,
                0.11700916333218744
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.11564072190110067,
                0.0676682553107678,
                0.03496231996318873,
                0.0,
                1.0,
                0.09651174109939835,
                0.20681308013441216
            ],
            [
                0.0,
                0.21102351400982255,
                0.07518980893318497,
                0.14286834363125483,
                0.0,
                0.09651174109939835,
                1.0,
                0.14028191663997502
            ],
            [
                0.0,
                0.33776268755276084,
                0.45496215691221087,
                0.11700916333218744,
                0.0,
                0.20681308013441216,
                0.14028191663997502,
                1.0
            ]
        ]
    },
    "W11-1310": {
        "input_sentences": [
            "Abstract",
            "Most models represent each word as a single prototype-based vector without addressing polysemy.",
            "We have participated in the shared task (Biemann and Giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations.",
            "Exemplar-Based Word-Space Model for Compositionality Detection: Shared Task System Description",
            "In this paper, we highlight the problems of polysemy in word space models of compositionality detection.",
            "We propose an exemplar-based model which is designed to handle polysemy.",
            "This model is tested for compositionality detection and it is found to outperform existing prototype-based models."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Most models represent each word as a single prototype-based vector without addressing polysemy.": 0,
            "We have participated in the shared task (Biemann and Giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations.": 0,
            "Exemplar-Based Word-Space Model for Compositionality Detection: Shared Task System Description": 0,
            "In this paper, we highlight the problems of polysemy in word space models of compositionality detection.": 0,
            "We propose an exemplar-based model which is designed to handle polysemy.": 0,
            "This model is tested for compositionality detection and it is found to outperform existing prototype-based models.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.14244395413538447,
                0.23636084851403488,
                0.15767928124582833,
                0.25107584531250565
            ],
            [
                0.0,
                0.0,
                1.0,
                0.23950191783717964,
                0.0,
                0.1035278478154242,
                0.039795697721812656
            ],
            [
                0.0,
                0.14244395413538447,
                0.23950191783717964,
                1.0,
                0.36532528554236937,
                0.23929062456439046,
                0.29935522118777713
            ],
            [
                0.0,
                0.23636084851403488,
                0.0,
                0.36532528554236937,
                1.0,
                0.09256765269503545,
                0.24836383725981231
            ],
            [
                0.0,
                0.15767928124582833,
                0.1035278478154242,
                0.23929062456439046,
                0.09256765269503545,
                1.0,
                0.14242575501716828
            ],
            [
                0.0,
                0.25107584531250565,
                0.039795697721812656,
                0.29935522118777713,
                0.24836383725981231,
                0.14242575501716828,
                1.0
            ]
        ]
    },
    "A00-2030": {
        "input_sentences": [
            "Abstract",
            "In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
            "A Novel Use of Statistical Parsing to Extract Information from Text",
            "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.": 0,
            "A Novel Use of Statistical Parsing to Extract Information from Text": 0,
            "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.05519964129862864,
                0.0
            ],
            [
                0.0,
                0.05519964129862864,
                1.0,
                0.21268347212012056
            ],
            [
                0.0,
                0.0,
                0.21268347212012056,
                1.0
            ]
        ]
    },
    "W12-3134": {
        "input_sentences": [
            "Abstract",
            "Joshua 4.0: Packing, PRO, and Paraphrases",
            "The main con",
            "We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Joshua 4.0: Packing, PRO, and Paraphrases": 0,
            "The main con": 0,
            "We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.09581273984464105
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.09581273984464105,
                0.0,
                1.0
            ]
        ]
    },
    "W04-1506": {
        "input_sentences": [
            "Abstract",
            "This scheme was used both to the manual tagging of the corpus and to develop the parser.",
            "Previous to the completion of the grammar for the dependency parsing, the design of the Dependency Structure-based Scheme had to be accomplished; we concentrated on issues that must be resolved by any practical system that uses such models.",
            "The manually tagged corpus has been used to evaluate the accuracy of the parser.",
            "Towards A Dependency Parser For Basque",
            "The Dependency syntactic model is applied to establish the dependency-based grammatical relations between the components within the clause.",
            "Such a deep analysis is used to improve the output of the shallow parsing where syntactic structure ambiguity is not fully and explicitly resolved.",
            "We present the Dependency Parser, for the linguistic processing of Basque, which can serve as a representative of agglutinative languages that are also characterized by the free order of its constituents.",
            "We have evaluated the application of the grammar to corpus, measuring the linking of the verb with its dependents, with satisfactory results."
        ],
        "authority_scores": {
            "Abstract": 0,
            "This scheme was used both to the manual tagging of the corpus and to develop the parser.": 0,
            "Previous to the completion of the grammar for the dependency parsing, the design of the Dependency Structure-based Scheme had to be accomplished; we concentrated on issues that must be resolved by any practical system that uses such models.": 0,
            "The manually tagged corpus has been used to evaluate the accuracy of the parser.": 0,
            "Towards A Dependency Parser For Basque": 0,
            "The Dependency syntactic model is applied to establish the dependency-based grammatical relations between the components within the clause.": 0,
            "Such a deep analysis is used to improve the output of the shallow parsing where syntactic structure ambiguity is not fully and explicitly resolved.": 0,
            "We present the Dependency Parser, for the linguistic processing of Basque, which can serve as a representative of agglutinative languages that are also characterized by the free order of its constituents.": 0,
            "We have evaluated the application of the grammar to corpus, measuring the linking of the verb with its dependents, with satisfactory results.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.08076941908715798,
                0.2800714983916237,
                0.14785326322207504,
                0.0,
                0.06997941984624487,
                0.05203995729482315,
                0.07765188176513191
            ],
            [
                0.0,
                0.08076941908715798,
                1.0,
                0.0,
                0.17453230791250895,
                0.19490563731238153,
                0.16390650070343799,
                0.06143018863703928,
                0.06062567271503728
            ],
            [
                0.0,
                0.2800714983916237,
                0.0,
                1.0,
                0.1439488657651009,
                0.0,
                0.06813145610886143,
                0.050665725353673724,
                0.07560130944034218
            ],
            [
                0.0,
                0.14785326322207504,
                0.17453230791250895,
                0.1439488657651009,
                1.0,
                0.2123299746149681,
                0.0,
                0.3519702992057696,
                0.0
            ],
            [
                0.0,
                0.0,
                0.19490563731238153,
                0.0,
                0.2123299746149681,
                1.0,
                0.06646766158434347,
                0.07473384469558378,
                0.0
            ],
            [
                0.0,
                0.06997941984624487,
                0.16390650070343799,
                0.06813145610886143,
                0.0,
                0.06646766158434347,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.05203995729482315,
                0.06143018863703928,
                0.050665725353673724,
                0.3519702992057696,
                0.07473384469558378,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.07765188176513191,
                0.06062567271503728,
                0.07560130944034218,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P04-1006": {
        "input_sentences": [
            "Abstract",
            "We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling.",
            "The parser\u2019s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited.",
            "Attention Shifting For Parsing Speech",
            "Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice.",
            "This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling.": 0,
            "The parser\u2019s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited.": 0,
            "Attention Shifting For Parsing Speech": 0,
            "Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice.": 0,
            "This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.1667820723808189,
                0.24856154671058772,
                0.052451732967176284,
                0.16620583337924102
            ],
            [
                0.0,
                0.1667820723808189,
                1.0,
                0.11956851162506964,
                0.060629692183762206,
                0.1360358069787404
            ],
            [
                0.0,
                0.24856154671058772,
                0.11956851162506964,
                1.0,
                0.0,
                0.23673571205965177
            ],
            [
                0.0,
                0.052451732967176284,
                0.060629692183762206,
                0.0,
                1.0,
                0.0705518201881785
            ],
            [
                0.0,
                0.16620583337924102,
                0.1360358069787404,
                0.23673571205965177,
                0.0705518201881785,
                1.0
            ]
        ]
    },
    "P10-1155": {
        "input_sentences": [
            "Abstract",
            "We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain.",
            "Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.",
            "All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision",
            "In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal.",
            "However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy.",
            "Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation.",
            "We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus.",
            "Accuracy figures close to self domain training lend credence to the viability of our approach.",
            "Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora.",
            "Many supervised WSD systems have been built, but the effort of creatthe training corpus sense corpora has always been a matter of concern.",
            "Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain.": 0,
            "Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.": 0,
            "All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision": 0,
            "In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal.": 0,
            "However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy.": 0,
            "Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation.": 0,
            "We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus.": 0,
            "Accuracy figures close to self domain training lend credence to the viability of our approach.": 0,
            "Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora.": 0,
            "Many supervised WSD systems have been built, but the effort of creatthe training corpus sense corpora has always been a matter of concern.": 0,
            "Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.20403717898389143,
                0.14504778654429226,
                0.053400312515239357,
                0.06482859309149283,
                0.06518266607397166,
                0.3268671179029049,
                0.12807962900907519,
                0.1618662687898673,
                0.0978627986892229,
                0.0
            ],
            [
                0.0,
                0.20403717898389143,
                1.0,
                0.14048847388665534,
                0.09829780620103273,
                0.0,
                0.047485558414859894,
                0.10925030967815741,
                0.08223287197614777,
                0.03035966978223983,
                0.0293961470890394,
                0.027096527476643813
            ],
            [
                0.0,
                0.14504778654429226,
                0.14048847388665534,
                1.0,
                0.1374731167388025,
                0.0,
                0.0,
                0.09501268680501822,
                0.05189285791696318,
                0.042459120802652274,
                0.041111598687947774,
                0.30171018549577344
            ],
            [
                0.0,
                0.053400312515239357,
                0.09829780620103273,
                0.1374731167388025,
                1.0,
                0.03868003063301379,
                0.07778257710835895,
                0.0,
                0.0,
                0.10002627957855832,
                0.09685175261625478,
                0.055805920451512316
            ],
            [
                0.0,
                0.06482859309149283,
                0.0,
                0.0,
                0.03868003063301379,
                1.0,
                0.0,
                0.0,
                0.08974028504522838,
                0.04552523991579468,
                0.044080408595596234,
                0.0
            ],
            [
                0.0,
                0.06518266607397166,
                0.047485558414859894,
                0.0,
                0.07778257710835895,
                0.0,
                1.0,
                0.05368444560621506,
                0.058641417306959445,
                0.0,
                0.06916927035065581,
                0.06375825474403
            ],
            [
                0.0,
                0.3268671179029049,
                0.10925030967815741,
                0.09501268680501822,
                0.0,
                0.0,
                0.05368444560621506,
                1.0,
                0.1349166612884673,
                0.05007142579923426,
                0.12560239102484771,
                0.0
            ],
            [
                0.0,
                0.12807962900907519,
                0.08223287197614777,
                0.05189285791696318,
                0.0,
                0.08974028504522838,
                0.058641417306959445,
                0.1349166612884673,
                1.0,
                0.0,
                0.08424098743137524,
                0.0
            ],
            [
                0.0,
                0.1618662687898673,
                0.03035966978223983,
                0.042459120802652274,
                0.10002627957855832,
                0.04552523991579468,
                0.0,
                0.05007142579923426,
                0.0,
                1.0,
                0.13033900878380153,
                0.1090498763343432
            ],
            [
                0.0,
                0.0978627986892229,
                0.0293961470890394,
                0.041111598687947774,
                0.09685175261625478,
                0.044080408595596234,
                0.06916927035065581,
                0.12560239102484771,
                0.08424098743137524,
                0.13033900878380153,
                1.0,
                0.089378628086321
            ],
            [
                0.0,
                0.0,
                0.027096527476643813,
                0.30171018549577344,
                0.055805920451512316,
                0.0,
                0.06375825474403,
                0.0,
                0.0,
                0.1090498763343432,
                0.089378628086321,
                1.0
            ]
        ]
    },
    "N12-1007": {
        "input_sentences": [
            "Abstract",
            "On an Arabic-English corpus that contains seven different text genres, our best model yields a 24.3% F1 gain over the baseline.",
            "When co-referent text mentions appear in different languages, these techniques cannot be easily applied.",
            "Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures.",
            "Crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown.",
            "Consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters.",
            "Standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like English WordNet.",
            "Entity Clustering Across Languages"
        ],
        "authority_scores": {
            "Abstract": 0,
            "On an Arabic-English corpus that contains seven different text genres, our best model yields a 24.3% F1 gain over the baseline.": 0,
            "When co-referent text mentions appear in different languages, these techniques cannot be easily applied.": 0,
            "Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures.": 0,
            "Crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown.": 0,
            "Consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters.": 0,
            "Standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like English WordNet.": 0,
            "Entity Clustering Across Languages": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.12029029721638025,
                0.0,
                0.0,
                0.04052766516095265,
                0.05045978588648968,
                0.0
            ],
            [
                0.0,
                0.12029029721638025,
                1.0,
                0.0,
                0.0,
                0.1852345935021228,
                0.0,
                0.16632280815500608
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.17215896679745854,
                0.1596108733100837,
                0.1149917206030925
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.07139725761794005,
                0.06619334939289764,
                0.21430958025990043
            ],
            [
                0.0,
                0.04052766516095265,
                0.1852345935021228,
                0.17215896679745854,
                0.07139725761794005,
                1.0,
                0.06234765327247069,
                0.3331500977760966
            ],
            [
                0.0,
                0.05045978588648968,
                0.0,
                0.1596108733100837,
                0.06619334939289764,
                0.06234765327247069,
                1.0,
                0.18714583513156666
            ],
            [
                0.0,
                0.0,
                0.16632280815500608,
                0.1149917206030925,
                0.21430958025990043,
                0.3331500977760966,
                0.18714583513156666,
                1.0
            ]
        ]
    },
    "D07-1026": {
        "input_sentences": [
            "Abstract",
            "In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text.",
            "Instance Based Lexical Entailment",
            "We demonstrate the effectiveness of our technique largelysurpassing both the random and most fre quent baselines and outperforming current state-of-the-art unsupervised approaches ona benchmark ontology available in the liter ature.",
            "The approach is fully unsupervised and based on kernel methods.",
            "Instance Based Lexical Entailment for Ontology Population"
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text.": 0,
            "Instance Based Lexical Entailment": 0,
            "We demonstrate the effectiveness of our technique largelysurpassing both the random and most fre quent baselines and outperforming current state-of-the-art unsupervised approaches ona benchmark ontology available in the liter ature.": 0,
            "The approach is fully unsupervised and based on kernel methods.": 0,
            "Instance Based Lexical Entailment for Ontology Population": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.44740520227188313,
                0.03662634384627384,
                0.052509860270876604,
                0.5735648087725097
            ],
            [
                0.0,
                0.44740520227188313,
                1.0,
                0.0,
                0.11736533237485014,
                0.7800429793267447
            ],
            [
                0.0,
                0.03662634384627384,
                0.0,
                1.0,
                0.06854846473934505,
                0.06385737633495707
            ],
            [
                0.0,
                0.052509860270876604,
                0.11736533237485014,
                0.06854846473934505,
                1.0,
                0.09155000353535173
            ],
            [
                0.0,
                0.5735648087725097,
                0.7800429793267447,
                0.06385737633495707,
                0.09155000353535173,
                1.0
            ]
        ]
    },
    "P05-1061": {
        "input_sentences": [
            "Abstract",
            "complex relation is any relation in which some of the arguments may be be unspecified.",
            "The first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances.",
            "We present here a simple two-stage method for extracting complex relations between named entities in text.",
            "Simple Algorithms For Complex Relation Extraction With Applications To Biomedical IE",
            "We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text."
        ],
        "authority_scores": {
            "Abstract": 0,
            "complex relation is any relation in which some of the arguments may be be unspecified.": 0,
            "The first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances.": 0,
            "We present here a simple two-stage method for extracting complex relations between named entities in text.": 0,
            "Simple Algorithms For Complex Relation Extraction With Applications To Biomedical IE": 0,
            "We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.14870531685831168,
                0.06411348975645113,
                0.27879292819665347,
                0.0
            ],
            [
                0.0,
                0.14870531685831168,
                1.0,
                0.20906479552219825,
                0.08565944892116031,
                0.0
            ],
            [
                0.0,
                0.06411348975645113,
                0.20906479552219825,
                1.0,
                0.16946911700792627,
                0.33086182423769284
            ],
            [
                0.0,
                0.27879292819665347,
                0.08565944892116031,
                0.16946911700792627,
                1.0,
                0.09659496656536723
            ],
            [
                0.0,
                0.0,
                0.0,
                0.33086182423769284,
                0.09659496656536723,
                1.0
            ]
        ]
    },
    "P11-2067": {
        "input_sentences": [
            "Abstract",
            "There are a number of systems that use a syntax-based reordering step prior to phrasebased statistical MT.",
            "We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT?",
            "An early work proposing this idea showed improved translation performance, but subsequent work has had mixed results.",
            "Clause Restructuring For SMT Not Absolutely Helpful",
            "Speculations as to cause have suggested the parser, the data, or other factors."
        ],
        "authority_scores": {
            "Abstract": 0,
            "There are a number of systems that use a syntax-based reordering step prior to phrasebased statistical MT.": 0,
            "We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT?": 0,
            "An early work proposing this idea showed improved translation performance, but subsequent work has had mixed results.": 0,
            "Clause Restructuring For SMT Not Absolutely Helpful": 0,
            "Speculations as to cause have suggested the parser, the data, or other factors.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.12061682783202671,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.12061682783202671,
                1.0,
                0.0,
                0.0,
                0.08144334807210837
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.08144334807210837,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "N10-1035": {
        "input_sentences": [
            "Abstract",
            "Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems",
            "We present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems.",
            "Linear Context-Free Rewriting Systems",
            "Our result is obtained through a linear space construction of a binary normal form for the grammar at hand.",
            "The use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems": 0,
            "We present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems.": 0,
            "Linear Context-Free Rewriting Systems": 0,
            "Our result is obtained through a linear space construction of a binary normal form for the grammar at hand.": 0,
            "The use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.1802383229236981,
                0.6797836732901541,
                0.05512248833033561,
                0.3899263656413529
            ],
            [
                0.0,
                0.1802383229236981,
                1.0,
                0.13560344654673268,
                0.0,
                0.04948867524055049
            ],
            [
                0.0,
                0.6797836732901541,
                0.13560344654673268,
                1.0,
                0.08108827925146052,
                0.45178353923139836
            ],
            [
                0.0,
                0.05512248833033561,
                0.0,
                0.08108827925146052,
                1.0,
                0.029593285568207332
            ],
            [
                0.0,
                0.3899263656413529,
                0.04948867524055049,
                0.45178353923139836,
                0.029593285568207332,
                1.0
            ]
        ]
    },
    "D07-1122": {
        "input_sentences": [
            "Abstract",
            "Then we present evaluation results and error analyses focusing on Chinese.",
            "The parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem.",
            "We describe the features used ineach stage.",
            "For four languages with different values of ROOT, we design some spe cial features for the ROOT labeler.",
            "A Two-Stage Parser for Multilingual Dependency Parsing",
            "We present a two-stage multilingual de pendency parsing system submitted to the Multilingual Track of CoNLL-2007."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Then we present evaluation results and error analyses focusing on Chinese.": 0,
            "The parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem.": 0,
            "We describe the features used ineach stage.": 0,
            "For four languages with different values of ROOT, we design some spe cial features for the ROOT labeler.": 0,
            "A Two-Stage Parser for Multilingual Dependency Parsing": 0,
            "We present a two-stage multilingual de pendency parsing system submitted to the Multilingual Track of CoNLL-2007.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.08665635549866692
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.17844733462538206,
                0.045083185753217195
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.112795770076881,
                0.15314452174471074,
                0.09164609671121099
            ],
            [
                0.0,
                0.0,
                0.0,
                0.112795770076881,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.17844733462538206,
                0.15314452174471074,
                0.0,
                1.0,
                0.42163781359175756
            ],
            [
                0.0,
                0.08665635549866692,
                0.045083185753217195,
                0.09164609671121099,
                0.0,
                0.42163781359175756,
                1.0
            ]
        ]
    },
    "P14-1078": {
        "input_sentences": [
            "Abstract",
            "Our approach integrates domain specific parsing and typing systems, and can utilize labeled as well as unlabeled examples.",
            "In this paper, we present a manifold model for medical relation extraction.",
            "Relation Extraction with Manifold Models",
            "Our model is built upon a medical corpus containing 80M sentences (11 gigabyte text) and designed to accurately and efficiently detect the key medical relations that can facilitate clinical decision making.",
            "To provide users with more flexibility, we also take label weight into consideration.",
            "Effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments.",
            "Medical Relation Extraction with Manifold Models"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Our approach integrates domain specific parsing and typing systems, and can utilize labeled as well as unlabeled examples.": 0,
            "In this paper, we present a manifold model for medical relation extraction.": 0,
            "Relation Extraction with Manifold Models": 0,
            "Our model is built upon a medical corpus containing 80M sentences (11 gigabyte text) and designed to accurately and efficiently detect the key medical relations that can facilitate clinical decision making.": 0,
            "To provide users with more flexibility, we also take label weight into consideration.": 0,
            "Effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments.": 0,
            "Medical Relation Extraction with Manifold Models": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.4846129411178613,
                0.16086053220160193,
                0.0,
                0.06388398970868361,
                0.582553259980236
            ],
            [
                0.0,
                0.0,
                0.4846129411178613,
                1.0,
                0.0,
                0.0,
                0.0,
                0.9015750672636621
            ],
            [
                0.0,
                0.0,
                0.16086053220160193,
                0.0,
                1.0,
                0.0,
                0.030226480888060292,
                0.13781649408064703
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.06388398970868361,
                0.0,
                0.030226480888060292,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.582553259980236,
                0.9015750672636621,
                0.13781649408064703,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "C10-1135": {
        "input_sentences": [
            "Abstract",
            "As tokenization is usually ambiguous for many natural languages such as Chineseand Korean, tokenization errors might po tentially introduce translation mistakes fortranslation systems that rely on 1-best tokenizations.",
            "While using lattices to offer more alternatives to translation systems have elegantly alleviated this prob lem, we take a further step to tokenize and translate jointly.",
            "Interestingly, as a tokenizer, our joint de coder achieves significant improvements over monolingual Chinese tokenizers.",
            "By integrat ing tokenization and translation features in a discriminative framework, our jointdecoder outperforms the baseline trans lation systems using 1-best tokenizationsand lattices significantly on both ChineseEnglish and Korean-Chinese tasks.",
            "Taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on thetarget side simultaneously.",
            "Joint Tokenization and Translation"
        ],
        "authority_scores": {
            "Abstract": 0,
            "As tokenization is usually ambiguous for many natural languages such as Chineseand Korean, tokenization errors might po tentially introduce translation mistakes fortranslation systems that rely on 1-best tokenizations.": 0,
            "While using lattices to offer more alternatives to translation systems have elegantly alleviated this prob lem, we take a further step to tokenize and translate jointly.": 0,
            "Interestingly, as a tokenizer, our joint de coder achieves significant improvements over monolingual Chinese tokenizers.": 0,
            "By integrat ing tokenization and translation features in a discriminative framework, our jointdecoder outperforms the baseline trans lation systems using 1-best tokenizationsand lattices significantly on both ChineseEnglish and Korean-Chinese tasks.": 0,
            "Taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on thetarget side simultaneously.": 0,
            "Joint Tokenization and Translation": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.05574790285309895,
                0.0,
                0.1663014165171296,
                0.06391502530143615,
                0.23722522559572537
            ],
            [
                0.0,
                0.05574790285309895,
                1.0,
                0.0,
                0.14431951133332674,
                0.02075227549774136,
                0.0770235670463815
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.05266814465095064,
                0.04128717113877712,
                0.1532403130783967
            ],
            [
                0.0,
                0.1663014165171296,
                0.14431951133332674,
                0.05266814465095064,
                1.0,
                0.038646829246552925,
                0.14344049373891105
            ],
            [
                0.0,
                0.06391502530143615,
                0.02075227549774136,
                0.04128717113877712,
                0.038646829246552925,
                1.0,
                0.2694276088933261
            ],
            [
                0.0,
                0.23722522559572537,
                0.0770235670463815,
                0.1532403130783967,
                0.14344049373891105,
                0.2694276088933261,
                1.0
            ]
        ]
    },
    "D07-1030": {
        "input_sentences": [
            "Abstract",
            "The interpolated model achieves an abso lute improvement of 0.0245 BLEU score (13.1% relative) as compared with the in dividual model trained on the real bilingual corpus.",
            "Using RBMT Systems to Produce Bilingual Corpus for SMT",
            "With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus.",
            "We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora.",
            "We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus.",
            "In our experi ments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora.",
            "This paper proposes a method using the ex isting Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Ma chine Translation (SMT) system."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The interpolated model achieves an abso lute improvement of 0.0245 BLEU score (13.1% relative) as compared with the in dividual model trained on the real bilingual corpus.": 0,
            "Using RBMT Systems to Produce Bilingual Corpus for SMT": 0,
            "With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus.": 0,
            "We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora.": 0,
            "We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus.": 0,
            "In our experi ments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora.": 0,
            "This paper proposes a method using the ex isting Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Ma chine Translation (SMT) system.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.062079790067758406,
                0.17821371169492073,
                0.3576808029038223,
                0.07345877166237355,
                0.23013415814701874,
                0.02423705167464986
            ],
            [
                0.0,
                0.062079790067758406,
                1.0,
                0.37685661536350357,
                0.12316760494894012,
                0.24472821928760635,
                0.266446534779969,
                0.27626221983011795
            ],
            [
                0.0,
                0.17821371169492073,
                0.37685661536350357,
                1.0,
                0.3678681907870886,
                0.345328104725285,
                0.1046839129335441,
                0.1797506077706563
            ],
            [
                0.0,
                0.3576808029038223,
                0.12316760494894012,
                0.3678681907870886,
                1.0,
                0.17524589575995328,
                0.14882191892398924,
                0.07145588475044208
            ],
            [
                0.0,
                0.07345877166237355,
                0.24472821928760635,
                0.345328104725285,
                0.17524589575995328,
                1.0,
                0.11269863862022628,
                0.1228909724164206
            ],
            [
                0.0,
                0.23013415814701874,
                0.266446534779969,
                0.1046839129335441,
                0.14882191892398924,
                0.11269863862022628,
                1.0,
                0.16827200351357138
            ],
            [
                0.0,
                0.02423705167464986,
                0.27626221983011795,
                0.1797506077706563,
                0.07145588475044208,
                0.1228909724164206,
                0.16827200351357138,
                1.0
            ]
        ]
    },
    "P11-1061": {
        "input_sentences": [
            "Abstract",
            "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.",
            "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).",
            "Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.",
            "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections",
            "Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.": 0,
            "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).": 0,
            "Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.": 0,
            "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections": 0,
            "Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.029767875056616735,
                0.13169363614263954,
                0.14243875773907877,
                0.07399922069329141
            ],
            [
                0.0,
                0.029767875056616735,
                1.0,
                0.03813501822591,
                0.17722115273305802,
                0.0
            ],
            [
                0.0,
                0.13169363614263954,
                0.03813501822591,
                1.0,
                0.07593843807864022,
                0.028120434072375446
            ],
            [
                0.0,
                0.14243875773907877,
                0.17722115273305802,
                0.07593843807864022,
                1.0,
                0.0
            ],
            [
                0.0,
                0.07399922069329141,
                0.0,
                0.028120434072375446,
                0.0,
                1.0
            ]
        ]
    },
    "W06-2932": {
        "input_sentences": [
            "Abstract",
            "We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.",
            "Multilingual Dependency Analysis with a Two-Stage Discriminative Parser",
            "present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.",
            "The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.",
            "The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.": 0,
            "Multilingual Dependency Analysis with a Two-Stage Discriminative Parser": 0,
            "present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.": 0,
            "The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.": 0,
            "The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09500267527973784,
                0.07023371956586016,
                0.0,
                0.05134397122608517
            ],
            [
                0.0,
                0.09500267527973784,
                1.0,
                0.32594717748762453,
                0.0976442128844634,
                0.11673273974410865
            ],
            [
                0.0,
                0.07023371956586016,
                0.32594717748762453,
                1.0,
                0.03056413098443932,
                0.10634825651019743
            ],
            [
                0.0,
                0.0,
                0.0976442128844634,
                0.03056413098443932,
                1.0,
                0.0527715839787818
            ],
            [
                0.0,
                0.05134397122608517,
                0.11673273974410865,
                0.10634825651019743,
                0.0527715839787818,
                1.0
            ]
        ]
    },
    "D12-1129": {
        "input_sentences": [
            "Abstract",
            "We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD).",
            "Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique.",
            "The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD.",
            "Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.",
            "A New Minimally-Supervised Framework for Domain Word Sense Disambiguation"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD).": 0,
            "Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique.": 0,
            "The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD.": 0,
            "Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.": 0,
            "A New Minimally-Supervised Framework for Domain Word Sense Disambiguation": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.2257733282778034,
                0.08426432362980506,
                0.7459445501008777
            ],
            [
                0.0,
                0.0,
                1.0,
                0.09882405606522016,
                0.0627563784199212,
                0.0
            ],
            [
                0.0,
                0.2257733282778034,
                0.09882405606522016,
                1.0,
                0.0,
                0.17788609254768248
            ],
            [
                0.0,
                0.08426432362980506,
                0.0627563784199212,
                0.0,
                1.0,
                0.11296325392873985
            ],
            [
                0.0,
                0.7459445501008777,
                0.0,
                0.17788609254768248,
                0.11296325392873985,
                1.0
            ]
        ]
    },
    "D11-1039": {
        "input_sentences": [
            "Abstract",
            "Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation.",
            "In this paper, we present an approach for using conversational interactions of this type to induce semantic parsers.",
            "Conversations provide rich opportunities for interactive, continuous learning.",
            "This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system.",
            "Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations.",
            "Bootstrapping Semantic Parsers from Conversations",
            "When something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals.",
            "We demonstrate learning without any explicit annotation of the meanings of user utterances."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation.": 0,
            "In this paper, we present an approach for using conversational interactions of this type to induce semantic parsers.": 0,
            "Conversations provide rich opportunities for interactive, continuous learning.": 0,
            "This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system.": 0,
            "Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations.": 0,
            "Bootstrapping Semantic Parsers from Conversations": 0,
            "When something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals.": 0,
            "We demonstrate learning without any explicit annotation of the meanings of user utterances.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.05219501452842094,
                0.06097585719412181,
                0.0,
                0.0,
                0.08682102967552979
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.10839636743737886,
                0.0721147399967092,
                0.2468899149563858,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.043334317605004016,
                0.05062451243209536,
                0.16724133387954587,
                0.0,
                0.07208217314901301
            ],
            [
                0.0,
                0.05219501452842094,
                0.10839636743737886,
                0.043334317605004016,
                1.0,
                0.031964553538249174,
                0.07982919461584698,
                0.0,
                0.04551302071038251
            ],
            [
                0.0,
                0.06097585719412181,
                0.0721147399967092,
                0.05062451243209536,
                0.031964553538249174,
                1.0,
                0.0,
                0.0,
                0.23335430744010727
            ],
            [
                0.0,
                0.0,
                0.2468899149563858,
                0.16724133387954587,
                0.07982919461584698,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.08682102967552979,
                0.0,
                0.07208217314901301,
                0.04551302071038251,
                0.23335430744010727,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "P11-1060": {
        "input_sentences": [
            "Abstract",
            "In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.",
            "On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.",
            "Learning Dependency-Based Compositional Semantics",
            "In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.",
            "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.": 0,
            "On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.": 0,
            "Learning Dependency-Based Compositional Semantics": 0,
            "In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.": 0,
            "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.05808337155681907,
                0.18113882241046905,
                0.05716975546285532,
                0.12715772395210728
            ],
            [
                0.0,
                0.05808337155681907,
                1.0,
                0.0,
                0.06472779823155181,
                0.09743181104032199
            ],
            [
                0.0,
                0.18113882241046905,
                0.0,
                1.0,
                0.0,
                0.07596271981172245
            ],
            [
                0.0,
                0.05716975546285532,
                0.06472779823155181,
                0.0,
                1.0,
                0.1875085053728042
            ],
            [
                0.0,
                0.12715772395210728,
                0.09743181104032199,
                0.07596271981172245,
                0.1875085053728042,
                1.0
            ]
        ]
    },
    "W07-1712": {
        "input_sentences": [
            "Named Entity Recognition for Ukrainian: A Resource-Light Approach",
            "Abstract",
            "The approach we follow uses a restricted number of features.",
            "We show that it is feasible to boost performance by considering several heuristics and patterns acquired from the Web data.",
            "Kruislaan 419, 1098VA the katrenko@science.uva.nl Pieter HCSL, University of Kruislaan 419, 1098VA the pitera@science.uva.nl Abstract Named entity recognition (NER) is a subtask of information extraction (IE) which can be used further on for different purposes.",
            "In this paper, we discuss named entity recognition for Ukrainian language, which is a Slavonic language with a rich morphology."
        ],
        "authority_scores": {
            "Named Entity Recognition for Ukrainian: A Resource-Light Approach": 0,
            "Abstract": 0,
            "The approach we follow uses a restricted number of features.": 0,
            "We show that it is feasible to boost performance by considering several heuristics and patterns acquired from the Web data.": 0,
            "Kruislaan 419, 1098VA the katrenko@science.uva.nl Pieter HCSL, University of Kruislaan 419, 1098VA the pitera@science.uva.nl Abstract Named entity recognition (NER) is a subtask of information extraction (IE) which can be used further on for different purposes.": 0,
            "In this paper, we discuss named entity recognition for Ukrainian language, which is a Slavonic language with a rich morphology.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.12909823740658932,
                0.0,
                0.1065040256857209,
                0.2894979496097446
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.13283122300642156,
                0.0
            ],
            [
                0.12909823740658932,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.1065040256857209,
                0.13283122300642156,
                0.0,
                0.0,
                1.0,
                0.06987810201070326
            ],
            [
                0.2894979496097446,
                0.0,
                0.0,
                0.0,
                0.06987810201070326,
                1.0
            ]
        ]
    },
    "D10-1025": {
        "input_sentences": [
            "Translingual Document Representations from Discriminative Projections",
            "Abstract",
            "We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters.",
            "Both of these variants start with a basic model of documents (PCA and PLSA).",
            "The OPCA method is shown to perform best.",
            "The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines.",
            "Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization.",
            "We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA).",
            "Each model is then made discriminative by encouraging comparable document pairs to have similar vector representations.",
            "The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel.",
            "We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space."
        ],
        "authority_scores": {
            "Translingual Document Representations from Discriminative Projections": 0,
            "Abstract": 0,
            "We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters.": 0,
            "Both of these variants start with a basic model of documents (PCA and PLSA).": 0,
            "The OPCA method is shown to perform best.": 0,
            "The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines.": 0,
            "Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization.": 0,
            "We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA).": 0,
            "Each model is then made discriminative by encouraging comparable document pairs to have similar vector representations.": 0,
            "The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel.": 0,
            "We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.09060215665712985,
                0.0,
                0.0,
                0.10028690881973341,
                0.0,
                0.10420791482068457,
                0.3702668234152687,
                0.0,
                0.2085528481298521
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.09060215665712985,
                0.0,
                1.0,
                0.04438679226211262,
                0.0,
                0.0,
                0.09926870018873031,
                0.0,
                0.061606682894261106,
                0.1910467552660308,
                0.033385554740046675
            ],
            [
                0.0,
                0.0,
                0.04438679226211262,
                1.0,
                0.0,
                0.0945103202226874,
                0.048636597809403534,
                0.060660686897515366,
                0.11654801310615488,
                0.056213235916515204,
                0.04884859070110405
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.10526845390267081,
                0.0,
                0.06756570824571763,
                0.0,
                0.0,
                0.0
            ],
            [
                0.10028690881973341,
                0.0,
                0.0,
                0.0945103202226874,
                0.10526845390267081,
                1.0,
                0.0,
                0.18921267671878295,
                0.068192016813507,
                0.0,
                0.05677266826327667
            ],
            [
                0.0,
                0.0,
                0.09926870018873031,
                0.048636597809403534,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.04209733192635549,
                0.036582048753306146
            ],
            [
                0.10420791482068457,
                0.0,
                0.0,
                0.060660686897515366,
                0.06756570824571763,
                0.18921267671878295,
                0.0,
                1.0,
                0.0,
                0.0,
                0.058992359502842685
            ],
            [
                0.3702668234152687,
                0.0,
                0.061606682894261106,
                0.11654801310615488,
                0.0,
                0.068192016813507,
                0.0,
                0.0,
                1.0,
                0.10087795228428627,
                0.14180952921521559
            ],
            [
                0.0,
                0.0,
                0.1910467552660308,
                0.056213235916515204,
                0.0,
                0.0,
                0.04209733192635549,
                0.0,
                0.10087795228428627,
                1.0,
                0.04228082204552289
            ],
            [
                0.2085528481298521,
                0.0,
                0.033385554740046675,
                0.04884859070110405,
                0.0,
                0.05677266826327667,
                0.036582048753306146,
                0.058992359502842685,
                0.14180952921521559,
                0.04228082204552289,
                1.0
            ]
        ]
    },
    "D07-1066": {
        "input_sentences": [
            "Abstract",
            "Wealso provide extensive past-parsing crosstreebank conversion.",
            "(2006), the question whether or not Ger man is harder to parse than English remains undecided.",
            "Recent studies focussed on the question whether less-configurational languages like German are harder to parse than English, or whether the lower parsing scores are an artefact of treebank encoding schemes and data structures, as claimed by Ku?bler et al(2006).",
            "The results of the ex periments show that, contrary to Ku?bler etal.",
            "Treebank Annotation Schemes and Parser Evaluation for German",
            "We use thePARSEVAL metric, the Leaf-Ancestor metric as well as a dependency-based evaluation, and present novel approaches measur ing the effect of controlled error insertion on treebank trees and parser output.",
            "This claim is based on the assumption that PARSEVAL metrics fully re flect parse quality across treebank encodingschemes.",
            "In this paper we present new ex periments to test this claim."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Wealso provide extensive past-parsing crosstreebank conversion.": 0,
            "(2006), the question whether or not Ger man is harder to parse than English remains undecided.": 0,
            "Recent studies focussed on the question whether less-configurational languages like German are harder to parse than English, or whether the lower parsing scores are an artefact of treebank encoding schemes and data structures, as claimed by Ku?bler et al(2006).": 0,
            "The results of the ex periments show that, contrary to Ku?bler etal.": 0,
            "Treebank Annotation Schemes and Parser Evaluation for German": 0,
            "We use thePARSEVAL metric, the Leaf-Ancestor metric as well as a dependency-based evaluation, and present novel approaches measur ing the effect of controlled error insertion on treebank trees and parser output.": 0,
            "This claim is based on the assumption that PARSEVAL metrics fully re flect parse quality across treebank encodingschemes.": 0,
            "In this paper we present new ex periments to test this claim.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.058198427407869645,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.2637650298807632,
                0.0,
                0.0,
                0.0,
                0.06473802515255109,
                0.0
            ],
            [
                0.0,
                0.058198427407869645,
                0.2637650298807632,
                1.0,
                0.12465329568975456,
                0.1889149960733471,
                0.018856216174169486,
                0.06625371965465406,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.12465329568975456,
                1.0,
                0.0,
                0.0,
                0.0,
                0.24374324412260565
            ],
            [
                0.0,
                0.0,
                0.0,
                0.1889149960733471,
                0.0,
                1.0,
                0.18936472146247565,
                0.066464093158657,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                0.018856216174169486,
                0.0,
                0.18936472146247565,
                1.0,
                0.0784500094968681,
                0.062475020798521644
            ],
            [
                0.0,
                0.0,
                0.06473802515255109,
                0.06625371965465406,
                0.0,
                0.066464093158657,
                0.0784500094968681,
                1.0,
                0.09623780061216919
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.24374324412260565,
                0.0,
                0.062475020798521644,
                0.09623780061216919,
                1.0
            ]
        ]
    },
    "W06-3119": {
        "input_sentences": [
            "Syntax Augmented Machine Translation Via Chart Parsing",
            "Abstract",
            "We present translation results on the shared task \u201dExploiting Parallel Texts for Statistical Machine Translation\u201d generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories.",
            "Our translation system is available open-source under the GNU General",
            "We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence.",
            "Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar.",
            "We present results on the French-to-English task for this workshop, representing significant improvements over the workshop\u2019s baseline system."
        ],
        "authority_scores": {
            "Syntax Augmented Machine Translation Via Chart Parsing": 0,
            "Abstract": 0,
            "We present translation results on the shared task \u201dExploiting Parallel Texts for Statistical Machine Translation\u201d generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories.": 0,
            "Our translation system is available open-source under the GNU General": 0,
            "We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence.": 0,
            "Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar.": 0,
            "We present results on the French-to-English task for this workshop, representing significant improvements over the workshop\u2019s baseline system.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.4067168198606376,
                0.1070457964411118,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.4067168198606376,
                0.0,
                1.0,
                0.09856495701393257,
                0.12697448880933024,
                0.10210544908686836,
                0.13274058189231178
            ],
            [
                0.1070457964411118,
                0.0,
                0.09856495701393257,
                1.0,
                0.06695902729809794,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.12697448880933024,
                0.06695902729809794,
                1.0,
                0.15992627412049565,
                0.0
            ],
            [
                0.0,
                0.0,
                0.10210544908686836,
                0.0,
                0.15992627412049565,
                1.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.13274058189231178,
                0.0,
                0.0,
                0.0,
                1.0
            ]
        ]
    },
    "D07-1119": {
        "input_sentences": [
            "Abstract",
            "For the domain adaptation track we applied a tree revision method which learns how to correct the mistakes made by the base parser on the adaptation domain.",
            "For the multi lingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language.",
            "Multilingual Dependency Parsing and Domain Adaptation using DeSR",
            "We describe our experiments using the DeSR parser in the multilingual and do main adaptation tracks of the CoNLL 2007 shared task.",
            "DeSR implements an incre mental deterministic Shift/Reduce parsing algorithm, using specific rules to handle non-projective dependencies."
        ],
        "authority_scores": {
            "Abstract": 0,
            "For the domain adaptation track we applied a tree revision method which learns how to correct the mistakes made by the base parser on the adaptation domain.": 0,
            "For the multi lingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language.": 0,
            "Multilingual Dependency Parsing and Domain Adaptation using DeSR": 0,
            "We describe our experiments using the DeSR parser in the multilingual and do main adaptation tracks of the CoNLL 2007 shared task.": 0,
            "DeSR implements an incre mental deterministic Shift/Reduce parsing algorithm, using specific rules to handle non-projective dependencies.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.044088925103243484,
                0.2921670661719771,
                0.13960924640846675,
                0.0
            ],
            [
                0.0,
                0.044088925103243484,
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.2921670661719771,
                0.0,
                1.0,
                0.31965760336780896,
                0.20201771013882103
            ],
            [
                0.0,
                0.13960924640846675,
                0.0,
                0.31965760336780896,
                1.0,
                0.08012477440813684
            ],
            [
                0.0,
                0.0,
                0.0,
                0.20201771013882103,
                0.08012477440813684,
                1.0
            ]
        ]
    },
    "C02-1154": {
        "input_sentences": [
            "Abstract",
            "We also investigate the relative merits of several evaluation strategies.",
            "Examples of these are names of diseases and infectious agents, such as bacteria and viruses.",
            "Nomen uses a novel form of bootstrap ping to grow sets of textual instances and of their contextual patterns.",
            "Unsupervised Learning Of Generalized Names",
            "These names exhibitcertain properties that make their identi ca tion more complex than that of regular propernames.",
            "We present an algorithm, Nomen, for learning generalized names in text.",
            "The algorithm makes use of competing evidence to boost the learning of several categories of names simultaneously.",
            "We present results of the algorithm on a large corpus."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We also investigate the relative merits of several evaluation strategies.": 0,
            "Examples of these are names of diseases and infectious agents, such as bacteria and viruses.": 0,
            "Nomen uses a novel form of bootstrap ping to grow sets of textual instances and of their contextual patterns.": 0,
            "Unsupervised Learning Of Generalized Names": 0,
            "These names exhibitcertain properties that make their identi ca tion more complex than that of regular propernames.": 0,
            "We present an algorithm, Nomen, for learning generalized names in text.": 0,
            "The algorithm makes use of competing evidence to boost the learning of several categories of names simultaneously.": 0,
            "We present results of the algorithm on a large corpus.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                1.0,
                0.0,
                0.08279044279147635,
                0.043590455220157406,
                0.06241079796383836,
                0.04591527238641204,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0976750065781454,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.08279044279147635,
                0.0,
                1.0,
                0.06820226820658229,
                0.4625460326198179,
                0.18741332523381707,
                0.0
            ],
            [
                0.0,
                0.0,
                0.043590455220157406,
                0.0,
                0.06820226820658229,
                1.0,
                0.05141363952403883,
                0.0378247248896072,
                0.0
            ],
            [
                0.0,
                0.0,
                0.06241079796383836,
                0.0976750065781454,
                0.4625460326198179,
                0.05141363952403883,
                1.0,
                0.22840385922939163,
                0.2846511994536956
            ],
            [
                0.0,
                0.0,
                0.04591527238641204,
                0.0,
                0.18741332523381707,
                0.0378247248896072,
                0.22840385922939163,
                1.0,
                0.09015736405362032
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.2846511994536956,
                0.09015736405362032,
                1.0
            ]
        ]
    },
    "W11-0131": {
        "input_sentences": [
            "Abstract",
            "As such, syntax and semantics are fully interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse.",
            "Evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.",
            "Distributed models of semantics assume that word meanings can be discovered from \u201cthe company they keep.\u201d Many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a docu- In contrast, this paper proposes a semantic framework, in which semantic vectors are defined and composed in syntactic context.",
            "Structured Composition of Semantic Vectors"
        ],
        "authority_scores": {
            "Abstract": 0,
            "As such, syntax and semantics are fully interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse.": 0,
            "Evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.": 0,
            "Distributed models of semantics assume that word meanings can be discovered from \u201cthe company they keep.\u201d Many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a docu- In contrast, this paper proposes a semantic framework, in which semantic vectors are defined and composed in syntactic context.": 0,
            "Structured Composition of Semantic Vectors": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.07178879245877602,
                0.20646509079773384,
                0.30100717674366195
            ],
            [
                0.0,
                0.07178879245877602,
                1.0,
                0.08829086321297627,
                0.0579399888553989
            ],
            [
                0.0,
                0.20646509079773384,
                0.08829086321297627,
                1.0,
                0.12501026304708251
            ],
            [
                0.0,
                0.30100717674366195,
                0.0579399888553989,
                0.12501026304708251,
                1.0
            ]
        ]
    },
    "A97-1014": {
        "input_sentences": [
            "Abstract",
            "An Annotation Scheme for Free Word Order Languages",
            "The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- \u2022lar representational strata.",
            "Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme.",
            "We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages."
        ],
        "authority_scores": {
            "Abstract": 0,
            "An Annotation Scheme for Free Word Order Languages": 0,
            "The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- \u2022lar representational strata.": 0,
            "Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme.": 0,
            "We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.041384884321766735,
                0.11876052256333934,
                0.22387863664261495
            ],
            [
                0.0,
                0.041384884321766735,
                1.0,
                0.02834292804578929,
                0.02888333946430978
            ],
            [
                0.0,
                0.11876052256333934,
                0.02834292804578929,
                1.0,
                0.1533259351413394
            ],
            [
                0.0,
                0.22387863664261495,
                0.02888333946430978,
                0.1533259351413394,
                1.0
            ]
        ]
    },
    "W10-1403": {
        "input_sentences": [
            "Abstract",
            "We then investigate the best way to incorporate this information during dependency parsing.",
            "In this paper we explore two strategies to incorporate local morphosyntactic features in Hindi dependency parsing.",
            "This paper is also the first attempt at complete sentence level parsing for Hindi.",
            "All the experiments were done with two data-driven parsers, MaltParser and MSTParser, on a part of multi-layered and multi-representational Hindi Treebank which is under development.",
            "We first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in Hindi dependency parsing.",
            "Further, we compare the results of various experiments based on various criterions and do some error analysis.",
            "These features are obtained using a shallow parser.",
            "Two Methods to Incorporate &rsquo;Local Morphosyntactic&rsquo; Features in Hindi Dependency Parsing"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We then investigate the best way to incorporate this information during dependency parsing.": 0,
            "In this paper we explore two strategies to incorporate local morphosyntactic features in Hindi dependency parsing.": 0,
            "This paper is also the first attempt at complete sentence level parsing for Hindi.": 0,
            "All the experiments were done with two data-driven parsers, MaltParser and MSTParser, on a part of multi-layered and multi-representational Hindi Treebank which is under development.": 0,
            "We first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in Hindi dependency parsing.": 0,
            "Further, we compare the results of various experiments based on various criterions and do some error analysis.": 0,
            "These features are obtained using a shallow parser.": 0,
            "Two Methods to Incorporate &rsquo;Local Morphosyntactic&rsquo; Features in Hindi Dependency Parsing": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.24555712935614807,
                0.06455318738222236,
                0.0,
                0.2182776060511059,
                0.0,
                0.0,
                0.20301229130393805
            ],
            [
                0.0,
                0.24555712935614807,
                1.0,
                0.2529941150676302,
                0.0379396527227308,
                0.23642929717533284,
                0.0,
                0.09104714588958873,
                0.465709796506648
            ],
            [
                0.0,
                0.06455318738222236,
                0.2529941150676302,
                1.0,
                0.038545423416096354,
                0.04873566515340281,
                0.0,
                0.0,
                0.10133611381097317
            ],
            [
                0.0,
                0.0,
                0.0379396527227308,
                0.038545423416096354,
                1.0,
                0.015085016480587571,
                0.05814854226158433,
                0.0,
                0.031366288776516156
            ],
            [
                0.0,
                0.2182776060511059,
                0.23642929717533284,
                0.04873566515340281,
                0.015085016480587571,
                1.0,
                0.0,
                0.15888029721960767,
                0.15326801746335666
            ],
            [
                0.0,
                0.0,
                0.0,
                0.0,
                0.05814854226158433,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.0,
                0.09104714588958873,
                0.0,
                0.0,
                0.15888029721960767,
                0.0,
                1.0,
                0.07527246206287576
            ],
            [
                0.0,
                0.20301229130393805,
                0.465709796506648,
                0.10133611381097317,
                0.031366288776516156,
                0.15326801746335666,
                0.0,
                0.07527246206287576,
                1.0
            ]
        ]
    },
    "W06-2207": {
        "input_sentences": [
            "Abstract",
            "Furthermore, using a modular architecture we investigate several algorithms for pattern ranking, the most important component of the decision list learner.",
            "We show that the combination of the two methods always outperforms the decision list learner alone.",
            "A Hybrid Approach For The Acquisition Of Information Extraction Patterns",
            "In this paper we present a hybrid approach for the acquisition of syntacticosemantic patterns from raw text.",
            "Our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an Expectation Maximization clustering algorithm that uses the text words as attributes."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Furthermore, using a modular architecture we investigate several algorithms for pattern ranking, the most important component of the decision list learner.": 0,
            "We show that the combination of the two methods always outperforms the decision list learner alone.": 0,
            "A Hybrid Approach For The Acquisition Of Information Extraction Patterns": 0,
            "In this paper we present a hybrid approach for the acquisition of syntacticosemantic patterns from raw text.": 0,
            "Our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an Expectation Maximization clustering algorithm that uses the text words as attributes.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.20182005259661406,
                0.0,
                0.0,
                0.10290813642323277
            ],
            [
                0.0,
                0.20182005259661406,
                1.0,
                0.0,
                0.0,
                0.16520927315758088
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.4204070902332055,
                0.11184679172329436
            ],
            [
                0.0,
                0.0,
                0.0,
                0.4204070902332055,
                1.0,
                0.14947085335172045
            ],
            [
                0.0,
                0.10290813642323277,
                0.16520927315758088,
                0.11184679172329436,
                0.14947085335172045,
                1.0
            ]
        ]
    },
    "W09-0402": {
        "input_sentences": [
            "Abstract",
            "We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the the on the detailed tags as well as the precision, recall and F-measure obtained We also introduced Fbased on both word and grams.",
            "Syntax-Oriented Evaluation Measures for Machine Translation Output"
        ],
        "authority_scores": {
            "Abstract": 0,
            "We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the the on the detailed tags as well as the precision, recall and F-measure obtained We also introduced Fbased on both word and grams.": 0,
            "Syntax-Oriented Evaluation Measures for Machine Translation Output": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.38191611176970136
            ],
            [
                0.0,
                0.38191611176970136,
                1.0
            ]
        ]
    },
    "C04-1180": {
        "input_sentences": [
            "Abstract",
            "This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.",
            "We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.",
            "Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.",
            "Wide-Coverage Semantic Representations From A CCG Parser"
        ],
        "authority_scores": {
            "Abstract": 0,
            "This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.": 0,
            "We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.": 0,
            "Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.": 0,
            "Wide-Coverage Semantic Representations From A CCG Parser": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.16474399971835538,
                0.0986475525687801,
                0.5965458025766242
            ],
            [
                0.0,
                0.16474399971835538,
                1.0,
                0.052796769408046736,
                0.2761632032390249
            ],
            [
                0.0,
                0.0986475525687801,
                0.052796769408046736,
                1.0,
                0.16536459085404295
            ],
            [
                0.0,
                0.5965458025766242,
                0.2761632032390249,
                0.16536459085404295,
                1.0
            ]
        ]
    },
    "D12-1133": {
        "input_sentences": [
            "Abstract",
            "Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.",
            "A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing",
            "We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees.",
            "Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.": 0,
            "A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing": 0,
            "We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees.": 0,
            "Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.13741352649104216,
                0.13741352649104216,
                0.021879298147270645
            ],
            [
                0.0,
                0.13741352649104216,
                1.0,
                0.4256812239687888,
                0.06699485318218065
            ],
            [
                0.0,
                0.13741352649104216,
                0.4256812239687888,
                1.0,
                0.06699485318218065
            ],
            [
                0.0,
                0.021879298147270645,
                0.06699485318218065,
                0.06699485318218065,
                1.0
            ]
        ]
    },
    "P14-1126": {
        "input_sentences": [
            "Abstract",
            "Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization",
            "We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization.",
            "Our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages.",
            "We perform experiments on three Data sets \u2014 Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages.",
            "We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems.",
            "We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resourcerich language."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization": 0,
            "We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization.": 0,
            "Our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages.": 0,
            "We perform experiments on three Data sets \u2014 Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages.": 0,
            "We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems.": 0,
            "We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resourcerich language.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.2696270710081269,
                0.037922311959405625,
                0.03641518043396105,
                0.11909545045072263,
                0.09897449708224025
            ],
            [
                0.0,
                0.2696270710081269,
                1.0,
                0.16550058152222671,
                0.024649700453862968,
                0.04030828824742507,
                0.0810808779084621
            ],
            [
                0.0,
                0.037922311959405625,
                0.16550058152222671,
                1.0,
                0.07045360772938293,
                0.028947567962028978,
                0.08230290353824651
            ],
            [
                0.0,
                0.03641518043396105,
                0.024649700453862968,
                0.07045360772938293,
                1.0,
                0.07826987106732412,
                0.0790319716211499
            ],
            [
                0.0,
                0.11909545045072263,
                0.04030828824742507,
                0.028947567962028978,
                0.07826987106732412,
                1.0,
                0.07555106302228452
            ],
            [
                0.0,
                0.09897449708224025,
                0.0810808779084621,
                0.08230290353824651,
                0.0790319716211499,
                0.07555106302228452,
                1.0
            ]
        ]
    },
    "P12-3012": {
        "input_sentences": [
            "Abstract",
            "In this paper we present an API for programmatic access to BabelNet \u2013 a wide-coverage multilingual lexical knowledge base \u2013 and multilingual knowledge-rich Word Sense Disambiguation (WSD).",
            "Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction.",
            "Multilingual WSD with Just a Few Lines of Code: the BabelNet API"
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this paper we present an API for programmatic access to BabelNet \u2013 a wide-coverage multilingual lexical knowledge base \u2013 and multilingual knowledge-rich Word Sense Disambiguation (WSD).": 0,
            "Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction.": 0,
            "Multilingual WSD with Just a Few Lines of Code: the BabelNet API": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.08205924893266757,
                0.26691664308354
            ],
            [
                0.0,
                0.08205924893266757,
                1.0,
                0.04431832591036525
            ],
            [
                0.0,
                0.26691664308354,
                0.04431832591036525,
                1.0
            ]
        ]
    },
    "W11-2205": {
        "input_sentences": [
            "Abstract",
            "The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research.",
            "Finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing.",
            "However, this advantage makes them difficult to evaluate against a manually labeled gold standard.",
            "Using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods.",
            "The primary advantage of these methods is that they do not require annotated data to learn a model.",
            "Evaluating unsupervised learning for natural language processing tasks",
            "Inwe argue that the rarely used evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research.": 0,
            "Finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing.": 0,
            "However, this advantage makes them difficult to evaluate against a manually labeled gold standard.": 0,
            "Using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods.": 0,
            "The primary advantage of these methods is that they do not require annotated data to learn a model.": 0,
            "Evaluating unsupervised learning for natural language processing tasks": 0,
            "Inwe argue that the rarely used evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.20750098405686662,
                0.0,
                0.15419701440686306,
                0.05144133324060721,
                0.5321279871639477,
                0.03970512208597641
            ],
            [
                0.0,
                0.20750098405686662,
                1.0,
                0.0,
                0.14934042029530017,
                0.0,
                0.29702317925409344,
                0.04675253793303023
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.09494932783760593,
                0.0,
                0.0
            ],
            [
                0.0,
                0.15419701440686306,
                0.14934042029530017,
                0.0,
                1.0,
                0.039443122446556254,
                0.1694013622336534,
                0.10964966235679997
            ],
            [
                0.0,
                0.05144133324060721,
                0.0,
                0.09494932783760593,
                0.039443122446556254,
                1.0,
                0.0,
                0.04368111417006723
            ],
            [
                0.0,
                0.5321279871639477,
                0.29702317925409344,
                0.0,
                0.1694013622336534,
                0.0,
                1.0,
                0.0
            ],
            [
                0.0,
                0.03970512208597641,
                0.04675253793303023,
                0.0,
                0.10964966235679997,
                0.04368111417006723,
                0.0,
                1.0
            ]
        ]
    },
    "W12-3154": {
        "input_sentences": [
            "Abstract",
            "Nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data.",
            "Analysing the Effect of Out-of-Domain Data on SMT Systems",
            "Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words.",
            "In statistical machine translation (SMT), it is known that performance declines when the training data is in a different domain from the test data.",
            "In this paper, we first try to relate the effect of the outof-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the outof-domain data at different parts of the training pipeline (alignment, phrase extraction, and phrase scoring)."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data.": 0,
            "Analysing the Effect of Out-of-Domain Data on SMT Systems": 0,
            "Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words.": 0,
            "In statistical machine translation (SMT), it is known that performance declines when the training data is in a different domain from the test data.": 0,
            "In this paper, we first try to relate the effect of the outof-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the outof-domain data at different parts of the training pipeline (alignment, phrase extraction, and phrase scoring).": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.20807764281407393,
                0.09800991081612427,
                0.2627754996307716,
                0.1864208420340233
            ],
            [
                0.0,
                0.20807764281407393,
                1.0,
                0.06389687180626558,
                0.2435002723112735,
                0.2256959738103299
            ],
            [
                0.0,
                0.09800991081612427,
                0.06389687180626558,
                1.0,
                0.13717798640858053,
                0.08916195734702938
            ],
            [
                0.0,
                0.2627754996307716,
                0.2435002723112735,
                0.13717798640858053,
                1.0,
                0.23592917823834067
            ],
            [
                0.0,
                0.1864208420340233,
                0.2256959738103299,
                0.08916195734702938,
                0.23592917823834067,
                1.0
            ]
        ]
    },
    "P11-1144": {
        "input_sentences": [
            "Abstract",
            "Semi-Supervised Frame-Semantic Parsing for Unknown Predicates",
            "The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement full frame-semantic parsing on a blind test set, over a state-of-the-art supervised baseline.",
            "We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data.",
            "We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones.",
            "Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Semi-Supervised Frame-Semantic Parsing for Unknown Predicates": 0,
            "The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement full frame-semantic parsing on a blind test set, over a state-of-the-art supervised baseline.": 0,
            "We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data.": 0,
            "We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones.": 0,
            "Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.43459329487293835,
                0.11094747514805589,
                0.1009856292122636,
                0.18216870770731633
            ],
            [
                0.0,
                0.43459329487293835,
                1.0,
                0.05719698508414797,
                0.10884874081238126,
                0.052110541572472364
            ],
            [
                0.0,
                0.11094747514805589,
                0.05719698508414797,
                1.0,
                0.1794183706830721,
                0.12984789715901676
            ],
            [
                0.0,
                0.1009856292122636,
                0.10884874081238126,
                0.1794183706830721,
                1.0,
                0.16031095416988947
            ],
            [
                0.0,
                0.18216870770731633,
                0.052110541572472364,
                0.12984789715901676,
                0.16031095416988947,
                1.0
            ]
        ]
    },
    "W09-1116": {
        "input_sentences": [
            "Abstract",
            "Pronoun resolution systems can use this fact to filter noun candidates that do not agree with the pronoun gender.",
            "Previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class.",
            "pronouns like reflect the gender and number of the entities to which they refer.",
            "While this provides useful statistics for frequent nouns, many infrequent nouns cannot be classified using this method.",
            "Indeed, broad-coverage models of noun gender have proved to be the most important source of world knowledge in automatic pronoun resolution systems.",
            "Glen Glenda or Glendale: Unsupervised and Semi-supervised Learning of English Noun Gender",
            "Our model collectively classifies all occurrences of a noun in a document using a wide variety of contextual, morphological, and categorical gender features.",
            "Rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model.",
            "By leveraging large volumes of unlabeled data, our full semi-supervised system reduces error by 50% over the existing stateof-the-art in gender classification."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Pronoun resolution systems can use this fact to filter noun candidates that do not agree with the pronoun gender.": 0,
            "Previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class.": 0,
            "pronouns like reflect the gender and number of the entities to which they refer.": 0,
            "While this provides useful statistics for frequent nouns, many infrequent nouns cannot be classified using this method.": 0,
            "Indeed, broad-coverage models of noun gender have proved to be the most important source of world knowledge in automatic pronoun resolution systems.": 0,
            "Glen Glenda or Glendale: Unsupervised and Semi-supervised Learning of English Noun Gender": 0,
            "Our model collectively classifies all occurrences of a noun in a document using a wide variety of contextual, morphological, and categorical gender features.": 0,
            "Rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model.": 0,
            "By leveraging large volumes of unlabeled data, our full semi-supervised system reduces error by 50% over the existing stateof-the-art in gender classification.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.04487921048827463,
                0.02601463091151459,
                0.0,
                0.32956313574590307,
                0.07167392934829447,
                0.05903353230723692,
                0.08658340821460599,
                0.01731343083610139
            ],
            [
                0.0,
                0.04487921048827463,
                1.0,
                0.16272757579023037,
                0.15855504694422126,
                0.04067139009217369,
                0.04915601348281999,
                0.04048687069927022,
                0.1160432383547007,
                0.03821963228838854
            ],
            [
                0.0,
                0.02601463091151459,
                0.16272757579023037,
                1.0,
                0.0,
                0.023575530638680946,
                0.028493717557065677,
                0.023468572342174046,
                0.023738440378366282,
                0.022154347564024138
            ],
            [
                0.0,
                0.0,
                0.15855504694422126,
                0.0,
                1.0,
                0.0,
                0.0,
                0.04959575453494642,
                0.05016606229311226,
                0.0
            ],
            [
                0.0,
                0.32956313574590307,
                0.04067139009217369,
                0.023575530638680946,
                0.0,
                1.0,
                0.06495386857852566,
                0.05349861984791068,
                0.016812030216499112,
                0.01569014453926103
            ],
            [
                0.0,
                0.07167392934829447,
                0.04915601348281999,
                0.028493717557065677,
                0.0,
                0.06495386857852566,
                1.0,
                0.064659183583258,
                0.020319255922231983,
                0.15804874937949223
            ],
            [
                0.0,
                0.05903353230723692,
                0.04048687069927022,
                0.023468572342174046,
                0.04959575453494642,
                0.05349861984791068,
                0.064659183583258,
                1.0,
                0.12508602478379263,
                0.015618960939723595
            ],
            [
                0.0,
                0.08658340821460599,
                0.1160432383547007,
                0.023738440378366282,
                0.05016606229311226,
                0.016812030216499112,
                0.020319255922231983,
                0.12508602478379263,
                1.0,
                0.0737353885737864
            ],
            [
                0.0,
                0.01731343083610139,
                0.03821963228838854,
                0.022154347564024138,
                0.0,
                0.01569014453926103,
                0.15804874937949223,
                0.015618960939723595,
                0.0737353885737864,
                1.0
            ]
        ]
    },
    "W11-1002": {
        "input_sentences": [
            "Abstract",
            "We argue that failing to capture the degree of contribution of each semantic frame in a sentence explains puzzling results in recent work on the MEANT family of semantic MT evaluation metrics, which have disturbingly indicated that dissociating semantic roles and fillers from their predicates actually improves correlation with human adequacy judgments even though, intuitively, properly segregating event frames should more accurately reflect the preservation of meaning.",
            "We then show that the correlation of HMEANT, the human variant of MEANT, can be greatly improved by introducing a simple length-based weighting scheme that approximates the degree of contribution of each semantic frame to the overall sentence.",
            "Structured vs. Flat Semantic Role Representations for Machine Translation Evaluation",
            "Our analysis finds that both properly structured and flattened representations fail to adequately account for the contribution of each semantic frame to the overall sentence.",
            "The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame\u2019s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER."
        ],
        "authority_scores": {
            "Abstract": 0,
            "We argue that failing to capture the degree of contribution of each semantic frame in a sentence explains puzzling results in recent work on the MEANT family of semantic MT evaluation metrics, which have disturbingly indicated that dissociating semantic roles and fillers from their predicates actually improves correlation with human adequacy judgments even though, intuitively, properly segregating event frames should more accurately reflect the preservation of meaning.": 0,
            "We then show that the correlation of HMEANT, the human variant of MEANT, can be greatly improved by introducing a simple length-based weighting scheme that approximates the degree of contribution of each semantic frame to the overall sentence.": 0,
            "Structured vs. Flat Semantic Role Representations for Machine Translation Evaluation": 0,
            "Our analysis finds that both properly structured and flattened representations fail to adequately account for the contribution of each semantic frame to the overall sentence.": 0,
            "The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame\u2019s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.19321923791399026,
                0.09051603843706325,
                0.14118717138046705,
                0.14413695986351557
            ],
            [
                0.0,
                0.19321923791399026,
                1.0,
                0.025152254367455825,
                0.17486544036665372,
                0.18749220123980392
            ],
            [
                0.0,
                0.09051603843706325,
                0.025152254367455825,
                1.0,
                0.19022155987424194,
                0.025280780290011757
            ],
            [
                0.0,
                0.14118717138046705,
                0.17486544036665372,
                0.19022155987424194,
                1.0,
                0.13598739548759917
            ],
            [
                0.0,
                0.14413695986351557,
                0.18749220123980392,
                0.025280780290011757,
                0.13598739548759917,
                1.0
            ]
        ]
    },
    "W06-2905": {
        "input_sentences": [
            "Abstract",
            "In this MWEs have no special status, but emerge in a general procedure for finding the best statistical grammar to describe the training corpus.",
            "We explore a novel computational approach to identifying \u201cconstructions\u201d or \u201cmulti-word expressions\u201d (MWEs) in an annotated corpus.",
            "We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank.",
            "We report quantitative results on the ATIS corpus of phrase-structure annotated sentences, and give examples of the MWEs extracted from this corpus.",
            "What Are The Productive Units Of Natural Language Grammar? A DOP Approach To The Automatic Identification Of Constructions",
            "The statistical grammar formalism used is that of stochastic tree substitution grammars (STSGs), such as used in Data-Oriented Parsing."
        ],
        "authority_scores": {
            "Abstract": 0,
            "In this MWEs have no special status, but emerge in a general procedure for finding the best statistical grammar to describe the training corpus.": 0,
            "We explore a novel computational approach to identifying \u201cconstructions\u201d or \u201cmulti-word expressions\u201d (MWEs) in an annotated corpus.": 0,
            "We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank.": 0,
            "We report quantitative results on the ATIS corpus of phrase-structure annotated sentences, and give examples of the MWEs extracted from this corpus.": 0,
            "What Are The Productive Units Of Natural Language Grammar? A DOP Approach To The Automatic Identification Of Constructions": 0,
            "The statistical grammar formalism used is that of stochastic tree substitution grammars (STSGs), such as used in Data-Oriented Parsing.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.09933093400866931,
                0.0,
                0.135358725180165,
                0.052894650434400246,
                0.10021779528629218
            ],
            [
                0.0,
                0.09933093400866931,
                1.0,
                0.0,
                0.1983358932641405,
                0.14569063270283417,
                0.0
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.035798190747123836
            ],
            [
                0.0,
                0.135358725180165,
                0.1983358932641405,
                0.0,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.052894650434400246,
                0.14569063270283417,
                0.0,
                0.0,
                1.0,
                0.04533977086933603
            ],
            [
                0.0,
                0.10021779528629218,
                0.0,
                0.035798190747123836,
                0.0,
                0.04533977086933603,
                1.0
            ]
        ]
    },
    "D10-1044": {
        "input_sentences": [
            "Abstract",
            "Instance Weighting",
            "We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.",
            "We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.",
            "This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure.",
            "Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation"
        ],
        "authority_scores": {
            "Abstract": 0,
            "Instance Weighting": 0,
            "We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.": 0,
            "We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.": 0,
            "This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure.": 0,
            "Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.27703027930910296,
                0.0,
                0.09092684698416184,
                0.3770018457037177
            ],
            [
                0.0,
                0.27703027930910296,
                1.0,
                0.0,
                0.025189489816718418,
                0.10444092661534823
            ],
            [
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.1946613010370093
            ],
            [
                0.0,
                0.09092684698416184,
                0.025189489816718418,
                0.0,
                1.0,
                0.09977178919585633
            ],
            [
                0.0,
                0.3770018457037177,
                0.10444092661534823,
                0.1946613010370093,
                0.09977178919585633,
                1.0
            ]
        ]
    },
    "I08-2105": {
        "input_sentences": [
            "Abstract",
            "Unsupervised All-words Word Sense Disambiguation with Grammatical Dependencies",
            "We also com pare two methods for computing selectional preferences between the senses of every two grammatically related words: one using a Lesk-based measure on WordNet, the other using dependency relations from the British National Corpus.",
            "The best configurationuses the syntactically-constrained graph, se lectional preferences computed from thecorpus and a PageRank tie-breaking algo rithm.",
            "We especially note good performancewhen disambiguating verbs with grammati cally constrained links.",
            "We show that allowing only grammatically related words to influence each other?s senses leads to disambiguation results on a par with thebest graph-based systems, while greatly reducing the computation load.",
            "We present experiments that analyze the necessity of using a highly interconnectedword/sense graph for unsupervised all words word sense disambiguation."
        ],
        "authority_scores": {
            "Abstract": 0,
            "Unsupervised All-words Word Sense Disambiguation with Grammatical Dependencies": 0,
            "We also com pare two methods for computing selectional preferences between the senses of every two grammatically related words: one using a Lesk-based measure on WordNet, the other using dependency relations from the British National Corpus.": 0,
            "The best configurationuses the syntactically-constrained graph, se lectional preferences computed from thecorpus and a PageRank tie-breaking algo rithm.": 0,
            "We especially note good performancewhen disambiguating verbs with grammati cally constrained links.": 0,
            "We show that allowing only grammatically related words to influence each other?s senses leads to disambiguation results on a par with thebest graph-based systems, while greatly reducing the computation load.": 0,
            "We present experiments that analyze the necessity of using a highly interconnectedword/sense graph for unsupervised all words word sense disambiguation.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.038545040222792204,
                0.0,
                0.0,
                0.10197983895350819,
                0.46809724957050003
            ],
            [
                0.0,
                0.038545040222792204,
                1.0,
                0.0417937807873519,
                0.0,
                0.18210114516311482,
                0.11366951427249579
            ],
            [
                0.0,
                0.0,
                0.0417937807873519,
                1.0,
                0.05941377442654629,
                0.03472353268383927,
                0.03866975582120471
            ],
            [
                0.0,
                0.0,
                0.0,
                0.05941377442654629,
                1.0,
                0.0,
                0.0
            ],
            [
                0.0,
                0.10197983895350819,
                0.18210114516311482,
                0.03472353268383927,
                0.0,
                1.0,
                0.10195790328289574
            ],
            [
                0.0,
                0.46809724957050003,
                0.11366951427249579,
                0.03866975582120471,
                0.0,
                0.10195790328289574,
                1.0
            ]
        ]
    },
    "N10-1002": {
        "input_sentences": [
            "Abstract",
            "The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features.",
            "As an illustration of the functionality of our proposed technique, we develop a lexical acquisition model for English verb particle constructions which operates over unlexicalised features mined from a partial parsing chart.",
            "Chart Mining-based Lexical Acquisition with Precision Grammars",
            "In this paper, we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars.",
            "The general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks, and is particularly suited to domain-specific lexical tuning and lexical acquisition using lowcoverage grammars."
        ],
        "authority_scores": {
            "Abstract": 0,
            "The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features.": 0,
            "As an illustration of the functionality of our proposed technique, we develop a lexical acquisition model for English verb particle constructions which operates over unlexicalised features mined from a partial parsing chart.": 0,
            "Chart Mining-based Lexical Acquisition with Precision Grammars": 0,
            "In this paper, we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars.": 0,
            "The general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks, and is particularly suited to domain-specific lexical tuning and lexical acquisition using lowcoverage grammars.": 0
        },
        "adjacency_matrix": [
            [
                1.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            [
                0.0,
                1.0,
                0.11886132395716843,
                0.07311767500725395,
                0.0748368887012481,
                0.02994963273077772
            ],
            [
                0.0,
                0.11886132395716843,
                1.0,
                0.19358924138690117,
                0.09907054969946234,
                0.18502360182699323
            ],
            [
                0.0,
                0.07311767500725395,
                0.19358924138690117,
                1.0,
                0.37349882133570905,
                0.38731385335087853
            ],
            [
                0.0,
                0.0748368887012481,
                0.09907054969946234,
                0.37349882133570905,
                1.0,
                0.08494730407269302
            ],
            [
                0.0,
                0.02994963273077772,
                0.18502360182699323,
                0.38731385335087853,
                0.08494730407269302,
                1.0
            ]
        ]
    }
}