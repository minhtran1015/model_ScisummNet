{
    "W11-2139": {
        "abstract": "the cmu-ark german-english translation system abstract this paper describes the german-english translation system developed by the ark research group at carnegie mellon university for the sixth workshop on machine translation (wmt11). we present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of oovs; and using development set translations into other languages to create additional pseudoreferences for training.",
        "conclusion": "summary we have presented a summary of the enhancements made to a hierarchical phrase-based translation system for the wmt11 shared translation task. some of our results are still preliminary (the source parse 10the model used is p(y i x)p(y). while this model is somewhat unusual (the conditional probability is backwards from a noisy channel model), it is a standard and effective technique for case restoration. model), but a number of changes we made were quite simple (oov handling, using mt output to provide additional references for training) and also led to improved results.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D12-1126": {
        "abstract": "dynamic features part-of-speech tagging for chinese-english mixed texts with dynamic features abstract in modern chinese articles or conversations, it is very popular to involve a few english words, especially in emails and internet literature. therefore, it becomes an important and challenging topic to analyze chinese-english mixed texts. the underlying problem is how to tag part-of-speech (pos) for the english words involved. due to the lack of specially annotated corpus, most of the english words are tagged as the oversimplified type, \u201cforeign words\u201d. in this paper, we present a method using dynamic features to tag pos of mixed texts. experiments show that our method achieves higher performance than traditional sequence labeling methods. meanwhile, our method also boosts the performance of pos tagging for pure chinese texts.",
        "conclusion": "conclusion in this paper, we focus on chinese-english mixed texts and use dynamic features for pos tagging. to overcome the problem of the lack of annotated corpus on mixed texts, our features use both local and non-local information and take advantage of the characteristics of chinese-english mixed texts. the experiments demonstrate the effectiveness of our method. it should be noted that our method is also effective for the mixed texts of chinese and any foreign languages since we use \u201cunified replacement\u201d. for future works, we plan to improve our approximate tagging algorithm to reduce error propagation. in addition, we will refer to an english dictionary to generate some useful features to distinguish between \u201cnr\u201d and \u201cnn\u201d in chinese-english mixed texts and add some statistical features derived from english resources, such as the most common tag of each english word. we would also like to investigate these features in more applications of natural language processing, such as name entity recognition, information extraction, etc.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W01-0510": {
        "abstract": "information extraction using the structured language model abstract the paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (slm) as a statistical parser. the task of template filling is cast as constrained parsing using the slm. the model is automatically trained from a set of sentences annotated with frame/slot labels and spans. training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage. despite the small amount of training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the mipad personal information management task.",
        "conclusion": "conclusions and future directions we have presented a data-driven approach to information extraction that, despite the small amount of training data used, is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the mipad personal information management task. the performance of the baseline model could be improved with more authoring effort, although this is expensive. the big difference in performance between training and test and the fact that we are using so little training data, makes improvements by using more training data very likely, although this may be expensive. a framework which utilizes the vast amounts of text data collected once such a system is deployed would be desirable. statistical modeling techniques that make more effective use of the training data should be used in the slm, maximum entropy (berger et al., 1996) being a good candidate. as for using the slm as the language understanding component of a speech driven application, such as mipad, it would be interesting to evaluate the impact of incorporating the semantic constraints on the word-level accuracy of the system. another possible research direction is to modify the framework such that it finds the most likely semantic parse given the acoustics thus treating the word sequence as a hidden variable.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W06-2204": {
        "abstract": "transductive pattern learning for information extraction abstract the requirement for large labelled training corpora is widely recognized as a key bottleneck in the use of learning algorithms for extraction. we present a semi-supervised learning algorithm for information extraction that can acquire extraction patterns from a small amount of labelled text in conjunction with a large amount of unlabelled text. compared to previous work, two novel features. first, the algorithm does not require redundancy in the fragments to be extracted, but only redundancy of the extraction patterns themselves. second, most bootstrapping methods identify the highest quality fragments in the unlabelled data and then assume that they are as reliable as manually labelled data in subsequent iterations. contrast, scoring mechanism prevents errors from snowballing by recording the reliability of fragments extracted from unlabelled data. our experiments with several demonstrate that usually competitive with various fully-supervised algorithms when very little labelled training data is available.",
        "conclusion": "discussion we have described tplex, a semi-supervised algorithm for learning information extraction patterns. the key idea is to exploit the following recursive definition: good patterns are those that extract good fragments, and good fragments are those that are extracted by good patterns. this definition allows tplex to perform well with very little training data in domains where other approaches that assume fragment redundancy would fail. conclusions. from our experiments we have observed that our algorithm is particularly competitive in scenarios where very little labelled training data is available. we contend that this is a result of our algorithm\u2019s ability to use the unlabelled test data to validate the patterns learned from the training data. we have also observed that the number of fields that are being extracted in the given domain affects the performance of our algorithm. tplex extracts all fields simultaneously and uses the scores from each of the figure 3: f1 averaged across all fields, for the seminar dataset trained on only labeled data and trained on labeled and unlabeled data patterns that extract a given position to determine the most likely field for that position. with more fields in the problem domain there is potentially more information on each of the candidate positions to constrain these decisions. future work. we are currently extending tplex in several directions. first, position filtering is currently performed as a distinct post-processing step. it would be more elegant (and perhaps more effective) to incorporate the filtering heuristics directly into the position scoring mechanism. second, so far we have focused on a bwi-like pattern language, but we speculate that richer patterns permitting (for example) optional or reordered tokens may well deliver substantial increases in accuracy. we are also exploring ideas for semi-supervised learning from the machine learning community. specifically, probabilistic finite-state methods such hidden markov models and conditional random fields have been shown to be competitive with more traditional pattern-based approaches to information extraction (fuchun and mccallum, 2004), and these methods can exploit the expectation maximization algorithm to learn from a mixture of labelled and unlabelled data (lafferty et al., 2004). it remains to be seen whether this approach would be effective for information extraction. another possibility is to explore semi-supervised extensions to boosting (d\u2019alch\u00b4e buc et al., 2002). boosting is a highly effective ensemble learning technique, and bwi uses boosting to tune the weights of the learned patterns, so if we generalize boosting to handle unlabelled data, then the learned weights may well be more effective than those calculated by tplex.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D08-1094": {
        "abstract": "a structured vector space model for word meaning in context abstract we address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. this task is a crucial step towards a robust, vector-based compositional account of sentence meaning. we argue that existing models for this task do not take syntactic structure sufficiently into account. present a novel vector space model that addresses these issues by incorporating the selectional preferences for words\u2019 argument positions. this makes it possible to integrate syntax into the computation of word meaning in context. in addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.",
        "conclusion": "conclusion in this paper, we have considered semantic space models that can account for the meaning of word occurrences in context. arguing that existing models do not sufficiently take syntax into account, we have introduced the new structured vector space (svs) model of word meaning. in addition to a vector representing a word\u2019s lexical meaning, it contains vectors representing the word\u2019s selectional preferences. these selectional preferences play a central role in the computation of meaning in context. we have evaluated the svs model on two datasets on the task of predicting the felicitousness of paraphrases in given contexts. on the m&l dataset, svs outperforms the state-of-the-art model of m&l, though the difference is not significant. on the lexical substitution dataset, svs significantly outperforms the state-of-the-art. this is especially interesting as the lexical substitution dataset, in contrast to the m&l data, uses \u201crealistic\u201d paraphrase candidates that are not necessarily maximally distinct. the most important limitation of the evaluation that we have given in this paper is that we have only considered single words as context. our next step will be to integrate information from multiple relations (such as both the subject and object positions of a verb) into the computation of context-specific meaning. our eventual aim is a model that can give a compositional account of a word\u2019s meaning in context, where all words in an expression disambiguate one another according to the relations between them. we will explore the usability of vector space models of word meaning in nlp applications, formulated as the question of how to perform inferences on them in the context of the textual entailment task (dagan et al., 2006). paraphrase-based inference rules play a large role in several recent approaches to textual entailment (e.g. szpektor et al (2008)); appropriateness judgments of paraphrases in context, the task of experiments 1 and 2 above, can be viewed as testing the applicability of these inferences rules.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W09-1210": {
        "abstract": "efficient parsing of syntactic and semantic dependency structures abstract in this paper, we describe our system for the 2009 conll shared task for joint parsing of syntactic and semantic dependency structures of multiple languages. our system combines and implements efficient parsing techniques to get a high accuracy as well as very good parsing and training time. for the applications of syntactic and semantic parsing, the parsing time and memory footprint are very important. we think that also the development of systems can profit from this since one can perform more experiments in the given time. for the subtask of syntactic dependency parsing, we could reach the second place with an accuracy in average of 85.68 which is only 0.09 points behind the first ranked system. for this task, our system has the highest accuracy for english with 89.88, german with 87.48 and the out-of-domain data in average with 78.79. the semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro f1 score) in the joint task for syntactic and semantic dependency parsing.",
        "conclusion": "conclusion we provided a fast implementation with good parsing time and memory footprint. even if we traded off a lot of the speed improvement by using a more expensive decoder and more attributes to get a higher accuracy. for some languages, features are not provided or the parser does not profit from using these features. for instance, the english parser does not profit from the lemmas and the chinese as well as the japanese corpus does not have lemmas different from the word forms, etc. therefore, a possible further accuracy and parsing speed improvement would be to select different features sets for different languages or to leave out some features.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C08-1081": {
        "abstract": "parsing the syntagrus treebank of russian abstract we present the first results on parsing the syntagrus treebank of russian with a data-driven dependency parser, achieving a labeled attachment score of over 82% and an unlabeled attachment score of 89%. a feature analysis shows that high parsing accuracy is crucially dependent on the use of both lexical and morphological features. we conjecture that the latter result can be generalized to richly inflected languages in general, provided that sufficient amounts of training data are available.",
        "conclusion": "conclusion we have presented the first results on parsing thesyntagrus treebank of russian using a data driven dependency parser. besides establishing a first benchmark for the syntagrus treebank, we have analyzed the influence of different kindsof features on parsing accuracy, showing conclu sively that both lexical and morphological features are crucial for obtaining good parsing accuracy. we hypothesize that this result can be generalized to other richly inflected languages, provided that sufficient amounts of data are available.future work includes a deeper analysis of the in fluence of individual features, both morphological and lexical, as well as an evaluation of the parserunder more realistic conditions without gold stan dard annotation in the input. this will require notonly automatic morphological analysis and disambiguation but also a mechanism for inserting so called phantom tokens in elliptical constructions. acknowledgments we want to thank ivan chardin for initiating this collaboration and jens nilsson for converting the syntagrus data to the conll format. we aregrateful to the russian foundation of basic re search for partial support of this research (grant no. 07-06-00339).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D11-1132": {
        "abstract": "relation extraction with relation topics abstract this paper describes a novel approach to the semantic relation detection problem. instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. specifically, we detect a new semantic relation by projecting the new relation\u2019s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. first, we construct a large relation repository of more than 7,000 relations from wikipedia. second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. third, we integrate the relation topics in a kernel function, and use it together with svm to construct detectors for new relations. the experimental results on wikipedia and ace data have confirmed that backgroundknowledge-based topics generated from the wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-1060": {
        "abstract": "vector space semantics with frequency-driven motifs abstract traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items. in this work, we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohelineal constituents, or the framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents, and circumvents some problems of data sparsity by design. we design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated. hellinger pca embeddings learnt using the framework show competitive results on empirical tasks.",
        "conclusion": "conclusion we have presented a new frequency-driven framework for distributional semantics of not only lexical items but also longer cohesive motifs. the theme of this work is a general paradigm of seeking motifs that are recurrent in common parlance, are semantically coherent, and are possibly noncompositional. such a framework for distributional models avoids the issue of data sparsity in learning of representations for larger linguistic structures. the approach depends on drawing features from frequency statistics, statistical correlations, and linguistic theories; and this work provides a computational framework to jointly model recurrence and semantic cohesiveness of motifs through compositional penalties and affinity scores in a data driven way. while being deliberately vague in our working definition of motifs, we have presented simple efficient formulations to extract such motifs that uses both annotated as well as partially unannotated data. the qualitative and quantitative analyis of results from our preliminary motif segmentation model indicate that such motifs can help to disambiguate contexts of single tokens, and provide cleaner, more interpretable representations. finally, we obtain motif representations in form of low-dimensional vector-space embeddings, and our experimental findings indicate value of the learnt representations in downstream applications. we believe that the approach has considerable theoretical as well as practical merits, and provides a simple and clean formulation for modeling phrasal and sentential semantics. in particular, we believe that ours is the first method that can invoke different meaning representations for a token depending on textual context of the sentence. the flexibility of having separate representations to model different semantic senses has considerable valuable, as compared with extant approaches that assign a single representation to each token, and are hence constrained to conflate several semantic senses into a common representation. the approach also elegantly deals with the problematic issue of differential compositional and non-compositional usage of words. future work can focus on a more thorough quantitative evaluation of the paradigm, as well as extension to model non-contiguous motifs.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W05-0104": {
        "abstract": "a core-tools statistical nlp course abstract in the fall term of 2004, i taught a new statistical nlp course focusing on core tools and machine-learning algorithms. the course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a pcfg parser, and a word-alignment system. using provided scaffolding, students built realistic tools with nearly state-of-theart performance in most cases. this paper briefly outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way.",
        "conclusion": "conclusions there are certainly changes i will make when i teach this course again this fall. i will likely shuffle the topics around so that word alignment comes earlier (closer to hmms for tagging) and i will likely teach dynamic programming solutions to parsing and tagging in more depth than graph-search based methods. some students needed remedial linguistics sections and other students needed remedial math sections, and i would hold more such sessions, and ear3there was also verbose error reporting for assignment 4, which displayed each sentence\u2019s guessed and gold alignments in a grid, but since most students didn\u2019t speak french, this didn\u2019t have the same effect. lier in the term. however, i will certainly keep the substantial implementation component of the course, partially in response to very positive student feedback on the assignments, partially from my own reaction to the high quality of student work on those assignments, and partially from how easily students with so much handson experience seem to be able to jump into nlp research.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-2802": {
        "abstract": "toward learning perceptually grounded word meanings from unaligned parallel data abstract in order for robots to effectively understand natural language commands, they must be ableto acquire a large vocabulary of meaning rep resentations that can be mapped to perceptualfeatures in the external world. previous ap proaches to learning these grounded meaning representations require detailed annotations at training time. in this paper, we present an approach which is capable of jointly learninga policy for following natural language com mands such as ?pick up the tire pallet,? as well as a mapping between specific phrases in the language and aspects of the external world; for example the mapping between the words ?the tire pallet? and a specific object in the environment. we assume the action policy takes a parametric form that factors based on the structure of the language, based on the g3 framework and use stochastic gradient ascentto optimize policy parameters. our prelimi nary evaluation demonstrates the effectivenessof the model on a corpus of ?pick up? com mands given to a robotic forklift by untrained users.",
        "conclusion": "conclusion in this paper we described an approach for learningperceptually grounded word meanings from an un aligned parallel corpus of language paired with robotactions. the training algorithm jointly infers poli cies that correspond to natural language commands as well as alignments between noun phrases in the command and groundings in the external world. inaddition, our approach learns grounded word mean ings or distributions corresponding to words in the language, that the system can use to follow novelcommands that it may have never encountered dur ing training. we presented a preliminary evaluation on a small corpus, demonstrating that the system isable to infer meanings for concrete noun phrases de spite having no direct supervision for these values. there are many directions for improvement. weplan to train our system using a large dataset of language paired with robot actions in more complex en vironments, and on more than one robotic platform. our approach points the way towards a framework that can learn a large vocabulary of general groundedword meanings, enabling systems that flexibly respond to a wide variety of natural language com mands given by untrained users.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W04-0837": {
        "abstract": "using automatically acquired predominant senses for word sense disambiguation abstract word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. the first (or predominant) sense heuristic assumes the availability of handtagged data. whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently. in this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text. we evaluate on the and english alldata. for accurate first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough. in this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from semcor many systems in",
        "conclusion": "conclusions we have demonstrated that it is possible to acquire predominant senses from raw textual corpora, and that these can be used as an unsupervised first sense heuristic that does not not rely on manually produced corpora such as semcor. this approach is useful for words where there is no manually-tagged data available. our predominant senses have been used within a wsd system as a back-off method when data is not available from other resources (villarejo et al., 2004). the method could be particularly useful when tailoring a wsd system to a particular domain. we intend to experiment further using a wider variety of grammatical relations, which we hope will improve performance for verbs, and with data from larger corpora, such as the gigaword corpus and the web, which should allow us to cover a great many more words which do not occur in manually created resources such as semcor. we also intend to apply our method to domain specific text.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-0610": {
        "abstract": "classification of atypical language in autism abstract atypical or idiosyncratic language is a characteristic of autism spectrum disorder (asd). in this paper, we discuss previous work identifying language errors associated with atypical language in asd and describe a procedure for reproducing those results. we describe our data set, which consists of transcribed data from a widely used clinical diagnostic instrument (the ados) for children with autism, children with developmental language disorder, and typically developing children. we then present methods for automatically extracting lexical and syntactic features from transcripts of children\u2019s speech to 1) identify certain syntactic and semantic errors that have previously been found to distinguish asd language from that of children with typical development; and 2) perform diagnostic classification. our classifiers achieve results well above chance, demonstrating the potential for using nlp techniques to enhance neurodevelopmental diagnosis and atypical language analysis. we expect further improvement with additional data, features, and classification techniques.",
        "conclusion": "future work although our classifiers using automatically extracted features were generally robust, we expect that including additional classification techniques, subjects (especially asd subjects without dld), and features will further improve our results. in particular, we would like to explore semantic and lexical features that are less dependent on linear order and syntactic structure, such as resnik similarity and features derived using latent semantic analysis. we also plan to expand the training input for the language model and parser to include children\u2019s speech. the switchboard corpus is conversational speech, but it may fail to adequately model many linguistic features characteristic of small children. the childes database of children\u2019s speech, although it is not large enough to be used on its own for our analysis and would require significant manual syntactic annotation, might provide enough data for us to adapt our models to the child language domain. finally, we would like to investigate how informative the error types are and whether they can be reliably coded by multiple judges. when we examined the output of our error-type classifier, we noticed that many of the misclassified examples could be construed, upon closer inspection, as belonging to multiple error classes. the sentence he\u2019s flying in a lily-pond, for instance, could contain a developmental error (i.e., the child has not yet acquired the correct meaning of in) or a semantic error (i.e., the child is using the word flying instead of swimming). without knowing the context in which the sentence was uttered, it is not possible to determine the type of error through any manual or automatic means. the seemingly large number of misclassifications of sentences like this indicates the need for further investigation of the existing coding procedure and in-depth classification error analysis. conclusions our method of automatically identifying error type shows promise as a supplement to, or substitute for, the time-consuming and subjective manual coding process described in volden and lord (volden and lord, 1991). however, the superior performance of our automatically extracted language features suggests that perhaps it may not be the errors themselves that characterize the speech of children with asd and dld but rather a preference for certain structures and word sequences that sometimes manifest themselves as clear language errors. such variations in complexity and likelihood might be too subtle for humans to reliably observe. in summary, the methods explored in this paper show potential for improving diagnostic discrimination between typically developing children and those with these neurodevelopmental disorders. further research is required, however, in finding the most reliable markers that can be derived from such spoken language samples.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "I08-1012": {
        "abstract": "dependency parsing with short dependency relations in unlabeled data abstract this paper presents an effective dependencyparsing approach of incorporating short de pendency information from unlabeled data. the unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words. we thentrain another parser which uses the informa tion on short dependency relations extractedfrom the output of the first parser. our proposed approach achieves an unlabeled at tachment score of 86.52, an absolute 1.24% improvement over the baseline system on the data set of chinese treebank.",
        "conclusion": "conclusion this paper presents an effective approach to improvedependency parsing by using unlabeled data. we ex tract the information on short dependency relations 93in an automatically generated corpus parsed by a basic parser. we then train a new parser with the information. the new parser achieves an absolute im provement of 1.24% over the state-of-the-art parser on chinese treebank (from 85.28% to 86.52%). there are many ways in which this research should be continued. first, feature representationneeds to be improved. here, we use a simple fea ture representation on short dependency relations.we may use a combined representation to use the in formation from long dependency relations even they are not so reliable. second, we can try to select more accurately parsed sentences. then we may collect more reliable information than the current one.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-2006": {
        "abstract": "fast and scalable decoding with language model look-ahead for phrase-based statistical machine translation abstract in this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (smt), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. we compare our approach with moses and observe the same performance, but a substantially better trade-off between translation quality and speed. at a speed of roughly 70 words per second, moses 17.2% whereas our approach yields 20.0% with identical models.",
        "conclusion": "conclusions this work introduces two extensions to the wellknown beam search algorithm for phrase-based machine translation. both pre-sorting the phrase translation candidates with an lm score estimate and lm look-ahead during search are shown to have a positive effect on translation speed. we compare our decoder to moses, reaching a similar highest bleu score, but clearly outperforming it in terms of scalability with respect to the trade-off ratio between translation quality and speed. in our experiments, the fastest settings of our decoder and moses differ in translation speed by a factor of 22 on the wmt data and a factor of 19 on the iwslt data. our software is part of the open source toolkit jane.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-3706": {
        "abstract": "opinum: statistical sentiment analysis for opinion classification abstract the classification of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach. we propose an approach based on the order of the words without using any syntactic and semantic information. it consists of building one probabilistic model for the positive and another one for the negative opinions. then the test opinions are compared to both models and a decision and confidence measure are calculated. in order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards. we present an accuracy above 81% for spanish opinions in the financial products domain.",
        "conclusion": "conclusions and future work opinum is a sentiment analysis system designed for classifying customer opinions in positive and negative. its approach based on morphological simplification, entity substitution and n-gram language models, makes it easily adaptable to other classification targets different from positive/negative. in this work we present experiments for spanish in the financial domain but opinum could easily be trained for a different language or domain. to this end an apertium morphological analyser would be necessary (30 languages are currently available) as well as a labeled data set of opinions. setting n for the ngram models depends on the size of the corpus but it would usually range from 4 to 6, 5 in our case. there are other parameters which have to be experimentally tuned and they are not related to the positive or negative classification but to the subjective qualifier very/somewhat/little and to the confidence measure. the classification performance of opinum in our financial-domain experiments is 81,98% which would be difficult to improve because of the noise in the data and the subjectivity of the labeling in positive and negative. the next steps would be to study the possibility to classify in more than two classes by using several language models. the use of an external neutral corpus should also be considered in the future. it is necessary to perform a deeper analysis of the impact of lexical simplification on the accuracy of the language models. it is also very important to establish the limitations of this approach for different domains. is it equally successful for a wider domain? for instance, trying to build the models from a mixed set of opinions of the financial domain and the it domain. would it work for a general domain? regarding applications, opinum could be trained for a given domain without expert knowledge. its queries are very fast which makes it feasible for free on-line services. an interesting application would be to exploit the named entity recognition and associate positive/negative scores to the entities based on their surrounding text. if several domains were available, then the same entities would have different scores depending on the domain, which would be a valuable analysis.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P13-1092": {
        "abstract": "grounded unsupervised semantic grounded unsupervised semantic parsing abstract we present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. our gusp system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using em. to compensate for the lack of example annotations or question-answer pairs, gusp adopts a novel grounded-learning approach to leverage database for indirect supervision. on the challenging atis dataset, gusp attained an accuracy of 84%, effectively tying with the best published results by supervised approaches.",
        "conclusion": "conclusion this paper introduces grounded unsupervised semantic parsing, which leverages available database for indirect supervision and uses a grounded meaning representation to account for syntax-semantics mismatch in dependency-based semantic parsing. the resulting gusp system is the first unsupervised approach to attain an accuracy comparable to the best supervised systems in translating complex natural-language questions to database queries. directions for future work include: joint syntactic-semantic parsing, developing better features for learning; interactive learning in a dialog setting; generalizing distant supervision; application to knowledge extraction from database-rich domains such as biomedical sciences.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W07-2214": {
        "abstract": "pomset mcfgs abstract this paper identifies two orthogonal dimensions of context sensitivity, the first being context sensitivity in concurrency and the second being structural context sensitivity. we present an example from natural language which seems to require both types of context sensitivity, and introduce partially ordered multisets (pomsets) mcfgs as a formalism which succintly expresses both.",
        "conclusion": "conclusion in this paper we identified two types of context sensitivity, and provided a natural language example which exhibits both types of context sensitivity. we introduced pomset mcfgs as a formalism for describing grammars with both types of context sensitivity, and outlined an informal proof of the its polynomialtime parsing complexity.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P05-1022": {
        "abstract": "coarse-to-fine n-best parsing and maxent discriminative reranking abstract discriminative reranking is one method for constructing high-performance statistical parsers (collins, 2000). a discriminative reranker requires a source of candidate parses for each sentence. this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (charniak, 2000). this method generates 50-best lists that are of substantially higher quality than previously obtainable. we used these parses as the input to a maxent reranker (johnson et al., 1999; riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.",
        "conclusion": "conclusion this paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of parses. because the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient. we use the 50-best parses produced by this algorithm as input to a maxent discriminative reranker. the reranker selects the best parse from this set of parses using a wide variety of features. the system we described here has an f-score of 0.91 when trained and tested using the standard parseval framework. this result is only slightly higher than the highest reported result for this test-set, bod\u2019s (.907) (bod, 2003). more to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the charniak or collins parsers. a 91.0 f-score represents a 13% reduction in fmeasure error over the best of these parsers.2 both the 50-best parser, and the reranking parser can be found at ftp://ftp.cs.brown.edu/pub/nlparser/, named parser and reranker respectively. acknowledgements we would like to thanks michael collins for the use of his data and many helpful comments, and liang huang for providing an early draft of his paper and very useful comments on our paper. finally thanks to the national science foundation for its support (nsf iis-0112432, nsf 9721276, and nsf dms-0074276).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C08-1074": {
        "abstract": "random restarts in minimum error rate training for statistical machine translation abstract och?s (2003) minimum error rate training (mert) procedure is the most commonly used method for training feature weights instatistical machine translation (smt) models. the use of multiple randomized start ing points in mert is a well-established practice, although there seems to be nopublished systematic study of its benefits. we compare several ways of perform ing random restarts with mert. we findthat all of our random restart methods out perform mert without random restarts,and we develop some refinements of ran dom restarts that are superior to the most common approach with regard to resulting model quality and training time.",
        "conclusion": "conclusions we believe that our results show very convincingly that using random restarts in mert improves the bleu scores produced by the result ing models. they also seem to show that starting point selection by random walk is slightly superiorto uniform random selection. finally, our exper iments suggest that time to carry out mert canbe significantly reduced by using as few as 5 starting points per decoding iteration, performing post restart pruning of hypothesis sets, and cutting off training after a fixed number of decoding iterations (perhaps 7) rather than waiting for convergence.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P09-1065": {
        "abstract": "joint decoding joint decoding with multiple translation models abstract current smt systems usually decode with single translation models and cannot benefit from the strengths of other models in phase. we instead propose a method that combines multiple translation models in one decoder. our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually. therefore, one model can share translations and even derivations with other models. comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 bleu points over individual decoding.",
        "conclusion": "conclusion we have presented a framework for including multiple translation models in one decoder. representing search space as a translation hypergraph, individual models are accessible to others via sharing nodes and even hyperedges. as our decoder accounts for multiple derivations, we extend the mert algorithm to tune feature weights with respect to bleu score for max-translation decoding. in the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the latticebased mert (macherey et al., 2008).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D11-1094": {
        "abstract": "latent vector weighting for word meaning in context abstract this paper presents a novel method for the computation of word meaning in context. we make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. the factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. the evaluation on a lexical substitution task \u2013 carried out for both english and french \u2013 indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations.",
        "conclusion": "conclusion in this paper, we presented a novel method for the modeling of word meaning in context. we make use of a factorization model based on non-negative matrix factorization, in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. the factorization model allows us to determine which particular dimensions are important for a target word in a particular context. a key feature of the algorithm is that we adapt the original dependency-based feature vector of the target word through the latent semantic space. by doing so, our model is able to make accurate similarity calculations for word meaning in context across the whole word space. our evaluation shows that the approach presented here is able to improve upon the state-of-the art performance on paraphrase ranking. moreover, our approach scores well for both paraphrase ranking and paraphrase induction, whereas previous approaches only seem capable of improving performance on the former task at the expense of the latter. during our research, a number of topics surfaced that we consider worth exploring in the future. first of all, we would like to further investigate the optimal configuration for combining window-based and dependency-based contexts. at the moment, the performance of the combined model does not yield a uniform picture. the results might improve further if window-based context and dependency-based context are combined in an optimal way. secondly, we would like to subject our approach to further evaluation, in particular on a number of different evaluation tasks, such as semantic compositionality. and thirdly, we would like to transfer the general idea of the approach presented in this paper to a tensorbased framework (which is able to capture the multiway co-occurrences of words, together with their window-based and dependency-based context features, in a natural way) and investigate whether such a framework proves beneficial for the modeling of word meaning in context.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-2093": {
        "abstract": "effective selection of translation model training data abstract data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest. most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus. by contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model. in this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection. the results show that our methods outperform previous methods. when the selected sentence pairs are evaluated on an end-to-end mt task, our methods can increase the translation performance by 3 bleu points.",
        "conclusion": "conclusion we present three novel methods for translation model training data selection, which are based on the translation model and language model. compared with the methods which only employ language model for data selection, we observe that our methods are able to select high-quality domain-relevant sentence pairs and improve the translation performance by nearly 3 bleu points. in addition, our methods make full use of the limited in-domain data and are easily implemented. in the future, we are interested in applying our methods into domain adaptation task of statistical machine translation in model level.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P05-1063": {
        "abstract": "discriminative syntactic language modeling for speech recognition abstract we describe a method for discriminative training of a language model that makes use of syntactic features. we follow where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second \u201creranking\u201d model is then used to choose an utterance from these 1000-best lists. the reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm. we describe experiments on the switchboard speech recognition task. the syntactic features provide an additional 0.3% reduction in test\u2013set error rate beyond the model of (roark et al., 2004a; roark et al., 2004b) (signifiat < which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline switchboard system.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-2123": {
        "abstract": "kenlm: faster and smaller language model queries abstract we present kenlm, a library that implements two data structures for efficient language model queries, reducing both time and costs. the structure uses linear probing hash tables and is designed for speed. compared with the widely- srilm, our is 2.4 times as fast while using 57% of the mem- the structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less cpu than the baseline. our code is thread-safe, and integrated into the moses, cdec, and joshua translation systems. this paper describes the several performance techniques used and presents benchmarks against alternative implementations.",
        "conclusion": "future work there any many techniques for improving language model speed and reducing memory consumption. for speed, we plan to implement the direct-mapped cache from berkeleylm. much could be done to further reduce memory consumption. raj and whittaker (2003) show that integers in a trie implementation can be compressed substantially. quantization can be improved by jointly encoding probability and backoff. for even larger models, storing counts (talbot and osborne, 2007; pauls and klein, 2011; guthrie and hepple, 2010) is a possibility. beyond optimizing the memory size of trie, there are alternative data structures such as those in guthrie and hepple (2010). finally, other packages implement language model estimation while we are currently dependent on them to generate an arpa file. while we have minimized forward-looking state in section 4.1, machine translation systems could also benefit by minimizing backward-looking state. for example, syntactic decoders (koehn et al., 2007; dyer et al., 2010; li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state. if they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram, then three or even fewer words could be kept in the backward state. this information is readily available in trie where adjacent records with equal pointers indicate no further extension of context is possible. exposing this information to the decoder will lead to better hypothesis recombination. generalizing state minimization, the model could also provide explicit bounds on probability for both backward and forward extension. this would result in better rest cost estimation and better pruning.10 in general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement. conclusion we have described two data structures for language modeling that achieve substantial reductions in time and memory cost. the probing model is 2.4 times as fast as the fastest alternative, srilm, and uses less memory too. the trie model uses less memory than the smallest lossless alternative and is still faster than srilm. these performance gains transfer to improved system runtime performance; though we focused on moses, our code is the best lossless option with cdec and joshua. we attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns. the code is opensource, has minimal dependencies, and offers both c++ and java interfaces for integration.",
        "citations": [],
        "summary": [
            "This paper talks about KenLM: Faster and Smaller Language Model Queries.  The PROBING data structure uses linear probing hash tables and is designed for speed. This paper presents methods to query N-gram language models, minimizing time and space costs. Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed. The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM. The code is open source, has minimal dependencies, and offers both C++ and Java interfaces for integration.  The authors presented, in this paper, KenLM, a library that implemented two data structures for efficient language model queries, reducing both time and costs. The structure used linear probing hash tables and was designed for speed. The PROBING model was 2.4 times as fast as the fastest alternative, SRILM, and used less memory too. The TRIE model used less memory than the smallest lossless alternative and was still faster than SRILM. These performance gained transfer to improved system run time performance. Though they focused on Moses, their code was the best lossless option with cdec and Joshua. They attained these results using several optimizations: hashing, custom look up tables, bit-level packing, and state for left-to-right query patterns. The code was open source, had minimal dependencies, and offered both C++ and Java interfaces for integration. This paper presents KenLM, a library that implements two data structures for efficient language model queries.The paper explains methods to query N-gram language models, minimizing time and space costs.It implements two data structures: PROBING , designed for speed, and TRIE , optimized for memory.It attains results using several optimizations: hashing,custom lookup tables, bit-level packing, and state for left-to-right query patterns.The PROBING model is 2.4 times as fast as the fastest alternative, SRILM, and uses less memory too. The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.\n"
        ]
    },
    "D09-1092": {
        "abstract": "polylingual topic models abstract topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. meanwhile, massive collections of interlinked documents in dozens of languages, such as wikipedia, are now widely available, calling for tools that can characterize content in many languages. we introduce a polylingual topic model that discovers topics aligned across multiple languages. we explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.",
        "conclusion": "conclusions we introduced a polylingual topic model (pltm) that discovers topics aligned across multiple languages. we analyzed the characteristics of pltm in comparison to monolingual lda, and demonstrated that it is possible to discover aligned topics. we also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora. additionally, pltm can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks. when applied to comparable document collections such as wikipedia, pltm supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
        "citations": [],
        "summary": [
            "In this paper the author aims at introducing a polylingual topic model that discovers topics aligned across multiple languages. They explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. Statistical topic models have emerged as an increasingly useful analysis tool for large text collections. Topic models have been used for analyzing topic trends in research literature, inferring captions for images, social network analysis in email, and expanding queries with topically related words in information retrieval. The author argues that topic modelling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages. By linking topics across languages, polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language. This paper talks about Polylingual Topic Models. The main keywords annotated in this paper are Polylingual Topic Model, languages and parallel documents. This paper introduces a Polylingual topic model that discovers topics aligned across multiple languages. The Polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) for modeling Polylingual document tuples. First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents. This property is useful for building machine translation systems as well as for human readers. Second, because comparable texts may not use exactly the same topics, it becomes crucially important to be able to characterize differences in topic prevalence at the document level and at the language-wide level. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language. Authors Mimno et all introduced a poly-lingual topic model that discovered topics aligned across multiple languages. They explored the models characteristics using two large corpora, each with over ten different languages, and demonstrated its usefulness in supporting machine translation and tracking topic trends across languages. They analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora. When applied to comparable document collections such as Wikipedia, PLTM supported data-driven analysis of differences and similarities across all languages for readers who understand any one language."
        ]
    },
    "P13-1155": {
        "abstract": "social text normalization using contextual graph random walks abstract we introduce a social media text normalization system that can be deployed as a preprocessing step for machine translation and various nlp applications to handle social media text. the proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. the proposed approach uses random walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. we show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). when used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. the proposed approach is domain and language independent and can be deployed as a preprocessing step for any nlp application to handle social media text.",
        "conclusion": "conclusion and future work we introduced a social media text normalization system that can be deployed as a preprocessor for mt and various nlp applications to handle social media text. the proposed approach is very scalable, adaptive to any domain and language. we show that the proposed unsupervised approach provides a normalization system with very high precision and a reasonable recall. we compared the system with conventional correction approaches and with recent previous work; and we showed that it highly outperforms other systems. finally, we have used the system as a preprocessing step for a machine translation system which improved the translation quality by 6%. as an extension to this work, we will extend the approach to handle many-to-many normalization pairs; also we plan to apply the approach to more languages. furthermore, the approach can be easily extended to handle similar problems such as accent restoration and generic entity normalization. dancing and should not be corrected to \u201d\u2018tweeting\u201d\u2019. table 5 lists a number of examples and their normalization using both baseline1 and rw3. at the first example, rw3 got the correct normalization as \u201dinteresting\u201d which apparently is not the one with the shortest edit distance, though it is the most frequent candidate at the generated lexicon. the baseline system did not get it right; it got a wrong normalization with shorter edit distance. example(2) shows the same effect by getting \u201dcuz\u201d normalized to \u201dbecause\u201d. at example(3), both the baseline and rw3 did not get the correct normalization of \u201dyur\u201d to \u201dyou are\u201d which is currently a limitation in our system since we only allow one-to-one word mapping in the generated lexicons not one-to-many or many-tomany. at example(4), rw3 did not normalize \u201ddure\u201d to \u201dsure\u201d ; however the baseline normalized it by mistake to \u201ddare\u201d. this shows a characteristic of the proposed approach; it is very conservative in proposing normalization which is desirable as a preprocessing step for nlp applications. this limitation can be marginalized by providing more data for generating the lexicon. finally, example 4 shows also that the system normalize \u201dgr8\u201d which is mainly due to having a flexible similarity cost during the normalization lexicon construction.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P08-1043": {
        "abstract": "a single generative model for joint morphological segmentation and syntactic parsing abstract morphological processes in semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. these words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
        "conclusion": "discussion and conclusion employing a pcfg-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound. the overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results. better grammars are shown here to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones. we conjecture that this trend may continue by incorporating additional information, e.g., three-dimensional models as proposed by tsarfaty and sima\u2019an (2007). in the current work morphological analyses and lexical probabilities are derived from a small treebank, which is by no means the best way to go. using a wide-coverage morphological analyzer based on (itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. (levinger et al., 1995; goldberg et al., ; adler et al., 2008)) will make the parser more robust and suitable for use in more realistic scenarios. acknowledgments we thank meni adler and michael elhadad (bgu) for helpful comments and discussion. we further thank khalil simaan (illcuva) for his careful advise concerning the formal details of the proposal. the work of the first author was supported by the lynn and william frankel center for computer sciences. the work of the second author as well as collaboration visits to israel was financed by nwo, grant number 017.001.271.",
        "citations": [],
        "summary": [
            "In this paper they propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a Treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique their model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of space delimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Using a wide-coverage morphological analyzer based on should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus will make the parser more robust and suitable for use in more realistic scenarios. Yoav Goldberg and Reut Tsarfaty in their paper'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing' propose a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.The data was taken from the Hebrew Treebank.They employed PCFG and used BitPar, an efficient general purpose parser,together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis and a data-driven morphological analyzer .Their model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.\n They proposed a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. They employed a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions which were not only theoretically clean and linguistically justified but also probabilistically appropriate and empirically sound. The overall performance of their joint framework demonstrated that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperformed upper bounds proposed by previous joint disambiguation systems and achieved segmentation and parsing results on a par with state-of-the-art standalone applications results. They showed better grammars to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones. Using a treebank grammar, the data-driven lexicon, and a linguistically motivated unknown-tokens handling technique their model outperformed previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far."
        ]
    },
    "N12-1052": {
        "abstract": "cross-lingual word clusters for direct transfer of linguistic structure abstract it has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. while previous work has focused primarily on english, we extend these results to other languages along two dimensions. first, we show that these results hold true for a number of languages across families. second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on english treebanks and transferred to foreign languages, can be reduced by up to 13%. when applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.",
        "conclusion": "conclusion in the first part of this study, we showed that word clusters induced from a simple class-based language model can be used to significantly improve on stateof-the-art supervised dependency parsing and ner for a wide range of languages and even across language families. although the improvements vary between languages, the addition of word cluster features never has a negative impact on performance. this result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models. given previous work on word clusters for various linguistic structure prediction tasks, these results are not too surprising. however, to our knowledge this is the first study to apply the same type of word cluster features across languages and tasks. in the second part, we provided two simple methods for inducing cross-lingual word clusters. the first method works by projecting word clusters, induced from monolingual data, from a source language to a target language directly via word alignments. the second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering. we then showed that by using these cross-lingual word clusters, we can significantly improve on direct transfer of discriminative models for both parsing and ner. as in the monolingual case, both types of cross-lingual word cluster features yield improvements across the board, with the more complex method providing a significantly larger improvement for ner. although the performance of transfer systems is still substantially below that of supervised systems, this research provides one step towards bridging this gap. further, we believe that it opens up an avenue for future work on multilingual clustering methods, cross-lingual feature projection and domain adaptation for direct transfer of linguistic structure.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P07-1051": {
        "abstract": "is the end of supervised parsing in sight? abstract how far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted? we present a new algorithm for unsupervised parsing using an all-subtrees model, termed u-dop*, which parses directly with packed forests of all binary trees. we train both on penn\u2019s wsj data and on the (much larger) nanc corpus, showing that u-dop* outperforms a treebank-pcfg on the standard wsj test set. while u-dop* performs worse than state-of-the-art supervised parsers on handannotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on europarl. we argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight.",
        "conclusion": "conclusion: future of supervised parsing in this paper we have shown that the accuracy of unsupervised parsing under u-dop* continues to grow when enlarging the training set with additional data. however, except for the simple treebank pcfg, u-dop* scores worse than supervised parsers if evaluated on hand-annotated data. at the same time u-dop* significantly outperforms the supervised dop* if evaluated in a practical application like mt. we argued that this can be explained by the fact that u-dop learns both constituents and (non-syntactic) phrases while supervised parsers learn constituents only. what should we learn from these results? we believe that parsing, when separated from a task-based application, is mainly an academic exercise. if we only want to mimick a treebank or implement a linguistically motivated grammar, then supervised, grammar-based parsers are preferred to unsupervised parsers. but if we want to improve a practical application with a syntaxbased language model, then an unsupervised parser like u-dop* might be superior. the problem with most supervised (and semi-supervised) parsers is their rigid notion of constituent which excludes \u2018constituents\u2019 like the german ich m\u00f6chte or the french il y a. instead, it has become increasingly clear that the notion of constituent is a fluid which may sometimes be in agreement with traditional syntax, but which may just as well be in opposition to it. any sequence of words can be a unit of combination, including noncontiguous word sequences like closest x to y. a parser which does not allow for this fluidity may be of limited use as a language model. since supervised parsers seem to stick to categorical notions of constituent, we believe that in the field of syntax-based language models the end of supervised parsing has come in sight.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P13-2112": {
        "abstract": "simpler unsupervised pos tagging with bilingual projections abstract figure 1: overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of known tokens for italian (left) and dutch (right). table 2 shows results for our seed model, self training and revision, and the results reported by das and petrov. self training and revision improve the accuracy for every language over the seed model, and gives an average improvement of roughly two percentage points. the average accuracy of self training and revision is on par with that reported by das and petrov. on individual languages, self training and revision and the method of das and petrov are split \u2014 each performs better on half of the cases. interestingly, our method achieves higher accuracies on germanic languages \u2014 the family of our source language, english \u2014 while das and petrov perform better on romance languages. this might be because our model relies on alignments, which might be more accurate for more-related languages, whereas das and petrov additionally rely on label propagation. compared to das and petrov, our model performs poorest on italian, in terms of percentage point difference in accuracy. figure 1 (left panel) shows accuracy, accuracy on known words, accuracy on unknown words, and proportion of known tokens for each iteration of our model for italian; iteration 0 is the seed model, and iteration 31 is the final model. our model performs poorly on unknown words as indicated by the low accuracy on unknown words, and high accuracy on known words compared to the overall accuracy. the poor performance on unknown words is expected because we do not use any language-specific rules to handle this case. moreover, on average for the final model, approximately 10% of the test data tokens are unknown. one way to improve the performance of our tagger might be to reduce the proportion of unknown words by using a larger training corpus, as das and petrov did. we examine the impact of self-training and revision over training iterations. we find that for all languages, accuracy rises quickly in the first 5\u20136 iterations, and then subsequently improves only slightly. we exemplify this in figure 1 (right panel) for dutch. (findings are similar for other languages.) although accuracy does not increase much in later iterations, they may still have some benefit as the vocabulary size continues to grow. 6 conclusion we have proposed a method for unsupervised pos tagging that performs on par with the current stateof-the-art (das and petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based hmm). complexity of our algorithm is to that of das and petrov 637 where the size of training we our code are available for in future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy. given the improvements of our model over that of das and petrov on languages from the same family as our source language, and the observation of snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language. using our final model with unsupervised hmm methods might improve the final performance too, i.e. use our final model as the initial state for hmm, then experiment with different inference algorithms such as expectation maximization (em), variational bayers (vb) or gibbs gao and johnson (2008) compare em, vb and gs for unsupervised english pos tagging. in many cases, gs outperformed other methods, thus we would like to try gs first for our model. 7 acknowledgements this work is funded by erasmus mundus european masters program in language and communication technologies (em-lct) and by the czech science foundation (grant no. p103/12/g084). we would like to thank prokopis prokopidis for providing us the greek treebank and antonia marti for the spanish conll 06 dataset. finally, we thank siva reddy and spandana gella for many discussions and suggestions. references thorsten brants. 2000. tnt: a statistical part-oftagger. in of the sixth conference on applied natural language processing pages 224\u2013231. seattle, washington, usa. dipanjan das and slav petrov. 2011. unsupervised part-of-speech tagging with bilingual projections. in of re-implemented label propagation from das and petrov (2011). it took over a day to complete this step on an eight core intel xeon 3.16ghz cpu with 32 gb ram, but only 15 minutes for our model. in fact have tried em, but it did not help. the overall performance dropped slightly. this might be because selftraining with revision already found the local maximal point. the 49th annual meeting of the association for computational linguistics: human language - volume 1 (acl pages 600\u2013609. portland, oregon, usa. pascal denis and benoit sagot. 2009. coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less effort. in of the 23rd pacific asia conference on language, information",
        "conclusion": "conclusion we have proposed a method for unsupervised pos tagging that performs on par with the current stateof-the-art (das and petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based hmm). the complexity of our algorithm is o(nlogn) compared to o(n2) for that of das and petrov (2011) where n is the size of training data.3 we made our code are available for download.4 in future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy. given the improvements of our model over that of das and petrov on languages from the same family as our source language, and the observation of snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language. using our final model with unsupervised hmm methods might improve the final performance too, i.e. use our final model as the initial state for hmm, then experiment with different inference algorithms such as expectation maximization (em), variational bayers (vb) or gibbs sampling (gs).5 gao and johnson (2008) compare em, vb and gs for unsupervised english pos tagging. in many cases, gs outperformed other methods, thus we would like to try gs first for our model.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W06-3120": {
        "abstract": "talp phrase-based statistical translation system for european language pairs abstract this paper reports translation results for the \u201cexploiting parallel texts for statistical machine translation\u201d (hlt-naacl workshop on parallel texts 2006). we have studied different techniques to improve the standard phrase-based translation system. mainly we introduce two reordering approaches and add morphological information.",
        "conclusion": "conclusions reordering is important when using a phrasebased system. although local reordering is supposed to be included in the phrase structure, performing local reordering improves the translation quality. in fact, local reordering, provided by the reordering approaches, allows for those generalizations which phrases could not achieve. reordering in the deen task is left as further work.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W05-1518": {
        "abstract": "improving parsing accuracy by combining diverse dependency parsers abstract this paper explores the possibilities of improving parsing results by combining outputs of several parsers. to some extent, we are porting the ideas of henderson and brill (1999) to the world of dependency structures. we differ from them in exploring context features more deeply. all our experiments were conducted on czech but the method is language-independent. we were able to significantly improve over the best parsing result for the given setting, known so far. moreover, our experiments show that even parsers far below the state of the art can contribute to the total improvement.",
        "conclusion": "conclusion we have tested several approaches to combining of dependency parsers. accuracy-aware voting of the four best parsers turned out to be the best method, as it significantly improved the accuracy of the best component from 85.0 to 87.0 % (13 % error also alternatively called unrestricted rate reduction). the unbalanced voting lead to the precision as high as 90.2 %, while the f-measure of 87.3 % outperforms the best result of balanced voting (87.0). at the same time, we found that employing context to this task is very difficult even with a wellknown and widely used machine-learning approach. the methods are language independent, though the amount of accuracy improvement may vary according to the performance of the available parsers. although voting methods are themselves not new, as far as we know we are the first to propose and evaluate their usage in full dependency parsing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W10-1401": {
        "abstract": "statistical parsing of morphologically rich languages (spmrl) what how and whither abstract the term morphologically rich languages(mrls) refers to languages in which signif icant information concerning syntactic units and relations is expressed at word-level. thereis ample evidence that the application of read ily available statistical parsing models to suchlanguages is susceptible to serious performance degradation. the first workshop on statistical parsing of mrls hosts a variety of contributions which show that despite languagespecific idiosyncrasies, the problems associ ated with parsing mrls cut across languagesand parsing frameworks. in this paper we re view the current state-of-affairs with respectto parsing mrls and point out central challenges. we synthesize the contributions of re searchers working on parsing arabic, basque, french, german, hebrew, hindi and korean to point out shared solutions across languages. the overarching analysis suggests itself as a source of directions for future investigations.",
        "conclusion": "conclusion this paper presents the synthesis of 11 contributionsto the first workshop on statistical parsing for mor phologically rich languages. we have shown that architectural, representational, and estimation issuesassociated with parsing mrls are found to be chal lenging across languages and parsing frameworks. the use of morphological information in the nongold-tagged input scenario is found to cause sub stantial differences in parsing performance, and inthe kind of morphological features that lead to per formance improvements.whether or not morphological features help pars ing also depends on the kind of model in which they are embedded, and the different ways they aretreated within. furthermore, sound statistical esti mation methods for morphologically rich, complexlexica, turn out to be crucial for obtaining good pars ing accuracy when using general-purpose models and algorithms. in the future we hope to gain better understanding of the common pitfalls in, and novel solutions for, parsing morphologically ambiguousinput, and to arrive at principled guidelines for selecting the model and features to include when pars ing different kinds of languages. such insights may be gained, among other things, in the context of more morphologically-aware shared parsing tasks. acknowledgements the program committee would like to thank naacl for hosting the workshop and sigparsefor their sponsorship. we further thank inria al page team for their generous sponsorship. we are finally grateful to our reviewers and authors for their dedicated work and individual contributions.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P13-1007": {
        "abstract": "plurality, negation, and quantification:towards comprehensive quantifier scope disambiguation abstract recent work on statistical quantifier scope disambiguation (qsd) has improved upon earlier work by scoping an arbitrary number and type of noun phrases. no corpusbased method, however, has yet addressed qsd when incorporating the implicit universal of plurals and/or operators such as negation. in this paper we report early, though promising, results for automatic qsd when handling both phenomena. we also present a general model for learning to build partial orders from a set of pairpreferences. we give an algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice. finally, we significantly improve the performance of the previous model using a rich set of automatically generated features.",
        "conclusion": "summary and future work we develop the first statistical qsd model addressing the interaction of quantifiers with negation and the implicit universal of plurals, defining a baseline for this task on quantext data (manshadi et al., 2012). in addition, our work improves upon manshadi and allen (2011a)\u2019s work by (approximately) optimizing a well justified criterion, by using automatically generated features instead of hand-annotated dependencies, and by boosting the performance by a large margin with the help of a rich feature vector. this work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P04-1013": {
        "abstract": "discriminative training of a neural network statistical parser abstract discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing. one problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem. we show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model. we present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative. the latter model outperforms the previous two, achieving state-ofthe-art levels of performance (90.1% f-measure on constituents).",
        "conclusion": "conclusions this article has investigated the application of discriminative methods to broad coverage natural language parsing. we distinguish between two different ways to apply discriminative methods, one where the probability model is changed to a discriminative one, and one where the probability model remains generative but the training method optimizes a discriminative criteria. we find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. performance of the latter model on the standard test set achieves 90.1% f-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (bod, 2003). this paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model. this training method successfully satisfies the conflicting constraints that it be computationally tractable and that it be a good approximation to the theoretically optimal method. this approach contrasts with previous approaches to scaling up discriminative methods to broad coverage natural language parsing, which have parameterizations which depart substantially from the successful previous generative models of parsing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-3160": {
        "abstract": "large-margin learning optimization strategies for online large-margin learning in machine translation abstract the introduction of large-margin based dis criminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process. by removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features. however, these methods have not yet met with wide-spread adoption. thismay be partly due to the perceived complex ity of implementation, and partly due to the lack of standard methodology for applying these methods to mt. this papers aims to shedlight on large-margin learning for mt, explic itly presenting the simple passive-aggressivealgorithm which underlies many previous ap proaches, with direct application to mt, andempirically comparing several widespread op timization strategies.",
        "conclusion": "discussion although the performance of the two strategies iscompetitive on the evaluation sets, this does not re lay the entire story. for a more complete view of the differences between optimization strategies, we turn to figures 1-6. figure 1 and 2 present thecomparison of performance on the nt08 development set for cs-en and fr-en, respectively, when using lu/mc to select the oracle and prediction ver sus m?c selection. m?c is indicated with a solid black line, while lu/mc is a dotted red line. the corpus-level oracle and prediction bleu scores at each iteration are indicated with error bars around each point, using solid lines for m?c and dotted lines for lu/mc. as can be seen in figure 1, while optimizing with m?c is stable and smooth, where we converge on our optimum after several iterations, optimizing with lu/mc is highly unstable. this is at least in part due to the wide range in bleu scores for the oracle and prediction, which are in the range of 10 bleu points higher or lower than the current model best. on the contrary, the range of bleu scores for the m?c optimizer is on the order of 2 bleu points, leading to more gradual changes. we see a similar, albeit slightly less pronounced behavior on fr-en in figure 2. m?c optimization is once again smooth, and converges quickly, with a small range for the oracle and prediction scores around the model best. lu/mc remains unstable, oscillating up to 2 bleu points between iterations. figures 3-6 compare the different optimization strategies further. in figures 3 and 5, we use m-cas the oracle, and show performance on the develop ment set while using the three prediction selection strategies, m+c with a solid blue line, pb with a dotted green line, and mc with a dashed red line. error bars indicate the oracle and prediction bleu scores for each pairing as before. in all three cases, the oracle bleu score is in about the same range,as expected, since all are using the same oracle se lection strategy. we can immediately observe that pb has no error bars going down, indicating that the pb method for selecting the prediction keeps pace with the model best at each iteration. on the other hand, mc selection also stands out, since it is the only one with a large drop in prediction bleu score. crucially, all learners are stable, and move toward convergence smoothly, which serves to validate our earlier observation that m-c oracle selection can bepaired with any prediction selection strategy and op timize effectively. in both cs-en and fr-en, we can observe that m?c performs the best. in figures 4 and 6, we use lu as the oracle, andshow performance using the three prediction selec tion strategies, with each line representing the same strategy as described above. the major difference, which is immediately evident, is that the optimizers are highly unstable. the only pairing which showssome stability is lu/mc, with both the other predic 486 0.05 0.07 0.09 0.11 0.13 0.15 0.17 0.19 0.21 0.23 0.25 1 2 3 4 5 6 7 8 9 10 ble u iteration figure 3: comparison of performance on development set for cs-en of the three prediction selection strategies when using m-c selection as oracle. 0.05 0.1 0.15 0.2 0.25 0.3 0.35 1 2 3 4 5 6 7 8 9 10 ble u iteration figure 4: comparison of performance on development set for cs-en of the three prediction selection strategies when using lu selection as oracle. 0.05 0.1 0.15 0.2 0.25 0.3 1 2 3 4 5 6 7 8 9 10 ble u iteration figure 5: comparison of performance on development set for fr-en of the three prediction selection strategies when using m-c selection as oracle. 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 1 2 3 4 5 6 7 8 9 10 ble u iteration figure 6: comparison of performance on development set for fr-en of the three prediction selection strategies when using lu selection as oracle. tion selection methods, pb and m+c significantly underperforming it.given that the translation performance of optimiz ing the loss functions represented by lu/mc and m?c selection is comparable on the evaluation sets for fr-en and cs-en, it may be premature to make a general recommendation for one over the other. however, taking the unstable nature of lu/mc intoaccount, the extent of which may depend on the tun ing set, as well as other factors which need to befurther examined, the current more prudent alterna tive is selecting the oracle and prediction pair based on m?c. conclusion in this paper, we strove to elucidate aspects of large margin structured learning with concrete application to the mt setting. towards this goal, we presented the mira passive-aggressive algorithm, which can be used directly to effectively tune a statistical mt system with millions of parameters, in the hope that some confusion surrounding mira-based methods may be cleared, and more mt researchers can adoptit for their own use. we then used the presented al gorithm to empirically compare several widespreadloss functions and strategies for selecting hypothe ses for optimization. we showed that although thereare two competing strategies with comparable per formance, one is an unstable learner, and before weunderstand more regarding the nature of the insta bility, the preferred alternative is to use m?c as the hypothesis pair in optimization. acknowledgments we would like to thank the anonymous reviewers for their comments. the author is supported by the department of defense through the nationaldefense science and engineering graduate fellow 487ship. any opinions, findings, conclusions, or rec ommendations expressed are the author?s and do not necessarily reflect those of the sponsors.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W04-1505": {
        "abstract": "fast deep-linguistic statistical dependency parsing abstract we present and evaluate an implemented statistical minimal parsing strategy exploiting dg charateristics to permit fast, robust, deeplinguistic analysis of unrestricted text, and compare its probability model to (collins, 1999) and an adaptation, (dubey and keller, 2003). we show that dg allows for the expression of the majority of english ldds in a context-free way and offers simple yet powerful statistical models.",
        "conclusion": "conclusions we have discussed how dg allows the expression of the majority of ldds in a contextfree way and shown that dg allows for simple but powerful statistical models. an evaluation shows that the performance of its implementation is state-of-the-art10. its parsing speed of about 300,000 words per hour is very good for a deep-linguistic parser and makes it fast enough for unlimited application.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N06-1045": {
        "abstract": "a better n-best list: practical determinization of weighted finite tree automata abstract ranked lists of output trees from syntactic statistical nlp applications frequently contain multiple repeated entries. this redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes. is due to nondeterminism in the weighted automata that produce the results. we introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees. we also demonstrate our algorithm\u2019s effectiveness on two large-scale tasks.",
        "conclusion": "conclusion we have shown that weighted determinization is useful for recovering -best unique trees from a weighted forest. as summarized in figure 9, the number of repeated trees prior to determinization was typically very large, and thus determinization is critical to recovering true tree weight. we have improved evaluation scores by incorporating the presented algorithm into our mt work and we believe that other nlp researchers working with trees can similarly benefit from this algorithm. further advances in determinization will provide additional benefit to the community. the translation system detailed here is a string-to-tree system, and the determinization algorithm returns the -best unique trees from a packed forest. users of mt systems are generally interested in the string yield of those trees, and not the trees per se. thus, an algorithm that can return the -best unique strings from a packed forest would be a useful extension. we plan for our weighted determinization algorithm to be one component in a generally available tree automata package for intersection, composition, training, recognition, and generation of weighted and unweighted tree automata for research tasks such as the ones described above.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E09-1034": {
        "abstract": "parsing mildly non-projective dependency structures abstract we present parsing algorithms for various mildly non-projective dependency formalisms. in particular, algorithms are presented for: all well-nested structures of degree at most with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power; all well-nested structures with degree bounded by any constant and a new class of structures with gap deup to includes some ill-nested structures. the third case includes all the degree in a number of dependency treebanks.",
        "conclusion": "conclusions and future work we have defined a parsing algorithm for wellnested dependency structures with bounded gap degree. in terms of computational complexity, this algorithm is comparable to the best parsers for related constituency-based formalisms: when the gap degree is at most 1, it runs in o(n7), like the fastest known parsers for ltag, and can be made o(n6) if we use unlexicalised dependencies. when the gap degree is greater than 1, the time complexity goes up by a factor of n2 for each extra unit of gap degree, as in parsers for coupled context-free grammars. most of the non-projective sentences appearing in treebanks are well-nested and have a small gap degree, so this algorithm directly parses the vast majority of the non-projective constructions present in natural languages, without requiring the construction of a constituency grammar as an intermediate step. additionally, we have defined a set of structures for any gap degree k which we call mildly ill-nested. this set includes ill-nested structures verifying certain conditions, and can be parsed in o(n3k+4) with a variant of the parser for wellnested structures. the practical interest of mildly ill-nested structures can be seen in the data obtained from several dependency treebanks, showing that all of the ill-nested structures in them are mildly ill-nested for their corresponding gap degree. therefore, our o(n3k+4) parser can analyse all the gap degree k structures in these treebanks. the set of mildly ill-nested structures for gap degree k is defined as the set of structures that have a binarisation of gap degree at most k. this definition is directly related to the way the mgk parser works, since it implicitly finds such a binarisation. an interesting line of future work would be to find an equivalent characterisation of mildly ill-nested structures which is more grammar-oriented and would provide a more linguistic insight into these structures. another research direction, which we are currently working on, is exploring how variants of the mgk parser\u2019s strategy can be applied to the problem of binarising lcfrs (g\u00b4omezrodriguez et al., 2009).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P10-1044": {
        "abstract": "a latent dirichlet allocation method for selectional preferences abstract computation of preferthe admissible argument values for a relation, is a well-known nlp task with applicability. we present which utilizes linklda (erosheva et al., 2004) to model selectional preferences. by simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation\u2019s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (erk, 2007). we also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement pantel system (pantel et al.,",
        "conclusion": "conclusions and future work we have presented an application of topic modeling to the problem of automatically computing selectional preferences. our method, lda-sp, learns a distribution over topics for each relation while simultaneously grouping related words into these topics. this approach is capable of producing human interpretable classes, however, avoids the drawbacks of traditional class-based approaches (poor lexical coverage and ambiguity). lda-sp achieves state-of-the-art performance on predictive tasks such as pseudo-disambiguation, and filtering incorrect inferences. because lda-sp generates a complete probabilistic model for our relation data, its results are easily applicable to many other tasks such as identifying similar relations, ranking inference rules, etc. in the future, we wish to apply our model to automatically discover new inference rules and paraphrases. finally, our repository of selectional preferences for 10,000 relations is available at http://www.cs.washington.edu/ research/ldasp.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P06-1109": {
        "abstract": "an all-subtrees approach to unsupervised parsing abstract we investigate generalizations of the allsubtrees &quot;dop&quot; approach to unsupervised parsing. unsupervised dop models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees. we will test both a relative frequency estimator for unsupervised dop and a maximum likelihood estimator which is known to be statistically consistent. we report state-ofthe-art results on english (wsj), german (negra) and chinese (ctb) data. to the best of our knowledge this is the first paper which tests a maximum likelihood estimator for dop on the wall street journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank pcfg).",
        "conclusion": "7 conclusion: is the end of supervised parsing in sight? now that we have outperformed a well-known supervised parser by an unsupervised one, we may raise the question as to whether the end of supervised nlp comes in sight. all supervised parsers are reaching an asymptote and further improvement does not seem to come from more hand-annotated data but by adding unsupervised or semi-unsupervised techniques (cf. mcclosky et al. 2006). thus if we modify our question as: does the exclusively supervised approach to parsing come to an end, we believe that the answer is certainly yes. yet we should neither rule out the possibility that entirely unsupervised methods will in fact surpass semi-supervised methods. the main problem is how to quantitatively compare these different parsers, as any evaluation on handannotated data (like the penn treebank) will unreasonably favor semi-supervised parsers. there is thus is a quest for designing an annotationindependent evaluation scheme. since parsers are becoming increasingly important in applications like syntax-based machine translation and structural language models for speech recognition, one way to go would be to compare these different parsing methods by isolating their contribution in improving a concrete nlp system, rather than by testing them against gold standard annotations which are inherently theory-dependent. the initially disappointing results of inducing trees entirely from raw text was not so much due to the difficulty of the bootstrapping problem per se, but to (1) the poverty of the initial models and (2) the difficulty of finding theoryindependent evaluation criteria. the time has come to fully reappraise unsupervised parsing models which should be trained on massive amounts of data, and be evaluated in a concrete application. there is a final question as to how far the dop approach to unsupervised parsing can be stretched. in principle we can assign all possible syntactic categories, semantic roles, argument structures etc. to a set of given sentences and let the statistics decide which assignments are most useful in parsing new sentences. whether such a massively maximalist approach is feasible can only be answered by empirical investigation in due time.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-2121": {
        "abstract": "getting the most out of transition-based dependency parsing abstract this paper suggests two ways of improving transition-based, non-projective dependency parsing. first, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. the new addition to the algorithm shows a clear advantage in parsing speed. the bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set.",
        "conclusion": "conclusion and future work we present two ways of improving transition-based, non-projective dependency parsing. the additional transition gives improvements to both parsing speed and accuracy, showing a linear time parsing speed with respect to sentence length. the bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-the-art performance with respect to other parsing approaches. in the future, we will test the robustness of these approaches in more languages.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N12-1090": {
        "abstract": "translation-based projection for multilingual coreference resolution abstract to build a coreference resolver for a new language, the typical approach is to first coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques. however, the high cost associated with manually coreference-annotating documents needed by a supervised approach makes it difficult to deploy coreference technologies across a large number of natural languages. to alleviate this corpus annotation bottleneck, we examine a translation-based projection approach to multilingual coreference resolution. experimental results on two target languages demonstrate the promise of our approach.",
        "conclusion": "conclusions and future work we explored the under-investigated yet challenging task of performing coreference resolution for a language for which we have no coreference-annotated data and no linguistic knowledge of the language. our translation-based projection approach has the flexibility to exploit any available knowledge about the target language. in experiments with spanish and italian, we obtained promising results: our approach achieved around 90% of the performance of a supervised resolver when only a mention extractor for the target language was available. we believe that this approach has the potential to allow coreference technologies to be deployed across a larger number of languages than is currently possible, and that this is just the beginning of a new line of work. to gain additional insights into our approach, we plan to pursue several directions. first, we will isolate the impact of each factor that adversely affects its performance, including errors in projection, translation, and coreference resolution in the resource-rich language. second, we will perform an empirical comparison of two approaches to projecting coreference annotations, our translation-based approach and camargo de souza and orasan\u2019s (2011) approach, where annotations are projected via a parallel corpus. third, rather than translate from the target to the source language, we will examine whether it is better to translate all the coreference-annotated data available in the source language to the target language, and train a coreference model for the target language on the translated data. fourth, since the success of our projection approach depends heavily on the accuracies of machine translation as well as coreference resolution in the source language, we will determine whether their accuracies can be improved via an ensemble approach, where we employ multiple mt engines and multiple coreference resolvers. finally, we plan to employ our approach to alleviate the corpus-annotation bottleneck, specifically by using the annotated data it produces to augment the manual coreference annotations that capture the specific properties of the target language.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E09-1018": {
        "abstract": "em works for pronoun anaphora resolution abstract we present an algorithm for pronounanaphora (in english) that uses expectation maximization (em) to learn virtually all of its parameters in an unsupervised fashion. while em frequently fails to find good models for the tasks to which it is set, in this case it works quite well. we have compared it to several systems available on the web (all we have found so far). our program significantly outperforms all of them. the algorithm is fast and robust, and has been made publically available for downloading.",
        "conclusion": "conclusion we have presented a generative model of pronounanaphora in which virtually all of the parameters are learned by expectation maximization. we find it of interest first as an example of one of the few tasks for which em has been shown to be effective, and second as a useful program to be put in general use. it is, to the best of our knowledge, the best-performing system available on the web. to down-load it, go to (to be announced). the current system has several obvious limitation. it does not handle cataphora (antecedents occurring after the pronoun), only allows antecedents to be at most two sentences back, does not recognize that a conjoined np can be the antecedent of a plural pronoun, and has a very limited grasp of pronominal syntax. perhaps the largest limitation is the programs inability to recognize the speaker of a quoted segment. the result is a very large fraction of first person pronouns are given incorrect antecedents. fixing these problems would no doubt push the system\u2019s performance up several percent. however the most critical direction for future research is to push the approach to handle full np anaphora. besides being of the greatest importance in its own right, it would also allow us to add one piece of information we currently neglect in our pronominal system \u2014 the more times a document refers to an entity the more likely it is to do so again.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1099": {
        "abstract": "mixing multiple translation models in statistical machine translation abstract statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. we propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. in this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.",
        "conclusion": "conclusion & future work in this paper, we presented a new approach for domain adaptation using ensemble decoding. in this approach a number of mt systems are combined at decoding time in order to form an ensemble model. the model combination can be done using various mixture operations. we showed that this approach can gain up to 2.2 bleu points over its concatenation baseline and 0.39 bleu points over a powerful mixture model. future work includes extending this approach to use multiple translation models with multiple language models in ensemble decoding. different mixture operations can be investigated and the behaviour of each operation can be studied in more details. we will also add capability of supporting syntax-based ensemble decoding and experiment how a phrase-based system can benefit from syntax information present in a syntax-aware mt system. furthermore, ensemble decoding can be applied on domain mixing settings in which development sets and test sets include sentences from different domains and genres, and this is a very suitable setting for an ensemble model which can adapt to new domains at test time. in addition, we can extend our approach by applying some of the techniques used in other system combination approaches such as consensus decoding, using n-gram features, tuning using forest-based mert, among other possible extensions.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W99-0613": {
        "abstract": "unsupervised models for named entity classification collins abstract this paper discusses the use of unlabeled examples for the problem of named entity classification. a large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- however, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules. the approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. we present two algorithms. the first method uses a similar algorithm to that of (yarowsky 95), with modifications motivated by (blum and mitchell 98). the second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (blum and mitchell 98).",
        "conclusion": "conclusions unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules. in addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (blum and mitchell 98). the method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function. we are currently exploring other methods that employ similar ideas and their formal properties. future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them. the contextual rules are restricted and may not be applicable to every example, but the spelling rules are generally applicable and should have good coverage. the problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.",
        "citations": [],
        "summary": [
            "This paper discusses the use of unlabeled examples for the problem of named entity classification.The task is to learn a function from an in-\nput string (proper name) to its type.The approach uses both spelling and contextual rules. A spelling rule might be a simple look-up for the string or a rule that looks at words within a string.They introduced a new algorithm for learning from unlabeled examples, which they called DL-\nCo'Train (DL stands for decision list, the term Cot r a i n is taken from (Blum and Mitchell 98)).It also presents a boosting-like framework that builds on ideas from (Blum and Mitchell 98). The method uses a \"soft\" measure of the agreement between two classifiers as an objective function;it describes an algorithm which directly optimizes this function.\n\n\n In this paper the author aims at discussing the use of unlabeled examples for the problem of named entity classification. Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labelled training examples. The task is to learn a function from an input string to its type, which we will assume to be one of the categories Person, Organization, or Location. Supervised methods have been applied quite successfully to the full MUC named-entity task. The algorithm can be viewed as heuristically optimizing an objective function; empirically it is shown to be quite successful in optimizing this criterion. The AdaBoost algorithm was developed for supervised learning. They are currently exploring other methods that employ similar ideas and their formal properties. Future work should also extend the approach to build a complete named entity extractor, a method that pulls proper names from text and then classifies them. Collins and Singer, in this paper, discussed the use of unlabeled examples for the problem of named entity classification. They showed that the use of data could reduce the requirements for supervision to just 7 simple rules. The approach gained leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appeared were sufficient to determine its type. They presented two algorithms. The first method used a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extended ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98) and boosted the objective functions. This paper talks about Unsupervised Models for Named Entity Classification. A large number of rules are needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. Here we present two algorithms. The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98). The second algorithm builds on a boosting algorithm called AdaBoost. The AdaBoost algorithm was developed for supervised learning. We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories. The contextual rules are restricted and may not be applicable to every example, but the spelling rules are generally applicable and should have good coverage. "
        ]
    },
    "E03-1005": {
        "abstract": "an efficient implementation of a new dop model abstract two apparently opposing dop models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. this paper proposes an integration of the two models which outperforms each of them separately. together with a pcfgreduction of dop we obtain improved accuracy and efficiency on the wall street journal treebank our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per wsj sentence.",
        "conclusion": "conclusion as our second experimental goal, we compared the models sl-dop and ls-dop explained in section 3.2. recall that for n=1, sl-dop is equal to the pcfg-reduction of bod (2001) (which we also called likelihood-dop) while ls-dop is equal to simplicity-dop. table 2 shows the results for sentences 100 words for various values of n. note that there is an increase in accuracy for both sl-dop and ls-dop if the value of n increases from 1 to 12. but while the accuracy of sl-dop decreases after n=14 and converges to simplicity dop, the accuracy of ls-dop continues to increase and converges to likelihood-dop. the highest accuracy is obtained by sl-dop at 12 n 14: an lp of 90.8% and an lr of 90.7%. this is roughly an 11% relative reduction in error rate over charniak (2000) and bods pcfg-reduction reported in table 1. compared to the reranking technique in collins (2000), who obtained an lp of 89.9% and an lr of 89.6%, our results show a 9% relative error rate reduction. while sl-dop and ls-dop have been compared before in bod (2002), especially in the context of musical parsing, this paper presents the the dop approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. while the first feature has been generally adopted in statistical nlp, the second feature has for a long time been a serious bottleneck, as it results in exponential processing time when the most probable parse tree is computed. this paper showed that a pcfg-reduction of dop in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the wall street journal treebank. this paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in collins 1999 and charniak 2000).",
        "citations": [],
        "summary": [
            "Rens Bod in his paper 'An Efficient Implementation of a New DOP Model' presents a DOP approach which is based on two distinctive features:first,the use of corpus fragments rather than grammar rules, and second,the use of arbitrarily large fragments rather than restricted ones.Together with a PCFG-reduction of DOP he obtains improved accuracy and efficiency on the Wall Street Journal treebank.The results show an 11%\nrelative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.The highest accuracy is obtained by SL-DOP(Simplicity-DOP) at 12 smaller than or equal to n smaller than or equal to 14: an LP(Label precision) of 90.8% and an LR(Label Recall) of 90.7%.The accuracy of SL-DOP decreases after n=14 and converges to Simplicity-DOP but the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.\n\n\n In this paper, Ren Bods proposes an integration of two existing DOP models (one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank)which outperforms each of them separately. They showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree resulted in fast processing times and very competitive accuracy on the Wall Street Journal treebank. They also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic dependencies. The results showed an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence. In this paper the author aims at proposing an integration of the two models which outperforms each of them separately. Together with a PCFG reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal Treebank.  It also presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. They show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part, the author extends his experiments with a new notion of the best parse tree. The DOP approach is based on two distinctive features as the first being the use of corpus fragments rather than grammar rules, and the other as the use of arbitrarily large fragments rather than restricted ones. Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence."
        ]
    },
    "P13-2083": {
        "abstract": "a structured distributional semantic model for event co-reference abstract in this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. we argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. we test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.",
        "conclusion": "conclusion and future work we outlined an approach that introduces structure into distributed semantic representations gives us an ability to compare the identity of two representations derived from supposedly semantically identical phrases with different surface realizations. we employed the task of event coreference to validate our representation and achieved significantly higher predictive accuracy than several baselines. in the future, we would like to extend our model to other semantic tasks such as paraphrase detection, lexical substitution and recognizing textual entailment. we would also like to replace our syntactic relations to semantic relations and explore various ways of dimensionality reduction to solve this problem.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N12-1086": {
        "abstract": "graph-based lexicon expansion with sparsity-inducing penalties abstract we present novel methods to construct compact natural language lexicons within a graph based semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data. to achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in gaussian and entropic pairwise markovnetworks constructed from labeled and unla beled data. sparse measures are desirable forhigh-dimensional multi-class learning problems such as the induction of labels on natu ral language types, which typically associate with only a few labels. compared to standard graph-based learning methods, for two lexicon expansion problems, our approach producessignificantly smaller lexicons and obtains bet ter predictive performance.",
        "conclusion": "conclusion we have presented a family of graph-based ssl objective functions that incorporate penalties encour aging sparse measures at each graph vertex. ourmethods relax the oft-used assumption that the measures at each vertex form a normalized probabil ity distribution, making optimization and the use ofcomplex penalties easier than prior work. optimiza tion is also easy when there are additional terms in a graph objective suited to a specific problem; ourgeneric optimizer would simply require the compu tation of new partial derivatives, unlike prior workthat required specialized techniques for a novel ob jective function. finally, experiments on two natural language lexicon learning problems show that our methods produce better performance with respect to state-of-the-art graph-based ssl methods, and also result in much smaller lexicons. acknowledgments we thank andre? martins, amar subramanya, and partha talukdar for helpful discussion during the progress of thiswork and the three anonymous reviewers for their valuable feedback. this research was supported by qatar na tional research foundation grant nprp 08-485-1-083, google?s support of the worldly knowledge project, andteragrid resources provided by the pittsburgh supercom puting center under nsf grant number tg-dbs110003. 685",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W08-0406": {
        "abstract": "syntactic reordering integrated with phrase-based smt abstract we present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based smt. this is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules. in decoding, the alternatives are scored based on the output word order, not the order of the input. unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based smt. on an english- danish task, we achieve an absolute improvement in translation quality of 1.1 % bleu. manual evaluation supports the claim that the present approach is significantly superior to previous approaches.",
        "conclusion": "conclusion and future plans we have described a novel approach to word reordering in smt, which successfully integrates syntactically motivated reordering in phrase-based smt. this is achieved by reordering the input string, but scoring on the output string. as opposed to previous approaches, this neither biases against phrase internal nor external reorderings. we achieve an absolute improvement in translation quality of 1.1 % bleu. a result that is supported by manual evaluation, which shows that the spto approach is significantly superior to previous approaches. in the future, we plan to apply this approach to english-arabic translation. we expect greater gains, due to the higher need for reordering between these less-related languages. we also want to examine the relation between word alignment method and the extracted rules and the relationship between reordering and word selection. finally, a limitation of the current experiments is that they only allow rule-based external reorderings. since the spto scoring is not tied to a source reordering approach, we want to examine the effect of simply adding it as an additional parameter to the baseline psmt system. this way, all external reorderings are made possible, but only the rule-supported ones get promoted.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P09-2003": {
        "abstract": "an earley parsing algorithm for range concatenation grammars abstract we present a cyk and an earley-style algorithm for parsing range concatenation grammar (rcg), using the deductive parsing framework. the characteristic property of the earley parser is that we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible. experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart.",
        "conclusion": "conclusion and future work we have presented a new cyk and earley parsing algorithms for the full class of rcg. the crucial difference between previously proposed topdown rcg parsers and the new earley-style algorithm is that while the former compute all clause instantiations during predict operations, the latter 4of course, the use of constraints makes comparisons between items more complex and more expensive which means that for an efficient implementation, an integer-based representation of the constraints and adequate techniques for constraint solving are required. a(x) \u2192 \u03b5 \u2208 p with an instantiation \u03c8 satisfying hp, ci such that \u03c8(a(x)) = a(0) avoids this using a technique of dynamic updating of a set of constraints on range boundaries. experiments show that this significantly decreases the number of generated items, which confirms that range boundary constraint propagation is a viable method for a lazy computation of ranges. the earley parser could be improved by allowing to process the predicates of the righthand sides of clauses in any order, not necessarily from left to right. this way, one could process predicates whose range boundaries are better known first. we plan to include this strategy in future work.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N12-1049": {
        "abstract": "parsing time: learning to interpret time expressions abstract we present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference. while most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a comof time this is used to construct a parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity. in this way, we can employ a loosely supervised em-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework. we achieve an accuracy of 72% on an adapted tempeval-2 task \u2013 comparable to state of the art systems.",
        "conclusion": "conclusion we present a new approach to resolving temporal expressions, based on synchronous parsing of a fixed grammar with learned parameters and a compositional representation of time. the system allows for output which captures uncertainty both with respect to the syntactic structure of the phrase and the pragmatic ambiguity of temporal utterances. we also note that the approach is theoretically better adapted for phrases more complex than those found in tempeval-2. furthermore, the system makes very few language-specific assumptions, and the algorithm could be adapted to domains beyond temporal resolution. we hope to improve detection and explore system performance on multilingual and complex datasets in future work. acknowledgements the authors would like to thank valentin spitkovsky, david mcclosky, and angel chang for valuable discussion and insights. we gratefully acknowledge the support of the defense advanced research projects agency (darpa) machine reading program under air force research laboratory (afrl) prime contract no. fa8750-09-c-0181. any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of darpa, afrl, or the us government.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1013": {
        "abstract": "characterizing the errors of data-driven dependency parsing models abstract we present a comparative error analysisof the two dominant approaches in datadriven dependency parsing: global, exhaus tive, graph-based models, and local, greedy, transition-based models. we show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. this analysisleads to new directions for parser develop ment.",
        "conclusion": "conclusion we have presented a thorough study of the dif ference in errors made between global exhaustivegraph-based parsing systems (mstparser) and local greedy transition-based parsing systems (malt parser). we have shown that these differences can be quantified and tied to theoretical expectations of each model, which may provide insights leading to better models in the future. 130",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C10-1045": {
        "abstract": "better arabic parsing: baselines evaluations and analysis abstract in this paper, we offer broad insightinto the underperformance of arabic constituency parsing by analyzing the inter play of linguistic phenomena, annotationchoices, and model design. first, we identify sources of syntactic ambiguity under studied in the existing parsing literature. second, we show that although the pennarabic treebank is similar to other tree banks in gross statistical terms, annotation consistency remains problematic. third,we develop a human interpretable grammar that is competitive with a latent vari able pcfg. fourth, we show how to build better models for three different parsers.finally, we show that in application set tings, the absence of gold segmentation lowers parsing performance by 2?5% f1.",
        "conclusion": "conclusion by establishing significantly higher parsing baselines, we have shown that arabic parsing perfor mance is not as poor as previously thought, butremains much lower than english. we have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation er rors. with a human evaluation we also showed that atb inter-annotator agreement remains low relative to the wsj corpus. our results suggest that current parsing models would benefit frombetter annotation consistency and enriched anno tation in certain syntactic configurations. acknowledgments we thank steven bethard, evan rosen, and karen shiells for material contributions to this work. we are also grateful to markus dickinson, ali farghaly, nizar habash, seth kulick, david mccloskey, claude reichard, ryan roth, and reut tsarfaty for constructive discussions. the first author is supported by a national defense science and engineering graduate (ndseg) fellowship. this paper is based on work supported in part by darpa through ibm. the content does not necessarily reflect the views of the u.s. government, and no official endorsement should be inferred. 401",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "H05-1094": {
        "abstract": "composition of conditional random fields for transfer learning abstract many learning tasks have subtasks for which much training data exists. therefore, we wantto transfer learning from the old, general purpose subtask to a more specific new task, for which there is often less data. while work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old. specifically, we perform joint decoding ofseparately-trained sequence models, preserv ing uncertainty between the tasks and allowinginformation from the new task to affect predic tions on the old task. on two standard text data sets, we show that joint decoding outperforms cascaded decoding.",
        "conclusion": "conclusion in this paper we have shown that joint decoding improves transfer between interdependent nlp tasks, even when the old task is named-entity recognition, for which highly accurate systems exist. the rich features afforded by aconditional model allow the new task to influence the pre 753 dictions of the old task, an effect that is only possible with joint decoding. it is now common for researchers to publicly release trained models for standard tasks such as part-of-speechtagging, named-entity recognition, and parsing. this paper has implications for how such standard tools are pack aged. our results suggest that off-the-shelf nlp tools will need not only to provide a single-best prediction, but also to be engineered so that they can easily communicate distributions over predictions to models for higher-level tasks. acknowledgmentsthis work was supported in part by the center for intelligent in formation retrieval, in part by the central intelligence agency, the national security agency and national science foundation under nsf grants #iis-0326249 and #iis-0427594, and in part by the defense advanced research projects agency (darpa),through the department of the interior, nbc, acquisition ser vices division, under contract number nbchd030010. anyopinions, findings and conclusions or recommendations ex pressed in this material are the author(s) and do not necessarily reflect those of the sponsor.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E09-1038": {
        "abstract": "enhancing unlexicalized parsing performance using a wide coverage lexicon fuzzy tag-set mapping and em-hmm-based lexical probabilities abstract we present a framework for interfacing a pcfg parser with lexical information from an external resource following a different tagging scheme than the treebank. this is achieved by defining a stochastic mapping layer between the two resources. lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora. we show that this solution greatly enhances the performance of an unlexicalized hebrew pcfg parser, resulting in state-of-the-art hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens.",
        "conclusion": "conclusions we present a framework for interfacing a parser with an external lexicon following a different annotation scheme. unlike other studies (yang huang et al., 2005; szolovits, 2003) in which such interfacing is achieved by a restricted heuristic mapping, we propose a novel, stochastic approach, based on a layered representation. we show that using an external lexicon for dealing with rare lexical events greatly benefits a pcfg parser for hebrew, and that results can be further improved by the incorporation of lexical probabilities estimated in a semi-supervised manner using a wide-coverage lexicon and a large unannotated corpus. in the future, we plan to integrate this framework with a parsing model that is specifically crafted to cope with morphologically rich, free-word order languages, as proposed in (tsarfaty and sima\u2019an, 2008). apart from hebrew, our method is applicable in any setting in which there exist a small treebank and a wide-coverage lexical resource. for example parsing arabic using the arabic treebank and the buckwalter analyzer, or parsing english biomedical text using a biomedical treebank and the umls specialist lexicon.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C10-2096": {
        "abstract": "machine translation with lattices and forests abstract traditional 1-best translation pipelinessuffer a major drawback: the errors of 1 best outputs, inevitably introduced by each module, will propagate and accumulate along the pipeline. in order to alleviate this problem, we use compact structures, lattice and forest, in each module insteadof 1-best results. we integrate both lat tice and forest into a single tree-to-stringsystem, and explore the algorithms of lattice parsing, lattice-forest-based rule ex traction and decoding. more importantly,our model takes into account all the probabilities of different steps, such as segmen tation, parsing, and translation. the main advantage of our model is that we can make global decision to search for the best segmentation, parse-tree and translation in one step. medium-scale experiments show an improvement of +0.9 bleu points over a state-of-the-art forest-based baseline.",
        "conclusion": "conclusion and future work in this paper, we have proposed a lattice-forestbased model to alleviate the problem of error propagation in traditional single-best pipeline frame work. unlike previous works, which only focus onone module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-to string system. we have explored the algorithms of lattice parsing, rule extraction and decoding. ourmodel postpones the disambiguition of segmenta tion and parsing into the final translation step, so that we can make a more global decision to searchfor the best segmentation, parse-tree and transla tion in one step. the experimental results showthat our lattice-forest approach achieves an abso lute improvement of +0.9 points in term of bleu score over a state-of-the-art forest-based model. for future work, we would like to pay more attention to word alignment between lattice pairs and forest pairs, which would be more principledthan our current method of word alignment be tween most-refined segmentation and string. acknowledgementwe thank steve deneefe and the three anonymous reviewers for comments. the work is sup ported by national natural science foundation of china, contracts 90920004 and 60736014, and 863 state key project no. 2006aa010108 (h. m and q. l.), and in part by darpa gale contract no. hr0011-06-c-0022, and darpa under doi-nbc grant n10ap20031 (l. h and h. m). 844",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E12-1047": {
        "abstract": "linear context-free rewriting systems efficient parsing with linear context-free rewriting systems abstract previous work on treebank parsing with discontinuous constituents using linear rewriting systems has been limited to sentences of up to 30 words, for reasons of computational complexity. there have been some results on an a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible. instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy. the resulting parser has been applied to a discontinuous treebank with favorable results.",
        "conclusion": "conclusion our results show that optimal binarizations are clearly not the answer to parsing lcfrs efficiently, as they do not significantly reduce parsing complexity in our experiments. while they provide some efficiency gains, they do not help with the main problem of longer sentences. we have presented a new technique for largescale parsing with lcfrs, which makes it possible to parse sentences of any length, with favorable accuracies. the availability of this technique may lead to a wider acceptance of lcfrs as a syntactic backbone in computational linguistics.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P05-1039": {
        "abstract": "what to do when lexicalization fails: parsing german with suffix analysis and smoothing abstract in this paper, we present an unlexicalized parser for german which employs smoothing and suffix analysis to achieve labelled bracket of 76.2, higher than previously reported results on the negra corpus. in addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results.",
        "conclusion": "5 discussion there are two primary results: first, although lp/id rules have been suggested as suitable for german\u2019s flexible word order, it appears that markov rules actually perform better. second, adding suffix analysis provides a clear benefit, but only after the inclusion of the coord gf transformation. while the sbar transformation slightly reduces performance, recall that we argued the s gf transformation only made sense if the sbar transformation is already in place. to test if this was indeed the case, we re-ran the final experiment, but excluded the sbar transformation. we did indeed find that applying s gf without the sbar transformation reduced performance. conclusions in this paper, we presented the best-performing parser for german, as measured by labelled bracket scores. the high performance was due to three factors: (i) treebank transformations (ii) an integrated model of morphology in the form of a suffix analyzer and (iii) the use of smoothing in an unlexicalized grammar. moreover, there are possible paths for improvement: lexicalization could be added to the model, as could some of the treebank transformations suggested by schiehlen (2004). indeed, the suffix analyzer could well be of value in a lexicalized model. while we only presented results on the german negra corpus, there is reason to believe that the techniques we presented here are also important to other languages where lexicalization provides little benefit: smoothing is a broadly-applicable technique, and if difficulties with lexicalization are due to sparse lexical data, then suffix analysis provides a useful way to get more information from lexical elements which were unseen while training. in addition to our primary results, we also provided a detailed error analysis which shows that pp attachment and co-ordination are problematic for our parser. furthermore, while pos tagging is highly accurate, the error analysis also shows it does have surprisingly large effect on parsing errors. because of the strong impact of pos tagging on parsing results, we conjecture that increasing pos tagging accuracy may be another fruitful area for future parsing research.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "A00-2005": {
        "abstract": "bagging and boosting a treebank parser abstract bagging and boosting, two effective machine learning techniques, are applied to natural language parsing. experiments using these techniques with a trainable statistical parser are described. the best resulting system provides roughly as large of a gain in f-measure as doubling the corpus size. error analysis of the result of the boosting technique reveals some inconsistent annotations in the penn treebank, suggesting a semi-automatic method for finding inconsistent treebank annotations.",
        "conclusion": "conclusion we have shown two methods, bagging and boosting, for automatically creating ensembles of parsers that produce better parses than any individual in the ensemble. neither of the algorithms exploit any specialized knowledge of the underlying parser induction algorithm, and the data used in creating the ensembles has been restricted to a single common training set to avoid issues of training data quantity affecting the outcome. our best bagging system performed consistently well on all metrics, including exact sentence accuracy. it resulted in a statistically significant fmeasure gain of 0.6 over the performance of the baseline parser. that baseline system is the best known treebank parser. this gain compares favorably with a bound on potential gain from increasing the corpus size. even though it is computationally expensive to create and evaluate a small (15-30) ensemble of parsers, the cost is far outweighed by the opportunity cost of hiring humans to annotate 40000 more sentences. the economic basis for using ensemble methods will continue to improve with the increasing value (performance per price) of modern hardware. our boosting system, although dominated by the bagging system, also performed significantly better than the best previously known individual parsing result. we have shown how to exploit the distribution created as a side-effect of the boosting algorithm to uncover inconsistencies in the training corpus. a semi-automated technique for doing this as well as examples from the treebank that are inconsistently annotated were presented. perhaps the biggest advantage of this technique is that it requires no a priori notion of how the inconsistencies can be characterized.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1091": {
        "abstract": "factored translation models abstract we present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes. in a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.",
        "conclusion": "conclusion and future work we presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes. we reported on experiments that showed gains over standard phrase-based models, both in terms of automatic scores (gains of up to 2% bleu), as well as a measure of grammatical coherence. these experiments demonstrate that within the frameworkof factored translation models additional informa tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase based statistical approach. the framework of factored translation models is very general. many more models that incorporatedifferent factors can be quickly built using the ex isting implementation. we are currently exploringthese possibilities, for instance use of syntactic in formation in reordering and models with augmented input information.we have not addressed all computational problems of factored translation models. in fact, compu tational problems hold back experiments with morecomplex factored models that are theoretically pos sible but too computationally expensive to carry out.our current focus is to develop a more efficient im plementation that will enable these experiments. moreover, we expect to overcome the constraints of the currently implemented synchronous factored models by developing a more general asynchronous framework, where multiple translation steps mayoperate on different phrase segmentations (for instance a part-of-speech model for large scale re ordering). acknowledgments this work was supported in part under the gale program of the defense advanced research projects agency, contract no nr0011-06-c-0022 and inpart under the euromatrix project funded by the eu ropean commission (6th framework programme).we also benefited greatly from a 2006 summer workshop hosted by the johns hopkins uni versity and would like thank the other workshop participants for their support and insights, namelynicola bertoldi, ondrej bojar, chris callison burch, alexandra constantin, brooke cowan, chris dyer, marcello federico, evan herbst christine moran, wade shen, and richard zens.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E06-1010": {
        "abstract": "constraints on non-projective dependency parsing abstract we investigate a series of graph-theoretic constraints on non-projective dependency and their effect on i.e. whether they allow naturally occurring syntactic constructions to be adequately and i.e. whether they reduce the search space for the parser. in particular, we define a new measure the non-projectivity in an acyclic dependency graph obeying the single-head constraint. the constraints are evaluated experimentally using data from the prague dependency treebank and the danish dependency treebank. the results indicate that, whereas complete linguistic coverage in principle requires unrestricted non-projective dependency graphs, limiting the degree of non-projectivity to at most 2 can reduce average running time from quadratic to linear, while excluding less than 0.5% of the dependency graphs found in the two treebanks. this is a substantial improvement over the commonly used projective approximation (degree 0), which excludes 15\u201325% of the graphs.",
        "conclusion": "conclusion we have investigated a series of graph-theoretic constraints on dependency structures, aiming to find a better approximation than projectivity for the structures found in naturally occurring data, while maintaining good parsing efficiency. in particular, we have defined the degree of nonprojectivity in terms of the maximum number of connected components that occur under a dependency arc without being dominated by the head of that arc. empirical experiments based on data from two treebanks, from different languages and with different annotation schemes, have shown that limiting the degree d of non-projectivity to 1 or 2 gives an average case running time that is linear in practice and allows us to capture about 98% of the dependency graphs actually found in the treebanks with d < 1, and about 99.5% with d < 2. this is a substantial improvement over the projective approximation, which only allows 75\u201385% of the dependency graphs to be captured exactly. this suggests that the integration of such constraints into non-projective parsing algorithms will improve both accuracy and efficiency, but we have to leave the corroboration of this hypothesis as a topic for future research.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-3117": {
        "abstract": "dcu-symantec submission for the wmt 2012 quality estimation task abstract this paper describes the features and the machine learning methods used by dublin city (dcu) and the wmt 2012 quality estimation task. two sets features are proposed: one i.e. respecting the data limitation suggested by the organisers, and one i.e. using data or tools trained on data that was not provided by the workshop organisers. in total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data. in this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the latent dirichlet allocation approach, and features based on source and target language syntax extracted using part-of-speech (pos) taggers and parsers. we evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques.",
        "conclusion": "conclusion we presented in this paper our submission for the wmt12 quality estimation shared task. we also presented further experiments using different machine learning techniques and we evaluated the impact of two sets of features - one set which is based on linguistic features extracted using pos tagging and parsing, and a second set which is based on topic modelling. the best results are obtained by our unconstrained system containing all features and using an e-svr regression method with a radial basis function kernel. this setup leads to a mean average error of 0.62 and a root mean squared error of 0.78. unfortunately, we did not submit our best configuration for the shared task. we plan to continue working on the task of machine translation quality estimation. our immediate next steps are to continue to investigate the contribution of individual features, to explore feature selection in a more detailed fashion and to apply our best system to other types of data including sentences taken from an online discussion forum.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N01-1023": {
        "abstract": "applying co-training methods to statistical parsing abstract we propose a novel co-training method for statistical parsing. the algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. the algorithm iteratively labels the entire data set with parse trees. using empirical results based on parsing the wall street journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.",
        "conclusion": "conclusion in this paper, we proposed a new approach for training a statistical parser that combines labeled with unlabeled data. it uses a co-training method where a pair of models attempt to increase their agreement on labeling the data. the algorithm takes as input a small corpus of 9695 sentences (234467 word tokens) of bracketed data, a large pool of unlabeled text and a tag dictionary of lexicalized structures for each word in this training set (based on the ltag formalism). the algorithm presented iteratively labels the unlabeled data set with parse trees. we then train a statistical parser on the combined set of labeled and unlabeled data. we obtained 80.02% and 79.64% labeled bracketing precision and recall respectively. the baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and recall. these results show that training a statistical parser using our co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data. it is important to note that unlike previous studies, our method of moving towards unsupervised parsing can be directly compared to the output of supervised parsers. unlike previous approaches to unsupervised parsing our method can be trained and tested on the kind of representations and the complexity of sentences that are found in the penn treebank. in addition, as a byproduct of our representation we obtain more than the phrase structure of each sentence. we also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilistic treatment.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W09-1207": {
        "abstract": "multilingual dependency-based syntactic and semantic parsing abstract dependency parser. in emnlp/conll- chang and chih-jen lin, 2001. a for support vector wanxiang che, zhenghua li, yuxuan hu, yongqiang li, bing qin, ting liu, and sheng li. 2008. a cascaded syntactic and semantic dependency parsing system. in jason eisner. 2000. bilexical grammars and their cubicparsing algorithms. in in probabilistic",
        "conclusion": "conclusion and future work our conll 2009 shared task system is composed of three cascaded components. the pseudoprojective high-order syntactic dependency model outperforms our conll 2008 model (in english). the additional in-domain (devel) srl data can help the in-domain test. however, it is harmful to the ood test. our final system achieves promising results. in the future, we will study how to solve the domain adaptive problem and how to do joint learning between syntactic and semantic parsing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N09-1061": {
        "abstract": "optimal reduction of rule length in linear context-free rewriting systems abstract linear context-free rewriting systems (lcfrs) is an expressive grammar formalism with applications in syntax-based machine translation. the parsing complexity of an is exponential in both the of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, in this paper, we present an algorithm that transforms an lcfrs into a strongly equivalent form in which productions have rank at most and has minimal fan-out. our results generalize previous work on synchronous context-free grammar, and are particularly relevant for machine translation from or to languages that require syntactic analyses with discontinuous constituents.",
        "conclusion": "discussion to conclude this paper, we now discuss a number of aspects of the results that we have presented, including various other pieces of research that are particularly relevant to this paper.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E09-1055": {
        "abstract": "treebank grammar techniques for non-projective dependency parsing abstract an open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. we propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms. in this paper, we provide two key tools for this approach. first, we show how to reduce nonprojective dependency parsing to parsing with linear context-free rewriting systems (lcfrs), by presenting a technique for extracting lcfrs from dependency treebanks. for efficient parsing, the extracted grammars need to be transformed in order to minimize the number of nonterminal symbols per production. our second contribution is an algorithm that computes this transformation for a large, empirically relevant class of grammars.",
        "conclusion": "discussion we have shown how to extract mildly contextsensitive grammars from dependency treebanks, and presented an efficient algorithm that attempts to convert these grammars into an efficiently parseable binary form. due to previous results (rambow and satta, 1999), we know that this is not always possible. however, our algorithm may fail even in cases where a binarization exists\u2014our notion of adjacency is not strong enough to capture all binarizable cases. this raises the question about the practical relevance of our technique. in order to get at least a preliminary answer to this question, we extracted lcfrs productions from the data used in the 2006 conll shared task on data-driven dependency parsing (buchholz and marsi, 2006), and evaluated how large a portion of these productions could be binarized using our algorithm. the results are given in table 1. since it is easy to see that our algorithm always succeeds on context-free productions (productions where each nonterminal has fan-out 1), we evaluated our algorithm on the 102 687 productions with a higher fan-out. out of these, only 24 (0.02%) could not be binarized using our technique. we take this number as an indicator for the usefulness of our result. it is interesting to compare our approach with techniques for well-nested dependency trees (kuhlmann and nivre, 2006). well-nestedness is a property that implies the binarizability of the extracted grammar; however, the classes of wellnested trees and those whose corresponding productions can be binarized using our algorithm are incomparable\u2014in particular, there are well-nested productions that cannot be binarized in our framework. nevertheless, the coverage of our technique is actually higher than that of an approach that relies on well-nestedness, at least on the conll 2006 data (see again table 1). we see our results as promising first steps in a thorough exploration of the connections between non-projective and mildly context-sensitive parsing. the obvious next step is the evaluation of our technique in the context of an actual parser. as a final remark, we would like to point out that an alternative technique for efficient non-projective dependency parsing, developed by g\u00f3mez rodr\u00edguez et al. independently of this work, is presented elsewhere in this volume. acknowledgements we would like to thank ryan mcdonald, joakim nivre, and the anonymous reviewers for useful comments on drafts of this paper, and carlos g\u00f3mez rodr\u00edguez and david j. weir for making a preliminary version of their paper available to us. the work of the first author was funded by the swedish research council. the second author was partially supported by miur under project prin no. 2007tjnzre_002.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-2138": {
        "abstract": "improving translation model by monolingual data abstract we use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation. this method called \u201creverse self-training\u201d improves the decoder\u2019s ability to produce grammatically correct translations into languages with morphology richer than the source language esp. in small-data setting. we empirically evaluate the gains for several pairs of european languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words. we also provide a description of the systems we",
        "conclusion": "conclusion we introduced a technique for exploiting monolingual data to improve the quality of translation into morphologically rich languages. we carried out experiments showing improvements in bleu when using our method for translating into czech, finnish, german and slovak with small parallel data. we discussed the issues of including similar translation models as separate components in mert. we showed that gains in bleu score increase with growing size of monolingual data. on the other hand, growing parallel data size diminishes the effect of our method quite rapidly. we also documented our experiments with several back-off techniques for english to czech translation. finally, we described our primary submissions to the wmt 2011 shared translation task.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D12-1127": {
        "abstract": "wiki-ly supervised part-of-speech tagging abstract despite significant recent work, purely unsupervised techniques for part-of-speech (pos) tagging have not achieved useful accuracies required by many language processing tasks. use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy. however, parallel text is not always available and techniques for using it require multiple complex algorithmic steps. in this paper we show that we can build pos-taggers exceeding state-of-the-art bilingual methods by using simple hidden markov models and a freely available and naturally growing resource, the wiktionary. across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods. we achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised penn",
        "conclusion": "conclusion we have shown that the wiktionary can be used to train a very simple model to achieve state-ofart weakly-supervised and out-of-domain pos taggers. the methods outlined in the paper are standard and easy to replicate, yet highly accurate and should serve as baselines for more complex proposals. these encouraging results show that using free, collaborative nlp resources can in fact produce results of the same level or better than using expensive annotations for many languages. furthermore, the wiktionary contains other possibly useful information, such as glosses and translations. it would be very interesting and perhaps necessary to incorporate this additional data in order to tackle challenges that arise across a larger number of language types, specifically non-european languages.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-2133": {
        "abstract": "bilingual latent semantic models topic adaptation for lecture translation through bilingual latent semantic models abstract this work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing probabilistic latent semantic analysis (plsa) during model training. during inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language\u2019s vocabulary, from which we perform minimum discrimination information (mdi) adaptation on a background language model (lm). we apply our approach on the english-french iwslt 2010 ted talk exercise, and report a 15% reduction in perplexity and relative bleu and nist improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background lm over the entire translation task. our topic modeling approach is simpler to construct than its counterparts.",
        "conclusion": "conclusions an alternative approach to bilingual topic modeling has been presented that integrates the plsa framework with mdi adaptation that can effectively adapt a background language model when given a document in the source language. rather than training two topic models and enforcing a one-to-one correspondence for translation, we use the assumption that parallel texts refer to the same topics and have a very similar topic distribution. preliminary experiments show a reduction in perplexity and an overall improvement in bleu and nist scores on speech translation. we also note that, unlike previous works involving topic modeling, we did not remove stop words and punctuation, but rather assumed that these features would have a relatively uniform topic distribution. one downside to the mdi adaptation approach is that the computation of the normalization term z(h) is expensive and potentially prohibitive during continuous speech translation tasks. further investigation is needed to determine if there is a suitable approximation that avoids computing probabilities across all n-grams.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D12-1069": {
        "abstract": "weakly supervised training of semantic parsers abstract we present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: supervision a knowledge base, supervision dependencyparsed sentences. we apply our approach to train a semantic parser that uses 77 relations from freebase in its knowledge representation. this semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. we demonstrate recovery of this richer structure by extracting logical forms from natural language queries against freebase. on this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.",
        "conclusion": "discussion this paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences. our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base of facts, and syntactic supervision in the form of a standard dependency parser. we presented an algorithm for training a semantic parser in the form of a probabilistic combinatory categorial grammar, using these two types of weak supervision. we used this algorithm to train a semantic parser for an ontology of 77 freebase predicates, using freebase itself as the weak semantic supervision. experimental results show that our trained semantic parser extracts binary relations as well as a state-of-the-art weakly supervised relation extractor (hoffmann et al., 2011). further experiments tested our trained parser\u2019s ability to extract more complex meanings from sentences, including logical forms involving conjunctions of multiple relation and category predicates with shared arguments (e.g., ax.musician(x) \u2227 personbornin(x, london) \u2227 cityincountry(london, england)). to test this capability, we applied the trained parser to natural language queries against freebase. the semantic parser correctly interpreted 56% of these queries, despite the broad domain and never having seen an annotated logical form. together, these two experimental analyses suggest that the combination of syntactic and semantic weak supervision is indeed a sufficient basis for training semantic parsers for a diverse range of corpora and predicate ontologies. one limitation of our method is the reliance on hand-built dependency parse patterns for lexicon initialization. although these patterns capture a variety of linguistic phenomena, they require manual engineering and may miss important relations. an area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (kwiatkowski et al., 2010) to the weakly supervised setting. such an algorithm seems especially important if one wishes to model phenomena such as adjectives, which are difficult to initialize heuristically without generating large numbers of lexical entries. an elegant aspect of semantic parsing is that it is easily extensible to include more complex linguistic phenomena, such as quantification and events (multi-argument relations). in the future, we plan to increase the expressivity of our parser\u2019s meaning representation to capture more linguistic and semantic phenomena. in this fashion, we can make progress toward broad coverage semantic parsing, and thus natural language understanding.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P08-1028": {
        "abstract": "vector-based models of semantic composition abstract this paper proposes a framework for representing the meaning of phrases and sentences in vector space. central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.",
        "conclusion": "discussion in this paper we presented a general framework for vector-based semantic composition. we formulated composition as a function of two vectors and introduced several models based on addition and multiplication. despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here. we conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials. previous applications of vector addition to document indexing (deerwester et al., 1990) or essay grading (landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences. importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components. the resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. further research is needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures (e.g., adjectivenoun, noun-noun) and also in terms of exploring the space of models more fully. we anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here. in particular, the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore. future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets. the applications of the framework discussed here are many and varied both for cognitive science and nlp. we intend to assess the potential of our composition models on context sensitive semantic priming (till et al., 1988) and inductive inference (heit and rubinstein, 1994). nlp tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (coccaro and jurafsky, 1998).",
        "citations": [],
        "summary": [
            "In this paper the author aims at proposing a framework for representing the meaning of phrases and sentences in vector space. Central to the approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, they introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Vector-based models of word meaning have become increasingly popular in natural language processing (NLP) and cognitive science. Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature. In fact, the commonest method for combining the vectors is to average them. Vector averaging is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary. Further research is needed to gain a deeper understanding of vector composition, both in terms of modelling a wider range of structures and also in terms of exploring the space of models more fully. Future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets. In this paper, Mitchell and Lapata proposed a framework for vector-based semantic composition. Central to their approach was vector composition which they operationalize in terms of additive and multiplicative functions. Under this framework, they introduced a wide range of composition models which they evaluated empirically on a sentence similarity task. Experimental results demonstrated that the multiplicative models were superior- at least, for the sentence similarity task attempted- to the additive alternatives when compared against human judgments. They conjectured that the additive models are not sensitive to the fine-grained meaning distinctions involved in their materials. Multiplicative models considered a subset, namely non-zero components whereas additive models captured composition by considering all vector components representing the meaning of the verb and its subject. The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. Further research would be needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures. Jeff Mitchell and Mirella Lapata in their paper 'Vector-based Models of Semantic Composition' present models of semantic\ncomposition that are empirically grounded and can represent similarity relations. They present a general framework for vector-based composition consider both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.They propose a framework to represent the meaning of the combination u+v as a function f operating on four components:p = f (u, v, R, K).R is the relation holding between u and v, and K additional knowledge. Their results show that the multiplicative models are superior to additive when compared to human judgement.The multiplicative model yields a better fit with the experimental data, \u03c1 = 0.17. The combined model is best overall with \u03c1 = 0.19. However,the difference between the two models is not statistically significant.In order to understand vector composition in deep more research in the area is required.\n"
        ]
    },
    "W05-0636": {
        "abstract": "joint parsing and semantic role labeling abstract a striking feature of human syntactic prois that it is that is, it seems to take into account semantic information from the discourse context and world knowledge. in this paper, we attempt to use this insight to bridge the gap between srl results from gold parses and from automatically-generated parses. to do this, we jointly perform parsing and semantic role labeling, using a probabilistic srl system to rerank the results of a probabilistic parser. our current results are negative, because a locallytrained srl model can return inaccurate probability estimates.",
        "conclusion": "conclusion and related work in this paper, we have considered several methods for reranking parse trees using information from semantic role labeling. so far, we have not been able to show improvement over selecting the 1-best parse tree. gildea and jurafsky (gildea and jurafsky, 2002) also report results on reranking parses using an srl system, with negative results. in this paper, we confirm these results with a maxent-trained srl model, and we extend them to show that weighting the probabilities does not help either. our results with collins-style reranking are too preliminary to draw definite conclusions, but the potential improvement does not appear to be great. in future work, we will explore the max-sum approach, which has promise to avoid the pitfalls of max-max reranking approaches.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "A00-2018": {
        "abstract": "a maximum-entropy-inspired parser * abstract we present a new parser for parsing down to penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the wall street journal treebank. this represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. the major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. we also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.",
        "conclusion": "conclusion we have presented a lexicalized markov grammar parsing model that achieves (using the now standard training/testing/development sections of the penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100. this corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of collins [9]. that the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it. the results reported here disprove this conjecture. the results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. indeed, it may be that adding this new parser to the mix may yield still higher results. from our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head. neither of these results were anticipated at the start of this research. as noted above, the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing. two aspects of this model deserve some comment. the first is the slight, but important, improvement achieved by using this model over conventional deleted interpolation, as indicated in figure 2. we expect that as we experiment with other, more semantic conditioning information, the importance of this aspect of the model will increase. more important in our eyes, though, is the flexibility of the maximum-entropy-inspired model. though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing. ultimately it is this flexibility that let us try the various conditioning events, to move on to a markov grammar approach, and to try several markov grammars of different orders, without significant programming. indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail. it is to this project that our future parsing work will be devoted.",
        "citations": [],
        "summary": [
            "This paper talks about a Maximum-Entropy-Inspired Parser. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that is to successfully to test and combine many different conditioning events. Here we present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1~ average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established \"standard\" sections of the Wall Street Journal tree-bank. In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. The generative model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c). Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming. Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail. In this paper the author aims at the major technical innovation that is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let successfully to test and combines many different conditioning events. They also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head. They also talk about the new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision or recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established  \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus. Indeed, they initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail. It is to this project that their future parsing work will be devoted. A maximum-entropy-inspired parser is a novel approach for parsing.the authors propose to improve the accuracy of parsing by employing probablistic generative method.The parser is inspired by maximum-entropy model for conditioning and smoothing.Feature sets are used for conditioning and deleted interpolation method is used for smoothing.the model uses a Markov-grammar approach.The lexicalized Markov grammar parsing model achieves an average accuracy of 91.1% on sentences of length less than or equal to 40 and 89.5% on sentences of length less than or equal to 100 which corresponds to an error reduction of 13%.the major achievement along with the accuracy is the strategy of guessing pre-terminal before guessing head,it improved the performance by 2%.This suggests that the flexibility offered by maximum-entropy model may give better results in future if conditioned with different grammars. \n"
        ]
    },
    "N06-1039": {
        "abstract": "preemptive information extraction using unrestricted relation discovery abstract surface text patterns for a question answering system. of the 40th annual meeting of the as",
        "conclusion": "conclusion in this paper we proposed preemptive information extraction as a new direction of ie research. as its key technique, we presented unrestricted relation discovery that tries to find parallel correspondences between multiple entities in a document, and perform clustering using basic patterns as features. to increase the number of basic patterns, we used a cluster of comparable articles instead of a single document. we presented the implementation of our preliminary system and its outputs. we obtained dozens of usable tables. sep. and nov. (from wikipedia). rows with a star (*) were actually extracted. the number of the source articles that contained a mention of the hurricane is shown in the right column.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W06-2920": {
        "abstract": "conll-x shared task on multilingual dependency parsing abstract each year the conference on computational natural language learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. the tenth conll (conll-x) saw a shared task on multilingual dependency parsing. in this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. we also give an overview of the parsing approaches that participants took and the results that they achieved. finally, we try to draw general conclusions about multi-lingual parsing: what makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser? acknowledgement many thanks to amit dubey and yuval krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing and helping with the also to alexander yeh for additional help with the paper reviews. his work was made possible by the mitre cor",
        "conclusion": "future research there are many directions for interesting research building on the work done in this shared task. one is the question which factors make data sets \u201ceasy\u201d or difficult. another is finding out how much of parsing performance depends on annotations such as the lemma and morphological features, which are not yet routinely part of treebanking efforts. in this respect, it would be interesting to repeat experiments with the recently released new version of the tiger treebank which now contains this information. one line of research that does not require additional annotation effort is defining or improving the mapping from coarse-grained to finegrained pos tags.34 another is harvesting and using large-scale distributional data from the internet. we also hope that by combining parsers we can achieve even better performance, which in turn would facilitate the semi-automatic enlargement of existing treebanks and possibly the detection of remaining errors. this would create a positive feedback loop. finally one must not forget that almost all of the lemma, (c)postag and feats values and even part of the form column (the multiword tokens used in many data sets and basically all tokenization for chinese and japanese, where words are normally not delimited by spaces) have been manually created or corrected and that the general parsing task has to integrate automatic tokenization, morphological analysis and tagging. we hope that the resources created and lessons learned during this shared task will be valuable for many years to come but also that they will be extended and improved by others in the future, and that the shared task website will grow into an informational hub on multilingual dependency parsing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P05-1065": {
        "abstract": "reading level assessment using support vector machines and statistical language models abstract reading proficiency is a fundamental component of language competency. however, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers. this task can be addressed with natural language processing technology to assess reading level. existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models. in this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level.",
        "conclusion": "conclusions and future work statistical lms were used to classify texts based on reading level, with trigram models being noticeably more accurate than bigrams and unigrams. combining information from statistical lms with other features using support vector machines provided the best results. future work includes testing additional classifier features, e.g. parser likelihood scores and features obtained using a syntax-based language model such as chelba and jelinek (2000) or roark (2001). further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data. we also plan to test these techniques on languages other than english, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C04-1074": {
        "abstract": "optimizing algorithms for pronoun resolution abstract the paper aims at a deeper understanding of sev eral well-known algorithms and proposes ways to optimize them. it describes and discusses factorsand strategies of factor interaction used in the algo rithms. the factors used in the algorithms and the algorithms themselves are evaluated on a germancorpus annotated with syntactic and coreference in formation (negra) (skut et al, 1997). a commonformat for pronoun resolution algorithms with sev eral open parameters is proposed, and the parameter settings optimal on the evaluation data are given.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "I05-6010": {
        "abstract": "some remarks on the annotation of quantifying noun groups in treebanks abstract this article is devoted to the problem of quantifying noun groups in german.after a thorough description of the phenom ena, the results of corpus-based in vestigations are described. moreover,some examples are given that under line the necessity of integrating somekind of information other than gram mar sensu stricto into the treebank. we argue that a more sophisticatedand fine-grained annotation in the tree bank would have very positve effects onstochastic parsers trained on the tree bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source ofdata for theoretical linguistic investigations. the information gained from cor pus research and the analyses that are proposed are realized in the framework of silva, a parsing and extraction tool for german text corpora.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W04-0305": {
        "abstract": "lookahead in deterministic left-corner parsing abstract to support incremental interpretation, any model of human sentence processing must not only process the sentence incrementally, it must to some degree restrict the number of analyses which it produces for any sentence prefix. deterministic parsing takes the extreme position that there can only be one analysis for any sentence prefix. experiments with an incremental statistical parser show that performance is severely degraded when the search for the most probable parse is pruned to only the most probable analysis after each prefix. one method which has been extensively used to address the difficulty of deterministic parsing is lookahead, where information about a bounded number of subsequent words is used to decide which analyses to pursue. we simulate the effects of lookahead by summing probabilities over possible parses for the lookahead words and using this sum to choose which parse to pursue. we find that a large improvement is achieved with one word lookahead, but that more lookahead results in relatively small additional improvements. this suggests that one word lookahead is sufficient, but that other modifications to our left-corner parsing model could make deterministic parsing more effective.",
        "conclusion": "discussion the large improvement in performance which results from adding the first word of lookahead, as compared to adding the subsequent words, indicates that the first word of lookahead has a qualitatively different effect on deterministic parsing. we believe that one word of lookahead is both necessary and sufficient for a model of deterministic parsing. the large gain provided by the first word of lookahead indicates that this lookahead is necessary for deterministic parsing. given the fact the with one word of lookahead the f-measure of the deterministic parser is only 2.7% below the maximum possible, it is unlikely that the family of deterministic parsers assumed here is so sub-optimal that the entire 5.6% improvement gained with one word lookahead is simply the result of compensating for limitations in the choice of this family. the performance curves in figure 1 also suggest that one word of lookahead is sufficient. we believe the gain provided by more than one word of lookahead is the result of compensating for limitations in the family of deterministic parsers assumed here. any limitations in this family will result in the deterministic search making choices before the necessary disambiguating information is available, thereby leading to additional errors. as the lookahead increases, some previously mistaken choices will become disambiguated by the additional lookahead information, thereby improving performance. in the limit as lookahead increases, the performance of \u00a2note that when the lookahead length is longer than the longest sentence, the deterministic and nondeterministic parsers become equivalent. the deterministic and non-deterministic parsers will become the same, no matter what family of deterministic parsers has been specified. the smooth curve of increasing performance as the lookahead is increased above one word is the type of results we would expect if the lookahead were simply correcting mistakes in this way. examples of possible limitations to the family of deterministic parsers assumed here include the choice of the left-corner ordering of parser decisions. the left-corner ordering completely determines when each decision about the phrase structure tree must be made. if the family of deterministic parsers had more flexibility in this ordering, then the optimal deterministic parser could use an ordering which was tailored to the statistics of the data, thereby avoiding being forced to make decisions before sufficient information is available. conclusions terministic parsing by characterizing these issues in terms of the search procedure used by a statistical parser. we use a neural network to estimate the probabilities for an incremental history-based probability model based on leftcorner parsing. using an unconstrained search procedure to try to find the most probable parse according to this probability model (i.e. nondeterministic parsing) results in state-of-the-art accuracy. deterministic parsing is simulated by allowing the sequence of decisions between two words to be combined into a single parser action, and choosing the best single combined action based on the probability calculated using the basic left-corner probability model. all parses which do not use this chosen action are then pruned from the search. when this pruning is applied directly after each word, there is a large reduction in accuracy (8.3% f-measure) as compared to the non-deterministic search. given the pervasive ambiguity in natural language, it is not surprising that this drastic pruning strategy results in a large reduction in accuracy. for this reason, deterministic parsers usually use some form of lookahead. lookahead gives the parser more information about the sentence at the point when the choice of the next parser action takes place. we simulate the optimal use of k-word lookahead by summing over all partial parses which continue the given partial parse to the point where all k words in the lookahead have been generated. when expressed in terms of search, this means that the deterministic pruning is done k words behind a non-deterministic search for the best parse, based on a sum over the partial parses found by the non-deterministic search. when accuracy is plotted as a function of k (figure 1), we found that there is a large increase in accuracy when the first word of lookahead is added (only 2.7% f-measure below non-deterministic search). further increases in the lookahead length have much less of an impact. we conclude that the first word of lookahead is necessary for the success of any deterministic parser, but that additional lookahead is probably not necessary. the remaining error created by this model of deterministic parsing is probably best dealt with by investigating other aspect of the model of deterministic parsing assumed here, in particular the strict adherence to the left-corner parsing order. despite the need to consider alternatives to the left-corner parsing order, these results do demonstrate that the left-corner parsing strategy proposed is surprisingly good at supporting deterministic parsing. this fact is important in making the non-deterministic search strategy used with this parser tractable. the observations made in this paper could lead to more sophisticated search strategies which further increase the speed of this or similar parsers without significant reductions in accuracy.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D11-1006": {
        "abstract": "multi-source transfer multi-source transfer of delexicalized dependency parsers abstract we present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. we first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. we then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. the projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.",
        "conclusion": "discussion one fundamental point the above experiments illustrate is that even for languages for which no resources exist, simple methods for transferring parsers work remarkably well. in particular, if one can transfer part-of-speech tags, then a large part of transferring unlabeled dependencies has been solved. this observation should lead to a new baseline in unsupervised and projected grammar induction \u2013 the uas of a delexicalized english parser. of course, our experiments focus strictly on indoeuropean languages. preliminary experiments for arabic (ar), chinese (zh), and japanese (ja) suggest similar direct transfer methods are applicable. for example, on the conll test sets, a dmv model obtains uas of 28.7/41.8/34.6% for ar/zh/ja respectively, whereas an english direct transfer parser obtains 32.1/53.8/32.2% and a multi-source direct transfer parser obtains 39.9/41.7/43.3%. in this setting only indo-european languages are used as source data. thus, even across language groups direct transfer is a reasonable baseline. however, this is not necessary as treebanks are available for a number of language groups, e.g., indo-european, altaic, semitic, and sino-tibetan. the second fundamental observation is that when available, multiple sources should be used. even through naive multi-source methods (concatenating data), it is possible to build a system that has comparable accuracy to the single-best source for all languages. this advantage does not come simply from having more data. in fact, if we randomly sampled from the multi-source data until the training set size was equivalent to the size of the english data, then the results still hold (and in fact go up slightly for some languages). this suggests that even better transfer models can be produced by separately weighting each of the sources depending on the target language \u2013 either weighting by hand, if we know the language group of the target language, or automatically, if we do not. as previously mentioned, the latter has been explored in both s\u00f8gaard (2011) and cohen et al. (2011). conclusions we presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data. central to our approach is the idea of delexicalizing the models, which combined with a standardized part-of-speech tagset allows us to directly transfer models between languages. we then use a constraint driven learning algorithm to adapt the transferred parsers to the respective target language, obtaining an additional 16% error reduction on average in a multi-source setting. our final parsers achieve state-of-the-art accuracies on eight indo-european languages, significantly outperforming previous unsupervised and projected systems. acknowledgements: we would like to thank kuzman ganchev, valentin spitkovsky and dipanjan das for numerous discussions on this topic and comments on earlier drafts of this paper. we would also like to thank shay cohen, dipanjan das, noah smith and anders s\u00f8gaard for sharing early drafts of their recent related work.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P03-1013": {
        "abstract": "probabilistic parsing for german using sister-head dependencies abstract we present a probabilistic parsing model for german trained on the negra treebank. we observe that existing lexicalized parsing models using head-head dependencies, while successful for english, fail to outperform an unlexicalized baseline model for german. learning curves show that this effect is not due to lack of training data. we propose an alternative model that uses sister-head dependencies instead of head-head dependencies. this model outperforms the baseline, achieving a labeled precision and recall of up to 74%. this indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as negra.",
        "conclusion": "conclusions we presented the first probabilistic full parsing model for german trained on negra, a syntactically annotated corpus. this model uses lexical sisterhead dependencies, which makes it particularly suitable for parsing negra\u2019s flat structures. the flatness of the negra annotation reflects the syntactic properties of german, in particular its semi-free wordorder. in experiment 1, we applied three standard parsing models from the literature to negra: an unlexicalized pcfg model (the baseline), carroll and rooth\u2019s (1998) head-lexicalized model, and collins\u2019s (1997) model based on head-head dependencies. the results show that the baseline model achieves a performance of up to 73% recall and 70% precision. both lexicalized models perform substantially worse. this finding is at odds with what has been reported for parsing models trained on the penn treebank. as a possible explanation we considered lack of training data: negra is about half the size of the penn treebank. however, the learning curves for the three models failed to produce any evidence that they suffer from sparse data. in experiment 2, we therefore investigated an alternative hypothesis: the poor performance of the lexicalized models is due to the fact that the rules in negra are flatter than in the penn treebank, which makes lexical head-head dependencies less useful for correctly determining constituent boundaries. based on this assumption, we proposed an alternative model hat replaces lexical head-head dependencies with lexical sister-head dependencies. this can the thought of as a way of binarizing the flat rules in negra. the results show that sister-head dependencies improve parsing performance not only for nps (which is well-known for english), but also for pps, vps, ss, and coordinate categories. the best performance was obtained for a model that uses sister-head dependencies for all categories. this model achieves up to 74% recall and precision, thus outperforming the unlexicalized baseline model. it can be hypothesized that this finding carries over to other treebanks that are annotated with flat structures. such annotation schemes are often used for languages that (unlike english) have a free or semi-free wordorder. testing our sister-head model on these languages is a topic for future research.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W08-1007": {
        "abstract": "a dependency-driven parser for german dependency and constituency representations abstract we present a dependency-driven parser that parses both dependency structures and constituent structures. constituency representations are automatically transformed into dependency representations with complex arc labels, which makes it possible to recover the constituent structure with both constituent labels and grammatical functions. we report a labeled attachment score close to 90% for dependency versions of the tiger and t\u00a8uba- d/z treebanks. moreover, the parser is able to recover both constituent labels and grammatical functions with an f-score over 75% for t\u00a8uba-d/z and over 65% for tiger.",
        "conclusion": "conclusion we have shown that a transition-based dependencydriven parser can be used for parsing german with both dependency and constituent representations. we can report state-of-the-art results for parsing the dependency versions of two german treebanks, and we have demonstrated, with promising results, how a dependency parser can parse full constituent structures by encoding the inverse mapping in complex arc labels of the dependency graph. we believe that this method can be improved by using, for example, head-finding rules.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-1086": {
        "abstract": "rule markov models for fast tree-to-string translation abstract most statistical machine translation systems on rules that can be formed out of smaller rules in the grammar). though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both trainand decoding here, we take the approach, where we only use minrules that cannot be formed out other rules), and instead rely on a model the derivation history to capture dependencies between minimal rules. large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using b ) as composed rules.",
        "conclusion": "conclusion in this paper, we have investigated whether we can eliminate composed rules without any loss in translation quality. we have developed a rule markov model that captures vertical bigrams and trigrams of minimal rules, and tested it in the framework of treeto-string translation. we draw three main conclusions from our experiments. first, our rule markov models dramatically improve a grammar of minimal rules, giving an improvement of 2.3 b . second, when we compare against vertically composed rules we are able to get about the same b score, but our model is much smaller and decoding with our model is faster. finally, when we compare against full composed rules, we find that we can reach the same level of performance under some conditions, but in order to do so consistently, we believe we need to extend our model to condition on horizontal context in addition to vertical context. we hope that by modeling context in both axes, we will be able to completely replace composed-rule grammars with smaller minimal-rule grammars.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N06-2033": {
        "abstract": "parser combination by reparsing abstract we present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. we apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.",
        "conclusion": "discussion we have presented a reparsing scheme that produces results with accuracy higher than the best individual parsers available by combining their results. we have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers. in constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score. by combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W10-2924": {
        "abstract": "joint entity and relation extraction using card-pyramid parsing abstract both entity and relation extraction can benefit from being performed jointly, allowing each task to correct the errors of the other. we present a new method for joint entity and relation extraction using a graph we call a \u201ccard-pyramid.\u201d this graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes. we give an efficient labeling algorithm that is analogous to parsing using dynamic programming. experimental results show improved results for our joint extraction method compared to a pipelined approach.",
        "conclusion": "future work there are several possible directions for extending the current approach. the card-pyramid structure could be used to perform other languageprocessing tasks jointly with entity and relation extraction. for example, co-reference resolution between two entities within a sentence can be easily incorporated in card-pyramid parsing by introducing a production like coref \u2014* person person, indicating that the two person entities are the same. in this work, and in most previous work, relations are always considered between two entities. however, there could be relations between more than two entities. in that case, it should be possible to binarize those relations and then use card-pyramid parsing. if the relations are between relations instead of between entities, then card-pyramid parsing can handle it by considering the labels of the immediate children as rhs nonterminals instead of the labels of the left-most and the right-most leaves beneath it. thus, it would be interesting to apply card-pyramid parsing to extract higher-order relations (such as causal or temporal relations). given the regular graph structure of the cardpyramid, it would be interesting to investigate whether it can be modeled using a probabilistic graphical model (koller and friedman, 2009). in that case, instead of using multiple probabilistic classifiers, one could employ a single jointlytrained probabilistic model, which is theoretically more appealing and might give better results. finally, we note that a better relation classifier could be used in the current approach which makes more use of linguistic information. for example, by using dependency-based kernels (bunescu and mooney, 2005a; kate, 2008) or syntactic kernels (qian et al., 2008; moschitti, 2009) or by including the word categories and their pos tags in the subsequences. also, it will be interesting to see if a kernel that computes the similarity between subcard-pyramids could be developed and used for relation classification. conclusions we introduced a card-pyramid graph structure and presented a new method for jointly extracting entities and their relations from a sentence using it. a card-pyramid compactly encodes the entities and relations in a sentence thus reducing the joint extraction task to jointly labeling its nodes. we presented an efficient parsing algorithm for jointly labeling a card-pyramid using dynamic programming and beam search. the experiments demonstrated the benefit of our joint extraction method over a pipelined approach.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-1141": {
        "abstract": "parsing the internal structure of words: a new paradigm for chinese word segmentation abstract annotation of a large corpus. lan- 11(2):207\u2013238. nianwen xue. 2003. chinese word segmentation as tagging. linguistics and language 8(1):29\u201348. yue zhang and stephen clark. 2007. chinese segmentawith a word-based perceptron algorithm. in pro",
        "conclusion": "conclusion and discussion in this paper we proposed a new paradigm for chinese word segmentation in which not only flat words were identified but words with structures were also parsed. we gave good reasons why this should be done, and we presented an effective method showing how this could be done. with the progress in statistical parsing technology and the development of large scale treebanks, the time has now come for this paradigm shift to happen. we believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications. we showed that word structures can be recovered with high precision, though there\u2019s still much room for improvement, especially for higher level constituent parsing. our model is generative, but discriminative models such as maximum entropy technique (berger et al., 1996) can be used in parsing word structures too. many parsers using these techniques have been proved to be quite successful (luo, 2003; fung et al., 2004; wang et al., 2006). another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (collins and koo, 2005; charniak and johnson, 2005). finally, we must note that the use of flat labels such as \u201cnnf\u201d is less than ideal. the most important reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in section 6. the problem with this approach is that word structures and phrase structures are then not treated in a truly unified way, and besides the 33 part-of-speech tags originally contained in penn chinese treebank, another 33 tags ending with \u2018f\u2019 are introduced. we leave this problem open for now and plan to address it in future work.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1051": {
        "abstract": "semantic parsing with bayesian tree transducers abstract many semantic parsing models use tree transformations to map between natural language and meaning representation. however, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata. this paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions. in particular, this paper further introduces a variational bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.",
        "conclusion": "conclusion we have argued that tree transformation based semantic parsing can benefit from the literature on formal language theory and tree automata, and have taken a step in this direction by presenting a tree transducer based semantic parser. drawing this connection facilitates a greater flow of ideas in the research community, allowing semantic parsing to leverage ideas from other work with tree automata, while making clearer how seemingly isolated efforts might relate to one another. we demonstrate this by both building on previous work in training tree transducers using em (graehl et al., 2008), and describing a general purpose variational inference algorithm for adapting tree transducers to the bayesian framework. the new vb algorithm results in an overall performance improvement for the transducer over em training, and the general effectiveness of the approach is further demonstrated by the bayesian transducer achieving highest accuracy among other tree transformation based approaches.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E12-1083": {
        "abstract": "structural and topical dimensions in multi-task patent translation abstract patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents. in this paper we analyze patents along the orthogonal dimensions of topic and textual structure. we view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance. we study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning. we find small but significant gains over task-specific training by techniques that model commonalities through shared parameters. a by-product of our work is a parallel patent corpus of 23 million german-english sentence pairs.",
        "conclusion": "conclusion the most straightforward approach to improve machine translation performance on patents is to enlarge the training set to include all available data. this question has been investigated by tins",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P07-1108": {
        "abstract": "pivot language approach for phrase-based statistical machine translation abstract this paper proposes a novel method for phrase-based statistical machine translation by using pivot language. to conduct transbetween languages with a small bilingual corpus, we bring in a third which is named the lan- for and there exist bilingual corpora. using only bilingual corpora, we can build a model for the advantage of this method lies in that we can perform between even if there is no bilingual corpus available for this language pair. using bleu as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly with 5,000 sentence pairs for french-spanish translation. moreover, with small bilingual corpus available, our method can further improve the translaquality by using the additional bilingual corpora.",
        "conclusion": "conclusion this paper proposed a novel method for phrasebased smt on language pairs with a small bilingual corpus by bringing in pivot languages. to perform translation between lf and le, we bring in a pivot language lp, via which the large corpora of lf-lp and lp-le can be used to induce a translation model for lf-le. the advantage of this method is that it can perform translation between the language pair lf-le even if no bilingual corpus for this pair is available. using bleu as a metric, our method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 sentence pairs for frenchspanish translation. and the translation quality is comparable with that of the model directly trained with 30,000 french-spanish sentence pairs. the results also indicate that using more pivot languages leads to better translation quality. with a small bilingual corpus available for lf-le, we built a translation model, and interpolated it with the pivot model trained with the large lf-lp and lp-le bilingual corpora. the results on both the europarl corpus and chinese-japanese translation indicate that the interpolated models achieve the best results. results also indicate that our pivot language approach is suitable for translation on language pairs with a small bilingual corpus. the less the lf-le bilingual corpus is, the bigger the improvement is. we also performed experiments using lf-lp and lp-le corpora of different sizes. the results indicate that using larger training corpora to train the pivot model leads to better translation quality.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W03-1509": {
        "abstract": "chinese named entity recognition combining statistical model wih human knowledge abstract named entity recognition is one of the key techniques in the fields of natural language processing, information retrieval, question answering and so on. unfortunately, chinese named entity recognition (ner) is more difficult for the lack of capitalization information and the uncertainty in word segmentation. in this paper, we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well. in order to avoid data sparseness problem, we employ a back-off model and yi ci ci lin a chinese thesaurus, to smooth the parameters in the model. the f-measure of person names, location names, and organization names on the newswire test data for the 1999 ieer evaluation in mandarin is 86.84%, 84.40% and 76.22% respectively.",
        "conclusion": "conclusions chinese ner is a more difficult task than english ner. though many approaches have been tried, the result is still not satisfactory. in this paper, we present a hybrid algorithm of incorporating human knowledge into statistical model. thus we only need a relative small-sized labeled corpus (onemonth\u2019s chinese people\u2019s daily tagged with ner tags at peking university) and human knowledge, but can achieve better performance. the main contribution of this paper is putting forward an approach which can make up for the limitation of using the statistical model or human knowledge purely by combining them organically. our lab was mainly devoted to cross-language information processing and its application. so in the future we will shift our algorithm to other languages. and fine-tune to a specific domain such as sports.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P05-1053": {
        "abstract": "exploring various knowledge in relation extraction abstract extracting semantic relationships between entities is challenging. this paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using svm. our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. this suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. we also demonstrate how semantic information such as wordnet and name list, can be used in feature-based relation extraction to further improve the performance. evaluation on the ace corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ace relation subtypes and significantly outperforms tree kernel-based systems by over 20 in f-measure on the 5 ace relation types.",
        "conclusion": "discussion and conclusion in this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information first. evaluation on the ace corpus shows that base phrase chunking contributes to most of the performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. this may be due to three reasons: first, most of relations defined in ace have two mentions being close to each other. while short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. second, it is well known that full parsing is always prone to long-distance parsing errors although the collins\u2019 parser used in our system achieves the state-of-the-art performance. therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially pp (preposition phrase) attachment. last, effective ways need to be explored to incorporate information embedded in the full parse trees. besides, we also demonstrate how semantic information such as wordnet and name list, can be used in feature-based relation extraction to further improve the performance. the effective incorporation of diverse features enables our system outperform previously bestreported systems on the ace corpus. although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ace where 5 types and 24 subtypes need to be extracted. evaluation on the ace rdc task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. the experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 f-measure on the extraction of 5 ace relation types. in the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. moreover, our current work is done when the entity detection and tracking (edt) has been perfectly done. therefore, it would be interesting to see how imperfect edt affects the performance in relation extraction.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E12-3010": {
        "abstract": "improving machine translation of null subjects in italian and spanish abstract null subjects are non overtly expressed pronouns found in languages such as italian and spanish. in this study we quantify and compare the occurrence of this phenomenon in these two languages. next, we evaluate null subjects\u2019 translation into french, a \u201cnon prodrop\u201d language. we use the europarl corpus to evaluate two mt systems on their performance regarding null subject translation: its-2, a rule-based system developed at latl, and a statistical system built using the moses toolkit. then we add a rule-based preprocessor and a statistical post-editor to the its-2 translation pipeline. a second evaluation of the improved its-2 system shows an average increase of 15.46% in correct pro-drop translations for italian-french and 12.80% for",
        "conclusion": "future work as already mentioned, we have not found the solution to some problems yet. first of all, we would like to include an ar module in our system. as it is a rule-base system, some problems as the subject pronoun mistranslation in subordinated sentences can be fixed by means of more specific rules and heuristics. besides, an approach based on binding theory (b\u00a8uring, 2005) could be effective as deep syntactic information is available, even though limited. for example, binding theory does not contain any formalization on gender, reason why a specific statistical component could be a more ideal option in order to tackle aspects such as masculine/feminine pronouns. secondly, an overt pronoun cannot be restored from a finite impersonal verb without making the sentence ungrammatical; therefore, our approach is not useful for treating impersonal sentences. as a consequence, we think that an annotation of the empty category, as done by chung and gildea (2010), could provide better results. also, in order to correctly render the meaning of a preprocessed sentence, we plan to mark restored subject pronouns in such a way that the information about their absence/presence in the original text is preserved as a feature in parsing and translation. finally, we would like to use a larger corpus to train the spe component and compare the effects of utilizing machine translations on the target side versus human reference translations. besides, we would like to further explore variations on the plain spe technique, for example, by injecting moses translation of sentences being translated into the phrase-table of the post-editor (chen and eisele, 2010). conclusion in this paper we measured and compared the occurrence of one syntactic feature \u2013 the null subject parameter \u2013 in italian and spanish. we also evaluated its translation into a \u201cnon pro-drop\u201d language, that is, french, obtaining better results for personal pro-drop than for impersonal pro-drop, for both its-2 and moses, the two mt systems we tested. we then improved the rule-based system using a rule-based preprocessor to restore pro-drop as overt pronouns and a statistical post-editor to correct the translation. results obtained from the second evaluation showed an improvement in the translation of both sorts of pronouns. in particular, the system now generates more pronouns in french than before, confirming the advantage of using a combination of preprocessing and postediting with rule-based machine translation.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-0314": {
        "abstract": "ulisse: an unsupervised algorithm for detecting reliable dependency parses abstract in this paper we present ulisse, an unsupervised linguistically\u2013driven algorithm to select reliable parses from the output of a dependency parser. different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains. in all cases, ulisse appears to outperform the baseline algorithms.",
        "conclusion": "conclusion ulisse is an unsupervised linguistically\u2013driven method to select reliable parses from the output of dependency parsers. to our knowledge, it represents the first unsupervised ranking algorithm operating on dependency representations which are more and more gaining in popularity and are arguably more useful for some applications than constituency parsers. ulisse shows a promising performance against the output of two supervised parsers selected for their behavioral differences. in all experiments, ulisse outperforms all baselines, including dpupa and sentence length (sl), the latter representing a very strong baseline selection method in a supervised scenario, where parsers have a very high performance with short sentences. the fact of carrying out the task of reliable parse selection in a supervised scenario represents an important novelty: however, the unsupervised nature of ulisse could also be used in an unsupervised scenario (reichart and rappoport, 2010). current direction of research include a careful study of a) the quality score function, in particular for what concerns the combination of individual feature weights, and b) the role and effectivess of the set of linguistic features. this study is being carried out with a specific view to nlp tasks which might benefit from the ulisse algorithm. this is the case, for instance, of the domain adaptation task in a self\u2013training scenario (mcclosky et al., 2006), of the treebank construction process by minimizing the human annotators\u2019 efforts (reichart and rappoport, 2009b), of n\u2013best ranking methods for machine translation (zhang, 2006).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1015": {
        "abstract": "structured prediction models via the matrix-tree theorem abstract this paper provides an algorithmic framework for learning statistical models involv ing directed spanning trees, or equivalently non-projective dependency structures. we show how partition functions and marginals for directed spanning trees can be computed by an adaptation of kirchhoff?s matrix-tree theorem. to demonstrate an application of the method, we perform experiments which use the algorithm in training both log-linear and max-margin dependency parsers. the new training methods give improvements in accuracy over perceptron-trained models.",
        "conclusion": "conclusions this paper describes inference algorithms forspanning-tree distributions, focusing on the funda mental problems of computing partition functionsand marginals. although we concentrate on loglinear and max-margin estimation, the inference al gorithms we present can serve as black-boxes in many other statistical modeling techniques. our experiments suggest that marginal-basedtraining produces more accurate models than per ceptron learning. notably, this is the first large-scale application of the eg algorithm, and shows that it is a promising approach for structured learning. in line with mcdonald et al (2005b), we confirmthat spanning-tree models are well-suited to depen dency parsing, especially for highly non-projective languages such as dutch. moreover, spanning-treemodels should be useful for a variety of other prob lems involving structured data. acknowledgmentsthe authors would like to thank the anonymous reviewers for their constructive comments. in addition, the authors gratefully acknowledge the follow ing sources of support. terry koo was funded by a grant from the nsf (dms-0434222) and a grantfrom ntt, agmt. dtd. 6/21/1998. amir globerson was supported by a fellowship from the roth schild foundation - yad hanadiv. xavier carreraswas supported by the catalan ministry of innova tion, universities and enterprise, and a grant from ntt, agmt. dtd. 6/21/1998. michael collins was funded by nsf grants 0347631 and dms-0434222. 7we ran the sign test at the sentence level to measure the statistical significance of the results aggregated across the six languages. out of 2,472 sentences total, log-linear models gave improved parses over the perceptron on 448 sentences, and worse parses on 343 sentences. the max-margin method gave improved/worse parses for 500/383 sentences. both results are significant with p ? 0.001. 149",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P13-1109": {
        "abstract": "graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation abstract out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. in this paper, we propose a novel approach to finding translations for oov words. we induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.",
        "conclusion": "conclusion we presented a novel approach for inducing oov translations from a monolingual corpus on the source side and a parallel data using graph propagation. our results showed improvement over the baselines both in intrinsic evaluations and on bleu. future work includes studying the effect of size of parallel corpus on the induced oov translations. increasing the size of parallel corpus on one hand reduces the number of oovs. but, on the other hand, there will be more labeled paraphrases that increases the chance of finding the correct translation for oovs in the test set. currently, we find paraphrases for oov words. however, oovs can be considered as n-grams (phrases) instead of unigrams. in this scenario, we also can look for paraphrases and translations for phrases containing oovs and add them to the phrase-table as new translations along with the translations for unigram oovs. we also plan to explore different graph propagation objective functions. regularizing these objective functions appropriately might let us scale to much larger data sets with an order of magnitude more nodes in the graph.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P05-1013": {
        "abstract": "pseudo-projective dependency parsing abstract in order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. we show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. experiments using data from the prague dependency treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. this leads to the best reported performance for robust non-projective parsing of czech.",
        "conclusion": "conclusion we have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques. the main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion, leading to the best reported performance for robust non-projective parsing of czech.",
        "citations": [],
        "summary": [
            "Authors Nilsson and Nivre showed how a data-driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank showed that the combined system can handle non-projective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. The combined system could recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion. This led to the best reported performance for robust non-projective parsing of Czech. Joakim Nivre and Jens Nilsson in their paper'Pseudo-Projective Dependency Parsing'show that how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of lifting operations and encoding information about these lifts in arc labels.When the parser is trained on the transformed data,it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts.The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a\nsignificant improvement in overall parsing accuracy.\n This paper talks about Pseudo-Projective Dependency Parsing. In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective tree banks is often neglected because of the relative scarcity of problematic constructions. Here we show how non-projective dependency parsing can be achieved by combining a data driven projective parser with special graph transformation techniques. The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers. The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion, leading to the best reported performance for robust non-projective parsing of Czech."
        ]
    },
    "W05-1505": {
        "abstract": "corrective modeling corrective modeling for non-projective dependency parsing abstract we present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers. the continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees. analysis of the types of dependency errors made by these parsers on a czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser. our model, based on a maxent classifier, improves overall dependency accuracy by .7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures.",
        "conclusion": "conclusion we have presented a maximum entropy-based corrective model for dependency parsing. the goal is to recover non-projective dependency structures that are lost when using state-of-the-art constituencybased parsers; we show that our technique recovers over 50% of these dependencies. our algorithm provides a simple framework for corrective modeling of dependency trees, making no prior assumptions about the trees. however, in the current model, we focus on trees with local errors. overall, our technique improves dependency parsing and provides the necessary mechanism to recover non-projective structures.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C10-1151": {
        "abstract": "heterogeneous parsing via collaborative decoding abstract there often exist multiple corpora for the same natural language processing (nlp)tasks. however, such corpora are generally used independently due to distinctions in annotation standards. for the purpose of full use of readily available human annotations, it is significant to simultaneously utilize multiple corpora of different annotation standards. in this paper, we focus on the challenge of con stituent syntactic parsing with treebanksof different annotations and propose a collaborative decoding (or co-decoding) ap proach to improve parsing accuracy byleveraging bracket structure consensus be tween multiple parsing decoders trainedon individual treebanks. experimental results show the effectiveness of the proposed approach, which outperforms state of-the-art baselines, especially on long sentences.",
        "conclusion": "conclusions this paper proposed a co-decoding approach tothe challenge of heterogeneous parsing. compared to previous work on this challenge, co decoding is able to directly utilize heterogeneous treebanks by incorporating consensus informationbetween partial output of individual parsers dur ing the decoding phase. experiments demonstratethe effectiveness of the co-decoding approach, es pecially the effectiveness on long sentences. acknowledgments this work was supported in part by the national science foundation of china (60873091). we would like to thank our anonymous reviewers for their comments. 1351",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1111": {
        "abstract": "dependency parsing and domain adaptation with lr models and parser ensembles abstract we present a data-driven variant of the lr algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized lr dependency parsing. parser actions are determined by a classifier, based on features that represent the current state of the parser. we apply this pars ing framework to both tracks of the conll 2007 shared task, in each case taking ad vantage of multiple models trained with different learners. in the multilingual track, we train three lr models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. in the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.",
        "conclusion": "conclusion our results demonstrate the effectiveness of even small ensembles of parsers that are relatively similar (using the same features and the same algorithm). there are several possible extensions and improvements to the approach we have described. for example, in section 3 we mention the use of different weighting schemes in dependency voting. we list additional ideas that were not attempted due to time constraints, but that are likely to produce improved results. one of the simplest improvements to our approach is simply to train more models with no oth er changes to our set-up. as mentioned in section 5, the addition of a backward svm model did im prove accuracy on the turkish set significantly, and it is likely that improvements would also be obtained in other languages. in addition, other learning approaches, such as memory-based lan guage processing (daelemans and van den bosch, 2005), could be used. a drawback of adding more models that became obvious in our experiments was the increased cost of both training (for example, the svm parsers we used required significant ly longer to train than the maxent parsers) and run-time (parsing with mbl models can be several times slower than with maxent, or even svm). a similar idea that may be more effective, but requires more effort, is to add parsers based on dif ferent approaches. for example, using mstparser (mcdonald and pereira, 2005), a large-margin all pairs parser, in our domain adaptation procedure results in significantly improved accuracy (83.2 las). of course, the use of different approaches used by different groups in the conll 2006 and 2007 shared tasks represents great opportunity for parser ensembles. acknowledgements we thank the shared task organizers and treebank providers. we also thank the reviewers for their comments and suggestions, and yusuke miyao for insightful discussions. this work was supported in part by grant-in-aid for specially promoted re search 18002007.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W99-0623": {
        "abstract": "exploiting diversity in natural language processing: combining parsers abstract three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable treebank parsing accuracy. two general approaches are presented and two combination techniques are described for each approach. both parametric and non-parametric models are explored. the resulting parsers surpass the best previously published performance results for the penn treebank.",
        "conclusion": "conclusion we have presented two general approaches to studying parser combination: parser switching and parse hybridization. for each experiment we gave an nonparametric and a parametric technique for combining parsers. all four of the techniques studied result in parsing systems that perform better than any previously reported. both of the switching techniques, as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments. through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result. combining multiple highly-accurate independent parsers yields promising results. we plan to explore more powerful techniques for exploiting the diversity of parsing methods.",
        "citations": [],
        "summary": [
            "In this paper, Henderson and Brill combined three state-of-the-art statistical parsers to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. The two general approaches presented to studying parser combination were parser switching and parse hybridization and two combination techniques were described for each approach. Both parametric and non-parametric techniques were given for each experiment. All four of the techniques studied resulted in parsing systems that performed better than any previously reported. Both of the switching techniques and the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiment. Through parser combination, they reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published performance results for the Penn Treebank.  In this paper the author aims at three state-of-the-art statistical parsers which are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. The natural language processing community is in the strong position of having many available approaches to solve some of its most fundamental problems. Recently, combination techniques have been investigated for part of speech tagging with positive results. In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation, speech recognition and named entity recognition. They have presented two general approaches to studying parser combination: parser switching and parse hybridization. Combining multiple highly-accurate independent parsers yields promising results. The author plans to explore more powerful techniques for exploiting the diversity of parsing methods. This paper talks about Exploiting Diversity in Natural Language Processing: Combining Parsers. Two general approaches are presented and two combination techniques are described for each approach. Here both parametric and non-parametric models are explored. One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure. The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank. Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result. Combining multiple highly-accurate independent parsers yields promising results. "
        ]
    },
    "P09-1111": {
        "abstract": "an optimal-time binarization algorithm for linear context-free rewriting systems with fan-out two abstract",
        "conclusion": "discussion we have presented an algorithm for the binarization of a lcfrs with fan-out 2 that does not increase the fan-out, and have discussed how this can be applied to improve parsing efficiency in several practical applications. in the algorithm of figure 1, we can modify line 14 to return r even in case of failure. if we do this, when a binarization with fan-out < 2 does not exist the algorithm will still provide us with a list of reductions that can be converted into a set of productions equivalent to p with fan-out at most 2 and rank bounded by some rb, with 2 < rb < r(p). in case rb < r(p), we are not guaranteed to have achieved an optimal reduction in the rank, but we can still obtain an asymptotic improvement in parsing time if we use the new productions obtained in the transformation. our algorithm has optimal time complexity, since it works in linear time with respect to the input production length. it still needs to be investigated whether the proposed technique, based on determinization of the choice of the reduction, can also be used for finding binarizations for lcfrs with fan-out larger than two, again without increasing the fan-out. however, it seems unlikely that this can still be done in linear time, since the problem of binarization for lcfrs in general, i.e., without any bound on the fan-out, might not be solvable in polynomial time. this is still an open problem; see (g\u00b4omez-rodr\u00b4\u0131guez et al., 2009) for discussion.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-0115": {
        "abstract": "computing semantic compositionality in distributional semantics abstract this article introduces and evaluates an approach to semantic compositionality in computational linguistics based on the combination of distributional semantics and supervised machine learning. in brief, distributional semantic spaces containing representations for complex constructions such as adjective-noun and verb-noun pairs, as well as for their constituent parts, are built. these representations are then used as feature vectors in a supervised learning model using multivariate multiple regression. in particular, the distributional semantic representations of the constituents are used to predict those of the complex structures. this approach outperforms the rivals in a series of experiments with adjective-noun pairs extracted from the bnc. in a second experimental setting based on verb-noun pairs, a comparatively much lower performance was obtained by all the models; however, the proposed approach gives the best results in combination with a random indexing semantic space.",
        "conclusion": "conclusions this paper proposes an improved framework to model the compositionality of meaning in distributional semantics. the method, partial least squares regression, is well known in other data-intensive fields of research, but to our knowledge had never been put to work in computational semantics. pls outperformed all the competing models in the reported experiments with an pairs. in particular, the pls model generates compositional predictions that are closer to the observed composed vectors than those of its rivals. this is an extremely promising result, indicating that it is possible to generalize linear transformation functions beyond single lexical items in distributional semantics\u2019 spaces. it is remarkable that pls did not actually have to compete against any of the previously proposed approaches to compositionality, but only against the noun- and adj-baselines, and in particular against the former. this fact is expected from a theoretical point of view: since the noun is the head of the an pair, it is likely that the complex expression and its head share much of their distributional properties. pls nearly always outperformed the noun-baseline, but only by small margins, which indicates that there is a still plenty of space for improvement. our experiments also show that an compositionality by regression performs nearly equally well in semantic spaces of very different nature (hal and ri). the second dataset used in this paper contained vn pairs. generally, this dataset did not produce good results with any of the considered approaches to model compositionality. this rather negative result may be due to its relatively smaller size, but this excuse may only be applied to pls, the only model that relies on parameter estimation. surprisingly, though, the gold-standard comparison of shared neighbours gave much better results, with add performing well in the hal space and pls performing very well in the ri space. even if the vn dataset did not produce excellent results, it highlights some interesting issues. first, not all syntactic relations may be equally &quot;easy&quot; to model. second, different evaluation methods may favor competing approaches. finally, some approaches may be particularly successful with a specific distributional space architecture (like pls and ri, and add and hal). this work has intentionally left the data as raw as possible, in order to keep the noise present in the models at a realistic level. the combination of machine learning and distributional semantics here advocated suggests a very promising perspective: transformation functions corresponding to different syntactic relations could be learned from suitably processed corpora and then combined to model larger, more complex structures, probably also recursive phenomena. it remains to prove if this approach is able to model the symbolic, logic-inspired kind of compositionality that is common in formal semantics; being inherently based on functional items, it is at present time very difficult and computationally intensive to attain, but hopefully this will change in the near future.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P07-1021": {
        "abstract": "mildly context-sensitive dependency languages abstract dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity. in previous work, several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense; the best-known but also most restrictive of these is projectivity. most constraints are formulated on fully specified structures, which makes them hard to integrate into models where structures are composed from lexical information. in this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P10-1074": {
        "abstract": "hierarchical joint learning: improving joint parsing and named entity recognition with non-jointly labeled data abstract one of the main obstacles to producing high quality joint models is the lack of jointly annotated data. joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data. in this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. experiments on joint parsing and named entity recognition, using the ontonotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.",
        "conclusion": "conclusion in this paper we presented a novel method for improving joint modeling using additional data which has not been labeled with the entire joint structure. while conventional wisdom says that adding more training data should always improve performance, this work is the first to our knowledge to incorporate singly-annotated data into a joint model, thereby providing a method for this additional data, which cannot be directly used by the non-hierarchical joint model, to help improve joint modeling performance. we built single-task models for the non-jointly labeled data, designing those single-task models so that they have features in common with the joint model, and then linked all of the different single-task and joint models via a hierarchical prior. we performed experiments on joint parsing and named entity recognition, and found that our hierarchical joint model substantially outperformed a joint model which was trained on only the jointly annotated data. future directions for this work include automatically learning the variances, qm and q* in the hierarchical model, so that the degree of information sharing between the models is optimized based on the training data available. we are also interested in ways to modify the objective function to place more emphasis on learning a good joint model, instead of equally weighting the learning of the joint and single-task models.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N06-1022": {
        "abstract": "multilevel coarse-to-fine pcfg parsing abstract we present a pcfg parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse. our approach requires the user to specify a sequence of nested partitions or equivalence classes of the pcfg nonterminals. we define a sequence of pcfgs corresponding to each partition, where the nonterminals of each pcfg are clusters of nonterminals of the original source pcfg. we use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level. we present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard cky parsing with the original pcfg. we suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results.",
        "conclusion": "conclusion and future research we have presented a novel parsing algorithm based upon the coarse-to-fine processing model. several aspects of the method recommend it. first, unlike methods that depend on best-first search, the method is \u201cholistic\u201d in its evaluation of constituents. for example, consider the impact of parent labeling. it has been repeatedly shown to improve parsing accuracy (johnson, 1998; charniak, 2000; klein and manning, 2003b), but it is difficult if not impossible to integrate with best-first search in bottom-up chart-parsing (as in charniak et al. (1998)). the reason is that when working bottom up it is difficult to determine if, say, ssbar is any more or less likely than ss, as the evidence, working bottom up, is negligible. since our method computes the exact outside probability of constituents (albeit at a coarser level) all of the top down information is available to the system. or again, another very useful feature in english parsing is the knowledge that a constituent ends at the right boundary (minus punctuation) of a string. this can be included only in an ad-hoc way when working bottom up, but could be easily added here. many aspects of the current implementation that are far from optimal. it seems clear to us that extracting the maximum benefit from our pruning would involve taking the unpruned constituents and specifying them in all possible ways allowed by the next level of granularity. what we actually did is to propose all possible constituents at the next level, and immediately rule out those lacking a corresponding constituent remaining at the previous level. this was dictated by ease of implementation. before using mlctf parsing in a production parser, the other method should be evaluated to see if our intuitions of greater efficiency are correct. it is also possible to combine mlctf parsing with queue reordering methods. the best-first search method of charniak et al. (1998) estimates equation 1. working bottom up, estimating the inside probability is easy (we just sum the probability of all the trees found to build this constituent). all the cleverness goes into estimating the outside probability. quite clearly the current method could be used to provide a more accurate estimate of the outside probability, namely the outside probability at the coarser level of granularity. there is one more future-research topic to add before we stop, possibly the most interesting of all. the particular tree of coarser to finer constituents that governs our mlctf algorithm (figure 1) was created by hand after about 15 minutes of reflection and survives, except for typos, with only two modifications. there is no reason to think it is anywhere close to optimal. it should be possible to define \u201coptimal\u201d formally and search for the best mlctf constituent tree. this would be a clustering problem, and, fortunately, one thing statistical nlp researchers know how to do is cluster.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-2429": {
        "abstract": "scaling up wsd with automatically generated examples abstract the most accurate approaches to word sense disambiguation (wsd) for biomedical documents are based on supervised learning. however, these require manually labeled training examples which are expensive to create and consequently supervised wsd systems are normally limited to disambiguating a small set of ambiguous terms. an alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones. this paper describes a large scale wsd system based on automatically labeled examples generated using information from the umls metathesaurus. the labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches). the system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the umls metathesaurus.",
        "conclusion": "conclusion this paper describes the development of a large scale wsd system based on automatically labeled examples. we find that these examples can be generated for the majority of cuis in the umls metathesaurus. evaluation on the nlm-wsd and mshwsd data sets demonstrates that the wsd system outperforms the ppr approach without making any use of labeled data. three techniques for determining the number of examples to use for training are explored. it is found that a supervised approach (which makes use of manually labeled data) provides the best results. surprisingly it was also found that using information from the mbr did not improve performance. analysis showed that the sense distributions extracted from the mbr were different from those observed in the evaluation data, providing an explanation for this result. evaluation showed that accurate information about the bias of training examples is useful for wsd systems and future work will explore other unsupervised ways of obtaining this information. alternative techniques for generating labeled examples will also be explored. in addition, further evaluation of the wsd system will be carried out, such as applying it to an all words task and within applications.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E09-1053": {
        "abstract": "dependency trees and the strong generative capacity of ccg abstract we propose a novel algorithm for extracting dependencies from the derivations of a large fragment of ccg. unlike earlier proposals, our dependency structures are always tree-shaped. we then use these dependency trees to compare the strong generative capacities of ccg and tag and obtain surprising results: both formalisms generate the same languages of derivation trees \u2013 but the mechanisms they use to bring the words in these trees into a linear order are incomparable.",
        "conclusion": "conclusion in this paper, we have shown how to read derivations of pf-ccg as dependency trees. unlike previous proposals, our view on ccg dependencies is in line with the mainstream dependency parsing literature, which assumes tree-shaped dependency structures; while our dependency trees are less informative than the ccg derivations themselves, they contain sufficient information to reconstruct the semantic representation. we used our new dependency view to compare the strong generative capacity of pf-ccg with other mildly contextsensitive grammar formalisms. it turns out that the valency trees generated by a pf-ccg grammar form regular tree languages, as in tag and lcfrs; however, unlike these formalisms, the sets of dependency trees including word order are not regular, and in particular can be more non-projective than the other formalisms permit. finally, we found new formal evidence for the importance of restricting rule schemata for describing non-context-free languages in ccg. all these results were technically restricted to the fragment of pf-ccg, and one focus of future work will be to extend them to as large a fragment of ccg as possible. in particular, we plan to extend the lambda notation used in figure 3 to cover typeraising and higher-order categories. we would then be set to compare the behavior of wide-coverage statistical parsers for ccg with statistical dependency parsers. we anticipate that our results about the strong generative capacity of pf-ccg will be useful to transfer algorithms and linguistic insights between formalisms. for instance, the crisp generation algorithm (koller and stone, 2007), while specified for tag, could be generalized to arbitrary grammar formalisms that use regular tree languages\u2014 given our results, to ccg in particular. on the other hand, we find it striking that ccg and tag generate the same string languages from the same tree languages by incomparable mechanisms for ordering the words in the tree. indeed, the exact characterization of the class of ccg-inducable dependency languages is an open issue. this also has consequences for parsing complexity: we can understand why tag and lcfrs can be parsed in polynomial time from the bounded block-degree of their dependency trees (kuhlmann and m\u00f6hl, 2007), but ccg can be parsed in polynomial time (vijay-shanker and weir, 1990) without being restricted in this way. this constitutes a most interesting avenue of future research that is opened up by our results.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W10-1407": {
        "abstract": "direct parsing of discontinuous constituents in german abstract discontinuities occur especially frequently in languages with a relatively free word order, such as german. generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of probabilistic cfg, i.e., they cannot be directly reconstructed by a pcfg parser. in this paper, we use a parser for probabilistic linear context-free rewriting systems (plcfrs), a formalism with high expressivity, to directly parse the german negra and tiger treebanks. in both treebanks, discontinuities are annotated with crossing branches. based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of pcfg-based systems.",
        "conclusion": "conclusion and future work we have investigated the possibility of using probabilistic linear context-free rewriting systems for direct parsing of discontinuous constituents. consequently, we have applied a plcfrs parser on the german negra and tiger treebanks. our evaluation, which used different metrics, showed that a plcfrs parser can achieve competitive results. in future work, all of the presented evaluation methods will be investigated to greater detail. in order to do this, we will parse our data sets with current state-of-the-art systems. especially a more elaborate dependency conversion should enable a more informative comparison between the output of pcfg parsers and the output of the plcfrs parser. last, since an algorithm is available which extracts lcfrss from dependency structures (kuhlmann and satta, 2009), the parser is instantly ready for parsing them. we are currently performing the corresponding experiments.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P04-1042": {
        "abstract": "deep dependencies from context-free statistical parsers: correcting the surface dependency approximation abstract we present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks. we use an algorithm based on loglinear classifiers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation. we find that our algorithm compares favorably with prior work on english using an existing evaluation metric, and also introduce and argue for a new dependency-based evaluation metric. by this new evaluation metric our algorithm achieves 60% error reduction on gold-standard input trees and 5% error reduction on state-ofthe-art machine-parsed input trees, when compared with the best previous work. we also present the first results on nonlocal dependency reconstruction for a language other than english, comparing performance on english and german. our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order, the surface dependencies in context-free parse trees are a poorer approximation to underlying dependency structure.",
        "conclusion": "discussion the wsj results shown in tables 2 and 3 suggest that discriminative models incorporating both nonlocal and local lexical and syntactic information can achieve good results on the task of non-local dependency identification. on the parseval metric, our algorithm performed particularly well on null complementizer and control locus insertion, and on s node relocation. in particular, johnson noted that the proper insertion of control loci was a difficult issue involving lexical as well as structural sensitivity. we found the loglinear paradigm a good one in which to model this feature combination; when run in isolation on gold-standard development trees, our model reached 96.4% f1 on control locus insertion, reducing error over the johnson model\u2019s 89.3% 14many head-dependent relations in negra are explicitly marked, but for those that are not we used a collins (1999)style head-finding algorithm independently developed for german pcfg parsing. by nearly two-thirds. the performance of our algorithm is also evident in the substantial contribution to typed dependency accuracy seen in table 3. for gold-standard input trees, our algorithm reduces error by over 80% from the surface-dependency baseline, and over 60% compared with johnson\u2019s results. for parsed input trees, our algorithm reduces dependency error by 23% over the baseline, and by 5% compared with johnson\u2019s results. note that the dependency figures of dienes lag behind even the parsed results for johnson\u2019s model; this may well be due to the fact that dienes built his model as an extension of collins (1999), which lags behind charniak (2000) by about 1.3-1.5%. manual investigation of errors on english goldstandard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size. first, we noted that annotation inconsistency accounted for a large number of errors, particularly false positives. vps from which an s has been extracted ([sshut up,] he [vp said t]) are inconsistently given an empty sbar daughter, suggesting the cross-model low-70\u2019s performance on null sbar insertion models (see table 2) may be a ceiling. control loci were often under-annotated; the first five development-set false positive control loci we checked were all due to annotation error. and why-whadvps under sbar, which are always dislocations, were not so annotated 20% of the time. second, both control locus insertion and dislocated np remapping must be sensitive to the presence of argument nps under classified nodes. but temporal nps, indistinguishable by gross category, also appear under such nodes, creating a major confound. we used customized features to compensate to some extent, but temporal annotation already exists in wsj and could be used. we note that klein and manning (2003) independently found retention of temporal np marking useful for pcfg parsing. as can be seen in table 3, the absolute improvement in dependency recovery is smaller for both our and johnson\u2019s postprocessing algorithms when applied to parsed input trees than when applied to gold-standard input trees. it seems that this degradation is not primarily due to noise in parse tree outputs reducing recall of nonlocal dependency identification: precision/recall splits were largely the same between gold and parsed data, and manual inspection revealed that incorrect nonlocal dependency choices often arose from syntactically reasonable yet incorrect input from the parser. for example, the gold-standard parse right-wing whites ... will [vp step up [np their threats [s [vp * to take matters into their own hands ]]]] has an unindexed control locus because treebank annotation specifies that infinitival vps inside nps are not assigned controllers. charniak\u2019s parser, however, attaches the infinitival vp into the higher step up ... vp. infinitival vps inside vps generally do receive controllers for their null subjects, and our algorithm accordingly yet mistakenly assigns right-wing-whites as the antecedent. the english/german comparison shown in tables 4 and 5 is suggestive, but caution is necessary in its interpretation due to the fact that differences in both language structure and treebank annotation may be involved. results in the g column of table 5, showing the accuracy of the context-free dependency approximation from gold-standard parse trees, quantitatively corroborates the intuition that nonlocal dependency is more prominent in german than in english. manual investigation of errors made on german gold-standard data revealed two major sources of error beyond sparsity. the first was a widespread ambiguity of s and vp nodes within s and vp nodes; many true dislocations of all sorts are expressed at the s and vp levels in cfg parse trees, such as vp1 of figure 2, but many adverbial and subordinate phrases of s or vp category are genuine dependents of the main clausal verb. we were able to find a number of features to distinguish some cases, such as the presence of certain unambiguous relativeclause introducing complementizers beginning an s node, but much ambiguity remained. the second was the ambiguity that some matrix s-initial nps are actually dependents of the vp head (in these cases, negra annotates the finite verb as the head of s and the non-finite verb as the head of vp). this is not necessarily a genuine discontinuity per se, but rather corresponds to identification of the subject np in a clause. obviously, having access to reliable case marking would improve performance in this area; such information is in fact included in negra\u2019s morphological annotation, another argument for the utility of involving enhanced annotation in cf parsing. as can be seen in the right half of table 4, performance falls off considerably on vanilla pcfgwsj(full) wsj(sm) negra parsed data. this fall-off seems more dramatic than that seen in sections 4.1 and 4.2, no doubt partly due to the poorer performance of the vanilla pcfg, but likely also because only non-relativization dislocations are considered in section 4.3. these dislocations often require non-local information (such as identity of surface lexical governor) for identification and are thus especially susceptible to degradation in parsed data. nevertheless, seemingly dismal performance here still provided a strong boost to typed dependency evaluation of parsed data, as seen in a \u25e6 p of table 5. we suspect this indicates that dislocated terminals are being usefully identified and mapped back to their proper governors, even if the syntactic projections of these terminals and governors are not being correctly identified by the parser. further work against the background of cfg as the standard approximation of dependency structure for broadcoverage parsing, there are essentially three options for the recovery of nonlocal dependency. the first option is to postprocess cf parse trees, which we have closely investigated in this paper. the second is to incorporate nonlocal dependency information into the category structure of cf trees. this was the approach taken by dienes and dubey (2003a,b) and dienes (2003); it is also practiced in recent work on broad-coverage ccg parsing (hockenmaier, 2003). the third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart. this approach was tentatively investigated by plaehn (2000). as the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-3131": {
        "abstract": "translation system the cmu-avenue french-english translation system abstract this paper describes the french-english translation system developed by the avenue research group at carnegie mellon university for the seventh workshop on statistical machine translation (naacl wmt12). we present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building.",
        "conclusion": "summary we have presented the french-english translation system built for the naacl wmt12 shared translation task, including descriptions of our data selection and text processing techniques. experimental results have shown incremental improvement for each addition to our baseline system. we have finally discussed the impact of the availability of wmtscale data on system building decisions and provided comparative experimental results.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P04-1015": {
        "abstract": "incremental parsing with the perceptron algorithm abstract this paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. a beam-search algorithm is used during both training and decoding phases of the method. the perceptron approach was implemented with the same feature set as that of an existing generative model (roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the penn treebank. we demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent f-measure improvement over the generative model alone, to 88.8 percent.",
        "conclusion": "conclusions in this paper we have presented a discriminative training approach, based on the perceptron algorithm with a couple of effective refinements, that provides a model capable of effective heuristic search over a very difficult search space. in such an approach, the unnormalized discriminative parsing model can be applied without either cluding labeled precision (lp), labeled recall (lr), and f-measure an external model to present it with candidates, or potentially expensive dynamic programming. when the training algorithm is provided the generative model scores as an additional feature, the resulting parser is quite competitive on this task. the improvement that was derived from the additional punctuation features demonstrates the flexibility of the approach in incorporating novel features in the model. future research will look in two directions. first, we will look to include more useful features that are difficult for a generative model to include. this paper was intended to compare search with the generative model and the perceptron model with roughly similar feature sets. much improvement could potentially be had by looking for other features that could improve the models. secondly, combining with the generative model can be done in several ways. some of the constraints on the search technique that were required in the absence of the generative model can be relaxed if the generative model score is included as another feature. in the current paper, the generative score was simply added as another feature. another approach might be to use the generative model to produce candidates at a word, then assign perceptron features for those candidates. such variants deserve investigation. overall, these results show much promise in the use of discriminative learning techniques such as the perceptron algorithm to help perform heuristic search in difficult domains such as statistical parsing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E99-1016": {
        "abstract": "cascaded markov models abstract this paper presents a new approach to partial parsing of context-free structures. the approach is based on markov mod- els. each layer of the resulting structure is represented byits own markov model, and output of a lower layer is passed as input to the next higher layer. an em- pirical evaluation of the method yields very good results for np/pp chunking of german ewspaper texts.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C08-1049": {
        "abstract": "word lattice word lattice reranking for chinese word segmentation and part-of-speech tagging abstract in this paper, we describe a new rerank ing strategy named word lattice reranking,for the task of joint chinese word segmen tation and part-of-speech (pos) tagging. as a derivation of the forest reranking for parsing (huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking. with aperceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local fea tures that can?t be easily incorporated intothe perceptron baseline. experimental results show that, this strategy achieves im provement on both segmentation and pos tagging, above the perceptron baseline and the n-best list reranking.",
        "conclusion": "conclusion this paper describes a reranking strategy calledword lattice reranking. as a derivation of the forest reranking of huang (2008), it performs rerank ing on pruned word lattice, instead of on n-best list. using word- and pos- gram information, this reranking technique achieves an error reduction of 16.3% on joint s&t, and 11.9% on segmentation, over the baseline classifier, and it also outperformsreranking on n-best list. it confirms that word lattice reranking can effectively use non-local infor mation to select the best candidate result, from a relative small representation structure while with aquite high oracle f-measure. however, our rerank ing implementation is relative coarse, and it must have many chances for improvement. in futurework, we will develop more precise pruning al gorithm for word lattice generation, to further cutdown the search space while maintaining the ora cle f-measure. we will also investigate the featureselection strategy under the word lattice architec ture, for effective use of non-local information. acknowledgementthis work was supported by national natural sci ence foundation of china, contracts 60736014 and 60573188, and 863 state key project no. 2006aa010108. we show our special thanks to liang huang for his valuable suggestions.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P04-1054": {
        "abstract": "dependency tree kernels for relation extraction abstract we extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. using this kernel within a support vector machine, we detect and classify relations between entities in the automatic content extraction (ace) corpus of news articles. we examine the utility of different features such as wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% f1 improvement over a \u201cbag-of-words\u201d kernel.",
        "conclusion": "conclusions we have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel. while the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low. detecting relations is a difficult task for a kernel method because the set of all non-relation instances is extremely heterogeneous, and is therefore difficult to characterize with a similarity metric. an improved system might use a different method to detect candidate relations and then use this kernel method to classify the relations. future work the most immediate extension is to automatically learn the feature compatibility function c(vq, vr). a first approach might use tf-idf to weight each feature. another approach might be to calculate the information gain for each feature and use that as its weight. a more complex system might learn a weight for each pair of features; however this seems computationally infeasible for large numbers of features. one could also perform latent semantic indexing to collapse feature values into similar \u201ccategories\u201d \u2014 for example, the words \u201cfootball\u201d and \u201cbaseball\u201d might fall into the same category. here, c(vq, vr) might return \u03b11 if vq = vr, and \u03b12 if vq and vr are in the same category, where \u03b11 > \u03b12 > 0. any method that provides a \u201csoft\u201d match between feature values will sharpen the granularity of the kernel and enhance its modeling power. further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel. these results contradict those given in zelenko et al. (2003), where the sparse kernel achieves 2-3% better f1 performance than the contiguous kernel. it is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P08-1013": {
        "abstract": "language model applying a grammar-based language model to a simplified broadcast-news transcription task abstract we propose a language model based on a precise, linguistically motivated grammar (a hand-crafted head-driven phrase structure grammar) and a statistical model estimating the probability of a parse tree. the language model is applied by means of an n-best rescoring step, which allows to directly measure the performance gains relative to the baseline system without rescoring. to demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified german broadcast-news transcription task. we report a significant reduction in word error rate compared to a state-of-the-art baseline system.",
        "conclusion": "conclusions and outlook we have presented a language model based on a precise, linguistically motivated grammar, and we have successfully applied it to a difficult broad-domain task. it is a well-known fact that natural language is highly ambiguous: a correct and seemingly unambiguous sentence may have an enormous number of readings. a related \u2013 and for our approach even more relevant \u2013 phenomenon is that many weirdlooking and seemingly incorrect word sequences are in fact grammatical. this obviously reduces the benefit of pure grammaticality information. a solution is to use additional information to asses how \u201cnatural\u201d a reading of a word sequence is. we have done a mum number of best hypotheses n. first step in this direction by estimating the probability of a parse tree. however, our model only looks at the structure of a parse tree and does not take the actual words into account. as n-grams and statistical parsers demonstrate, word information can be very valuable. it would therefore be interesting to investigate ways of introducing word information into our grammar-based model.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P10-1097": {
        "abstract": "contextualizing semantic representations using syntactically enriched vector models abstract we present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. it employs a systematic combination of firstand second-order context vectors. we apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.",
        "conclusion": "conclusion we have presented a novel method for adapting the vector representations of words according to their context. in contrast to earlier approaches, our model incorporates detailed syntactic information. we solved the problems of data sparseness and incompatibility of dimensions which are inherent in this approach by modeling contextualization as an interplay between first- and second-order vectors. evaluating on the semeval 2007 lexical substitution task dataset, our model performs substantially better than all earlier approaches, exceeding the state of the art by around 9% in terms of generalized average precision and around 7% in terms of precision out of ten. also, our system is the first unsupervised method that has been applied to erk and mccarthy\u2019s (2009) graded word sense assignment task, showing a substantial positive correlation with the gold standard. we further showed that a weakly supervised heuristic, making use of wordnet sense ranks, can be significantly improved by incorporating information from our system. we studied the effect that context has on target words in a series of experiments, which vary the target word and keep the context constant. a natural objective for further research is the influence of varying contexts on the meaning of target expressions. this extension might also shed light on the status of the modelled semantic process, which we have been referring to in this paper as \u201ccontextualization\u201d. this process can be considered one of mutual disambiguation, which is basically the view of e&p. alternatively, one can conceptualize it as semantic composition: in particular, the head of a phrase incorporates semantic information from its dependents, and the final result may to some extent reflect the meaning of the whole phrase. another direction for further study will be the generalization of our model to larger syntactic contexts, including more than only the direct neighbors in the dependency graph, ultimately incorporating context information from the whole sentence in a recursive fashion.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P07-1083": {
        "abstract": "alignment-based discriminative string similarity abstract a character-based measure of similarity is an important component of many natural language processing systems, including approaches to transliteration, coreference, word alignment, spelling correction, and the identification of cognates in related vocabu- we propose an alignment-based disfor string similarity. we gather features from substring pairs consistent with a character-based alignment of the two strings. this approach achieves exceptional performance; on nine separate cognate identification experiments using six language pairs, we more than double the precision of traditional orthographic measures like longest common subsequence ratio and dice\u2019s coefficient. we also show strong improvements over other recent discriminative and heuristic similarity functions.",
        "conclusion": "conclusion this is the first research to apply discriminative string similarity to the task of cognate identification. we have introduced and successfully applied an alignment-based framework for discriminative similarity that consistently demonstrates improved performance in both bitext and dictionary-based cognate identification on six language pairs. our improved approach can be applied in any of the diverse applications where traditional similarity measures like edit distance and lcsr are prevalent. we have also made available our cognate identification data sets, which will be of interest to general string similarity researchers. furthermore, we have provided a natural framework for future cognate identification research. phonetic, semantic, or syntactic features could be included within our discriminative infrastructure to aid in the identification of cognates in text. in particular, we plan to investigate approaches that do not require the bilingual dictionaries or bitexts to generate training data. for example, researchers have automatically developed translation lexicons by seeing if words from each language have similar frequencies, contexts (koehn and knight, 2002), burstiness, inverse document frequencies, and date distributions (schafer and yarowsky, 2002). semantic and string similarity might be learned jointly with a co-training or bootstrapping approach (klementiev and roth, 2006). we may also compare alignmentbased discriminative string similarity with a more complex discriminative model that learns the alignments as latent structure (mccallum et al., 2005).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W10-3504": {
        "abstract": "expanding textual entailment corpora fromwikipedia using co-training abstract in this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones. the key idea is the combination of two intuitions: (1) the use of wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. we report empirical evidence that our method successfully expands existing textual entailment corpora.",
        "conclusion": "conclusions in this paper we proposed a method for expanding existing textual entailment corpora that leverages wikipedia. the method is extremely promising as it allows building corpora homogeneous to existing ones. the model we have presented is not strictly related to the rte corpora. this method can then be used to expand corpora such as the fracas test-suite (cooper et al., 1996) which is more oriented to specific semantic phenomena. even if the performance increase of the completely unsupervised cotraining method is not extremely high, this model can be used to semiautomatically expanding corpora by using active learning techniques (cohn et al., 1996). the initial increase of performances is an interesting starting point. in the future, we aim at releasing the annotated portion of the wiki corpus to the community; we will also carry out further experiments and refine the feature spaces. finally, as wikipedia is a multilingual resource, we will use the wiki methodology to semi-automatically build rte corpora for other languages.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-2074": {
        "abstract": "discriminative feature-tied mixture modeling for statistical machine translation abstract in this paper we present a novel discriminative mixture model for statistical machine translation (smt). we model the feature space with a log-linear combination of multiple mixture components. each component contains a large set of features trained in a maximumentropy framework. all features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance. this approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for smt. it is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. the proposed approach improves the translation performance significantly on a large-scale arabic-to-english mt task.",
        "conclusion": "conclusion in this paper we presented a novel discriminative mixture model for bridging the gap between the maximum-likelihood training and the discriminative training in smt. we partition the feature space into multiple regions. the features in each region are tied together to share the same mixture weights that are optimized towards the maximum bleu scores. it was shown that the same model structure can be effectively applied to feature combination, alignment combination and domain adaptation. we also point out that it is straightforward to combine any of these three. for example, we can cluster the features based on both feature types and alignments. further improvement may be achieved with other feature space partition approaches in the future.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W10-1404": {
        "abstract": "application of different techniques to dependency parsing of basque abstract we present a set of experiments on dependency parsing of the basque dependency treebank (bdt). the present work has examined several directions that try to explore the rich set of morphosyntactic features in the bdt: i) experimenting the impact of morphological features, ii) application of dependency tree transformations, iii) application of a two-stage parsing scheme (stacking), and iv) combinations of the individual experiments. all the tests were conducted using maltparser (vivre et al., 2007a), a freely available and state of the art dependency parser generator.",
        "conclusion": "conclusions and future work we studied several proposals for improving a baseline system for parsing the basque treebank. all the results were evaluated on the new version, bdt ii, three times larger than the previous one. we have obtained the following main results: \u2022 using rich morphological features. we have extended previous works, giving a finer grained description of morphosyntactic features on the learner\u2019s configuration, (fo: feature optimization; tp tc ts: pseudo-projective, coordination and subordinated sentence transformations; svg, snp, sc: stacking (feature passing) on verb groups, nps and coordination; sp, sch, sgp: stacking (category, features and dependency) on parent, children and grandparent; *: statistically significant in mcnemar's test, p < 0.005; **: statistically significant, p < 0.001) showing that it can significantly improve the results. in particular, differentiating case and the type of subordinated sentence gives the best las increase (+0.82%). the effect of a stacked learning scheme. some of the stacked features were languageindependent, as in (nivre and mcdonald. 2008), but we have also applied a generalization of the stacking mechanism to a morphologically rich language, as some of the stacked features are morphosyntactic features (such as case and number) which were propagated through a first stage dependency tree by the application of linguistic principles (noun phrases, verb groups and coordination). with respect to the individual systems, some others do not give a improvement over the basic systems. a careful study must be conducted to investigate whether the approaches are exclusive or complementary. for example, the transformation on subordinated sentences and feature propagation on verbal groups seem to be attacking the same problem, i. e., the relations between main and subordinated sentences. in this respect, they can be viewed as alternative approaches to dealing with these phenomena. the results show that the application of these techniques can give noticeable results, getting an overall improvement of 1.90% (from 77.08% until 78.98%), which can be roughly comparable to the effect of doubling the size of the treebank (see the last two lines of table 1).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E06-2025": {
        "abstract": "estimation methods theoretical evaluation of estimation methods for data-oriented parsing abstract we analyze estimation methods for data- oriented parsing, as well as the theoretical criteria used to evaluate them. we show that all current estimation methods are inconsistent in the \u201cweight-distribution test\u201d, and argue that these results force us to rethink both the methods proposed and the criteria used.",
        "conclusion": "discussion & conclusions a desideratum for parameter estimation methods is that they converge to the correct parameters with infinitely many data \u2013 that is, we like an estimator to be consistent. the stsg formalism, however, allows for many different derivations of the same parse tree, and for many different grammars to generate the same frequency-distribution. consistency in the weight-distribution test is therefore too stringent a criterion. we have shown that dop1 and methods based on correction factors also fail the weaker frequency-distribution test. however, the only current estimation methods that are consistent in the frequency-distribution test, have the linguistically undesirable property of converging to a distribution with all probability mass in complete parse trees. although these method fail the weight-distribution test for the whole class of stsgs, we argued earlier that this test is not the appropriate test either. both estimation methods for stsgs and the criteria for evaluating them, thus require thorough rethinking. in forthcoming work we therefore study yet another estimator, and the linguistically motivated evaluation criterion of convergence to a maximally general stsg consistent with the training data5.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P08-1006": {
        "abstract": "syntactic parsers and their representations task-oriented evaluation of syntactic parsers and their representations abstract this paper presents a comparative evaluation of several state-of-the-art english parsers based on different frameworks. our approach is to measure the impact of each parser when it is used as a component of an information extraction system that performs protein-protein interaction (ppi) identification in biomedical papers. we evaluate eight parsers (based on dependency parsing, phrase structure parsing, or deep parsing) using five different parse representations. we run a ppi system with several combinations of parser and parse representation, and examine their impact on ppi identification accuracy. our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data.",
        "conclusion": "conclusion and future work we have presented our attempts to evaluate syntactic parsers and their representations that are based on different frameworks; dependency parsing, phrase structure parsing, or deep parsing. the basic idea is to measure the accuracy improvements of the ppi extraction task by incorporating the parser output as statistical features of a machine learning classifier. experiments showed that state-of-theart parsers attain accuracy levels that are on par with each other, while parsing speed differs significantly. we also found that accuracy improvements vary when parsers are retrained with domainspecific data, indicating the importance of domain adaptation and the differences in the portability of parser training methods. although we restricted ourselves to parsers trainable with penn treebank-style treebanks, our methodology can be applied to any english parsers. candidates include rasp (briscoe and carroll, 2006), the c&c parser (clark and curran, 2004), the xle parser (kaplan et al., 2004), minipar (lin, 1998), and link parser (sleator and temperley, 1993; pyysalo et al., 2006), but the domain adaptation of these parsers is not straightforward. it is also possible to evaluate unsupervised parsers, which is attractive since evaluation of such parsers with goldstandard data is extremely problematic. a major drawback of our methodology is that the evaluation is indirect and the results depend on a selected task and its settings. this indicates that different results might be obtained with other tasks. hence, we cannot conclude the superiority of parsers/representations only with our results. in order to obtain general ideas on parser performance, experiments on other tasks are indispensable.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D08-1008": {
        "abstract": "dependency-based semantic role labeling of propbank abstract we present a propbank semantic role label ing system for english that is integrated with a dependency parser. to tackle the problemof joint syntactic?semantic analysis, the system relies on a syntactic and a semantic subcomponent. the syntactic model is a projective parser using pseudo-projective transfor mations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. the complete syntactic?semanticoutput is selected from a candidate pool gen erated by the subsystems.we evaluate the system on the conll 2005 test sets using segment-based and dependency-based metrics. using the segment-based conll-2005 metric, our system achieves a near state-of-the-art f1 figure of 77.97 on the wsj+brown test set, or 78.84 if punctuation is treated consistently. using a dependency-based metric, the f1 figure of our system is 84.29 on the test set from conll-2008. our system is the first dependency-based semantic role labeler for propbank that rivals constituent-based systems in terms of performance.",
        "conclusion": "conclusion we have described a dependency-based system1 for semantic role labeling of english in the propbankframework. our evaluations show that the perfor mance of our system is close to the state of theart. this holds regardless of whether a segmentbased or a dependency-based metric is used. in terestingly, our system has a complete proposition accuracy that surpasses other systems by nearly 3 percentage points. our system is the first semantic role labeler based only on syntactic dependency that achieves a competitive performance. evaluation and comparison is a difficult issuesince the natural output of a dependency-based sys tem is a set of semantic links rather than segments, as is normally the case for traditional systems. to handle this situation fairly to both types of systems, we carried out a two-way evaluation: conversion of dependencies to segments for the dependency-basedsystem, and head-finding heuristics for segment based systems. however, the latter is difficult since no structure is available inside segments, and we had to resort to computing upper-bound results usinggold-standard input; despite this, the dependency based system clearly outperformed the upper bound of the performance of the segment-based system. the comparison can also be slightly misleading since the dependency-based system was optimized for the dependency metric and previous systems for the segment metric.our evaluations suggest that the dependency based srl system is biased to finding argument heads, rather than argument text snippets, and thisis of course perfectly logical. whether this is an advantage or a drawback will depend on the applica tion ? for instance, a template-filling system might need complete segments, while an srl-based vectorspace representation for text categorization, or a rea soning application, might prefer using heads only. in the future, we would like to further investigatewhether syntactic and semantic analysis could be integrated more tightly. in this work, we used a sim 1our system is freely available for download at http://nlp.cs.lth.se/lth_srl. 76 plistic loose coupling by means of reranking a small set of complete structures. the same criticisms that are often leveled at reranking-based models clearly apply here too: the set of tentative analyses from the submodules is too small, and the correct analysis is often pruned too early. an example of a method to mitigate this shortcoming is the forest reranking byhuang (2008), in which complex features are evalu ated as early as possible. a classifier features features used in predicate disambiguation predword, predlemma. the lexical form and lemma of the predicate. predparentword and predparentpos. form and part-of-speech tag of the parent node of the predicate.childdepset, childwordset, childworddepset, childposset, child posdepset. these features represent the setof dependents of the predicate using combina tions of dependency labels, words, and parts of speech.depsubcat. subcategorization frame: the concatenation of the dependency labels of the pred icate dependents.predreltoparent. dependency relation be tween the predicate and its parent. features used in argument identification and labelingpredlemmasense. the lemma and sense num ber of the predicate, e.g. give.01. voice. for verbs, this feature is active or passive. for nouns, it is not defined. position. position of the argument with respect to the predicate: before, after, or on.argword and argpos. lexical form and part of-speech tag of the argument node.leftword, leftpos, rightword, rightpos. form/part-of-speech tag of the left most/rightmost dependent of the argument. leftsiblingword, leftsiblingpos. form/part-of-speech tag of the left sibling of the argument. predpos. part-of-speech tag of the predicate.relpath. a representation of the complex gram matical relation between the predicate and theargument. it consists of the sequence of de pendency relation labels and link directions in the path between predicate and argument, e.g. im?oprd?obj?. verbchainhassubj. binary feature that is set to true if the predicate verb chain has a subject. the purpose of this feature is to resolve verb coordination ambiguity as in figure 3. controllerhasobj. binary feature that is true if the link between the predicate verb chain andits parent is oprd, and the parent has an ob ject. this feature is meant to resolve control ambiguity as in figure 4.function. the grammatical function of the argument node. for direct dependents of the predi cate, this is identical to the relpath. i sbj eat drinkyouand coord sbjconj root sbj coord root drinkandeati conj figure 3: coordination ambiguity: the subject i is in an ambiguous position with respect to drink. i to imsbj want sleephim objoprd root im sleepi sbj want root to oprdfigure 4: subject/object control ambiguity: i is in an am biguous position with respect to sleep. 77",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W08-2107": {
        "abstract": "picking them up and figuring them out: verb-particle constructions noise and idiomaticity abstract this paper investigates, in a first stage, some methods for the automatic acquisition of verb-particle constructions (vpcs) taking into account their statistical properties and some regular patterns found in productive combinations of verbs and particles. given the limited coverage provided by lexical resources, such as dictionaries, and the constantly growing number of vpcs, possible ways of automatically identifying them are crucial for any nlp task that requires some degree of semantic interpretation. in a second stage we also study whether the combination of statistical and linguistic properties can provide some indication of the degree of idiomaticity of a given vpc. the results obtained show that such combination can successfully be used to detect vpcs and distinguish idiomatic from compositional cases.",
        "conclusion": "conclusions one of the important challenges for robust natural language processing systems is to be able to successfully deal with multiword expressions and related constructions. we investigated the identification of vpcs using a combination of statistical methods and linguistic information, and whether there is a correlation between the productivity of vpcs and their semantics that could help us detect if a vpc is idiomatic or compositional. the results confirm that the use of statistical and linguistic information to automatically identify verb-particle constructions presents a reasonable way of improving coverage of existing lexical resources in a very simple and straightforward manner. in terms of grammar engineering, the information about compositional candidates belonging to productive classes provides us with the basis for constructing a family of fine-grained redundancy rules for these classes. these rules are applied in a constrained way to verbs already in the lexicon, according to their semantic classes. the vpcs identified as idiomatic, on the other hand, need to be explicitly added to the lexicon, after their semantic is determined. this study can also be complemented with the results of investigations into the semantics of vpcs, as discussed by both bannard (2005) and mccarthy et al. (2003). in addition, the use of clustering methods is an interesting possibility for automatically identifying clusters of productive classes of both verbs and of particles that combine well together.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P07-1055": {
        "abstract": "structured models for fine-to-coarse sentiment analysis abstract in this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. inference in the model is based on standard sequence classification techniques using constrained viterbi to ensure consistent solutions. the primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another. experiments show that this method can significantly reduce classification error relative to models trained in isolation.",
        "conclusion": "future work 4 future work finally we should note that experiments using one important extension to this work is to augment crfs to train the structured models and logistic re- the models for partially labeled data. it is realistic gression to train the local models yielded similar re- to imagine a training set where many examples do sults to those in table 2. not have every level of sentiment annotated.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P08-1102": {
        "abstract": "cascaded linear model a cascaded linear model for joint chinese word segmentation and part-of-speech tagging abstract we propose a cascaded linear model for joint chinese word segmentation and partof-speech tagging. with a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. on the penn chinese treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.",
        "conclusion": "conclusions we proposed a cascaded linear model for chinese joint s&t. under this model, many knowledge sources that may be intractable to be incorporated into the perceptron directly, can be utilized effectively in the outside-layer linear model. this is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large. however, can the perceptron incorporate all the knowledge used in the outside-layer linear model? if this cascaded linear model were chosen, could more accurate generative models (lms, word-pos co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of mcclosky (2006)? in addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (ng and low, 2004). how can we utilize these knowledge sources effectively? we will investigate these problems in the following work.",
        "citations": [],
        "summary": [
            "Wenbin Jiang,Liang Huang,Qun Liu and Yajuan Lu in their paper 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging'proposes a cascaded linear model inspired by the log-linear model for joint Chinese word segmentation and POS tagging.the cascaded model has a two-layer architecture, with a character based perceptron as the core combined with other real-valued features such as language models.Other than the usual character-based features, additional features dependent on POS\u2019s or words are also employed to improve the performance.Experimental results show that, it achieves obvious improvement over the perceptron-only model, about from 0.973 to 0.978 on segmentation, and from 0.925 to 0.934 on Joint S&T, with error reductions of 18.5% and 12% respectively.\n In this paper, Jiang et all proposed a cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model was able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments showed that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, they obtained an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline. Under this model, many knowledge sources that might be intractable to be incorporated into the perceptron directly, could be utilized effectively in the outside-layer linear model. This was a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large. This paper aims at proposing a cascaded linear model for joint Chinese word segmentation and part-of- speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. However, as such features are generated dynamically during the decoding procedure, two limitations arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. To cope with this problem, we propose a cascaded linear model inspired by the log-linear model widely used in statistical machine translation to incorporate different kinds of knowledge sources. In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources can be used to improve performance."
        ]
    },
    "P11-2124": {
        "abstract": "joint hebrew segmentation and parsing using a pcfgla lattice parser abstract we experiment with extending a lattice parsing methodology for parsing hebrew (goldberg and tsarfaty, 2008; golderg et al., 2009) to make use of a stronger syntactic model: the pcfg-la berkeley parser. we show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented hebrew text with an f-score of almost 80%, an error reduction of over 20% over the best previous result for this task. this result indicates that lattice parsing with the berkeley parser is an effective methodology for parsing over uncertain inputs.",
        "conclusion": "conclusions and future work we demonstrated that the combination of lattice parsing with the pcfg-la berkeley parser is highly effective. lattice parsing allows much needed flexibility in providing input to a parser when the yield of the tree is not known in advance, and the grammar refinement and estimation techniques of the berkeley parser provide a strong disambiguation component. in this work, we applied the berkeley+lattice parser to the challenging task of joint segmentation and parsing of hebrew text. the result is the first constituency parser which can parse naturally occurring unsegmented hebrew text with an acceptable accuracy (an fi score of 80%). many other uses of lattice parsing are possible. these include joint segmentation and parsing of chinese, empty element prediction (see (cai et al., 2011) for a successful application), and a principled handling of multiword-expressions, idioms and named-entities. the code of the lattice extension to the berkeley parser is publicly available.10 despite its strong performance, we observed that the berkeley parser did not learn morphological agreement patterns. agreement information could be very useful for disambiguating various constructions in hebrew and other morphologically rich languages. we plan to address this point in future work.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "S12-1097": {
        "abstract": "university_of_sheffield: two approaches to semantic text similarity abstract this paper describes the university of sheffield?s submission to semeval-2012 task 6: semantic text similarity. two approaches were developed. the first is an unsupervised technique based on the widely used vector space model and information from wordnet.the second method relies on supervised ma chine learning and represents each sentence as a set of n-grams. this approach also makes use of information from wordnet. resultsfrom the formal evaluation show that both approaches are useful for determining the simi larity in meaning between pairs of sentences with the best performance being obtained bythe supervised approach. incorporating information from wordnet alo improves perfor mance for both approaches.",
        "conclusion": "conclusion and future work two approaches for computing semantic similaritybetween sentences were explored. the first, unsu pervised approach, uses a vector space model andcomputes similarity between sentences by comparing vectors while the second is supervised and rep resents the sentences as sets of n-grams. both approaches used wordnet to provide information about similarity between lexical items. results fromevaluation show that the supervised approach provides the best results on average but also that per formance of the unsupervised approach is better forsome data sets. the best overall results for the semeval evaluation were obtained using a hybrid system that attempts to choose the most suitable ap proach for each data set. the results reported here show that the semantic text similarity task can be successfully approached using lexical overlap techniques augmented withlimited semantic information derived from word net. in future, we would like to explore whether performance can be improved by applying deeper analysis to provide information about the structure and semantics of the sentences being compared. for example, parsing the input sentences would provide more information about their structure than can be obtained by representing them as a bag of words orset of n-grams. we would also like to explore methods for improving performance of the n-gram over lap approach and making it more robust to different data sets. acknowledgementsthis research has been supported by a google re search award.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W09-1218": {
        "abstract": "dependency parsing multilingual syntactic-semantic dependency parsing with three-stage approximate max-margin linear models abstract this paper describes a system for syntacticsemantic dependency parsing for multiple languages. the system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classifier, and an argument classifier for semantic dependency parsing. for semantic dependency parsing, we explore use of global features. all components are trained with an approximate max-margin learning algorithm. in the closed challenge of the conll-2009 shared task (haji\u02c7c et al., 2009), our system achieved the 3rd best performances for english and czech, and the 4th best performance for japanese.",
        "conclusion": "conclusion in this paper, we have described our system for syntactic and semantic dependency analysis in multilingual. although our system is not a joint approach but a pipeline approach, the system is comparable to the top system for some of the 7 languages. a further research direction we are investigating is the application of various types of global features. we believe that there is still room for improvements since we used only two types of global features for the argument classifier. another research direction is investigating joint approaches. to the best of our knowledge, three types of joint approaches have been proposed: n-best based approach (johansson and nugues, 2008), synchronous joint approach (henderson et al., 2008), and a joint approach where parsing and srl are performed simultaneously (llu\u00b4\u0131s and m`arquez, 2008). we attempted to perform nbest based joint approach, however, the expensive computational cost of the 2nd-order projective parser discouraged it. we would like to investigate syntactic-semantic joint approaches with reasonable time complexities.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W09-2208": {
        "abstract": "named entity recognition a simple semi-supervised algorithm for named entity recognition abstract we present a simple semi-supervised learning algorithm for named entity recognition (ner) using conditional random fields (crfs). the algorithm is based on exploiting evidence that is independent from the features used for a classifier, which provides high-precision labels to unlabeled data. such independent evidence is used to automatically extract highaccuracy and non-redundant data, leading to a much improved classifier at the next iteration. we show that our algorithm achieves an averimprovement of recall and precision compared to the supervised algorithm. we also show that our algorithm achieves high accuracy when the training and test sets are from different domains.",
        "conclusion": "conclusion we presented a simple semi-supervised learning algorithm for ner using conditional random fields (crfs). in addition we proposed using high precision label features to improve classification accuracy as well as to reduce training and test time. compared to other semi-supervised learning algorithm, our proposed algorithm has several advantages. it is domain and data independent. although it requires a small amount of labeled training data, the data is not required to be from the same domain as the one in which are interested to tag nes. it can be applied to different types of nes as long as independent evidence exists, which is usually available. it is simple and, we believe not limited by the choice of the classifier. although we used crfs in our framework, other models can be easily incorporated to our framework as long as they provide accurate confidence scores. with only a small amount of training data, our algorithm can achieve a better ne tagging accuracy than a supervised algorithm with a large amount of training data.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D11-1086": {
        "abstract": "improving bilingual projections via sparse covariance matrices abstract mapping documents into an interlingual representation can help bridge the language bar rier of cross-lingual corpora. many existing approaches are based on word co-occurrencesextracted from aligned training data, repre sented as a covariance matrix. in theory, sucha covariance matrix should represent seman tic equivalence, and should be highly sparse. unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. in this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. first, we explore word association measures and bilingual dictionaries to weigh the word pairs. later, we explore different selection strategies to remove the noisy pairsbased on the association scores. our experimental results on the task of aligning compa rable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.",
        "conclusion": "discussion in this paper, we have proposed the idea of sparsifyng covariance matrices to improve bilingual pro 938 jection directions. we are not aware of any nlp research that attempts to recover the sparseness of the covariance matrices to improve the projection directions. our work is different from the sparse cca (hardoon and shawe-taylor, 2011; rai and daume? iii, 2009) proposed in the machine learningliterature. their objective is to find projection directions such that the original documents are repre sented as a sparse vectors in the common sub-space. another seemingly relevant but different direction is the sparse covariance matrix selection research (banerjee et al, 2005). the objective in this workis to find matrices such that the inverse of the co variance matrix is sparse which has applications in gaussian processes.in this paper, we tried sparsification in the con text of cca only but our technique is general andcan be applied to its variants like opca. our experimental results show that using external informa tion such as bilingual dictionaries which is gleanedfrom cleaner resources brings significant improve ments. moreover, we also observe that computingword pair association measures from the same train ing data along with an appropriate selection criteriacan also yield significant improvements. this is cer tainly encouraging and in future we would like to explore more sophisticated techniques to recover the sparsity based on the training data itself.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1065": {
        "abstract": "bootstrapping via graph propagation abstract bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them. this paper introduces a novel variant of the yarowsky algorithm based on this view. it is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function. the experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets.",
        "conclusion": "conclusions n our novel algorithm achieves accuracy comparable to yarowsky-cautious, but is better theoretically motivated by combining ideas from haffari and sarkar (2007) and subramanya et al. (2010). it also achieves accuracy comparable to dl-cotrain, but does not require the features to be split into two independent views. as future work, we would like to apply our algorithm to a structured task such as part of speech tagging. we also believe that our method for adapting collins and singer (1999)\u2019s cautiousness to yarowsky-prop can be applied to similar algorithms with other underlying classifiers, even to structured output models such as conditional random fields.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N03-2024": {
        "abstract": "references to named entities: a corpus study abstract references included in multi-document summaries are often problematic. in this paper, we present a corpus study performed to derive a statistical model for the syntactic realization of referential expressions. the interpretation of the probabilistic data helps us gain insight on how extractive summaries can be rewritten in an efficient manner to produce more fluent and easy-to-read text.",
        "conclusion": "conclusion and future work as has been seen, a major improvement of summary readability can be achieved by using the simple set of rewrite rules that realize the highest probability path in the derived markov model. one possible usage of the model which is not discussed in the paper but is the focus of current and ongoing work, is to generate realizations \u201con demand\u201d. referring expressions can be generated by recombining different pieces of the input rather than the currently used extraction of full nps. this approach will make better use of the markov model, but it also requires work towards deeper semantic processing of the input. semantic information is needed in order to prevent the combination of almost synonymous premodifiers in the same np and also for the identification of properties that are more central for the enity with respect to the focus of the input cluster.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W07-0738": {
        "abstract": "linguistic features for automatic evaluation of heterogenous mt systems abstract evaluation results recently reported by callison-burch et al. (2006) and koehn and monz (2006), revealed that, in certain cases, may not be a reliable mt quality indicator. this happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon. the reason is that, while mt quality aspects are its scope to the lexical dimension. in this work, we suggest using metrics which take into account linguistic features at more abstract levels. we provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature.",
        "conclusion": "conclusions we have presented a comparative study on the behavior of a wide set of metrics for automatic mt evaluation at different linguistic levels (lexical, shallow-syntactic, syntactic, and shallow-semantic) under different scenarios. we have shown, through empirical evidence, that linguistic features at more abstract levels may provide more reliable system rankings, specially when the systems under evaluation do not share the same lexicon. we strongly believe that future mt evaluation campaigns should benefit from these results, by including metrics at different linguistic levels. for instance, the following set could be used: { \u2018dp-hwcr-4\u2019, \u2018dp-oc-*\u2019, \u2018dp-ol-*\u2019, \u2018dp-or-*\u2019, \u2018cpstm-9\u2019, \u2018sr-or-*\u2019, \u2018sr-orv\u2019 } all these metrics are among the top-scoring in all the translation tasks studied. however, none of these metrics provides, in isolation, a \u2018global\u2019 measure of quality. indeed, all these metrics focus on \u2018partial\u2019 aspects of quality. we believe that, in order to perform \u2018global\u2019 evaluations, different quality dimensions should be integrated into a single measure of quality. with that purpose, we are currently exploring several metric combination strategies. preliminary results, based on the queen measure inside the qarla framework (amig\u00b4o et al., 2005), indicate that metrics at different linguistic levels may be robustly combined. experimental results also show that metrics requiring linguistic analysis seem very robust against parsing errors committed by automatic linguistic processors, at least at the document level. that is very interesting, taking into account that, while reference translations are supposedly well formed, that is not always the case of automatic translations. however, it remains pending to test the behaviour at the sentence level, which could be very useful for error analysis. moreover, relying on automatic processors implies two other important limitations. first, these tools are not available for all languages. second, usually they are too slow to allow for massive evaluations, as required, for instance, in the case of system development. in the future, we plan to incorporate more accurate, and possibly faster, linguistic processors, also for languages other than english, as they become publicly available.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1045": {
        "abstract": "fast online lexicon learning for grounded language acquisition abstract learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language. it is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context. recent work by chen and mooney (2011) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs. while the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets. in this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results. we show that by changing the grammar of the formal meaning representation language and training on additional data collected from amazon\u2019s mechanical turk we can further improve the results. we also include experimental results on a chinese translation of the training data to demonstrate the generality of our approach.",
        "conclusion": "conclusion learning the semantics of language from the perceptual context in which it is uttered is a useful approach because only minimal human supervision is required. in this paper we presented a novel online algorithm for building a lexicon from ambiguously supervised relational data. in contrast to the previous approach that computed common subgraphs between different contexts in which an n-gram appeared, we instead focus on small, connected subgraphs and introduce an algorithm, sgoll, that is an order of magnitude faster. in addition to being more scalable, sgoll also performed better on the task of interpreting navigation instructions. in addition, we showed that changing the mrg and collecting additional training data from mechanical turk further improve the performance of the overall navigation system. finally, we demonstrated the generality of the system by using it to learn chinese navigation instructions and achieved similar results.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-2002": {
        "abstract": "joint evaluation of morphological segmentation and syntactic parsing abstract we present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. the protocol uses distance-based metrics defined for the space of trees over lattices. our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). our evaluation of segmentation and parsing for modern hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios.",
        "conclusion": "conclusion we presented distance-based metrics defined for trees over lattices and applied them to evaluating parsers on joint morphological and syntactic disambiguation. our contribution is both technical, providing an evaluation tool that can be straightforwardly applied for parsing scenarios involving trees over lattices,4 and methodological, suggesting to evaluate parsers in all possible scenarios in order to get a realistic indication of parser performance.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W07-0718": {
        "abstract": "(meta-) evaluation of machine translation abstract j schroeder ed ac uk abstract this paper evaluates the translation quality of machine translation systems for 8 language pairs: translating french, german, spanish, and czech to english and back. we carried out an extensive human evaluation which allowed us not only to rank the different mt systems, but also to perform higher-level analysis of the evaluation process. we measured timing and intraand inter-annotator agreement for three types of subjective evaluation. we measured the correlation of automatic evaluation metrics with human judgments. this meta-evaluation reveals surprising facts about the most commonly used methodologies.",
        "conclusion": "conclusions similar to last year\u2019s workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from four european languages into english, and vice versa. this year we substantially increased the number of automatic evaluation metrics and were also able to nearly double the efforts of producing the human judgments. there were substantial differences in the results results of the human and automatic evaluations. we take the human judgments to be authoritative, and used them to evaluate the automatic metrics. we measured correlation using spearman\u2019s coefficient and found that three less frequently used metrics were stronger predictors of human judgments than bleu. they were: semantic role overlap (newly introduced in this workshop) paraeval-recall and meteor. although we do not claim that our observations are indisputably conclusive, they again indicate that the choice of automatic metric can have a significant impact on comparing systems. understanding the exact causes of those differences still remains an important issue for future research. this year\u2019s evaluation also measured the agreement between human assessors by computing the kappa coefficient. one striking observation is that inter-annotator agreement for fluency and adequacy can be called \u2018fair\u2019 at best. on the other hand, comparing systems by ranking them manually (constituents or entire sentences), resulted in much higher inter-annotator agreement.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W10-2009": {
        "abstract": "surprisal theory modeling the noun phrase versus sentence coordination ambiguity in dutch: evidence from surprisal theory abstract this paper investigates whether surprisal theory can account for differential processing difficulty in the np-/s-coordination ambiguity in dutch. surprisal is estimated using a probabilistic context-free grammar (pcfg), which is induced from an automatically annotated corpus. we find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by frazier (1987). we argue that syntactic and lexical probabilities, as specified in a pcfg, are sufficient to account for what is commonly referred to as an np-coordination preference.",
        "conclusion": "discussion in this paper we have shown that a model of lexicalized surprisal, based on an automatically induced pcfg, can account for the np-/s-ambiguity reading time data of frazier (1987). we found these results to be robust for a critical model parameter (beam size), which suggests that syntactic processing in human comprehension might be based on limited parallelism only. surprisal theory models processing difficulty on a word level. a word\u2019s difficulty is related to the expectations the language processor forms, given the structural and lexical material that precedes it. the model showed a clear preference for np-coordination which suggests that structural and lexical expectations as estimated from a corpus might be sufficient to explain the np-coordination bias in human sentence processing. our account of this bias differs considerably from the original account proposed by frazier (minimal attachment principle) in a number of ways. frazier\u2019s explanation is based on a metric of syntactic complexity which in turn depends on quite specific syntactic representations of a language\u2019s phrase structure. surprisal theory, on the other hand, is largely neutral with respect to the form syntactic representations take in the human mind.4 moreover, differential processing in surprisal-based models does not require the specification of a notion of syntactic complexity. both these aspects make surprisal theory a parsimonious explanatory framework. the minimal attachment principle postulates that the bias towards np-coordination is an initial processing primitive. in contrast, the bias in our simulations is a function of the model\u2019s input history and linguistic experience from which the grammar is induced. it is further modulated by the immediate context from which upcoming words are predicted during processing. consequently, the model\u2019s preference for one structural type can vary across sentence tokens and even be reversed on occasion. we argued that our grammar showed an overall preference for np-coordination but this preference was not necessarily reflected on each and every rule that dealt with coordinations. some scoordination rules could have higher probability than np-coordination rules. in addition, syntactic expectations were modified by lexical expectations. thus, even when np-coordination was structurally favored over s-coordination, highly unexpected lexical material could lead to more processing difficulty for np-coordination than for s-coordination. surprisal theory allows us to build a formally precise computational model of reading time data which generates testable, quantitative predictions about the differential processing of individual test items. these predictions (figure 5) indicate that mean reading times for a set of np/s-coordination sentences may not be adequate to tap the origin of differential processing difficulty. our results are consistent with the findings of hoeks et al. (2002), who also found evidence for an np-coordination preference in a self-paced reading experiment as well as in an eye-tracking experiment. they suggested that np-coordination might be easier to process because it has a simpler topic structure than s-coordination. the former only has one topic, whereas the latter has two. hoeks et al. (2002) argue that having more than one topic is unexpected. sentences with more than one topic will therefore cause more processing difficulty. this preference for simple topic-structure that was evident in language comprehension may also be present in language production, and hence in language corpora. thus, it may very well be the case that the np-coordination preference that was present in our training corpus may have had a pragmatic origin related to topic-structure. the outcome of our surprisal model is also compatible with the results of hoeks et al. (2006) who found that thematic information can strongly reduce but not completely eliminate the np-coordination preference. surprisal theory is explicitly built on the assumption that multiple sources of information can interact in parallel at any point in time during sentence processing. accordingly, we suggest here that the residual preference for npcoordination found in the study of hoeks et al. (2006) might be explained in terms of syntactic and lexical expectation. and finally, our approach is consistent with a large body of evidence indicating that language comprehension is incremental and makes use of expectation-driven word prediction (pickering and garrod, 2007). it remains to be tested whether our model can explain behavioral data from the processing of ambiguities other than the dutch np- versus s-coordination case.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P06-1012": {
        "abstract": "estimating class priors in domain adaptation for word sense disambiguation abstract instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word). this in turn affects the accuracy of word sense disambiguation (wsd) systems trained and applied on different domains. this paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations. by using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve significant improvements in wsd accuracy.",
        "conclusion": "discussion the experimental results show that the sense priors estimated using the calibrated probabilities of naive bayes are effective in increasing the wsd accuracy. however, using a learning algorithm which already gives well calibrated posterior probabilities may be more effective in estimating the sense priors. one possible algorithm is logistic regression, which directly optimizes for getting approximations of the posterior probabilities. hence, its probability estimates are already well calibrated (zhang and yang, 2004; niculescumizil and caruana, 2005). in the rest of this section, we first conduct experiments to estimate sense priors using the predictions of logistic regression. then, we perform significance tests to compare the various methods. we trained logistic regression classifiers and evaluated them on the 4 datasets. however, the wsd accuracies of these unadjusted logistic regression classifiers are on average about 4% lower than those of the unadjusted naive bayes classifiers. one possible reason is that being a discriminative learner, logistic regression requires more training examples for its performance to catch up to, and possibly overtake the generative naive bayes learner (ng and jordan, 2001). although the accuracy of logistic regression as a basic classifier is lower than that of naive bayes, its predictions may still be suitable for estimating 'though not shown, we also calculated the accuracies of these binary classifiers without calibration, and found them to be similar to the accuracies of the multiclass naive bayes shown in the column l under nb in table 1. sense priors. to gauge how well the sense priors are estimated, we measure the kl divergence between the true sense priors and the sense priors estimated by using the predictions of (uncalibrated) multiclass naive bayes, calibrated naive bayes, and logistic regression. these results are shown in table 3 and the column em shows that using the predictions of logistic regression to estimate sense priors consistently gives the lowest kl divergence. results of the kl divergence test motivate us to use sense priors estimated by logistic regression on the predictions of the naive bayes classifiers. to elaborate, we first use the probability estimates of logistic regression in equations (2) and (3) to estimate the sense priors . these estimates and the predictions of the calibrated naive bayes classifier are then used in equation (4) to obtain the adjusted predictions. the resulting wsd accuracy is shown in the column em under nbcal in table 1. corresponding results when the predictions of the multiclass naive bayes is used in equation (4), are given in the column em under nb. the relative improvements against using the true sense priors, based on the calibrated probabilities, are given in the column em l in table 2. the results show that the sense priors provided by logistic regression are in general effective in further improving the results. in the case of dso nouns, this improvement is especially significant. paired t-tests were conducted to see if one method is significantly better than another. the t statistic of the difference between each test instance pair is computed, giving rise to a p value. the results of significance tests for the various methods on the 4 datasets are given in table 4, where the symbols (0.01, 0.05], and 0.01 respectively. the methods in table 4 are represented in the form a1-a2, where a1 denotes adjusting the predictions of which classifier, and a2 denotes how the sense priors are estimated. as an example, nbcal-em specifies that the sense priors estimated by logistic regression is used to adjust the predictions of the calibrated naive bayes classifier, and corresponds to accuracies in column em under nbcal in table 1. based on the significance tests, the adjusted accuracies of em and em in table 1 are significantly better than their respective unadjusted l accuracies, indicating that estimating the sense priors of a new domain via the em approach presented in this paper significantly improves wsd accuracy compared to just using the sense priors from the old domain. nb-em represents our earlier approach in (chan and ng, 2005b). the significance tests show that our current approach of using calibrated naive bayes probabilities to estimate sense priors, and then adjusting the calibrated probabilities by these estimates (nbcal-em ) performs significantly better than nb-em (refer to row 2 of table 4). for dso nouns, though the results are similar, the p value is a relatively low 0.06. using sense priors estimated by logistic regression further improves performance. for example, row 1 of table 4 shows that adjusting the predictions of multiclass naive bayes classifiers by sense priors estimated by logistic regression (nbem ) performs significantly better than using sense priors estimated by multiclass naive bayes (nb-em ). finally, using sense priors estimated by logistic regression to adjust the predictions of calibrated naive bayes (nbcal-em ) in general performs significantly better than most other methods, achieving the best overall performance. in addition, we implemented the unsupervised method of (mccarthy et al., 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense. as in our earlier work (chan and ng, 2005b), we normalized the prevalence score of each sense to obtain estimated sense priors for each word, which we then used to adjust the predictions of our naive bayes classifiers. we found that the wsd accuracies obtained with the method of (mccarthy et al., 2004) are on average 1.9% lower than our nbcal-em method, and the difference is statistically significant. conclusion differences in sense priors between training and target domain datasets will result in a loss of wsd accuracy. in this paper, we show that using well calibrated probabilities to estimate sense priors is important. by calibrating the probabilities of the naive bayes algorithm, and using the probabilities given by logistic regression (which is already well calibrated), we achieved significant improvements in wsd accuracy over previous approaches.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "J01-2004": {
        "abstract": "this paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition the paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling a lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers a new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model a small recognition experiment also demonstrates the utility of the model",
        "conclusion": "",
        "citations": [],
        "summary": [
            "This paper aims at describing the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modelling for speech recognition. With certain exceptions, computational linguists have in the past generally formed a separate research community from speech recognition researchers, despite obvious overlap of interest. While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task. Two features of our top-down parsing approach will emerge as key to its success. First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar. A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model. There are few implications that are to be taken further in this area. First, there is reason to believe that some of the conditioning information is not uniformly useful, and we would benefit from finer distinctions. Second, there are advantages to top-down parsing that have not been examined to date. Brian Roark in his paper'Probabilistic Top-Down Parsing and Language Modeling'proposes a new language model, based on probabilistic top-down parsing.Their top-down parser allows for the incremental calculation of generative conditional word probabilities and top-down guidance also improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.It uses a PCFG (Probabilistic (or stochastic) context-free grammars (PCFGs)) with a conditional probability model.The parser gave a remarkable precision rate of 94.1, for sentences of length less than or equal to 100.This implies that using multiple model in one parser to deal with different aspects of language can be fruitful.\n Roark, in this paper, described the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. They varied the base beam factor in trials on the Chelba and Jelinek corpora, keeping the level of conditioning information constant.    With a simple conditional probability model, and simple statistical search heuristics, they were able to find very accurate parses efficiently, and, assign word probabilities that yielded a perplexity improvement over previous results. Interpolation with a trigram model yielded an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by their parsing model was orthogonal to that captured by a trigram model."
        ]
    },
    "N07-1050": {
        "abstract": "incremental non-projective dependency parsing abstract an open issue in data-driven dependency parsing is how to handle non-projective dependencies, which seem to be required by linguistically adequate representations, but which pose problems in parsing with respect to both accuracy and efficiency. using data from five different languages, we evaluate an incremental deterministic parser that derives non-projective depenstructures in supported by svm classifiers for predicting the next parser action. the experiments show that unrestricted non-projective parsing gives a significant improvement in accuracy, compared to a strictly projective baseline, with up to 35% error reduction, leading to state-of-the-art results for the given data sets. moreover, by restricting the class of permissible structures to limited degrees of non-projectivity, the parsing time can be reduced by up to 50% without a significant decrease in accuracy.",
        "conclusion": "conclusion in this paper, we have investigated a data-driven approach to dependency parsing that combines a deterministic incremental parsing algorithm with historybased svm classifiers for predicting the next parser action. we have shown that, for languages with a non-negligible proportion of non-projective structures, parsing accuracy can be improved significantly by allowing non-projective structures to be derived. we have also shown that the parsing time can be reduced substantially, with only a marginal loss in accuracy, by limiting the degree of nonprojectivity allowed during parsing. a comparison with results from the conll-x shared task shows that the parsing accuracy is comparable to that of the best available systems, which means that incremental non-projective dependency parsing is a viable alternative to approaches based on post-processing of projective approximations.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1053": {
        "abstract": "strong lexicalization of tree adjoining grammars abstract it was shown tree-adjoining grammars are not closed unstrong comput. linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol. a more powerful model, the simple context-free tree grammar, admits such a normal form. it can be effectively constructed and the maximal rank of the nononly increases by thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.",
        "conclusion": "conclusion for k \u2208 icy, let cftg(k) be the set of those cftg whose nonterminals have rank at most k. since the normal form constructions preserve the nonterminal rank, the proof of theorem 17 shows that cftg(k) are strongly lexicalized by cftg(k+1). kepser and rogers (2011) show that non-strict tag are strongly equivalent to cftg(1). hence, non-strict tag are strongly lexicalized by cftg(2). it follows from section 6 of engelfriet et al. (1980) that the classes cftg(k) with k \u2208 icy induce an infinite hierarchy of string languages, but it remains an open problem whether the rank increase in our lexicalization construction is necessary. g\u00b4omez-rodriguez et al. (2010) show that wellnested lcfrs of maximal fan-out k can be parsed in time o(n2k+2), where n is the length of the input string w \u2208 a\u2217. from this result we conclude that cftg(k) can be parsed in time o(n2k+4), in the sense that we can produce a parse tree t that is generated by the cftg with yd\u03b4(t) = w. it is not clear yet whether lexicalized cftg(k) can be parsed more efficiently in practice.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W05-0602": {
        "abstract": "a statistical semantic parser that integrates syntax and semantics abstract we introduce a learning semantic parser,scissor, that maps natural-language sentences to a detailed, formal, meaning representation language. it first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. a compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. we evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. we present experimentalresults demonstrating that scissor produces more accurate semantic representa tions than several previous approaches.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P13-1126": {
        "abstract": "vector space model for adaptation in statistical machine translation abstract this paper proposes a new approach to domain adaptation in statistical machine translation (smt) based on a vector space model (vsm). the general idea is first to create a vector profile for the in-domain development (\u201cdev\u201d) set. this profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. thus, we obtain a decoding feature whose value represents the phrase pair\u2019s closeness to the dev. this is a simple, computationally cheap form of instance weighting for phrase pairs. experiments on large scale nist evaluation data show improvements over strong baselines: +1.8 bleu on arabic to english and +1.4 bleu on chinese to english over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation. an informal analysis suggests that vsm adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.",
        "conclusion": "conclusions and future work this paper proposed a new approach to domain adaptation in statistical machine translation, based on vector space models (vsms). this approach measures the similarity between a vector representing a particular phrase pair in the phrase table and a vector representing the dev set, yielding a feature associated with that phrase pair that will be used by the decoder. the approach is simple, easy to implement, and computationally cheap. for the two language pairs we looked at, it provided a large performance improvement over a non-adaptive baseline, and also compared favourably with linear mixture adaptation techniques. furthermore, vsm adaptation can be exploited in a number of different ways, which we have only begun to explore. in our experiments, we based the vector space on subcorpora defined by the nature of the training data. this was done purely out of convenience: there are many, many ways to define a vector space in this situation. an obvious and appealing one, which we intend to try in future, is a vector space based on a bag-of-words topic model. a feature derived from this topicrelated vector space might complement some features derived from the subcorpora which we explored in the experiments above, and which seem to exploit information related to genre and style.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P13-2073": {
        "abstract": "language independent connectivity strength features for phrase pivot statistical machine translation abstract an important challenge to statistical machine translation (smt) is the lack of parallel data for many language pairs. one common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. although pivoting is a robust technique, it introduces some low quality translations. in this paper, we present two language-independent features to improve the quality of phrase-pivot based smt. the features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. we show positive results (0.6 bleu points) on persian-arabic smt as a case study.",
        "conclusion": "conclusion and future work we presented an experiment showing the effect of using two language independent features, source connectivity score and target connectivity score, to improve the quality of pivot-based smt. we showed that these features help improving the overall translation quality. in the future, we plan to explore other features, e.g., the number of the pivot phases used in connecting the source and target phrase pair and the similarity between these pivot phrases. we also plan to explore language specific features which could be extracted from some seed parallel data, e.g., syntactic and morphological compatibility of the source and target phrase pairs.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N06-1037": {
        "abstract": "exploring syntactic features for relation extraction using a convolution tree kernel abstract this paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction. our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel. evaluation on the ace 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ace relation subtypes. it also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ace relation major types.",
        "conclusion": "conclusion and future work in this paper, we explore the syntactic features using convolution tree kernels for relation extraction. we conclude that: 1) the relations between entities can be well represented by parse trees with carefully calibrating effective portions of parse trees; 2) the syntactic features embedded in a parse tree are particularly effective for relation extraction; 3) the convolution tree kernel can effectively capture the syntactic features for relation extraction. the most immediate extension of our work is to improve the accuracy of relation detection. we may adopt a two-step method (culotta and sorensen, 2004) to separately model the relation detection and characterization issues. we may integrate more features (such as head words or wordnet semantics) into nodes of parse trees. we can also benefit from the learning algorithm to study how to solve the data imbalance and sparseness issues from the learning algorithm viewpoint. in the future, we would like to test our algorithm on the other version of the ace corpus and to develop fast algorithm (vishwanathan and smola, 2002) to speed up the training and testing process of convolution kernels. acknowledgements: we would like to thank dr. alessandro moschitti for his great help in using his tree kernel toolkits and fine-tuning the system. we also would like to thank the three anonymous reviewers for their invaluable suggestions.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-3407": {
        "abstract": "combining rule-based and statistical syntactic analyzers abstract this paper presents the results of a set of preliminary experiments combining two knowledge-based partial dependency analyzers with two statistical parsers, applied to the basque dependency treebank. the general idea will be to apply a stacked scheme where the output of the rule-based partial parsers will be given as input to maltparser and mst, two state of the art statistical parsers. the results show a modest improvement over the baseline, although they also present interesting lines for further research. in this paper we present a set of preliminary experiments on the combination of two knowledge-based partial syntactic analyzers with two state of the art data-driven statistical parsers. the experiments have been performed on the basque dependency treebank (aduriz et al., 2003). in the last years, many attempts have been performed trying to combine different parsers (surdeanu and manning, 2010), with significant improvements over the best individual parser\u2019s baseline. the two most successful approaches have been stacking (martins et al., 2008) and voting (sagae and lavie, 2006, nivre and mcdonald, 2008, mcdonald and nivre, 2011). in this paper we will experiment the use of the stacking technique, giving the tags obtained by the rulebased syntactic partial parsers as input to the statistical parsers. morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like english does not reach similar performance levels in languages like basque, greek or turkish (nivre et al., 2007a). as it was successfully done on part of speech (pos) tagging, where the use of rule-based pos taggers (tapanainen and voutilainen, 1994) or a combination of a rulebased pos tagger with a statistical one (aduriz et al., 1997, ezeiza et al., 1998) outperformed purely statistical taggers, we think that exploring the combination of knowledge-based and data-driven systems in syntactic processing can be an interesting line of research. most of the experiments on combined parsers have relied on different types of statistical parsers (sagae and lavie, 2006, martins et al., 2008, mcdonald and nivre, 2011), trained on an automatically annotated treebank. yeh (2000) used the output of several baseline diverse parsers to increase the performance of a second transformation-based parser. in our work we will study the use of two partial rule-based syntactic analyzers together with two data-driven parsers: set of predefined tags to each word, where each tag gives both the name of a dependency relation (e.g. subject) together with the direction of its head (left or right). in the rest of this paper, section 2 will first present the corpus and the different parsers we will combine, followed by the experimental results in section 3, and the main conclusions of the work. this section will describe the main resources that have been used in the experiments. first, subsection 2.1 will describe the basque dependency treebank, and then subsection 2.2 will explain the main details of the analyzers that have been employed. the analyzers are a rulebased chunker, a rule-based shallow dependency parser and two state of the art data-driven dependency parsers, maltparser and mst. our work will make use the second version of the basque dependency treebank (bdt ii, aduriz et al., 2003), containing 150,000 tokens (11,225 sentences). figure 1 presents an example of a syntactically annotated sentence. each word contains its form, lemma, category or coarse part of speech (cpos), pos, morphosyntactic features such as case, number of subordinate relations, and the dependency relation (headword + dependency). the information in figure 1 has been simplified due to space reasons, as typically each word contains many morphosyntactic features (case, number, type of subordinated sentence, ...), which are relevant for parsing. the last two lines of the sentence in figure 1 do not properly correspond to the treebank, but are the result of the rule-based partial syntactic analyzers (see subsection 2.2). for evaluation, we divided the treebank in three sets, corresponding to training, development, and test (80%, 10%, and 10%, respectively). the experiments were performed on the development set, leaving the best system for the final test. this subsection will present the four types of analyzers that have been used. the rule-based analyzers are based on the contraint grammar (cg) formalism (karlsson et al., 1995), based on the assignment of morphosyntactic tags to words using a formalism that has the capabilities of finite state automata or regular expressions, by means of a set of rules that examine mainly local contexts of words to determine the correct tag assignment. the rule-based chunker (rbc henceforth, aranzabe et al., 2009) uses 560 rules, where 479 of the rules deal with noun phrases and the rest with verb phrases. the chunker delimits the chunks with three tags, using a standard iob marking style (see figure 1). the first one is to mark the beginning of the phrase (b-vp if it is a verb phrase and b-np whether it's a noun phrase) and the other one to mark the continuation of the phrase (i-np or i-vp, meaning that the word is inside an np or vp). the last tag marks words that are outside a chunk. the evaluation of the chunker on the bdt gave a result of 87% precision and 85% recall over all chunks. we must take into account that this evaluation was performed on the gold pos tags, rather than on automatically assigned pos tasks, as in the present experiment. for that reason, the results can serve as an upper bound on the real results. the rule-based dependency analyzer (rbda, aranzabe et al., 2004) uses a set of 505 cg rules that try to assign dependency relations to wordforms. as the cg formalism only allows the assignment of tags, the rules only aim at marking the name of the dependency relation together with the direction of the head (left or right). for example, this analyzer assigns tags of the form &ncsubj> (see figure 1), meaning that the corresponding wordform is a non-clausal syntactic subject and that its head is situated to its right (the \u201c>\u201d or \u201c<\u201d symbols mark the direction of the head). this means that the result of this analysis is on the one hand a partial analysis and, on the other hand, it does not define a dependency tree, and can also be seen as a set of constraints on the shape of the tree. the system was evaluated on the bdt, obtaining f-scores between 90% for the auxmod dependency relation between the auxiliary and the main verb and 52% for the subject dependency relation, giving a (macro) average of 65%. regarding the data-driven parsers, we have made use of maltparser (nivre et al., 2007b) and mst parser (mcdonald et al., 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (mcdonald and nivre, 2007). maltparser (nivre, 2006) is a representative of local, greedy, transition-based dependency parsing models, where the parser obtains deterministically a dependency tree in a single pass over the input using two data structures: a stack of partially analyzed items and the remaining input sequence. to determine the best action at each step, the parser uses history-based feature models and discriminative machine learning. the learning configuration can include any kind of information (such as word-form, lemma, category, subcategory or morphological features). several variants of the parser have been implemented, and we will use one of its standard versions (maltparser version 1.4). in our experiments, we will use the stacklazy algorithm with the liblinear classifier. the mst parser can be considered a representative of global, exhaustive graph-based parsing (mcdonald et al., 2005, 2006). this algorithm finds the highest scoring directed spanning tree in a dependency graph forming a valid dependency tree. to learn arc scores, it uses large-margin structured learning algorithms, which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set. the learning procedure is global since model parameters are set relative to classifying the entire dependency graph, and not just over single arc attachments. this is in contrast to the local but richer contexts used by transition-based parsers. we use the freely available version of mstparser1. in the following experiments we will make use of the second order non-projective algorithm. we will experiment the effect of using the output of the knowledge-based analyzers as input to the data-driven parsers in a stacked learning scheme. figure 1 shows how the two last lines of the example sentence contain the tags assigned by the rule-based chunker (b-np, i-np, b-vp and i-vp) and the rule-based partial dependency analyzer (&ncsubj, &<ncmod, &<auxmod, &ccomp_obj and &mainv) . the first step consisted in applying the complete set of text processing tools for basque, including: properties, such as case, number, tense, or different types of subordination for verbs. consequently, the morphological analyzer for basque (aduriz et al. 2000) gives a high ambiguity. if only categorial (pos) ambiguity is taken into account, there is an average of 1.55 interpretations per wordform, which rises to 2.65 when the full morphosyntactic information is taken into account, giving an overall 64% of ambiguous word-forms. can pose an important problem, as determining the correct interpretation for each word-form requires in many cases the inspection of local contexts, and in some others, as the agreement of verbs with subject, object or indirect object, it could also suppose the examination of elements which can be far from each other, added to the free constituent order of the main sentence elements in basque. the erroneous assignment of incorrect part of speech or morphological features can difficult the work of the parser. when performing this task, we found the problem of matching the treebank tokens with those obtained from the analyzers, as there were divergences on the treatment of multiword units, mostly coming from named entities, verb compounds and complex postpositions (formed with morphemes appearing at two different words). for that reason, we performed a matching process trying to link the multiword units given by the morphological analysis module and the treebank, obtaining a correct match for 99% of the sentences. regarding the data-driven parsers, they are trained using two kinds of tags as input: syntactic analyzers (two last lines of the example in figure 1). these tags contain errors of the cg-based syntactic taggers. as the analyzers are applied after morphological processing, the errors can be propagated and augmented. table 1 shows the results of using the output of the knowledge-based analyzers as input to the statistical parsers. we have performed three experiments for each statistical parser, trying with the chunks provided by the chunker, the partial dependency parser, and both. the table shows modest gains, suggesting that the rule-based analyzers help the statistical ones, giving slight increases over the baseline, which are statistically significant when applying maltparser to the output of the rule-based dependency parser and a combination of the chunker and rule-based parsers. as table 1 shows, the parser type is relevant, as maltparser seems to be sensitive when using the stacked features, while the partial parsers do not seem to give any significant improvement to mst. looking with more detail at the errors made by the different versions of the parsers, we observe significant differences in the results for different dependency relations, seeing that the statistical parsers behave in a different manner regarding to each relation, as shown in table 2. the table shows the differences in f-score2 corresponding to five local dependency relations, (determination of verbal modifiers, such as subject, object and indirect object). mcdonald and nivre (2007) examined the types of errors made by the two data-driven parsers used in this work, showing how the greedy algorithm of maltparser performed better with local dependency relations, while the graph-based algorithm of mst was more accurate for global relations. as both the chunker and the partial dependency analyzer are based on a set of local rules in the cg formalism, we could expect that the stacked parsers could benefit mostly on the local dependency relations. 2 f-score = 2 * precision * recall / (precision + recall) (ncmod = non-clausal modifier, ncobj = non-clausal object, ncpred = non-clausal predicate, ncsubj = non-clausal subject, nciobj = non-clausal indirect object) table 2 shows how the addition of the rule-based parsers\u2019 tags performs in accord with this behavior, as maltparser gets f-score improvements for the local relations. although not shown in table 2, we also inspected the results on the long distance relations, where we did not observe noticeable improvements with respect to the baseline on any parser. for that reason, maltparser, seems to mostly benefit of the local nature of the stacked features, while mst does not get a significant improvement, except for some local dependency relations, such as ncobj and ncsubj. we performed an additional test using the partial dependency analyzer\u2019s gold dependency relations as input to maltparser. as could be expected, the gold tags gave a noticeable improvement to the parser\u2019s results, reaching 95% las. however, when examining the scores for the output dependency relations, we noticed that the gold partial dependency tags are beneficial for some relations, although negative for some others. for example the non-clausal modifier (ncmod) relation\u2019s f-score increases 3.25 points, while the dependency relation for clausal subordinate sentences functioning as indirect object decreases 0.46 points, which is surprising in principle. for all those reasons, the relation between the input dependency tags and the obtained results seems to be intricate, and we think that it deserves new experiments in order to determine their nature. as each type of syntactic information can have an important influence on the results on specific relations, their study can shed light on novel schemes of parser combination. we have presented a preliminary effort to integrate different syntactic analyzers, with the objective of getting the best from each system. although the potential gain is in theory high, the experiments have shown very modest improvements, which seem to happen in the set of local dependency relations. we can point out some avenues for further research: schemes, such as voting, trying to get the best from each type of parser. finally, we must also take into account that the rule-based analyzers were developed mainly having linguistic principles in mind, such as coverage of diverse linguistic phenomena or the treatment of specific syntactic constructions (aranzabe et al., 2004), instead of performanceoriented measures, such as precision and recall. this means that there is room for improvement in the first-stage knowledge-based parsers, which will have, at least in theory, a positive effect on the second-phase statistical parsers, allowing us to test whether knowledge-based and machine learningbased systems can be successfully combined.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P87-1015": {
        "abstract": "characterizing structural descriptions produced by various grammatical formalisms* abstract we consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. in considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by context-free grammars on the basis of this observation, we describe a class of formalisms which we call linear context- free rewriting systems, and show they are recognizable in polynomial time and generate only semilinear languages.",
        "conclusion": "discussion we have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems, and classified these formalisms on the basis of two features: path complexity; and path independence. we contrasted formalisms such as cfg's, hg's, tag's and mctag's, with formalisms such as ig's and unificational systems such as lfg's and fug's. we address the question of whether or not a formalism can generate only structural descriptions with independent paths. this property reflects an important aspect of the underlying linguistic theory associated with the formalism. in a grammar which generates independent paths the derivations of sibling constituents can not share an unbounded amount of information. the importance of this property becomes clear in contrasting theories underlying gpsg (gazdar, klein, pulluna, and sag, 1985), and gb (as described by berwick, 1984) with those underlying lfg and fug. it is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in lfg, fug and ig's. as illustrated by mctag's, it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of cfg's, hg's, and tag's. in order to observe the similarity between these constrained systems, it is crucial to abstract away from the details of the structures and operations used by the system. the similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of cfg's, hg's, tag's, and mctag's are all local sets. independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently. as suggested in section 4.3.2, a derivation with independent paths can be divided into subcomputations with limited sharing of information.",
        "citations": [],
        "summary": [
            "This paper talks about characterizing structural descriptions produced by various grammatical formalisms. We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. We consider properties of the tree sets generated by CFG's, Tree Adjoining Grammars (TAG's), Head Grammars (HG's), Categorial Grammars (CG's), and IG's. In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semi linearity and polynomial recognition of these systems follows. In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs. To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labeled by a pair to include the composition operations. Our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity. In this paper the author aims at to outline how such family of formalisms can be defined, and show that like CFG's, each member possesses a number of desirable linguistic and computational properties: in particular, the constant growth property and polynomial recognisability. In considering the relationship between formalisms, the author shows that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties of their derivation trees. Little attention has been paid to the structural descriptions that these formalisms can assign to strings, i.e. their strong generative capacity. This aspect of the formalism is both linguistically and computationally important. We consider properties of the tree sets generated by CFG's, Tree Adjoining Grammars (TAG's), Head Grammars (HG's), Categorial Grammars (CG's), and IG's. They examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths. These two properties of the tree sets are not only linguistically relevant, but also have computational importance. LCFRS's have only been loosely defined in this paper; they have to provide a complete set of formal properties associated with members of this class. The authors\u2019 goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of the strong generative capacity. Vijay-Shankar et all considered the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. They showed that it was useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees, find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars. On the basis of that observation, they described a class of formalisms which they called Linear Context- Free Rewriting Systems (LCFRs), and showed they were recognizable in polynomial time and generated only semilinear languages."
        ]
    },
    "D09-1034": {
        "abstract": "deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing abstract a number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of hale (2001; 2003; 2006). in this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized pcfg. we also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. empirical results demonstrate the utility of our methods in predicting human reading times.",
        "conclusion": "summary we have presented novel methods for teasing apart syntactic and lexical surprisal from a fully lexicalized parser, as well as for extending the operation of a predictive parser to capture novel entropy measures that are also shown to be relevant to psycholinguistic modeling. such automatic methods provide psycholinguistically relevant measures that are intractable to calculate by hand. the empirical validation presented here demonstrated that the new measures \u2013 particularly syntactic entropy and syntactic surprisal \u2013 have high utility for modeling human reading time data. our approach to calculating syntactic surprisal, based on fully lexicalized parsing, provided significant effects, while the pos-tag based (unlexicalized) surprisal \u2013 of the sort used in boston et al. (2008a) and demberg and keller (2008) \u2013 did not provide a significant effect in our trials. further, we showed an effect of lexical surprisal for closed class words even when combined with unigram and bigram probabilities in the same model. this work contributes to the important, developing enterprise of leveraging data-driven nlp approaches to derive new measures of high utility for psycholinguistic and neuropsychological studies.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W05-0638": {
        "abstract": "exploiting full parsing information to label semantic roles using an ensemble of me and svm via integer linear programming abstract in this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs. in addition, to take advantage of svm-based and maximum entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs. the experimental results show that full parsing information not only increases the f-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies, which increases the f-score by 0.64%. the ensemble of svm and me also boosts the f-score by 0.77%. our system achieves an f-score of 76.53% in the development set and 76.38% in test wsj.",
        "conclusion": "conclusion in this paper, we add more full parsing features to argument classification models, and represent full parsing information as constraints in ilps to resolve labeling inconsistencies. we also integrate two argument classification models, me and svm, by combining their argument classification results and applying them to the above-mentioned ilps. the results show full parsing information increases the total f-score by 1.34%. the ensemble of svm and me also boosts the f-score by 0.77%. finally, our system achieves an f-score of 76.53% in the development set and 76.38% in test wsj.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-3145": {
        "abstract": "kriya - the sfu system for translation task at wmt-12 abstract models for morpheme segmentation and morphology transactions on speech and language 4(1):3:1\u20133:34, february. kenneth heafield. 2011. kenlm: faster and smaller model queries. in of the sixth on statistical machine pages 187\u2013197.",
        "conclusion": "conclusion we submitted systems in two language pairs frenchenglish and english-czech for wmt-12 shared task. in french-english, we experimented the ensemble decoding framework that effectively utilizes the small amount of news genre data to improve the performance in the testset belonging to the same genre. we obtained a moderate gain of 0.4 bleu points with the ensemble decoding over the baseline system in newstest-2011. for newstest-2012, it performs comparably to that of the baseline and we are presently investigating the lack of improvement in newstest-2012. for cz-en, we found that the bleu scores do not substantially differ from each other and also the minor differences are not consistent for test-11 and test-12.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P04-1036": {
        "abstract": "finding predominant word senses in untagged text abstract word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. the problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data. whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. we present work on the use of a thesaurus acquired from raw textual corpora and the wordnet similarity package to find predominant noun senses automatically. the acquired predominant senses give a of 64% on the nouns of the 2 english all-words task. this is a very promising result given that our method does not require any hand-tagged text, such as semcor. furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.",
        "conclusion": "conclusions we have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in wordnet. we use an automatically acquired thesaurus and a wordnet similarity measure. the automatically acquired predominant senses were evaluated against the hand-tagged resources semcor and the senseval-2 english all-words task giving us a wsd precision of 64% on an all-nouns task. this is just 5% lower than results using the first sense in the manually labelled semcor, and we obtain 67% precision on polysemous nouns that are not in semcor. in many cases the sense ranking provided in semcor differs to that obtained automatically because we used the bnc to produce our thesaurus. indeed, the merit of our technique is the very possibility of obtaining predominant senses from the data at hand. we have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns. in the future, we will perform a large scale evaluation on domain specific corpora. in particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora. there is plenty of scope for further work. we want to investigate the effect of frequency and choice of distributional similarity measure (weeds et al., 2004). additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses. whilst we have used wordnet as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbours and the senses. the lesk measure for example, can be used with definitions in any standard machine readable dictionary.",
        "citations": [],
        "summary": [
            "This paper talks about Finding Predominant Word Senses in Untagged Text. We present work on the use of a thesaurus acquired from raw textual corpora and the Word Net similarity package to find predominant noun senses automatically. The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account. We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking. A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required. We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in Word Net. Whilst we have used Word Net as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbors and the senses. Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll in their paper 'Finding Predominant Word Senses in Untagged Text'present a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.ey chose documents from the SPORTS domain and a the FINANCE domain.they use an automatically acquired thesaurus and a WordNet Similarity measure.we use the SENSEVAL -2 all-words data (Palmer et al.,2001).This is a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II. The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL -2 English all-words task giving us a WSD precision of 64% on an all-nouns task.\n\n\n McCarthy et all presented work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses from two domain specific corpora automatically. The acquired predominant senses gave a of 64% on the nouns of the 2 English all-words task, a very promising result given that their method did not require any hand-tagged text. They had devised a method that used raw corpus data to automatically find a predominant sense for nouns in WordNet and used an automatically acquired thesaurus and a WordNet Similarity measure. They were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving them a WSD precision of 64% on an all-nouns task. In many cases, the sense ranking provided in SemCor differed to that obtained automatically because they used the BNC to produce their thesaurus.  This paper aims at presenting work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL- 2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account. They believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off and for systems that use it in lexical acquisition because of the limited size of hand-tagged resources. The first sense of star in SemCor is celestial body; however, if one were disambiguating popular news celebrity would be preferred. Later, they would perform a large scale evaluation on domain specific corpora. In particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora."
        ]
    },
    "N10-1091": {
        "abstract": "ensemble models for dependency parsing: cheap and good? abstract previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking. in this paper we implemented such a study for english dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. this study proves that fast and accurate ensemble parsers can be built with minimal effort.",
        "conclusion": "conclusions this study unearthed several non-intuitive yet important observations about ensemble models for dependency parsing. first, we showed that the diversity of base parsers is more important than complex learning models for parser combination, i.e., (a) ensemble models that combine several base parsers at runtime performs significantly better than a state-ofthe-art model that combines two parsers at learning time, and (b) meta-classification does not outperform unsupervised voting schemes for the re-parsing of candidate dependencies when six base models are available. second, we showed that well-formed dependency trees can be guaranteed without significant performance loss by linear-time approximate re-parsing algorithms. and lastly, our analysis indicates that unweighted voting performs as well as weighted voting for the re-parsing of candidate dependencies. considering that different base models are easy to generate, this work proves that ensemble parsers that are both accurate and fast can be rapidly developed with minimal effort.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E12-1055": {
        "abstract": "perplexity minimization for translation model domain adaptation in statistical machine translation abstract we investigate the problem of domain adaptation for parallel data in statistical machine translation (smt). while techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately. we also explore adapting multiple (4\u201310) data sets with no priori between in-domain and out-of-domain data except for an in-domain development set.",
        "conclusion": "conclusion this paper contributes to smt domain adaptation research in several ways. we expand on work by (foster et al., 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models.15 we demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted mle training, but are of little prominence in domain adaptation research. we also show that we can separately optimize the four variable features in the moses translation model through perplexity optimization. we break with prior domain adaptation research in that we do not rely on a binary clustering of in-domain and out-of-domain training data. we demonstrate that perplexity minimization scales well to a higher number of translation models. this is not only useful for domain adaptation, but for various tasks that profit from mixture mod15the source code is available in the moses repository http://github.com/moses-smt/mosesdecoder elling. we envision that a weighted combination could be useful to deal with noisy datasets, or applied after a clustering of training data.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D11-1033": {
        "abstract": "domain adaptation via pseudo in-domain data selection abstract we explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. these sentences may be selected with simple cross-entropy based methods, of which we present three. as these sentences are not themselves identical the in-domain data, we call them these subcorpora \u2013 1% the size of the original \u2013 can then used to train small domain-adapted statistical machine translation (smt) systems which outperform systems trained on the entire corpus. performance is further improved when we use these domain-adapted models in combination with a true in-domain model. the results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding.",
        "conclusion": "conclusions sentence pairs from a general-domain corpus that seem similar to an in-domain corpus may not actually represent the same distribution of language, as measured by language model perplexity. nonetheless, we have shown that relatively tiny amounts of this pseudo in-domain data can prove more useful than the entire general-domain corpus for the purposes of domain-targeted translation tasks. this paper has also explored three simple yet effective methods for extracting these pseudo indomain sentences from a general-domain corpus. a translation model trained on any of these subcorpora can be comparable \u2013 or substantially better \u2013 than a translation system trained on the entire corpus. in particular, the new bilingual moore-lewis method, which is specifically tailored to the machine translation scenario, is shown to be more efficient and stable for mt domain adaptation. translation models trained on data selected in this way consistently outperformed the general-domain baseline while using as few as 35k out of 12 million sentences. this fast and simple technique for discarding over 99% of the general-domain training corpus resulted in an increase of 1.8 bleu points. we have also shown in passing that the linear interpolation of translation models may work less well for translation model adaptation than the multiple paths decoding technique of (birch et al., 2007). these approaches of data selection and model combination can be stacked, resulting in a compact, two phrase-table, translation system trained on 1% of the available data that again outperforms a state-of-theart translation system trained on all the data. besides improving translation performance, this work also provides a way to mine very large corpora in a computationally-limited environment, such as on an ordinary computer or perhaps a mobile device. the maximum size of a useful general-domain corpus is now limited only by the availability of data, rather than by how large a translation model can be fit into memory at once.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P10-1021": {
        "abstract": "syntactic and semantic factors in processing difficulty: an integrated measure abstract the analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. there is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. in this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.",
        "conclusion": "discussion in this paper we investigated the contributions of syntactic and semantic constraint in modeling processing difficulty. our work departs from previous approaches in that we propose a single measure which integrates syntactic and semantic factors. evaluation on an eye-tracking corpus shows that our measure predicts reading time better than a baseline model that captures low-level factors in reading (word length, landing position, etc.). crucially, we were able to show that the semantic component of our measure improves reading time predictions over and above a model that includes syntactic measures (based on a trigram model and incremental parser). this means that semantic costs are a significant predictor of reading time in addition to the well-known syntactic surprisal. an open issue is whether a single, integrated measure (as evaluated in table 4) fits the eyemovement data significantly better than separate measures for trigram, syntactic, and semantic surprisal (as evaluated in table 3. however, we are not able to investigate this hypothesis: our approach to testing the significance of factors requires nested models; the log-likelihood test (see section 4) is only able to establish whether adding a factor to a model improves its fit; it cannot compare models with disjunct sets of factors (such as a model containing the integrated surprisal measure and one containing the three separate ones). however, we would argue that a single, integrated measure that captures human predictive processing is preferable over a collection of separate measures. it is conceptually simpler (as it is more parsimonious), and is also easier to use in applications (such as readability prediction). finally, an integrated measure requires less parameters; our definition of surprisal in 12 is simply the sum of the trigram, syntactic, and semantic components. an lme model containing separate factors, on the other hand, requires a coefficient for each of them, and thus has more parameters. in evaluating our model, we adopted a broad coverage approach using the reading time data from a naturalistic corpus rather than artificially constructed experimental materials. in doing so, we were able to compare different syntactic and semantic costs on the same footing. previous analyses of semantic constraint have been conducted on different eye-tracking corpora (dundee and embra corpus) and on different languages (english, french). moreover, comparisons of the individual contributions of syntactic and semantic factors were generally absent from the literature. our analysis showed that both of these factors can be captured by our integrated surprisal measure which is uniformly probabilistic and thus preferable to modeling semantic and syntactic costs disjointly using a mixture of probabilistic and nonprobabilistic measures. an interesting question is which aspects of semantics our model is able to capture, i.e., why does the combination of lsa or lda representations with an incremental parser yield a better fit of the behavioral data. in the psycholinguistic literature, various types of semantic information have been investigated: lexical semantics (word senses, selectional restrictions, thematic roles), sentential semantics (scope, binding), and discourse semantics (coreference and coherence); see keller (2010) of a detailed discussion. we conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). however, discourse coreference effects (such as the ones reported by altmann and steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see dubey 2010 for a model of human sentence processing that can handle coreference). a key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly. for example, we could envisage a parser that uses semantic representations to guide its search, e.g., by pruning syntactic analyses that have a low semantic probability. at the same time, the semantic model should have access to syntactic information, i.e., the composition of word representations should take their syntactic relationships into account, rather than just linear order.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "E12-1014": {
        "abstract": "toward statistical machine translation without parallel corpora abstract we estimate the parameters of a phrasebased statistical machine translation sysfrom instead of a corpus. we extend existing research on bilingual lexicon induction estimate and phrasal translation probabilities for mt-scale phrasetables. we propose a novel algorithm to estimate reordering probabilities from monolingual data. we report translation results for an end-to-end translation system using these monolingual features alone. our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights. in this paper, we examine an idealization where a phrase-table is given. we examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone. we further show that our monolingual features add 1.5 bleu points when combined with standard bilingually estimated phrase table features.",
        "conclusion": "conclusion this paper has demonstrated a novel set of techniques for successfully estimating phrase-based smt parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. we evaluated the performance of our algorithms in a full end-to-end translation system. assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover over 82% of bleu loss that resulted from removing the bilingual-corpus-derived phrase-table probabilities. we also showed that our monolingual features add 1.5 bleu points when combined with standard bilingually estimated features. thus our techniques have stand-alone efficacy when large bilingual corpora are not available and also make a significant contribution to combined ensemble performance when they are.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-2022": {
        "abstract": "faster phrase-based decoding by refining feature state abstract we contribute a faster decoding algorithm for phrase-based machine translation. translation hypotheses keep track of state, such as context for the language model and coverage of words in the source sentence. most features depend upon only part of the state, but traditional algorithms, including cube pruning, handle state atomically. for example, cube pruning will repeatedly query the language model with hypotheses that differ only in source coverage, despite the fact that source coverage is irrelevant to the language model. our key contribution avoids this behavior by placing hypotheses into equivalence classes, masking the parts of state that matter least to the score. moreover, we exploit shared words in hypotheses to iteratively refine language model scores rather than handling language model state atomically. since our algorithm and cube pruning are both approximate, improvement can be used to increase speed or accuracy. when tuned to attain the same accuracy, our algorithm is 4.0\u20137.7 times as fast as the moses decoder with cube pruning.",
        "conclusion": "conclusion we have contributed a new phrase-based search algorithm based on the principle that the language model cares the most about boundary words. this leads to two contributions: hiding irrelevant state from features and an incremental refinement algorithm to find high-scoring combinations. this algorithm is implemented in a new fast phrase-based decoder, which we release as open-source under the lgpl at kheafield.com/code/.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N07-2041": {
        "abstract": "simultaneous identification of biomedical named-entity and functional relation using statistical parsing techniques abstract in this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (nes) and extracts subcellular localization relations for bacterial proteins from the text in medline articles. we build a parser that derives both syntactic and domain-dependent semantic information achieves an f-score of the relation extraction task. we then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the f-score of our parser to our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research.",
        "conclusion": "discussion and future work in this paper we introduced a statistical parsingbased method to extract biomedical relations from medline articles. we made use of a large unlabeled data set to train our relation extraction model. experiments show that the semi-supervised method significantly outperforms the fully supervised method with f-score increasing from 48.4% to 83.2%. we have implemented a discriminative model (liu et al., 2007) which takes as input the examples with gold named entities and identifies bpl relations on them. in future work, we plan to let the discriminative model take the output of our parser and refine our current results further. we also plan to train a graphical model based on all extracted bp, pl and bpl relations to infer relations from multiple sentences and documents.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P13-2009": {
        "abstract": "semantic parsing as machine translation abstract semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. in experiments on the multilingual geoquery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. these results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.",
        "conclusion": "discussion our results validate the hypothesis that it is possible to adapt an ordinary mt system into a working semantic parser. in spite of the comparative simplicity of the approach, it achieves scores comparable to (and sometimes better than) many state-of-the-art systems. for this reason, we argue for the use of a machine translation baseline as a point of comparison for new methods. the results also demonstrate the usefulness of two techniques which are crucial for successful mt, but which are not widely used in semantic parsing. the first is the incorporation of a language model (or comparable long-distance structure-scoring model) to assign scores to predicted parses independent of the transformation model. the second is the use of large, composed rules (rather than rules which trigger on only one lexical item, or on tree portions of limited depth (lu et al., 2008)) in order to \u201cmemorize\u201d frequently-occurring largescale structures. conclusions we have presented a semantic parser which uses techniques from machine translation to learn mappings from natural language to variable-free meaning representations. the parser performs comparably to several recent purpose-built semantic parsers on the geoquery dataset, while training considerably faster than state-of-the-art systems. our experiments demonstrate the usefulness of several techniques which might be broadly applied to other semantic parsers, and provides an informative basis for future work.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-2147": {
        "abstract": "experiments with word alignment normalization and clause reordering for smt between english and german abstract this paper presents the liu system for thewmt 2011 shared task for translation be tween german and english. for english?german we attempted to improve the trans lation tables with a combination of standard statistical word alignments and phrase-basedword alignments. for german?english trans lation we tried to make the german text moresimilar to the english text by normalizing ger man morphology and performing rule-basedclause reordering of the german text. this resulted in small improvements for both transla tion directions.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-1008": {
        "abstract": "logical inference on dependency-based compositional semantics abstract dependency-based compositional semantics (dcs) is a framework of natural language semantics with easy-to-process structures as well as strict semantics. in this paper, we equip the dcs framework logical inference, by defining abdenotations an abstraction of the computing process of denotations in original dcs. an inference engine is built to achieve inference on abstract denotations. furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation. experiments on fracas and pascal rte datasets show promising results.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C10-1132": {
        "abstract": "a character-based joint model for chinese word segmentation abstract the character-based tagging approach is a dominant technique for chinese word segmentation, and both discrimi native and generative models can be adopted in that framework. however, generative and discriminative charac ter-based approaches are significantly different and complement each other. a simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches. experiments on the sec ond sighan bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one. in addition, closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best f score in four out of five corpora.",
        "conclusion": "conclusion from the error analysis of the character-based generative model and the discriminative o we found that thes each other on hanwords. to take advantage of these two ap proaches, a joint model is thus proposed to combine them. experiments on the second sighan bakeoff show that the joint model achieves 21% error reduction over the dis criminative model (14% over the generative model). moreover, closed tests on the second sighan bakeoff corpora show that this joint model significantly outperforms all the state of-the-art systems reported in the literature. last, it is found that weighting various features differently would give better result. how ever, further study is required to find out the true reason for this strange but interesting ph 1180 a generic-beam-search code and o ms. nanyan kuo for eric-beam-search code. m optimum entation. in proceedings of ghan workshop on chinese lan st th jia w jo hw fu ction using conditional random fields. ad mnlp, pages hu ld word segmenter ku d keh-yih su, 2009. yi scriminative ni ssing, 8 (1). pages hu second ru2006. subword-based tagging for con yi scores: how much im yu f acl, pages 840-847, cknowledgement the authors extend sincere thanks to wenbing jiang for his helps with our experiments. also, we thank behavior design corporation for using their show special thanks t her helps with the gen the research work has been partially funded by the natural science foundation of china under grant no. 60975053, 90820303 and 60736014, the national key technology r&d program under grant no. 2006bah03b02, and also the hi-tech research and develop ent program (?863? program) of china under grant no. 2006aa010108-4 as well.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-1089": {
        "abstract": "a discriminative model for joint morphological disambiguation and dependency parsing abstract most previous studies of morphological disambiguation and dependency parsing have been pursued independently. morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the \u201cpipeline\u201d approach, assuming that morphological information has been separately obtained. however, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. in this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. in evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.",
        "conclusion": "conclusions and future work we have proposed a discriminative model that jointly infers morphological properties and syntactic structures. in evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection. this model may be refined by incorporating richer features and improved decoding. in particular, we would like to experiment with higher-order features (\u00a76), and with maximum a posteriori decoding, via max-product bp or (relaxed) integer linear programming. further evaluation on other morphological systems would also be desirable.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W06-0508": {
        "abstract": "a hybrid approach for extracting semantic relations from texts abstract we present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases. the relations extracted can be used for various tasks, including semantic web annotation and ontology learning. we suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text.",
        "conclusion": "conclusions and future work we presented a hybrid approach for the extraction of semantic relations from text. it was designed mainly to enrich the annotations produced by a semantic web portal, but can be used for other domains and applications, such as ontology population and development. currently we are concluding the integration of the several modules composing our architecture. we will then carry experiments with our corpus of newsletters in order to evaluate the approach. subsequently, we will incorporate the architecture to a semantic web portal and accomplish an extrinsic evaluation in the context of that application. since the approach uses deep linguistic processing and corpus-based strategies not requiring any manual annotation, we expect it will accurately discover most of the relevant relations in the text.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C10-1061": {
        "abstract": "data-driven parsing with probabilistic linear context-free rewriting systems abstract this paper presents a first efficient imple mentation of a weighted deductive cykparser for probabilistic linear contextfree rewriting systems (plcfrs), to gether with context-summary estimatesfor parse items used to speed up parsing. lcfrs, an extension of cfg, can de scribe discontinuities both in constituencyand dependency structures in a straight forward way and is therefore a naturalcandidate to be used for data-driven parsing. we evaluate our parser with a gram mar extracted from the german negratreebank. our experiments show that data driven lcfrs parsing is feasible with a reasonable speed and yields output of competitive quality.",
        "conclusion": "conclusion we have presented the first parser for unrestrictedprobabilistic linear context-free rewriting sys tems (plcfrs), implemented as a cyk parser with weighted deductive parsing. to speed up parsing, we use context summary estimates for parse items. an evaluation on the negra treebank, both in terms of output quality and speed, showsthat data-driven parsing using plcfrs is feasible. already in this first attempt with a straight forward binarization, we obtain results that are comparable to state-of-the-art pcfg results in terms of f1, while yielding parse trees that are richer than context-free trees since they describediscontinuities. therefore, our approach demon strates convincingly that plcfrs is a natural and tractable alternative for data-driven parsing which takes non-local dependencies into consideration. 2note that these results were obtained on sentences with a length of ? 40 words and that those parser possibly would deliver better results if tested on our test set. 544",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1025": {
        "abstract": "reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations abstract we address the issue of consuming heterogeneous annotation data for chinese word segmentation and part-of-speech tagging. we empirically analyze the diversity between two representative corpora, i.e. penn chinese treebank (ctb) and pku\u2019s people\u2019s daily (ppd), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. the analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. evaluation on the ctb and ppd data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.",
        "conclusion": "conclusion our theoretical and empirical analysis of two representative popular corpora highlights two essential characteristics of heterogeneous annotations which are explored to reduce approximation and estimation errors for chinese word segmentation and pos tagging. we employ stacking models to incorporate features derived from heterogeneous analysis and apply them to convert heterogeneous labeled data for re-training. the appropriate application of heterogeneous annotations leads to a significant improvement (a relative error reduction of 11%) over the best performance for this task. although our discussion is for a specific task, the key idea to leverage heterogeneous annotations to reduce the approximation error with stacking models and the estimation error with automatically converted corpora is very general and applicable to other nlp tasks.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-1012": {
        "abstract": "learning new semi-supervised deep auto-encoder features for statistical machine translation abstract in this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep autoencoder (dae) paradigm for phrase-based translation model. using the unsupervised pre-trained deep belief net (dbn) to initialize dae\u2019s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised dae features, which are more effective and stable than the unsupervised dbn features. moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more daes for large hidden layers feature learning. on two chinese- english tasks, our semi-supervised dae features obtain statistically significant improvements of 1.34/2.45 (iwslt) and 0.82/1.52 (nist) bleu points over the unsupervised dbn features and the baseline features, respectively.",
        "conclusion": "conclusions in this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we have learned new features using the dae for the phrase-based translation model. using the unsupervised pre-trained dbn to initialize dae\u2019s parameters and using the input original phrase features as the \u201cteacher\u201d for semi-supervised backpropagation, our semi-supervised dae features are more effective and stable than the unsupervised dbn features (maskey and zhou, 2012). moreover, to further improve the performance, we introduce some simple but effective features as the input features for feature learning. lastly, to learn high dimensional feature representation, we introduce a natural horizontal composition of two daes for large hidden layers feature learning. on two chinese-english translation tasks, the results demonstrate that our solutions solve the two aforementioned shortcomings successfully. firstly, our dae features obtain statistically significant improvements of 1.34/2.45 (iwslt) and 0.82/1.52 (nist) bleu points over the dbn features and the baseline features, respectively. secondly, compared with the baseline phrase features xi, our introduced input original phrase features x significantly improve the performance of not only our dae features but also the dbn features. the results also demonstrate that dnn (dae and hcdae) features are complementary to the original features for smt, and adding them together obtain statistically significant improvements of 3.16 (iwslt) and 2.06 (nist) bleu points over the baseline features. compared with the original features, dnn (dae and hcdae) features are learned from the non-linear combination of the original features, they strong capture high-order correlations between the activities of the original features, and we believe this deep learning paradigm induces the original features to further reach their potential for smt.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-2110": {
        "abstract": "learning polylingual topic models from code-switched social media documents abstract code-switched documents are in social media, providing evidence polylingual topic models to infer aligned topics across languages. we code-switched lda (cslda), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis. we experiment on two code-switching corpora (english-spanish twitter data and english-chinese weibo data) and show that cslda improves perplexity over lda, and learns semantically coherent aligned topics as judged by human annotators.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D11-1140": {
        "abstract": "lexical generalization in ccg grammar induction for semantic parsing abstract we consider the problem of learning factored probabilistic ccg grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. traditional ccg lexicons list lexical items that pair words and phrases with syntactic and semantic content. such lexicons can be inefficient when words appear repeatedly with closely related lexical content. in this paper, we introduce factored lexicons, which both model word meaning model systematic variation in word usage. we also present an algorithm for learning factored ccg lexicons, along with a probabilistic parse-selection model. evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring.",
        "conclusion": "1 discussion we argued that factored ccg lexicons, which include both lexemes and lexical templates, provide a compact representation of lexical knowledge that can have advantages for learning. we also described a complete approach for inducing factored, probabilistic ccgs for semantic parsing, and demonstrated strong performance across a wider range of benchmark datasets that any previous approach. in the future, it will also be important to explore morphological models, to better model variation within the existing lexemes. the factored lexical representation also has significant potential for lexical transfer learning, where we would need to learn new lexemes for each target application, but much of the information in the templates could, potentially, be ported across domains.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P01-1018": {
        "abstract": "constraints on strong generative power abstract we consider the question \u201chow much strong generative power can be squeezed out of a formal system without increasing its weak generative power?\u201d and propose some theoretical and practical constraints on this problem. we then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar. finally, we generalize this result to formalisms beyond cfg.",
        "conclusion": "discussion we have proposed a more constrained version of joshi\u2019s question, \u201chow much strong generative power can be squeezed out of a formal system without increasing its weak generative power,\u201d and shown that within these constraints, a variant of tag called mmtag characterizes the limit of how much strong generative power can be squeezed out of cfg. moreover, using the notion of a meta-level grammar, this result is extended to formalisms beyond cfg. it remains to be seen whether rf-mmtag, whether used directly or for specifying meta-level grammars, provides further practical benefits on top of existing \u201csqueezed\u201d grammar formalisms like tree-local mctag, tree insertion grammar, or regular form tag. this way of approaching joshi\u2019s question is by no means the only way, but we hope that this work will contribute to a better understanding of the strong generative capacity of constrained grammar formalisms as well as reveal more powerful formalisms for linguistic analysis and natural language processing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D12-1046": {
        "abstract": "joint chinese word segmentation, pos tagging and parsing abstract in this paper, we propose a novel decoding algorithm for discriminative joint chinese word segmentation, part-of-speech (pos) tagging, and parsing. previous work often used a pipeline method \u2013 chinese word segmentation followed by pos tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. in our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. we extend the cyk parsing algorithm so that it can deal with word segmentation and pos tagging features. as far as we know, this is the first work on joint chinese word segmentation, pos tagging and parsing. our experimental results on chinese tree bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system.",
        "conclusion": "conclusion in this paper, we proposed a new algorithm for joint chinese word segmentation, pos tagging, and parsing. our algorithm is an extension of the cyk number of the corresponding pattern made by the pipeline tagging model. \u2193 and \u2191 mean the error number reduced or increased by the joint model. parsing method. the sub-models are independently trained for the three tasks to reduce model complexity and optimize individual sub-models. our experiments demonstrate the advantage of the joint models. in the future work, we will compare this joint model to the pipeline approach that uses multiple candidates or soft decisions in the early modules. we will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm. acknowledgments the authors thank zhongqiang huang for his help with experiments. this work is partly supported by darpa under contract no. hr0011-12-c-0016. any opinions expressed in this material are those of the authors and do not necessarily reflect the views of darpa.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-2058": {
        "abstract": "heuristic cube pruning in linear time abstract we propose a novel heuristic algorithm for cube pruning running in linear time in the beam size. empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W10-2803": {
        "abstract": "what is word meaning really? (and how can distributional models help us describe it?) abstract in this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation. more specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees. or move away from dictionary senses completely, and only model similarities between individual word usages. we argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models.",
        "conclusion": "conclusion and outlook in this paper, we have argued that it may be time to consider alternative computational models of word meaning, given that word sense disambiguation, after all this time, is still a tough problem for humans as well as machines. we have followed three hypotheses. the first two involve dictionary senses, suggesting that (a) senses may best be viewed as applying to a certain degree, rather than in a binary fashion, and (b) that it may make sense to describe an occurrence through multiple senses as a default rather than an exception. the third hypothesis then departs from dictionary senses, suggesting (c) focusing on individual word usages and their similarities instead. we have argued that distributional models are a good match for word meaning models following hypotheses (a)(c): they can represent individual word usages as points in vector space, and they can also represent dictionary senses in a way that allows for graded membership and overlapping senses, and we have discussed some existing models, both prototypebased and exemplar-based. one big question is, of course, about the usability of these alternative models of word meaning in nlp applications. will they do better than dictionary-based models? the current evaluations, testing paraphrase applicability in context, are a step in the right direction, but more task-oriented evaluation schemes have to follow. we have argued that it makes sense to look to cognitive models of mental concept representation. they are often based on feature vectors, and there are many interesting ideas in these models that have not yet been used (much) in computational models of word meaning. one of the most exciting ones, perhaps, is that cognitive models often have interpretable dimensions. while dimensions of distributional models are usually not individually interpretable, there are some first models (almuhareb and poesio, 2005; baroni et al., 2010) that use patterns to extract meaningful dimensions from corpus data. this offers many new perspectives: for which tasks can we improve performance by selecting dimensions that are meaningful specifically for that task (as in mitchell et al. (2008))? can interpretable dimensions be used for inferences? and, when we are computing vector space representations for word meaning in context, is it possible to select meaningful dimensions that are appropriate for a given context?",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1048": {
        "abstract": "translation model adaptation for statistical machine translation with monolingual topic information abstract to adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. in this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. experimental result on the nist chinese-english translation task shows that our approach significantly outperforms the baseline system.",
        "conclusion": "conclusion and future work this paper presents a novel method for smt system adaptation by making use of the monolingual corpora in new domains. our approach first estimates the translation probabilities from the out-ofdomain bilingual corpus given the topic information, and then rescores the phrase pairs via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. experimental results show that our method achieves better performance than the baseline system, without increasing the burden of the translation system. in the future, we will verify our method on other language pairs, for example, chinese to japanese. furthermore, since the in-domain phrase-topic distribution is currently estimated with simple smoothing interpolations, we expect that the translation system could benefit from other sophisticated smoothing methods. finally, the reasonable estimation of topic number for better translation model adaptation will also become our study emphasis.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P04-1040": {
        "abstract": "enriching the output of a parser using memory-based learning abstract we describe a method for enriching the output of a parser with information available in a corpus. the method is based on graph rewriting using memorybased learning, applied to dependency structures. this general framework allows us to accurately recover both grammatical and semantic information as well as non-local dependencies. it also facilitates dependency-based evaluation of phrase structure parsers. our method is largely independent of the choice of parser and corpus, and shows state of the art performance.",
        "conclusion": "discussion the experiments described in the previous sections indicate that although statistical parsers do not explicitly output some information available in the corpus they were trained on (grammatical and semantic tags, empty nodes, non-local dependencies), this information can be recovered with reasonably high accuracy, using pattern matching and machine learning methods. for our task, using dependency structures rather than phrase trees has several advantages. first, after converting both the treebank trees and parsers\u2019 outputs to graphs with head\u2013modifier relations, our method needs very little information about the linguistic nature of the data, and thus is largely corpusand parser-independent. indeed, after the conversion, the only linguistically informed operation is the straightforward extraction of features indicating the presence of subject and object dependents, and finiteness of verb groups. second, using a dependency formalism facilitates a very straightforward evaluation of the systems that produce structures more complex than trees. it is not clear whether the parseval evaluation can be easily extended to take non-local relations into account (see (johnson, 2002) for examples of such extension). finally, the independence from the details of the parser and the corpus suggests that our method can be applied to systems based on other formalisms, e.g., (hockenmaier, 2003), to allow a meaningful dependency-based comparison of very different parsers. furthermore, with the fine-grained set of dependency labels that our system provides, it is possible to map the resulting structures to other dependency formalisms, either automatically in case annotated corpora exist, or with a manually developed set of rules. our preliminary experiments with collins\u2019 parser and the corpus annotated with grammatical relations (carroll et al., 2003) are promising: the system achieves 76% precision/recall fscore, after the parser\u2019s output is enriched with our method and transformed to grammatical relations using a set of 40 simple rules. this is very close to the performance reported by carroll et al. (2003) for the parser specifically designed for the extraction of grammatical relations. despite the high-dimensional feature spaces, the large number of lexical features, and the lack of independence between features, we achieved high accuracy using a memory-based learner. timbl performed well on tasks where structured, more complicated and task-specific statistical models have been used previously (blaheta and charniak, 2000). for all subtasks we used the same settings for timbl: simple feature overlap measure, 5 nearest neighbours with majority voting. during further experiments with our method on different corpora, we found that quite different settings led to a better performance. it is clear that more careful and systematic parameter tuning and the analysis of the contribution of different features have to be addressed. finally, our method is not restricted to syntactic structures. it has been successfully applied to the identification of semantic relations (ahn et al., 2004), using framenet as the training corpus. for this task, we viewed semantic relations (e.g., speaker, topic, addressee) as dependencies between a predicate and its arguments. adding such semantic relations to syntactic dependency graphs was simply an additional graph transformation step. 0 conclusions we presented a method to automatically enrich the output of a parser with information that is not provided by the parser itself, but is available in a treebank. using the method with two state of the art statistical parsers and the penn treebank allowed us to recover functional tags (grammatical and semantic), empty nodes and traces. thus, we are able to provide virtually all information available in the corpus, without modifying the parser, viewing it, indeed, as a black box. our method allows us to perform a meaningful dependency-based comparison of phrase structure parsers. the evaluation on a dependency corpus derived from the penn treebank showed that, after our post-processing, two state of the art statistical parsers achieve 84% accuracy on a fine-grained set of dependency labels. finally, our method for enriching the output of a parser is, to a large extent, independent of a specific parser and corpus, and can be used with other syntactic and semantic resources.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-1004": {
        "abstract": "discovering latent structure in task-oriented dialogues abstract a key challenge for computational conversation models is to discover latent structure in task-oriented dialogue, since it provides a basis for analysing, evaluating, and building conversational systems. we propose three new unsupervised models to discover latent structures in task-oriented dialogues. our methods synthesize hidden markov models (for underlying state) and topic models (to connect words to states). we apply them to two real, non-trivial datasets: human-computer spoken dialogues in bus query service, and humanhuman text-based chats from a live technical support service. we show that our models extract meaningful state representations and dialogue structures consistent with human annotations. quantitatively, we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task.",
        "conclusion": "conclusion and future work we have presented three new unsupervised models to discover latent structures in task-oriented dialogues. we evaluated on two very different corpora\u2014logs from spoken, human-computer dialogues about bus time, and logs of textual, humanhuman dialogues about technical support. we have shown our models yield superior performance both qualitatively and quantitatively. one possible avenue for future work is scalability. parallelization (asuncion et al., 2012) or online learning (doucet et al., 2001) could significantly speed up inference. in addition to mcmc, another class of inference method is variational bayesian analysis (blei et al., 2003; beal, 2003), which is inherently easier to distribute (zhai et al., 2012) and online update (hoffman et al., 2010).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D10-1004": {
        "abstract": "dependency parsing. turbo parsers: dependency parsing by approximate variational inference abstract we present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of smith and eisner (2008) and the relaxed linear program of martins et al. (2009). by representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. we also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. the algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including crfs and structured svms. experiments show state-of-the-art performance for 14 languages.",
        "conclusion": "conclusion we presented a unified view of two recent approximate dependency parsers, by stating their underlying factor graphs and by deriving the variational problems that they address. we introduced new hard constraint factors, along with formulae for their messages, local belief constraints, and entropies. we provided an aggressive online algorithm for training the models with a broad family of losses. there are several possible directions for future work. recent progress in message-passing algorithms yield \u201cconvexified\u201d bethe approximations that can be used for marginal inference (wainwright et al., 2005), and provably convergent max-product variants that solve the relaxed lp (globerson and jaakkola, 2008). other parsing formalisms can be handled with the inventory of factors shown here\u2014 among them, phrase-structure parsing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-3029": {
        "abstract": "syntactic annotations syntactic annotations for the google books ngram corpus abstract we present a new edition of the google books ngram corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published. this new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodifier relationships are recorded. the annotations are produced automatically with statistical models that are specifically adapted to historical text. the corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.",
        "conclusion": "conclusions",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P01-1005": {
        "abstract": "scaling to very very large corpora for natural language disambiguation abstract the amount of readily available on-line text has reached hundreds of billions of words and continues to grow. yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. in this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. we are fortunate that for this particular application, correctly labeled training data is free. since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.",
        "conclusion": "conclusions in this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available. we have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets. we have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation. we propose that a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections, while deemphasizing the focus on comparing different learning techniques trained only on small training corpora. while it is encouraging that there is a vast amount of on-line text, much work remains to be done if we are to learn how best to exploit this resource to improve natural language processing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D12-1108": {
        "abstract": "document-wide decoding for phrase-based statistical machine translation abstract any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. consider, for instance, the work on cache-based language models by tiedemann (2010) and gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by le nagard and koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and hardmeier and federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. in this paper, we present a method for decoding complete documents in phrase-based smt. our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. the initial state is improved by the application of a series of operations using a hill climbing strategy to find a (local) maximum of the score function. this setup gives us complete freedom to define scoring functions over the entire document. moreover, by optionally initialising the state with the output of a traditional dp decoder, we can ensure that the final hypothesis is no worse than what would have been found by dp search alone. we start by describing the decoding algorithm and the state operations used by our decoder, then we present empirical results demonstrating the effectiveness of our approach and its usability with a document-level semantic language model, and finally we discuss some related work.",
        "conclusion": "conclusion in the last twenty years of smt research, there has been a strong assumption that sentences in a text are independent of one another, and discourse context has been largely neglected. several factors have contributed to this. developing good discourse-level models is difficult, and considering the modest translation quality that has long been achieved by smt, there have been more pressing problems to solve and lower hanging fruit to pick. however, we argue that the popular dp beam search algorithm, which delivers excellent decoding performance, but imposes a particular kind of local dependency structure on the feature models, has also had its share in driving researchers away from discourse-level problems. in this paper, we have presented a decoding procedure for phrase-based smt that makes it possible to define feature models with cross-sentence dependencies. our algorithm can be combined with dp beam search to leverage the quality of the traditional approach with increased flexibility for models at the discourse level. we have presented preliminary results on a cross-sentence semantic language model addressing the problem of lexical cohesion to demonstrate that this kind of models is worth exploring further. besides lexical cohesion, cross-sentence models are relevant for other linguistic phenomena such as pronominal anaphora or verb tense selection. we believe that smt research has reached a point of maturity where discourse phenomena should not be ignored any longer, and we consider our decoder to be a step towards this goal.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P08-1108": {
        "abstract": "integrating graph-based and transition-based dependency parsers abstract previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference. in this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model. by letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the conll-x shared task.",
        "conclusion": "conclusion in this paper, we have demonstrated how the two dominant approaches to data-driven dependency parsing, graph-based models and transition-based models, can be integrated by letting one model learn from features generated by the other. our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing. moreover, a comparative error analysis reveals that the improvements are largely predictable from theoretical properties of the two models, in particular the tradeoff between global learning and inference, on the one hand, and rich feature representations, on the other. directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1002": {
        "abstract": "joint feature selection in distributed joint feature selection in distributed stochastic learning for large-scale discriminative training in smt abstract with a few exceptions, discriminative training in statistical machine translation (smt) has been content with tuning weights for large feature sets on small development data. evidence from machine learning indicates that increasing the training sample size results in better prediction. the goal of this paper is to show that this common wisdom can also be brought to bear upon smt. we deploy local features for scfg-based smt that can be read off from rules at runtime, and present a learnalgorithm that applies regularization for joint feature selection over distributed stochastic learning processes. we present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.",
        "conclusion": "discussion we presented an approach to scaling discriminative learning for smt not only to large feature sets but also to large sets of parallel training data. since inference for smt (unlike many other learning problems) is very expensive, especially on large training sets, good parallelization is key. our approach is made feasible and effective by applying joint feature selection across distributed stochastic learning processes. furthermore, our local features are efficiently computable at runtime. our algorithms and features are generic and can easily be reimplemented and make our results relevant across datasets and language pairs. in future work, we would like to investigate more sophisticated features, better learners, and in general improve the components of our system that have been neglected in the current investigation of relative improvements by scaling the size of data and feature sets. ultimately, since our algorithms are inspired by multi-task learning, we would like to apply them to scenarios where a natural definition of tasks is given. for example, patent data can be characterized along the dimensions of patent classes and patent text fields (w\u00a8aschle and riezler, 2012) and thus are well suited for multi-task translation.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N03-1004": {
        "abstract": "in question answering two heads are better than one abstract motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multistrategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora. the answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques. we present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels. experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric.",
        "conclusion": "conclusions in this paper, we introduced a multi-strategy and multisource approach to question answering that enables combination of answering agents adopting different strategies and consulting multiple knowledge sources. in particular, we focused on two answering agents, one adopting a knowledge-based approach and one using statistical methods. we discussed our answer resolution component which employs a multi-level combination algorithm that allows for resolution at the question, passage, and answer levels. best performance using the % correct metric was achieved by the three-level algorithm that combines after each stage, while highest average precision was obtained by a two-level algorithm merging at the question and answer levels, supporting a tightly-coupled design for multi-agent question answering. our experiments showed that our best performing algorithms achieved a 35.0% relative improvement in the number of correct answers and a 32.8% improvement in average precision on a previously unseen test set.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-1110": {
        "abstract": "incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese abstract we propose the first joint model for word segmentation, pos tagging, and dependency parsing for chinese. based on an extension of the incremental joint model for pos tagging and dependency parsing (hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, pos tagging, and dependency parsing models. we also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. in experiments using the chinese treebank (ctb), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for pos tagging and 2.4% for dependency parsing. we also perform comparison experiments with the partially joint models.",
        "conclusion": "conclusion in this paper, we proposed the first joint model for word segmentation, pos tagging, and dependency parsing in chinese. the model demonstrated substantial improvements on the three tasks over the pipeline combination of the state-of-the-art joint segmentation and pos tagging model, and dependency parser. particularly, results showed that the accuracies of pos tagging and dependency parsing were remarkably improved by 0.6% and 2.4%, respectively corresponding to 8.3% and 10.2% error reduction. for word segmentation, although the overall improvement was only around 0.1%, greater than 1% improvements was observed for oov words. we conducted some comparison experiments of the partially joint and full joint models. compared to segtagdep, segtag+tagdep performs reasonably well in terms of dependency parsing accuracy, whereas the pos tagging accuracies are more than 0.5% lower. in future work, probabilistic pruning techniques such as the one based on a maximum entropy model are expected to improve the efficiency of the joint model further because the accuracies are apparently still improved if a larger beam can be used. more efficient decoding would also allow the use of the look-ahead features (hatori et al., 2011) and richer parsing features (zhang and nivre, 2011). acknowledgement we are grateful to the anonymous reviewers for their comments and suggestions, and to xianchao wu, kun yu, pontus stenetorp, and shinsuke mori for their helpful feedback.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D10-1069": {
        "abstract": "uptraining for accurate deterministic question parsing abstract it is well known that parsing accuracies drop significantly on out-of-domain data. what is less known is that some parsers suffer more from domain shifts than others. we show that dependency parsers have more difficulty parsing questions than constituency parsers. in particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. we propose an in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). uptraining with 100k unlabeled questions achieves results comparable to having 2k labeled questions for training. with 100k unlabeled and 2k labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.",
        "conclusion": "conclusions we presented a method for domain adaptation of deterministic shift-reduce parsers. we evaluated multiple state-of-the-art parsers on a question corpus and showed that parsing accuracies degrade substantially on this out-of-domain task. most notably, deterministic shift-reduce parsers have difficulty dealing with the modified word order and lose more than 20% in accuracy. we then proposed a simple, yet very effective uptraining method for domainadaptation. in a nutshell, we trained a deterministic shift-reduce parser on the output of a more accurate, but slower parser. uptraining with large amounts of unlabeled data gives similar improvements as having access to 2,000 labeled sentences from the target domain. with 2,000 labeled questions and a large amount of unlabeled questions, uptraining is able to close the gap between in-domain and out-of-domain accuracy.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-2084": {
        "abstract": "identifying word translations from comparable corpora using latent topic models abstract a topic model outputs a set of multinomial distributions over words for each topic. in this paper, we investigate the value of bilingual topic models, i.e., a bilingual latent dirichlet allocation model for finding translations of terms in comparable corpora without using any linguistic resources. experiments on a document-aligned english-italian wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. the best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.",
        "conclusion": "conclusion we have presented a generic, language-independent framework for mining translations of words from latent topic models. we have proven that topical knowledge is useful and improves the quality of word translations. the quality of translations depends only on the quality of a topic model and its ability to find latent relations between words. our next steps involve experiments with other topic models and other corpora, and combining this unsupervised approach with other tools for lexicon extraction and synonymy detection from unrelated and comparable corpora.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N09-2064": {
        "abstract": "combining constituent parsers abstract the output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (henderson and brill, 1999; sagae and lavie, 2006). we propose three ways to improve upon existing methods for parser combination. first, we propose a method of parse hybridization that recomproductions of conthereby preserving the structure of the output of the individual parsers to a greater extent. second, we propose an efficient lineartime algorithm for computing expected f-score using minimum bayes risk parse selection. third, we extend these parser combination from multiple outputs to muloutputs. we present results on wsj section 23 and also on the english side of a chinese-english parallel corpus.",
        "conclusion": "discussion & conclusion results are shown in tables 2, 3, and 4. on both test sets, constituent recombination achieves the best f-score (1.0 points on wsj test and 2.3 points on chinese-english test), followed by context-free production combination, then parse selection, though the differences in f-score among the combination methods are not statistically significant. increasing the n-best list size from 1 to 10 improves parse selection and context-free production recombination, though further increasing n does not, in general, help.4 chinese-english test set f-score gets a bigger boost from combination than wsj test set f-score, perhaps because the best individual parser\u2019s baseline f-score is lower on the out-of-domain data. we have presented an algorithm for parse hybridization by recombining context-free productions. while constituent recombination results in the highest f-score of the methods explored, contextfree production recombination produces trees which better preserve the syntactic structure of the individual parses. we have also presented an efficient linear-time algorithm for selecting the parse with maximum expected f-score.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W03-1022": {
        "abstract": "supersense tagging of unknown nouns in wordnet abstract we present a new framework for classifying common nouns that extends namedentity classification. we used a fixed set 26 semantic labels, which we called su- these are the labels used by lexicographers developing wordnet. this framework has a number of practical advantages. we show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. we also define a more realistic evaluation procedure than cross-validation.",
        "conclusion": "conclusion we presented a new framework for word sense classification, based on the wordnet lexicographer classes, that extends named-entity classification. within this framework it is possible to use the information contained in wordnet to improve classification and define a more realistic evaluation than standard cross-validation. directions for future research include the following topics: disambiguation of the training data, e.g. during training as in cotraining; learning unknown ambiguous nouns, e.g., studying the distribution of the labels the classifier guessed for the individual tokens of the new word.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D11-1022": {
        "abstract": "dual decomposition with many overlapping components abstract dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. however, in cases where lightweight decomare not readily available due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. we sidestep that difficulty by adopting an augmented lagrangian method that accelerates model consensus by regularizing towards the averaged votes. we show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.",
        "conclusion": "conclusion we have introduced new feature-rich turbo parsers. since exact decoding is intractable, we solve an lp relaxation through a recently proposed consensus algorithm, dd-admm, which is suitable for problems with many overlapping components. we study the empirical runtime and convergence properties of dd-admm, complementing the theoretical treatment in martins et al. (2011). dd-admm compares favourably against the subgradient method in several aspects: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. while its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. dd-admm may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by liang et al. (2011). non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (clarke and lapata, 2008; martins and smith, 2009; bergkirkpatrick et al., 2011). finally, dd-admm can be adapted to tighten its relaxations towards exact decoding, as in sontag et al. (2008) and rush and collins (2011). we defer this for future work.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1092": {
        "abstract": "translating unknown words by analogical learning abstract unknown words are a well-known hindranceto natural language applications. in particular, they drastically impact machine transla tion quality. an easy way out commercial translation systems usually offer their users is the possibility to add unknown wordsand their translations into a dedicated lex icon. recently, stroppa and yvon (2005) have shown how analogical learning alone deals nicely with morphology in differentlanguages. in this study we show that ana logical learning offers as well an elegant andeffective solution to the problem of identify ing potential translations of unknown words.",
        "conclusion": "discussion and future work in this paper, we have investigated the appropri ateness of analogical learning to handle unknown words in machine translation. on the contrary to several lines of work, our approach does not rely on massive additional resources but capitalizes instead on an information which is inherently pertaining tothe language. we measured that roughly 80% of or dinary unknown french words can receive a valid translation into english with our approach. this work is currently being developed in severaldirections. first, we are investigating why our ap proach remains silent for some words or phrases.this will allow us to better characterize the limitations of analog and will hopefully lead us to de sign a better strategy for identifying the stems of agiven word or phrase. second, we are investigat ing how a systematic enrichment of a phrase-transfer table will impact a phrase-based statistical machine translation engine. last, we want to investigate the training of a model that can learn regularities from the analogies we are making. this would relieve usfrom requiring the training material while translat ing, and would allow us to compare our approachwith other methods proposed for unsupervised mor phology acquisition.acknowledgement we are grateful to the anony mous reviewers for their useful suggestions and to pierre poulin for his fruitful comments. this study has been partially funded by nserc. 885",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W06-3114": {
        "abstract": "we evaluated machine translation performance for six european language pairs that participated in a shared task: translating french, german, spanish texts to english and back evaluation was done automatically using the bleu score and manually on fluency and adequacy",
        "conclusion": "conclusions we carried out an extensive manual and automatic evaluation of machine translation performance on european language pairs. while many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems. due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. the bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data. the manual evaluation of scoring translation on a graded scale from 1\u20135 seems to be very hard to perform. replacing this with an ranked evaluation seems to be more suitable. human judges also pointed out difficulties with the evaluation of long sentences.",
        "citations": [],
        "summary": [
            "Koehn and Monz carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs. While many systems had similar performance, the results offered interesting insights, especially, about the relative performance of statistical and rule-based systems. Due to many similarly performing systems, they are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favour of statistical systems seemed to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1&#8211;5 seemed to be very hard to perform. Human judges also pointed out difficulties with the evaluation of long sentences. They found replacing it with a ranked evaluation to be more suitable. The paper'Manual and Automatic Evaluation of Machine Translation between European Languages'by Philipp Koehn and Christof Monz talks about a shared task to evaluate machine translation performance.They assembled various forms of data and resources: a baseline MT system,language models, prepared training and test sets,resulting in actual machine translation output from several state-of-the-art systems and manual evaluations.Training and testing is based on the Europarl corpus.For translation they used automatic evaluation and manual evaluation.For automatic evaluation they used BLEU.They were not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.\n In this paper the author evaluates machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done automatically using the BLEU score and manually on fluency and adequacy. Due to many similarly performing systems, the author was not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favour of statistical systems seems to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1\u20135 seems to be very hard to perform. Replacing this with a ranked evaluation seems to be more suitable. Human judges also pointed out difficulties with the evaluation of long sentences."
        ]
    },
    "D09-1161": {
        "abstract": "k-best combination of syntactic parsers abstract in this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers. the proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model. as a result, it is able to fully utilize both the logarithm of the probability of each k-best parse tree from each individual parser and any additional useful features. for feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm. our experiments are carried out on both the chinese and english penn treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. experimental results show that our f-scores of 85.45 on chinese and 92.62 on english outperform the previously best-reported systems by 1.21 and 0.52, respectively.",
        "conclusion": "conclusions in this paper, we propose a linear model-based general framework for multiple parser combination. compared with previous methods, our method is able to use diverse features, including logarithm of the parse tree probability calculated by the individual systems. we verify our method by combining the two representative parsing models, lexicalized model and un-lexicalized model, on both chinese and english. experimental results show our method is very effective and advance the state-of-the-art results on both chinese and english syntax parsing. in the future, we will explore more features and study the forest-based combination methods for syntactic parsing.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-3401": {
        "abstract": "probabilistic lexical generalization for french dependency parsing abstract this paper investigates the impact on french dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis. a distributional thesaurus is created from a large text corpus and used for distributional clustering and wordnet automatic sense ranking. the standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class. we use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets. probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features. we obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the french treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.",
        "conclusion": "conclusion we have investigated the use of probabilistic lexical target spaces for reducing lexical data sparseness in a transition-based dependency parser for french. we built a distributional thesaurus from an automatically-parsed large text corpus, using it to generate word clusters and perform wordnet asr. we tested a standard approach to lexical generalization for parsing that has been previously explored, where a word is mapped to a single cluster or synset. we also introduced a novel probabilistic lexical generalization approach, where a lemma is represented by a categorical distribution over the space of lemmas, clusters, or synsets. probabilities for the lemma space were calculated using the distributional thesaurus, and probabilities for the wordnet synset space were calculated using asr sense prevalence scores, with probabilistic clusters left for future work. our experiments with an arc-eager transitionbased dependency parser resulted in modest but significant improvements in las over the baseline when parsing out-of-domain medical text. however, we did not see statistically significant improvements over the baseline when parsing in-domain text or out-of-domain parliamentary text. an explanation for this result is that the french treebank training set vocabulary has a very high lexical coverage over the evaluation sets in these domains, suggesting that lexical generalization does not provide much additional benefit. comparing the standard single-mapping approach to the probabilistic generalization approach, we found a slightly (though not significantly) better performance for probabilistic generalization across different parsing configurations and evaluation sets. however, the probabilistic approach also has the downside of a slower running time. based on the findings in this paper, our focus for future work on lexical generalization for dependency parsing is to continue improving parsing performance on out-of-domain text, specifically for those domains where lexical variation is high with respect to the training set. one possibility is to experiment with building a distributional thesaurus that uses text from both the source and target domains, similar to what candito et al. (2011) did with brown clustering, which may lead to a stronger bridging effect across domains for probabilistic lexical generalization methods.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-1310": {
        "abstract": "exemplar-based word-space model for compositionality detection: shared task system description abstract in this paper, we highlight the problems of polysemy in word space models of compositionality detection. most models represent each word as a single prototype-based vector without addressing polysemy. we propose an exemplar-based model which is designed to handle polysemy. this model is tested for compositionality detection and it is found to outperform existing prototype-based models. we have participated in the shared task (biemann and giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations.",
        "conclusion": "conclusions in this paper, we examined the effect of polysemy in word space models for compositionality detection. we showed exemplar-based wsm is effective in dealing with polysemy. also, we use multiple evidences for compositionality detection rather than basing our judgement on a single evidence. overall, performance of the exemplar-based models of compositionality detection is found to be superior to prototype-based models.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "A00-2030": {
        "abstract": "a novel use of statistical parsing to extract information from text abstract since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the upenn treebank as a gold standard. in this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on muc-7 template elements and template relations.",
        "conclusion": "1 conclusions we have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (lpcfghr) can be used effectively for information extraction. a single model proved capable of performing all necessary sentential processing, both syntactic and semantic. we were able to use the penn treebank to estimate the syntactic parameters; no additional syntactic training was required. the semantic training corpus was produced by students according to a simple set of guidelines. this simple semantic annotation was the only source of task knowledge used to configure the model.",
        "citations": [],
        "summary": [
            "The paper reports adapting a lexicalized,probabilistic context-free parser with head rules to information extraction.the approach is evaluated on two tasks:The Template Element (TE) task which identifies elements like organizations,persons, locations, and some artifacts and The Template Relations (TR) task which involves identifying instances of relations between elements in the text.TR builds on TE in that TR reports binary relations between elements of TE.The model can limit the propagation of errors by making all decisions jointly. For this reason,The authors designed an integrated model in which tagging, name finding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.The integrated model represents syntax and semantics jointly using augmented parse trees.In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.The system finished in second place among all entrants.Disabling the cross-sentence model entirely reduced overall F-Score by 2 points.\n In this paper the author aimed at reporting, adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate the new technique on MUC-7 template elements and template relations. The author is able to integrate both syntactic and semantic information into the parsing process, thus avoiding potential errors of syntax first followed by semantics. Their parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation. They were able to specify relatively simple guidelines that students with no training in computational linguistics could annotate. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model. This paper talks about A Novel Use of Statistical Parsing to Extract Information from Text. We report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations. We describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machine generated syntactic parse trees. We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model."
        ]
    },
    "W12-3134": {
        "abstract": "joshua 4.0: packing, pro, and paraphrases abstract we present joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. the main con",
        "conclusion": "conclusion we present a new iteration of the joshua machine translation toolkit. our system has been extended towards efficiently supporting large-scale experiments in parsing-based machine translation and text-to-text generation: joshua 4.0 supports compactly represented large grammars with its packed grammars, as well as large language models via kenlm and berkeleylm.we include an implementation of pro, allowing for stable and fast tuning of large feature sets, and extend our toolkit beyond pure translation applications by extending thrax with a large-scale paraphrase extraction module. acknowledgements this research was supported by in part by the euromatrixplus project funded by the european commission (7th framework programme), and by the nsf under grant iis-0713448. opinions, interpretations, and conclusions are the authors\u2019 alone.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W04-1506": {
        "abstract": "towards a dependency parser for basque abstract we present the dependency parser, for the linguistic processing of basque, which can serve as a representative of agglutinative languages that are also characterized by the free order of its constituents. the dependency syntactic model is applied to establish the dependency-based grammatical relations between the components within the clause. such a deep analysis is used to improve the output of the shallow parsing where syntactic structure ambiguity is not fully and explicitly resolved. previous to the completion of the grammar for the dependency parsing, the design of the dependency structure-based scheme had to be accomplished; we concentrated on issues that must be resolved by any practical system that uses such models. this scheme was used both to the manual tagging of the corpus and to develop the parser. the manually tagged corpus has been used to evaluate the accuracy of the parser. we have evaluated the application of the grammar to corpus, measuring the linking of the verb with its dependents, with satisfactory results.",
        "conclusion": "conclusions we have presented the application of the dependency grammar parser for the processing of basque, which can serve as a representative of agglutinative languages with free order of constituents. we have shown how dependency grammar approach provides a good solution for deeper syntactic analysis, being at this moment the best alternative for morphologically complex languages. we have also evaluated the application of the grammar to corpus, measuring the linking of the verb with its dependents, with satisfactory results. however, the development of a full dependency syntactic analyser is still a matter of research. for instance, all kinds of constructions without a clear syntactic head are difficult to analyse: ellipses, sentences without a verb (e.g., copula-less predicative), and coordination. all these aspects have been treated in our manually annotated corpus; our efforts now are oriented to deal with them automatically.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P04-1006": {
        "abstract": "attention shifting for parsing speech abstract we present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling. our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice. the parser\u2019s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. this attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model.",
        "conclusion": "conclusion we presented a parsing technique that shifts the attention of a word-lattice parser in order to ensure syntactic analyses for all lattice paths.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P10-1155": {
        "abstract": "all words domain adapted wsd: finding a middle ground between supervision and unsupervision abstract in spite of decades of research on word sense disambiguation (wsd), all-words general purpose wsd has remained a distant goal. many supervised wsd systems have been built, but the effort of creatthe training corpus sense corpora has always been a matter of concern. therefore, attempts have been made to develop unsupervised and knowledge based techniques for wsd which do not need sense marked corpora. however such approaches have not proved effective, since they typically do not better wordnet first sense baseline accuracy. our research reported here proposes to stick to the supervised approach, but with far less demand on annotation. we show that if we have any sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in any other domain can deliver the goods almost as if exhaustive sense marking were available in that domain. we have tested our approach across tourism and health domain corpora, using also the well known mixed domain semcor corpus. accuracy figures close to self domain training lend credence to the viability of our approach. our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised wsd. finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific wsd.",
        "conclusion": "conclusion and future work based on our study of wsd in 4 domain adaptation scenarios, we make the following conclusions: ing finding with the following implication: as long as one has sense marked corpus - be it from a mixed or specific domain - simply injecting any small amount of data from the target domain suffices to beget good accuracy. as future work, we would like to test our work on the environment domain data which was released as part of the semeval 2010 shared task on \u201callwords word sense disambiguation on a specific domain\u201d.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N12-1007": {
        "abstract": "entity clustering across languages abstract standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like english wordnet. when co-referent text mentions appear in different languages, these techniques cannot be easily applied. consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters. our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures. crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown. on an arabic-english corpus that contains seven different text genres, our best model yields a 24.3% f1 gain over the baseline.",
        "conclusion": "conclusion cross-lingual entity clustering is a natural step toward more robust natural language understanding. we proposed pipeline models that make clustering decisions based on cross-lingual similarity. we investigated two methods for mapping documents in different languages to a common representation: mt and the pltm. although mt may achieve more accurate results for some language pairs, the pltm training resources (e.g., wikipedia) are readily available for many languages. as for the clustering algorithms, hac appears to perform better than the dpmm on our dataset, but this may be due to the small corpus size. the instance-level constraints represent tendencies that could be learned from larger amounts of data. with more data, we might be able to relax the constraints and use an exchangeable dpmm, which might be more effective. finally, we have shown that significant quantities of within-document errors cascade into the cross-lingual clustering phase. as a result, we plan a model that clusters the mentions directly, thus removing the dependence on within-document coreference resolution. in this paper, we have set baselines and proposed models that significantly exceeded those baselines. the best model improved upon the cross-lingual entity baseline by 24.3% f1. this result was achieved without a knowledge base, which is required by previous approaches to cross-lingual entity linking. more importantly, our techniques can be used to extend existing cross-document entity clustering systems for the increasingly multilingual web. acknowledgments we thank jason eisner, david mimno, scott miller, jim mayfield, and paul mcnamee for helpful discussions. this work was started during the scale 2010 summer workshop at johns hopkins. the first author is supported by a national science foundation graduate fellowship.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1026": {
        "abstract": "instance based lexical entailment instance based lexical entailment for ontology population abstract in this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text. the approach is fully unsupervised and based on kernel methods. we demonstrate the effectiveness of our technique largelysurpassing both the random and most fre quent baselines and outperforming current state-of-the-art unsupervised approaches ona benchmark ontology available in the liter ature.",
        "conclusion": "conclusions and future work in this paper, we presented a novel unsupervised technique for recognizing lexical entailment in texts, namely instance based lexical entailment, and we exploited it to approach an ontology population task. the basic assumption is that if a word is entailed by another in a given context, then some of the contexts of the entailed word should be similar tothat of the word to be disambiguated. our tech nique is effective, as it largely surpasses both the random and most frequent baselines. in addition, it improves over the state-of-the-art for unsupervisedapproaches, achieving performances close to the su pervised rivaling techniques requiring hundreds of examples for each class. ontology population is only one of the possible applications of lexical entailment. for the future, we plan to apply our instance based approach to a wide variety of tasks, e.g., lexical substitution, word sense disambiguation and information retrieval. in addition, we plan to exploit our lexical entailment asa subcomponent of a more complex system to rec ognize textual entailment. finally, we are going toexplore more elaborated kernel functions to recog nize lexical entailment and more efficient learning strategies to apply our method to web-size corpora. acknowledgments the authors would like to thank bernardo magnini and hristo tanev for providing the benchmark, ido dagan for useful discussions and commentsregarding the connections between lexical entail ment and ontology population, and alberto lavellifor his thorough review. claudio giuliano is sup ported by the x-media project (http://www.x-media-project.org), sponsored by the european commission as part of the information so ciety technologies (ist) program under ec grantnumber ist-fp6-026978. alfio gliozzo is sup ported by the firb-israel research project n. rbin045pxh.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P05-1061": {
        "abstract": "simple algorithms for complex relation extraction with applications to biomedical ie abstract complex relation is any relation in which some of the arguments may be be unspecified. we present here a simple two-stage method for extracting complex relations between named entities in text. the first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. we evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text.",
        "conclusion": "conclusions and future work we presented a method for complex relation extraction, the core of which was to factorize complex relations into sets of binary relations, learn to identify binary relations and then reconstruct the complex relations by finding maximal cliques in graphs that represent relations between pairs of entities. the primary advantage of this method is that it allows for the use of almost any binary relation classifier, which have been well studied and are often accurate. we showed that such a method can be successful with an empirical evaluation on a large set of biomedical data annotated with genomic variation relations. in fact, this approach is both significantly quicker and more accurate then enumerating and classifying all possible instances. we believe this work provides a good starting point for continued research in this area. a distinction may be made between the factored system presented here and one that attempts to classify complex relations without factorization. this is related to the distinction between methods that learn local classifiers that are combined with global constraints after training and methods that incorporate the global constraints into the learning process. mccallum and wellner (2003) showed that learning binary co-reference relations globally improves performance over learning relations in isolation. however, their model relied on the transitive property inherent in the co-reference relation. our system can be seen as an instance of a local learner. punyakanok et al. (2004) argued that local learning actually outperforms global learning in cases when local decisions can easily be learnt by the classifier. hence, it is reasonable to assume that our binary factorization method will perform well when binary relations can be learnt with high accuracy. as for future work, there are many things that we plan to look at. the binary relation classifier we employ is quite simplistic and most likely can be improved by using features over a deeper representation of the data such as parse trees. other more powerful binary classifiers should be tried such as those based on tree kernels (zelenko et al., 2003). we also plan on running these algorithms on more data sets to test if the algorithms empirically generalize to different domains. perhaps the most interesting open problem is how to learn the complex reconstruction phase. one possibility is recent work on supervised clustering. letting the edge probabilities in the graphs represent a distance in some space, it may be possible to learn how to cluster vertices into relational groups. however, since a vertex/entity can participate in one or more relation, any clustering algorithm would be required to produce non-disjoint clusters. we mentioned earlier that the only restriction of our complex relation definition is that the arity of the relation must be known in advance. it turns out that the algorithms we described can actually handle dynamic arity relations. all that is required is to remove the constraint that maximal cliques must be consistent with the structure of the relation. this represents another advantage of binary factorization over enumeration, since it would be infeasible to enumerate all possible instances for dynamic arity relations.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-2067": {
        "abstract": "clause restructuring for smt not absolutely helpful abstract there are a number of systems that use a syntax-based reordering step prior to phrasebased statistical mt. an early work proposing this idea showed improved translation performance, but subsequent work has had mixed results. speculations as to cause have suggested the parser, the data, or other factors. we systematically investigate possible factors to give an initial answer to the question: under what conditions does this use of syntax help psmt?",
        "conclusion": "conclusion collins et al. (2005) reported that a reorderingas-preprocessing approach improved overall performance in german-to-english translation. the reimplementation of this system by howlett and dras (2010) came to the opposite conclusion. we have systematically varied several aspects of the howlett and dras (2010) system and reproduced results close to both papers, plus a full range in between. our results show that choices in the psmt system can completely erode potential gains of the reordering preprocessing step, with the largest effect due to simple choice of data. we have shown that a lack of overall improvement using reordering-aspreprocessing need not be due to the usual suspects, language pair and reordering process. significantly, our oracle experiments show that in all cases the reordering system does produce better translations for some sentences. we conclude that effort is best directed at determining for which sentences the improvement will appear.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N10-1035": {
        "abstract": "linear context-free rewriting systems efficient parsing of well-nested linear context-free rewriting systems abstract the use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order. we present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems. our result is obtained through a linear space construction of a binary normal form for the grammar at hand.",
        "conclusion": "conclusion in this paper, we have presented an efficient parsing algorithm for well-nested linear context-free rewriting systems, based on a new normal form for this formalism. the normal form takes up linear space with respect to grammar size, and the algorithm is based on a bottom-up process that can be applied to any lcfrs, achieving o(cp \u00b7 |g |\u00b7 |w|2cp+2) time complexity when applied to lcfrs of fan-out cp in our normal form. this complexity is an asymptotic improvement over existing results for this class, both from parsers specifically geared to well-nested lcfrs or equivalent formalisms (hotz and pitsch, 1996) and from applying general lcfrs parsing techniques to the well-nested case (seki et al., 1991). the class of well-nested lcfrs is an interesting syntactic formalism for languages with discontinuous constituents, providing a good balance between coverage of linguistic phenomena in natural language treebanks (kuhlmann and nivre, 2006; maier and lichte, 2009) and desirable formal properties (kanazawa, 2009). our results offer a further argument in support of well-nested lcfrs: while the complexity of parsing general lcfrs depends on two dimensions (rank and fan-out), this bidimensional hierarchy collapses into a single dimension in the well-nested case, where complexity is only conditioned by the fan-out. acknowledgments g\u00f3mez-rodr\u00edguez has been supported by mec/feder (hum2007-66607-c04) and xunta de galicia (pgidit07sin005206pr, redes galegas de pl e ri e de ling. de corpus, bolsas estad\u00edas incite/fse cofinanced). kuhlmann has been supported by the swedish research council.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1122": {
        "abstract": "a two-stage parser for multilingual dependency parsing abstract we present a two-stage multilingual de pendency parsing system submitted to the multilingual track of conll-2007. the parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem. we describe the features used ineach stage. for four languages with different values of root, we design some spe cial features for the root labeler. then we present evaluation results and error analyses focusing on chinese.",
        "conclusion": "conclusion in this paper, we presented our two-stage depen dency parsing system submitted to the multilingual track of conll-2007 shared task. we used nivre?smethod to produce the dependency arcs and the se quence labeler to produce the dependency labels. the experimental results showed that our system can provide good performance for all languages.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-1078": {
        "abstract": "relation extraction with manifold models medical relation extraction with manifold models abstract in this paper, we present a manifold model for medical relation extraction. our model is built upon a medical corpus containing 80m sentences (11 gigabyte text) and designed to accurately and efficiently detect the key medical relations that can facilitate clinical decision making. our approach integrates domain specific parsing and typing systems, and can utilize labeled as well as unlabeled examples. to provide users with more flexibility, we also take label weight into consideration. effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments.",
        "conclusion": "conclusions in this paper, we identify a list of key relations that can facilitate clinical decision making. we also present a new manifold model to efficiently extract these relations from text. our model is developed to utilize both labeled and unlabeled examples. it further provides users with the flexibility to take label weight into consideration. effectiveness of the new model is demonstrated both theoretically and experimentally. we apply the new model to construct a relation knowledge base (kb), and use it as a complement to the existing manually created kbs.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C10-1135": {
        "abstract": "joint tokenization and translation abstract as tokenization is usually ambiguous for many natural languages such as chineseand korean, tokenization errors might po tentially introduce translation mistakes fortranslation systems that rely on 1-best tokenizations. while using lattices to offer more alternatives to translation systems have elegantly alleviated this prob lem, we take a further step to tokenize and translate jointly. taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on thetarget side simultaneously. by integrat ing tokenization and translation features in a discriminative framework, our jointdecoder outperforms the baseline trans lation systems using 1-best tokenizationsand lattices significantly on both chineseenglish and korean-chinese tasks. interestingly, as a tokenizer, our joint de coder achieves significant improvements over monolingual chinese tokenizers.",
        "conclusion": "conclusion we have presented a novel method for joint tok enization and translation which directly combines the tokenization model into the decoding phase.allowing tokenization and translation to collaborate with each other, tokenization can be opti mized for translation, while translation also makes contribution to tokenization performance under a supervised way. we believe that our approach can be applied to other string-based model such asphrase-based model (koehn et al, 2003), stringto-tree model (galley et al, 2006) and string-to dependency model (shen et al, 2008). acknowledgement the authors were supported by sk telecom cibusiness, and national natural science founda tion of china, contracts 60736014 and 60903138.we thank the anonymous reviewers for their insightful comments. we are also grateful to wen bin jiang, zhiyang wang and zongcheng ji for their helpful feedback. 1207",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1030": {
        "abstract": "using rbmt systems to produce bilingual corpus for smt abstract this paper proposes a method using the ex isting rule-based machine translation (rbmt) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the statistical ma chine translation (smt) system. we use the existing rbmt system to translate the monolingual corpus into synthetic bilingual corpus. with the synthetic bilingual corpus, we can build an smt system even if there is no real bilingual corpus. in our experi ments using bleu as a metric, the system achieves a relative improvement of 11.7% over the best rbmt system that is used to produce the synthetic bilingual corpora. we also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora. the interpolated model achieves an abso lute improvement of 0.0245 bleu score (13.1% relative) as compared with the in dividual model trained on the real bilingual corpus.",
        "conclusion": "discussion 6.1 model interpolation vs. corpus merge. in section 5, we make use of the real bilingual corpus and the synthetic bilingual corpora by perform ing model interpolation. another available way is directly combining these two kinds of corpora to train a translation model, namely corpus merge. in order to compare these two methods, we use rbmt system 1 to translate the 1,087,651 monolingual english sentences to produce synthetic bi lingual corpus. then we train an smt system with the combination of this synthetic bilingual corpus and the real bilingual corpus. the bleu score of such system is 0.1887, while that of the model in terpolation system is 0.2020. it indicates that the model interpolation method is significantly better than the corpus merge method. 6.2 result analysis. as discussed in section 5.5, the number of the overlapped phrase pairs among the standard model and the synthetic models is very small. the newly added phrase pairs from the synthetic models can assist to improve the translation results of the in terpolated model. in this section, we will use an example to further discuss the reason behind the improvement of the smt system by using syn thetic bilingual corpus. table 5 shows an english sentence and its chinese translations produced by different methods. and figure 3 shows the phrase pairs used for translation. the results show that imperfect translations of rbmt systems can be also used to boost the performance of an smt sys tem. phrase pairs phrase pairs used new pairs used standard model 6,105,260 5,509 ? interpolated model 73,221,525 5,306 1993 table 6. statistics of phrase pairs further analysis is shown in table 6. after add ing the synthetic corpus produced by the rbmt systems, the interpolated model outperforms the standard models mainly for the following two rea sons: (1) some new phrase pairs are added into the interpolated model. 37.6% phrase pairs (1993 out of 5306) are newly learned and used for translation. for example, the phrase pair \"after-sale service <-> ???? (shouhoufuwu)\" is added; (2) the prob ability distribution of the phrase pairs is changed. for example, the probabilities of the two pairs \"a brand name <-> ?? (pinpai)\" and \"and the crea tion of <-> ? ?? (he chuangzao)\" increase. the probabilities of the other two pairs \"brand name < > ?? (pinpai)\" and \"and the creation of a <-> ? ?? (he jianli)\" decrease. we found that 930 phrase pairs, which are also in the phrase table of the standard model, are used by the interpolated model for translation but not used by the standard model. 6.3 human evaluation. according to (koehn and monz, 2006; callison burch et al, 2006), the rbmt systems are usually not adequately appreciated by bleu. we also manually evaluated the rbmt systems and smt systems in terms of both adequacy and fluency as defined in (koehn and monz, 2006). the evalua tion results show that the smt system with the interpolated model, which achieves the highest bleu scores in table 2, achieves slightly better adequacy and fluency scores than the two rbmt systems. conclusion and future work we presented a method using the existing rbmt system as a black box to produce synthetic bilin gual corpus, which was used as training data for the smt system. we used the existing rbmt system to translate the monolingual corpus into a syn thetic bilingual corpus. with the synthetic bilingual corpus, we could build an smt system even if there is no real bilingual corpus. in our experi ments using bleu as the metric, such a system achieves a relative improvement of 11.7% over the best rbmt system that is used to produce the syn thetic bilingual corpora. it indicates that using the existing rbmt systems to produce a synthetic bi lingual corpus, we can build an smt system that outperforms the existing rbmt systems. we also interpolated the model trained on a real bilingual corpus and the models trained on the syn thetic bilingual corpora, the interpolated model achieves an absolute improvement of 0.0245 bleu score (13.1% relative) as compared with the individual model trained on the real bilingual cor 293 pus. it indicates that we can build a better smt system by leveraging the real and the synthetic bi lingual corpus. further result analysis shows that after adding the synthetic corpus produced by the rbmt systems, the interpolated model outperforms the stan dard models mainly because of two reasons: (1) some new phrase pairs are added to the interpo lated model; (2) the probability distribution of the phrase pairs is changed. in the future work, we will investigate the possi bility of training a reverse smt system with the rbmt systems. for example, we will investigate to train chinese-to-english smt system based on natural english and rbmt-generated synthetic chinese.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-1061": {
        "abstract": "unsupervised part-of-speech tagging with bilingual graph-based projections abstract we describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. we use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (berg- kirkpatrick et al., 2010). across eight european languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden markov models induced with the expectation maximization algorithm.",
        "conclusion": "conclusion we have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. our results suggest that it is possible to learn accurate pos taggers for languages which do not have any annotated data, but have translations into a resource-rich language. our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised pos tagging models.",
        "citations": [],
        "summary": [
            "In this paper the author aims at describing a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labelled training data, but have translated text in a resource-rich language. They use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model. Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. Unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for training models. To bridge this gap, the author considers a practically motivated scenario, in which he wants to leverage existing resources from a resource-rich language when building tools for resource-poor foreign languages. Their final average POS tagging accuracy of 83.4% compares very favourably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).Their results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language. Their results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models. Das and Petrov, in this paper, approached inducing unsupervised part-of-speech taggers for languages that had no labeled training data, but had translated text in a resource-rich language. Their method did not assume any knowledge about the target language, making it applicable to a wide array of resource-poor languages. They used graph-based label propagation for cross-lingual knowledge transfer and used the projected labels as features in an unsupervised model. Across eight European languages, their approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm. They showed the efficacy of graph-based label propagation for projecting part-of-speech information across languages. Their results suggested that it was possible to learn accurate POS taggers for languages which did not have any annotated data, but have translations into a resource-rich language. It outperformed strong unsupervised baselines as well as approaches that relied on direct projections, and bridged the gap between purely supervised and unsupervised POS tagging models. Dipanjan Das and Slav Petrov in their paper 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections' propose a new approach for developing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.They use a novel graph-based framework for projecting syntactic information across language boundaries.They construct a bilingual graph\nover word types to establish a connection between the two languages, and then use graph label propagation to project syntactic information from\nEnglish to the foreign language. They treat the projected labels as features in an unsupervised model, rather than using them directly for supervised training. To make the projection practical,they used the twelve universal part-of-speech tags.The focus of this work is on building POS taggers for foreign languages, including an English POS tagger and some parallel text between the two languages.Their results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised\nPOS tagging models.\n"
        ]
    },
    "W06-2932": {
        "abstract": "multilingual dependency analysis with a two-stage discriminative parser abstract present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages. the first stage based on the unlabeled dependency parsing models described by mcdonald and pereira (2006) augmented with morphological features for a subset of the languages. the second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. we report results on the conll-x shared task (buchholz et al., 2006) data sets and present an error analysis.",
        "conclusion": "conclusions we have presented results showing that the spanning tree dependency parsing framework of mcdonald et al. (mcdonald et al., 2005b; mcdonald and pereira, 2006) generalizes well to languages other than english. in the future we plan to extend these models in two ways. first, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling. it is our hypothesis that for languages with fine-grained label sets, joint parsing and labeling will improve performance. second, we plan on integrating any available morphological features in a more principled manner. the current system simply includes all morphological bi-gram features. it is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.",
        "citations": [],
        "summary": [
            "In this paper the author aims at presenting a two-stage multilingual dependency parser and evaluates it on 13 diverse languages. With the availability of resources such as the Penn WSJ Treebank, much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure. However, recently there has been a revived interest in parsing models that produce dependency graph representations of sentences, which model words and their arguments through directed edges. Dependency graphs also encode much of the deep syntactic information needed for further processing. In the future, the author plans to extend these models in two ways. First, they plan on examining the performance difference between two-staged dependency parsing and joint parsing plus labelling. Second, they plan on integrating any available morphological features in a more principled manner. The current system simply includes all morphological bi-gram features. It is the hope that a better morphological feature set will help with both unlabeled parsing and labelling for highly inflected languages. \tThis paper talks about Multilingual Dependency Analysis with a Two-Stage Discriminative Parser. Here we present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages. The first stage of our system creates an unlabeled parse y for an input sentence x. The second stage takes the output parse y for sentence x and classifies each edge (i, j) E y with a particular label l (i,j). Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph. The current system simply includes all morphological bi-gram features. It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages. McDonald et all presented a two-stage multilingual pendency parser and evaluate it on 13 diverse languages. The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira was augmented with morphological features for a subset of the languages. The second stage took the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. They reported results on the CoNLL-X shared task data sets and presented an error analysis and presented results showing that the spanning tree dependency parsing framework of theirs generalized well to languages other than English."
        ]
    },
    "D12-1129": {
        "abstract": "a new minimally-supervised framework for domain word sense disambiguation abstract we present a new minimally-supervised framework for performing domain-driven word sense disambiguation (wsd). glossaries for several domains are iteratively acquired from the web by means of a bootstrapping technique. the acquired glosses are then used as the sense inventory for fullyunsupervised domain wsd. our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.",
        "conclusion": "conclusion we have here presented a new framework for domain word sense disambiguation. we depart from the use of general-purpose sense inventories like wordnet and propose a bootstrapping approach to the acquisition of sense inventories for virtually any domain. while we selected 30 domains for this study, nothing would prevent us from using a smaller or larger set of these domains, or a set of completely different domains. our work provides three main contributions: i) we propose a new, flexible approach to glossary bootstrapping which harvests hundreds of thousands of term/gloss pairs; the resulting multidomain glossary is shown to have wide coverage across domains and to include a large amount of terms not available in wordnet; ii) we propose a novel framework for fullyunsupervised domain wsd which uses the multi-domain glossary as our sense inventory; iii) we show that high performance can be achieved by means of simple, unsupervised wsd algorithms (around 80% and 69% in a coarse- and fine-grained setting, respectively). note that our aim here has not been to determine which system performs best, but rather to show that a reliable, full-fledged framework for domain wsd can be set up with minimal supervision. additionally, our framework can be applied to any language of interest, provided enough glossaries are available online, by simply translating the keywords used for our queries. the multi-domain glossary (and sense inventory) together with the seeds used for bootstrapping are available from http://lcl.uniroma1. it/dwsd.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D11-1039": {
        "abstract": "bootstrapping semantic parsers from conversations abstract conversations provide rich opportunities for interactive, continuous learning. when something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals. in this paper, we present an approach for using conversational interactions of this type to induce semantic parsers. we demonstrate learning without any explicit annotation of the meanings of user utterances. instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation. this loss drives the overall learning approach, which induces a weighted ccg grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system. experiments on darpa communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations.",
        "conclusion": "1 discussion we presented a loss-driven learning approach that induces the lexicon and parameters of a ccg parser for mapping sentences to logical forms. the loss was defined over the conversational context, without requiring annotation of user utterances meaning. the overall approach assumes that, in aggregate, the conversations contain sufficient signal (remediations such as clarification, etc.) to learn effectively. in this paper, we satisfied this requirement by using logs from automated systems that deployed reasonably effective recovery strategies. an important area for future work is to consider how this learning can be best integrated into a complete dialog system. this would include designing remediation strategies that allow for the most effective learning and considering how similar techniques could be used simultaneously for other dialog subproblems.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-1060": {
        "abstract": "learning dependency-based compositional semantics abstract compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. in this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. in tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. on two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.",
        "conclusion": "",
        "citations": [],
        "summary": [
            "The paper 'Learning Dependency-Based Compositional Semantics' by Percy Liang,Michael I. Jordan and Dan Klein propose a new way to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive. The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees,which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.The logical forms in DCS are called DCS trees,where nodes are labeled with predicates, and edges are labeled with relations.DCS trees are learned from Question-Answer pairs and database entries.They trained their model using an EM-like algorithm on two benchmarks, GEO and JOBS. Their system outperforms all existing systems despite using no annotated logical forms.\n In this paper, the author aims to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, he introduces a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner. CCG is one instantiation, which is used by many semantic parsers. The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive. The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient. Free from the burden of annotating logical forms, the author hopes to use their techniques in developing even more accurate and broader-coverage language understanding systems.  In this paper, the authors mapped questions to answers via latent logical forms, which are induced automatically from question-answer pairs. They introduced a new semantic representation which highlighted a parallel between dependency syntax and efficient evaluation of logical forms. On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms. They built a system that interpreted natural language borders which specified only that border can be used, but utterances much more accurately than existing not how. The combination rules are encoded in the systems, despite using no annotated logical forms. Their features were soft preferences. This yielded a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offered a simple and expressive alternative to lambda calculus search through and parametrize using features. Free from the burden, it also allowed them to easily add new lexical triggers of annotating logical forms."
        ]
    },
    "W07-1712": {
        "abstract": "named entity recognition for ukrainian: a resource-light approach abstract kruislaan 419, 1098va the katrenko@science.uva.nl pieter hcsl, university of kruislaan 419, 1098va the pitera@science.uva.nl abstract named entity recognition (ner) is a subtask of information extraction (ie) which can be used further on for different purposes. in this paper, we discuss named entity recognition for ukrainian language, which is a slavonic language with a rich morphology. the approach we follow uses a restricted number of features. we show that it is feasible to boost performance by considering several heuristics and patterns acquired from the web data.",
        "conclusion": "conclusions and future work in this paper, we focused on standard features used for the named entity recognition on the newswire data which have been used on many languages. to improve the results that we get by employing orthographic and contextual features, we add patterns extracted from the web and use a similarity measure to find the named entities similar to the nes in the training set. the results we received are, in general, lower than the performance of ner systems in other languages but higher than both baselines. the former might be explained by the size of the corpus we use and by the characteristics of the language. as ukrainian language is a language with a rich morphology, there are several directions we would like to explore in the future. from the language-oriented perspective, it would be useful to determine to which extent stemming and morphological analysis would boost performance. the other problem which we have not considered up to now is the ambiguity of some named entities. for example, a word \u2019ukraine\u2019 can belong to the category loc as well as to the category org (as it is a part of a complex named entity). in addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (collins and singer, 1999).",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D10-1025": {
        "abstract": "translingual document representations from discriminative projections abstract representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization. we use discriminative training to create a projection of documents from multiple languages into a single translingual vector space. we explore two variants to create these projections: oriented principal component analysis (opca) and coupled probabilistic latent semantic analysis (cplsa). both of these variants start with a basic model of documents (pca and plsa). each model is then made discriminative by encouraging comparable document pairs to have similar vector representations. we evaluate these algorithms on two tasks: parallel document retrieval for wikipedia and europarl documents, and cross-lingual text classification on reuters. the two discriminative variants, opca and cplsa, significantly outperform their corresponding baselines. the largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel. the opca method is shown to perform best.",
        "conclusion": "conclusions this paper presents two different methods for creating discriminative projections: opca and cplsa. both of these methods avoid the use of artificial concatenated documents. instead, they model documents in multiple languages, with the constraint that comparable documents should map to similar locations in the projected space. when compared to other techniques, opca had the highest accuracy while still having a run-time that allowed scaling to large data sets. we therefore recommend the use of opca as a pre-processing step for large-scale comparable document retrieval or cross-language text categorization.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1066": {
        "abstract": "treebank annotation schemes and parser evaluation for german abstract recent studies focussed on the question whether less-configurational languages like german are harder to parse than english, or whether the lower parsing scores are an artefact of treebank encoding schemes and data structures, as claimed by ku?bler et al(2006). this claim is based on the assumption that parseval metrics fully re flect parse quality across treebank encodingschemes. in this paper we present new ex periments to test this claim. we use theparseval metric, the leaf-ancestor metric as well as a dependency-based evaluation, and present novel approaches measur ing the effect of controlled error insertion on treebank trees and parser output. wealso provide extensive past-parsing crosstreebank conversion. the results of the ex periments show that, contrary to ku?bler etal. (2006), the question whether or not ger man is harder to parse than english remains undecided.",
        "conclusion": "conclusions in this paper we presented novel experiments assess ing the validity of parsing results measured along different dimensions: the tree-based parseval metric, the string-based leaf-ancestor metric anda dependency-based evaluation. by inserting con trolled errors into gold treebank trees and measuring the effects on parser evaluation results we gave new evidence for the downsides of parseval which,despite severe criticism, is still the standard measure for parser evaluation. we showed that par seval cannot be used to compare the output ofpcfg parsers trained on different treebank anno tation schemes, because the results correlate withthe ratio of non-terminal/terminal nodes. compar ing two different annotation schemes, parsevalconsistently favours the one with the higher node ra tio. we examined the influence of treebank annotationschemes on unlexicalised pcfg parsing, and re jected the claim that the german tu?ba-d/z treebankis more appropriate for pcfg parsing than the ger man tiger treebank and showed that converting the tu?ba-d/z trained parser output to a tiger-like format leads to parseval results slightly worse than the ones for the tiger treebank trained parser.additional evidence comes from a dependency based evaluation, showing that, for the output of the parser trained on the tiger treebank, the mapping from the cfg trees to dependency relations yields better results than for the grammar trained on thetu?ba-d/z annotation scheme, even though parse val scores suggest that the tiger-based parseroutput trees are substantial worse than tu?ba-d/z based parser output trees. we have shown that different treebank annotation schemes have a strong impact on parsing results forsimilar input data with similar (simulated) parser er rors. therefore the question whether a particular language is harder to parse than another language or not, can not be answered by comparing parsingresults for parsers trained on treebanks with different annotation schemes. comparing parseval based parsing results for a parser trained on the tu?ba-d/z or tiger to results achieved by a parser trained on the english penn-ii treebank (marcus et al, 1994) does not provide conclusive evidenceabout the parsability of a particular language, be cause the results show a bias introduced by thecombined effect of annotation scheme and evalua tion metric. this means that the question whether german is harder to parse than english, is still undecided. a possible way forward is perhaps a dependency-based evaluation of tiger/tu?ba-d/z with penn-ii trained grammars for ?similar? test andtraining sets and cross-treebank and -language con trolled error insertion experiments. even this is not entirely straightforward as it is not completely clear what constitutes ?similar? test/training sets across languages. we will attempt to pursue this in further research. 638 acknowledgements we would like to thank the anomymous reviewers for many helpful comments. this research has been supported by a science foundation ireland grant 04|in|i527.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W06-3119": {
        "abstract": "syntax augmented machine translation via chart parsing abstract we present translation results on the shared task \u201dexploiting parallel texts for statistical machine translation\u201d generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. we use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. we present results on the french-to-english task for this workshop, representing significant improvements over the workshop\u2019s baseline system. our translation system is available open-source under the gnu general",
        "conclusion": "conclusions in this work we applied syntax based resources (the target language parser) to annotate and generalize phrase translation tables extracted via existing phrase extraction techniques. our work reaffirms the feasibility of parsing approaches to machine translation in a large data setting, and illustrates the impact of adding syntactic categories to drive and constrain the structured search space. while no improvements were available at submission time, our subsequent performance highlights the importance of tight integration of n-gram language modeling within the syntax driven parsing environment. our translation system is available opensource under the gnu general public license at: www.cs.cmu.edu/\u02dczollmann/samt",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D07-1119": {
        "abstract": "multilingual dependency parsing and domain adaptation using desr abstract we describe our experiments using the desr parser in the multilingual and do main adaptation tracks of the conll 2007 shared task. desr implements an incre mental deterministic shift/reduce parsing algorithm, using specific rules to handle non-projective dependencies. for the multi lingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language. for the domain adaptation track we applied a tree revision method which learns how to correct the mistakes made by the base parser on the adaptation domain.",
        "conclusion": "conclusions for performing multilingual parsing in the conll 2007 shared task we employed desr, a classifier based shift/reduce parser. we used a second order averaged perceptron as classifier and achieved accuracy scores quite above the average in all lan guages. for proper comparison with other approaches, one should take into account that the parser is incremental and deterministic; hence it is typically faster than other non linear algorithms. for the adaptation track we used a novel ap proach, based on the technique of tree revision, applied to a parser trained on a corpus combining sentences from both the training and the adaptation domain. the technique achieved quite promising results and it also offers the interesting possibility of being iterated, allowing the parser to incorporate language knowledge from additional domains. since the technique is applicable to any parser, we plan to test it also with more accurate english parsers.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C02-1154": {
        "abstract": "unsupervised learning of generalized names abstract we present an algorithm, nomen, for learning generalized names in text. examples of these are names of diseases and infectious agents, such as bacteria and viruses. these names exhibitcertain properties that make their identi ca tion more complex than that of regular propernames. nomen uses a novel form of bootstrap ping to grow sets of textual instances and of their contextual patterns. the algorithm makes use of competing evidence to boost the learning of several categories of names simultaneously. we present results of the algorithm on a large corpus. we also investigate the relative merits of several evaluation strategies.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-0131": {
        "abstract": "structured composition of semantic vectors abstract distributed models of semantics assume that word meanings can be discovered from \u201cthe company they keep.\u201d many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a docu- in contrast, this paper proposes a semantic framework, in which semantic vectors are defined and composed in syntactic context. as such, syntax and semantics are fully interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse. evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.",
        "conclusion": "conclusion this paper has introduced a structured vectorial semantic (svs) framework in which vector composition and syntactic parsing are a single, interactive process. the framework thus fully integrates distributional semantics with traditional syntactic models of language. two standard parsing techniques were defined within svs and evaluated: headword-lexicalization svs (bilexical parsing) and relational-clustering svs (latent annotations). it was found that relationallyclustered svs outperformed the simpler lexicalized model and syntax-only models, and that additional clusters had a mildly positive effect. additionally, perplexity results showed that the integration of distributed semantics in relationally-clustered svs improved the model over a non-interactive baseline. it is hoped that this flexible framework will enable new generations of interactive interpretation models that deal with the syntax\u2013semantics interface in a plausible manner.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "A97-1014": {
        "abstract": "an annotation scheme for free word order languages abstract we describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. the resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- \u2022lar representational strata.",
        "conclusion": "conclusion as the annotation scheme described in this paper focusses on annotating argument structure rather than constituent trees, it differs from existing treebanks in several aspects. these differences can be illustrated by a comparison with the penn treebank annotation scheme. the following features of our formalism are then of particular importance: the current tagset comprises only 16 node labels and 34 function tags, yet a. finely grinned classification will take place in the near future. we have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order. in general, the resulting interpreted data also are closer to semantic annotation and more neutral with respect to particular syntactic theories. as modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data., interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research. in addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts. syntactically annotated corpora of german have been missing until now. in the second phase of the project verbmobil a. treebank for :30,000 german spoken sentences as well as for the same amount of english and japanese sentences will be created. we will closely coordinate the further development of our corpus with the annotation work in verbmobil and with other german efforts in corpus annotation. since the combinatorics of syntactic constructions creates a. demand for very large corpora., efficiency of annotation is an important. criterion for the success of the developed methodology and tools. our annotation tool supplies efficient manipulation and immediate visualization of argument structures. partial automation included in the current version significantly reduces the manna.1 effort. its extension is subject to further investigations.",
        "citations": [],
        "summary": [
            "This paper talks about an annotation scheme for free word order languages. The main key words annotated in this paper are tree bank, corpus, and free word order. It aims at providing syntactically annotated corpora ('tree banks') for stochastic grammar induction. The requirements for such formalism differ from those posited for configurational languages; several features have been added, influencing the architecture of the scheme. In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and refinements. This paper focuses on annotating argument structure rather than constituent trees; it differs from existing tree banks in several aspects. These differences are illustrated by a comparison with the Penn Treebank annotation scheme. Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.  The author aims at describing an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. It also aims at providing syntactically annotated corpora ('treebanks') for stochastic grammar induction. In particular, they focus on several methodological issues concerning the annotation of non-configurational languages. They will closely coordinate the further development of the corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation. Since the combinatorics of syntactic constructions creates a demand for very large corpora, efficiency of annotation is an important criterion for the success of the developed methodology and tools. Their annotation tool supplies efficient manipulation and immediate visualization of argument structures. Partial automation included it, the current version significantly reduces the manual effort. Its extension is subject to further investigations. In this paper, Skut et all described an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. The annotation scheme described in this paper focused on annotating argument structure rather than constituent trees. It differed from existing treebanks in several aspects which could be illustrated by a comparison with the Penn Treebank annotation scheme. They argued that their selected approach was better suited for producing high quality interpreted corpora in languages exhibiting free constituent order and provided empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linearizations etc. Their annotation tool supplied efficient manipulation and immediate visualization of argument structures. Partial automation included in the current version significantly reduced the manna.1 effort. Its extension was subject to further investigations according to them."
        ]
    },
    "W10-1403": {
        "abstract": "two methods to incorporate &rsquo;local morphosyntactic&rsquo; features in hindi dependency parsing abstract in this paper we explore two strategies to incorporate local morphosyntactic features in hindi dependency parsing. these features are obtained using a shallow parser. we first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in hindi dependency parsing. we then investigate the best way to incorporate this information during dependency parsing. further, we compare the results of various experiments based on various criterions and do some error analysis. all the experiments were done with two data-driven parsers, maltparser and mstparser, on a part of multi-layered and multi-representational hindi treebank which is under development. this paper is also the first attempt at complete sentence level parsing for hindi.",
        "conclusion": "discussion and future work we systematically explored the effect of various linguistic features in hindi dependency parsing. results show that pos, case, suffix, root, along with local morphosyntactic features help dependency parsing. we then described 2 methods to incorporate such features during the parsing process. these methods can be thought as different paradigms of modularity. for practical reasons (i.e. given the pos tagger/chunker accuracies), it is wiser to use this information as features rather than dividing the task into two stages. as mentioned earlier, this is the first attempt at complete sentence level parsing for hindi. so, we cannot compare our results with previous attempts at hindi dependency parsing, due to, (a) the data used here is different and (b) we produce complete sentence parses rather than chunk level parses. as mentioned in section 5.1, accuracies of intrachunk dependencies are very high compared to inter-chunk dependencies. inter-chunk dependencies are syntacto-semantic in nature. the parser depends on surface syntactic cues to identify such relations. but syntactic information alone is always not sufficient, either due to unavailability or due to ambiguity. in such cases, providing some semantic information can help in improving the inter-chunk dependency accuracy. there have been attempts at using minimal semantic information in dependency parsing for hindi (bharati et al., 2008). recently, ambati et al. (2009b) used six semantic features namely, human, non-human, in-animate, time, place, and abstract for hindi dependency parsing. using gold-standard semantic features, they showed considerable improvement in the core inter-chunk dependency accuracy. some attempts at using clause information in dependency parsing for hindi (gadde et al., 2010) have also been made. these attempts were at inter-chunk dependency parsing using gold-standard pos tags and chunks. we plan to see their effect in complete sentence parsing using automatic shallow parser information also. conclusion in this paper we explored two strategies to incorporate local morphosyntactic features in hindi dependency parsing. these features were obtained using a shallow parser. we first explored which information provided by the shallow parser is useful and showed that local morphosyntactic features in the form of chunk type, head/non-head info, chunk boundary info, distance to the end of the chunk and suffix concatenation are very crucial for hindi dependency parsing. we then investigated the best way to incorporate this information during dependency parsing. further, we compared the results of various experiments based on various criterions and did some error analysis. this paper was also the first attempt at complete sentence level parsing for hindi.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W06-2207": {
        "abstract": "a hybrid approach for the acquisition of information extraction patterns abstract in this paper we present a hybrid approach for the acquisition of syntacticosemantic patterns from raw text. our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an expectation maximization clustering algorithm that uses the text words as attributes. we show that the combination of the two methods always outperforms the decision list learner alone. furthermore, using a modular architecture we investigate several algorithms for pattern ranking, the most important component of the decision list learner.",
        "conclusion": "conclusions this paper introduces a hybrid, lightly-supervised method for the acquisition of syntactico-semantic patterns for information extraction. our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an expectation maximization clustering algorithm that uses the text words as attributes. furthermore, we customize the decision list learner with up to four criteria for pattern selection, which is the most important component of the acquisition algorithm. for the evaluation of the proposed approach we have used both an indirect evaluation based on text categorization and a direct evaluation where human experts evaluated the quality of the generated patterns. our results indicate that co-training the expectation maximization algorithm with the decision list learner tailored to acquire only high precision patterns is by far the best solution. for the same recall point, the proposed method increases the precision of the generated models up to 35% from the previous state of the art. furthermore, the combination of the two feature spaces (words and patterns) also increases the coverage of the acquired patterns. the direct evaluation of the acquired patterns by the human experts validates these results.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W09-0402": {
        "abstract": "syntax-oriented evaluation measures for machine translation output abstract we explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the the on the detailed tags as well as the precision, recall and f-measure obtained we also introduced fbased on both word and grams.",
        "conclusion": "conclusions the results presented in this article suggest that the syntactic information has the potential to strenghten automatic evaluation metrics, and there are many possible directions for future work. we proposed several syntax-oriented evaluation metrics based on the detailed pos tags: the posbleu score and pos-n-gram precision, recall and f-measure, i.e. the posp, posr, and posf score. in addition, we introduced a measure which takes into account both pos tags and words: the wpf score. we carried out an extensive analysis of the spearman\u2019s rank correlation coefficients between the syntactic evaluation metrics and the human judgments. the obtained results showed that the new metrics correlate well with human judgments, namely the adequacy and fluency scores, as well as the sentence ranking. the results also showed that the syntax-oriented metrics are competitive with the widely used evaluation measures bleu, meteor and ter. especially promising are the posbleu and the posf score. the correlations of the wpf score are slightly lower than those of the purely pos based metrics \u2013 however, this metric has advantage of taking both syntactic and lexical aspect into account.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "C04-1180": {
        "abstract": "wide-coverage semantic representations from a ccg parser abstract this paper shows how to construct semantic representations from the derivations producedby a wide-coverage ccg parser. unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation. we demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen wsj text.we believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of nlp.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D12-1133": {
        "abstract": "a transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing abstract most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. we present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. experimental evaluation on chinese, czech, english and german shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.",
        "conclusion": "conclusion we have presented the first system for joint partof-speech tagging and labeled dependency parsing with non-projective dependency trees. evaluation on four languages shows consistent improvements in both tagging and parsing accuracy over a pipeline system with state-of-the-art results across the board. the error analysis reveals improvements in tagging accuracy for syntactically central categories, mainly verbs, with improvement in syntactic accuracy for core grammatical functions as a result. in future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P14-1126": {
        "abstract": "unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization abstract we present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resourcerich language. we train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization. our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages. we perform experiments on three data sets \u2014 version 1.0 and version 2.0 of google universal dependency treebanks and treebanks from conll shared-tasks, across ten languages. we obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems.",
        "conclusion": "conclusion in this paper, we propose an unsupervised projective dependency parsing approach for resourcepoor languages, using existing resources from a resource-rich source language. by presenting a model training framework, our approach can utilize parallel text to estimate transferring distribution with the help of a well-developed resourcerich language dependency parser, and use unlabeled data as entropy regularization. the experimental results on three data sets across ten target languages show that our approach achieves significant improvement over previous studies.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P12-3012": {
        "abstract": "multilingual wsd with just a few lines of code: the babelnet api abstract in this paper we present an api for programmatic access to babelnet \u2013 a wide-coverage multilingual lexical knowledge base \u2013 and multilingual knowledge-rich word sense disambiguation (wsd). our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction.",
        "conclusion": "",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-2205": {
        "abstract": "evaluating unsupervised learning for natural language processing tasks abstract the development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research. the primary advantage of these methods is that they do not require annotated data to learn a model. however, this advantage makes them difficult to evaluate against a manually labeled gold standard. using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods. inwe argue that the rarely used evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied. finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing.",
        "conclusion": "conclusions in this position paper, we discussed the issue of evaluation of unsupervised learning methods for nlp tasks. using pos tagging as our case study, we examined recent attempts of evaluating unsupervised approaches and showed that a lot of confusion is caused due to evaluating their output against a labeled gold standard. instead, we argue that it is more appropriate to evaluate unsupervised methods in context, either as a pre-processing step for a downstream task or as a tool for data exploration. following this, we proposed that future work should focus on adapting to and evaluating unsupervised learning methods in the context in which they are intended to be used and that a shared task would facilitate research in this direction. finally, we hope that the adoption of in-context evaluation will result in the development of improved unsupervised learning methods for nlp tasks, so that researchers and practitioners can exploit the large amounts of textual data available.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W12-3154": {
        "abstract": "analysing the effect of out-of-domain data on smt systems abstract in statistical machine translation (smt), it is known that performance declines when the training data is in a different domain from the test data. nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data. in this paper, we first try to relate the effect of the outof-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the outof-domain data at different parts of the training pipeline (alignment, phrase extraction, and phrase scoring). through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words.",
        "conclusion": "conclusions in this paper we have attempted to give an indepth analysis of the domain adaptation problem for two different domain adaptation problems in phrasebased mt. the differences between the two problems are clearly illustrated by the results in figures 2 and 3, where we see that the difference between the in-domain and out-of-domain data are larger for the opensubtitles domain than for the news-commentary domain. this can be detected by the differences in word distribution and out-ofvocabulary rates observed in figure 2, and is reflected by the differing translation results in figure 3. however, the experiments of sections 3.4 and 3.5 show some common themes emerging in the two domains. in both cases, the out-of-domain data helps most when it is just allowed to add entries (i.e. \u201cfill in\u201d) the phrase-table, and using the scores provided by out-of-domain data has a tendency to be harmful to translation quality. the precision results of section 3.5 show out-of-domain data (when it is simply added to the training set) mainly helping with the low frequency words, and having a neutral or harmful effect for higher frequency words. this explains why approaches which try to weight the outof-domain data in some way (e.g. corpus weighting or instance weighting) can be more successful than simply concatenating data sets. it also suggests that the way forward is to look for methods that use the out-of-domain data mainly for rarer words, and not to change translations which have a lot of evidence in the in-domain data.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "P11-1144": {
        "abstract": "semi-supervised frame-semantic parsing for unknown predicates abstract we describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. we construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. the label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement full frame-semantic parsing on a blind test set, over a state-of-the-art supervised baseline.",
        "conclusion": "conclusion we have presented a semi-supervised strategy to improve the coverage of a frame-semantic parsing model. we showed that graph-based label propagation and resulting smoothed frame distributions over unseen targets significantly improved the coverage of a state-of-the-art semantic frame disambiguation model to previously unseen predicates, also improving the quality of full framesemantic parses. the improved parser is available at http://www.ark.cs.cmu.edu/semafor.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W09-1116": {
        "abstract": "glen glenda or glendale: unsupervised and semi-supervised learning of english noun gender abstract pronouns like reflect the gender and number of the entities to which they refer. pronoun resolution systems can use this fact to filter noun candidates that do not agree with the pronoun gender. indeed, broad-coverage models of noun gender have proved to be the most important source of world knowledge in automatic pronoun resolution systems. previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class. while this provides useful statistics for frequent nouns, many infrequent nouns cannot be classified using this method. rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model. our model collectively classifies all occurrences of a noun in a document using a wide variety of contextual, morphological, and categorical gender features. by leveraging large volumes of unlabeled data, our full semi-supervised system reduces error by 50% over the existing stateof-the-art in gender classification.",
        "conclusion": "conclusion we have shown how noun-pronoun co-occurrence counts can be used to automatically annotate the gender of millions of nouns in unlabeled text. training from these examples produced a classifier that clearly exceeds the state-of-the-art in gender classification. we incorporated thousands of useful but previously unexplored indicators of noun gender as features in our classifier. by combining the predictions of this classifier with the original gender counts, we were able to produce a gender predictor that achieves 95.5% classification accuracy on 2596 test nouns, a 50% reduction in error over the current state-of-the-art. a further name-matching post-processor reduced error even further, resulting in 96.7% accuracy on the test data. our final system is the broadest and most accurate gender model yet created, and should be of value to many pronoun and coreference resolution systems.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W11-1002": {
        "abstract": "structured vs. flat semantic role representations for machine translation evaluation abstract we argue that failing to capture the degree of contribution of each semantic frame in a sentence explains puzzling results in recent work on the meant family of semantic mt evaluation metrics, which have disturbingly indicated that dissociating semantic roles and fillers from their predicates actually improves correlation with human adequacy judgments even though, intuitively, properly segregating event frames should more accurately reflect the preservation of meaning. our analysis finds that both properly structured and flattened representations fail to adequately account for the contribution of each semantic frame to the overall sentence. we then show that the correlation of hmeant, the human variant of meant, can be greatly improved by introducing a simple length-based weighting scheme that approximates the degree of contribution of each semantic frame to the overall sentence. the new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame\u2019s contribution gives hmeant higher correlations than the previously bestperforming flattened model, as well as hter.",
        "conclusion": "conclusion",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "W06-2905": {
        "abstract": "what are the productive units of natural language grammar? a dop approach to the automatic identification of constructions abstract we explore a novel computational approach to identifying \u201cconstructions\u201d or \u201cmulti-word expressions\u201d (mwes) in an annotated corpus. in this mwes have no special status, but emerge in a general procedure for finding the best statistical grammar to describe the training corpus. the statistical grammar formalism used is that of stochastic tree substitution grammars (stsgs), such as used in data-oriented parsing. we present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an stsg, and a method for estimating the parameters of an stsg given observed frequencies in a tree bank. we report quantitative results on the atis corpus of phrase-structure annotated sentences, and give examples of the mwes extracted from this corpus.",
        "conclusion": "discussion calculating e[f(t)] using equation (8) can be extremely expensive in computational terms. one will typically want to calculate this value for all subtrees, the number of which is exponential in the size of the trees in the training data. for each subtree t, we will need to consider the set of all its derivations (exponential in the size of t), and for each derivation the set of supertwigs of the first elementary trees and, for incompletely lexicalized subtrees, the set of superprunes of all elementary trees in their derivations. the latter two sets, however, need not be constructed for every time the expected frequency e[f(t)] is calculated. instead, we can, as we do in the current implementation, keep track of the two sums for every change of the weights. however, there are many further possibilities for improving the efficiency of the algorithm that are currently not implemented. equation (8) remains valid under various restrictions on the elementary trees that we are willing to consider as productive units. some of these will remove the exponential dependence on the size of the trees in the training data. for instance, in the case where we restrict the productive units (with nonzero weights) to depth-1 trees (i.e. cfg rules), equation (8) collapses to the product of inside and outside probabilities, which can be calculated using dynamical programming in polynomial time (lari and young, 1990). a major topic for future research is to define linguistically motivated restrictions that allow for efficient computation. another concern is the size of the grammar the estimation procedure produces, and hence the time and space efficiency of the resulting parser. table 1 already showed that push-n-pull leads to a more concise grammar. the reason is that many potential elementary trees receive a score (and weight) 0. more generally, push-n-pull generates extremely tilted score distributions, which allows for even more compact but highly accurate approximations. in table 2 we show, for the d = 4 grammar of figure 1, that a 10-fold reduction of the grammar size by pruning elementary trees with low scores, leads only to a small decrease in the lp and lr measures. another interesting question is if and how the current algorithm can be extended to the full class of stochastic tree-adjoining grammars (schabes, 1992; resnik, 1992). with the added operation of adjunction, equation (8) is not valid anymore. given the computational complexities that it already gives rise to, however, it seems that issue of linguistically motivated restrictions (other than lexicalization) should be considered first. finally, given that the current approach is dependent on the availability of a large annotated corpus, an important question is if and how it can be extended to work with unlabeled data. that is, can we transform the push-npull algorithm to perform the unsupervised learning of stsgs? although most work on unsupervised grammar learning concerns scfgs (including some of our own (zuidema, 2003)) it is interesting to note that much of the evidence for construction grammar in fact comes from the language acquisition literature (tomasello, 2000). conclusions theoretical linguistics has long strived to account for the unbounded productivity of natural language syntax with as few units and rules of combination as possible. in contrast, construction grammar and related theories of grammar postulate a heterogeneous and redundant storage of \u201cconstructions\u201d. if this view is correct, we expect to see statistical signatures of these constructions in the distributional information that can be derived from corpora of natural language utterances. how can we recover those signatures? in this paper we have presented an approach to identifying the relevant statistical correlations in a corpus based on the assumption that the (a) the \u201cshow me np pp\u201d frame, (b) the complete parse tree (c) the frame for \u201cflights from np to which occurs very frequently in for the sentence \u201cwhich of np\u201d the training data and is repre- these flights\u201d, which occurs sented in several elementary trees 16 times in training data. with high weight. using the push-n-pull algorithm on the atis3 corpus. for use in the current implementation, the parse trees have been converted to chomsky normal form (all occurrences of a \u2192 b, b \u2192 w are replaced by a \u2192 w; all occurrences of a \u2192 bcw are replaced by a \u2192 ba\u2217, a\u2217 \u2192 cw), all non-terminal labels are made unique for a particular parse tree (address labeling not shown) and all top nodes are replaced by the non-terminal \u201ctop\u201d. listed are the elementary trees of the induced stsg with for each tree the score, the weight and the frequency with which it occurs in the training set. corpus is generated by an stsg, and by inferring the properties of that underlying stsg. given our best guess of the stsg that generated the data, we can start to ask questions like: which subtrees are overrepresented in the corpus? which correlations are so strong that it is reasonable to think of the correlated phrases as a single unit? we presented a new algorithm for estimating weights of an stsg from a corpus, and reported promising empirical results on a small corpus.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "D10-1044": {
        "abstract": "instance weighting discriminative instance weighting for domain adaptation in statistical machine translation abstract we describe a new approach to smt adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. this extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. we incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.",
        "conclusion": "conclusion in this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. the features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making map-smoothed relative-frequency estimates (different weights are learned for each conditioning direction). these estimates are in turn combined linearly with relative-frequency estimates from an in-domain phrase table. mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus. we obtained positive results using a very simple phrase-based system in two different adaptation settings: using english/french europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the nist09 training material to improve performance on the news-related corpora. in both cases, the instanceweighting approach improved over a wide range of baselines, giving gains of over 2 bleu points over the best non-adapted baseline, and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting). in future work we plan to try this approach with more competitive smt systems, and to extend instance weighting to other standard smt components such as the lm, lexical phrase weights, and lexicalized distortion. we will also directly compare with a baseline similar to the matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences. finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.",
        "citations": [],
        "summary": [
            "'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation' by George Foster, Cyril Goutte and Roland Kuhn study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material is availalable.They aim to explicitly characterize examples from OUT as belonging to general language or not and to apply instance weighting at the level of phrase pairs.Finally, they make some improvements to baseline approaches. they train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.they used a very simple phrase-based system in two different adaptation settings:The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus as OUT,for English/French translation in both directions.The second setting uses the news-related sub-corpora for the NIST09 MT Chinese to English evaluation 8 as IN, and the remaining NIST parallel Chinese/English corpora (UN, Hong Kong Laws,and Hong Kong Hansard) as OUT.They obtained positive results from the approach.\n Foster et all describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. They extended previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and used simpler training procedure. They incorporated instance-weighting into a mixture-model framework, and found that it yielded consistent improvements over a wide range of baselines.\n\nIn this paper, the authors proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair was characterized by a set of simple features intended to reflect how useful it would be. The features were weighted within a logistic model that gave an overall weight that was applied to the phrase pair and MAP-smoothed relative-frequency estimates which were combined linearly with relative-frequency estimates from an in-domain phrase table. Instance-weighting approach improved over a wide range of baselines, giving gains of over 2 BLEU points over the best non-adapted baseline. They plan on extending instance-weighting to other standard SMT components and \ncapture the degree of generality of phrase pairs.\n In this paper the author describes a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. Domain adaptation is a common concern when optimizing empirical NLP applications. For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components, which precludes a single universal approach to adaptation. The author also studies the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material. First, they aim to explicitly characterize examples from OUT as belonging to general language or not. Their second contribution is to apply instance weighting at the level of phrase pairs. Finally, they make some improvements to baseline approaches. In future work the author plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion. They will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences. Finally, they intend to explore more sophisticated instance weighting features for capturing the degree of generality of phrase pairs."
        ]
    },
    "I08-2105": {
        "abstract": "unsupervised all-words word sense disambiguation with grammatical dependencies abstract we present experiments that analyze the necessity of using a highly interconnectedword/sense graph for unsupervised all words word sense disambiguation. we show that allowing only grammatically related words to influence each other?s senses leads to disambiguation results on a par with thebest graph-based systems, while greatly reducing the computation load. we also com pare two methods for computing selectional preferences between the senses of every two grammatically related words: one using a lesk-based measure on wordnet, the other using dependency relations from the british national corpus. the best configurationuses the syntactically-constrained graph, se lectional preferences computed from thecorpus and a pagerank tie-breaking algo rithm. we especially note good performancewhen disambiguating verbs with grammati cally constrained links.",
        "conclusion": "conclusions we have studied the impact of grammatical in formation for constraining and guiding the word sense disambiguation process in an unsupervised all-words setup. compared with graph methods, the approach we described is computationally lighter, while performing at the same level on senseval-2and senseval-3 all-words tasks test data. grammat ical constraints serve both to limit the number of word-senses pair similarities necessary, and also to estimate selectional preferences from an untagged corpus. using only grammatically motivated connections leads to better disambiguation of verbs for both senseval-2 and senseval-3 test data, but while thedifference is consistent (1.4%, 1.9%) it is not statis tically significant. 6as opposed to other unsupervised approaches, the sense frequency information from wordnet was not used. we explored a new method for estimating sense association strength from a sense-untagged corpus.disambiguation when using sense relatedness com puted from wordnet is very close in performance with disambiguation based on sense association strength computed from the british national corpus,and on a par with state-of-the-art unsupervised systems on senseval-2. this indicates that grammatical relations and automatically derived sense association preference scores from a corpus have high potential for unsupervised all-word sense disambigua tion.",
        "citations": [],
        "summary": [
            ""
        ]
    },
    "N10-1002": {
        "abstract": "chart mining-based lexical acquisition with precision grammars abstract in this paper, we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars. the general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks, and is particularly suited to domain-specific lexical tuning and lexical acquisition using lowcoverage grammars. as an illustration of the functionality of our proposed technique, we develop a lexical acquisition model for english verb particle constructions which operates over unlexicalised features mined from a partial parsing chart. the proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features.",
        "conclusion": "discussion and future work the inventory of features we propose for vpc extraction is just one illustration of how partial parse results can be used in lexical acquisition tasks. the general chart mining technique can easily be adapted to learn other challenging linguistic phenomena, such as the countability of nouns (baldwin and bond, 2003), subcategorization properties of verbs or nouns (korhonen, 2002), and general multiword expression (mwe) extraction (baldwin and kim, 2009). with mwe extraction, e.g., even though some mwes are fixed and have no internal syntactic variability, such as ad hoc, there is a very large proportion of idioms that allow various degrees of internal variability, and with a variable number of elements. for example, the idiom spill the beans allows internal modification (spill mountains of beans), passivisation (the beans were spilled in the latest edition of the report), topicalisation (the beans, the opposition spilled), and so forth (sag et al., 2002). in general, however, the exact degree of variability of an idiom is difficult to predict (riehemann, 2001). the chart mining technique we propose here, which makes use of partial parse results, may facilitate the automatic recognition task of even more flexible idioms, based on the encouraging results for vpcs. the main advantage, though, of chart mining is that parsing with precision grammars does not any longer have to assume complete coverage, as has traditionally been the case. as an immediate consequence, the possibility of applying our chart mining technique to evolving medium-sized grammars makes it especially interesting for lexical acquisition over low-density languages, for instance, where there is a real need for rapid-prototyping of language resources. the chart mining approach we propose in this paper is couched in the bottom-up chart parsing paradigm, based exclusively on passive edges. as future work, we would also like to look into the top-level active edges (those active edges that are never completed), as an indication of failed assumptions. moreover, it would be interesting to investigate the applicability of the technique in other parsing strategies, e.g., head-corner or left-corner parsing. finally, it would also be interesting to investigate whether by using the features we acquire from chart mining enhanced with information on the prevalence of certain patterns, we could achieve performance improvements over broader-coverage treebank parsers such as the charniak parser. conclusion we have proposed a chart mining technique for lexical acquisition based on partial parsing with precision grammars. we applied the proposed method to the task of extracting english verb particle constructions from a prescribed set of corpus instances. our results showed that simple unlexicalised features mined from the chart can be used to effectively extract vpcs, and that the model outperforms a probabilistic baseline and the charniak parser at vpc extraction.",
        "citations": [],
        "summary": [
            ""
        ]
    }
}