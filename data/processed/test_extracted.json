{
    "W11-2139": {
        "input_sentences": [
            "We have presented a summary of the enhancements made to a hierarchical phrase-based translation system for the WMT11 shared translation task.",
            "We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training.",
            "Some of our results are still preliminary (the source parse 10The model used is p(y I x)p(y).",
            "Abstract",
            "While this model is somewhat unusual (the conditional probability is backwards from a noisy channel model), it is a standard and effective technique for case restoration. model), but a number of changes we made were quite simple (OOV handling, using MT output to provide additional references for training) and also led to improved results.",
            "This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11).",
            "Summary",
            "The CMU-ARK German-English Translation System"
        ]
    },
    "D12-1126": {
        "input_sentences": [
            "Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts.",
            "For future works, we plan to improve our approximate tagging algorithm to reduce error propagation.",
            "In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature.",
            "In this paper, we present a method using dynamic features to tag POS of mixed texts.",
            "The underlying problem is how to tag part-of-speech (POS) for the English words involved.",
            "Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, \u201cforeign words\u201d.",
            "Experiments show that our method achieves higher performance than traditional sequence labeling methods.",
            "Conclusion",
            "We would also like to investigate these features in more applications of natural language processing, such as name entity recognition, information extraction, etc.",
            "The experiments demonstrate the effectiveness of our method.",
            "It should be noted that our method is also effective for the mixed texts of Chinese and any foreign languages since we use \u201cUnified Replacement\u201d.",
            "Abstract",
            "Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features",
            "In addition, we will refer to an English dictionary to generate some useful features to distinguish between \u201cNR\u201d and \u201cNN\u201d in Chinese-English mixed texts and add some statistical features derived from English resources, such as the most common tag of each English word.",
            "To overcome the problem of the lack of annotated corpus on mixed texts, our features use both local and non-local information and take advantage of the characteristics of Chinese-English mixed texts.",
            "Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.",
            "Dynamic Features",
            "In this paper, we focus on Chinese-English mixed texts and use dynamic features for POS tagging."
        ]
    },
    "W01-0510": {
        "input_sentences": [
            "Despite the small amount of training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad personal information management task.",
            "The model is automatically trained from a set of sentences annotated with frame/slot labels and spans.",
            "We have presented a data-driven approach to information extraction that, despite the small amount of training data used, is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad personal information management task.",
            "A framework which utilizes the vast amounts of text data collected once such a system is deployed would be desirable.",
            "Conclusions and Future Directions",
            "The big difference in performance between training and test and the fact that we are using so little training data, makes improvements by using more training data very likely, although this may be expensive.",
            "Another possible research direction is to modify the framework such that it finds the most likely semantic parse given the acoustics thus treating the word sequence as a hidden variable.",
            "The task of template filling is cast as constrained parsing using the SLM.",
            "Abstract",
            "The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser.",
            "Training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage.",
            "Statistical modeling techniques that make more effective use of the training data should be used in the SLM, maximum entropy (Berger et al., 1996) being a good candidate.",
            "As for using the SLM as the language understanding component of a speech driven application, such as MiPad, it would be interesting to evaluate the impact of incorporating the semantic constraints on the word-level accuracy of the system.",
            "Information Extraction Using The Structured Language Model",
            "The performance of the baseline model could be improved with more authoring effort, although this is expensive."
        ]
    },
    "W06-2204": {
        "input_sentences": [
            "With more fields in the problem domain there is potentially more information on each of the candidate positions to constrain these decisions.",
            "We have also observed that the number of fields that are being extracted in the given domain affects the performance of our algorithm.",
            "It remains to be seen whether this approach would be effective for information extraction.",
            "We present a semi-supervised learning algorithm for information extraction that can acquire extraction patterns from a small amount of labelled text in conjunction with a large amount of unlabelled text.",
            "Abstract",
            "We are currently extending TPLEX in several directions.",
            "From our experiments we have observed that our algorithm is particularly competitive in scenarios where very little labelled training data is available.",
            "Our experiments with several demonstrate that usually competitive with various fully-supervised algorithms when very little labelled training data is available.",
            "This definition allows TPLEX to perform well with very little training data in domains where other approaches that assume fragment redundancy would fail.",
            "First, the algorithm does not require redundancy in the fragments to be extracted, but only redundancy of the extraction patterns themselves.",
            "We have described TPLEX, a semi-supervised algorithm for learning information extraction patterns.",
            "We are also exploring ideas for semi-supervised learning from the machine learning community.",
            "Second, so far we have focused on a BWI-like pattern language, but we speculate that richer patterns permitting (for example) optional or reordered tokens may well deliver substantial increases in accuracy.",
            "Specifically, probabilistic finite-state methods such hidden Markov models and conditional random fields have been shown to be competitive with more traditional pattern-based approaches to information extraction (Fuchun and McCallum, 2004), and these methods can exploit the Expectation Maximization algorithm to learn from a mixture of labelled and unlabelled data (Lafferty et al., 2004).",
            "TPLEX extracts all fields simultaneously and uses the scores from each of the Figure 3: F1 averaged across all fields, for the Seminar dataset trained on only labeled data and trained on labeled and unlabeled data patterns that extract a given position to determine the most likely field for that position.",
            "Boosting is a highly effective ensemble learning technique, and BWI uses boosting to tune the weights of the learned patterns, so if we generalize boosting to handle unlabelled data, then the learned weights may well be more effective than those calculated by TPLEX.",
            "Conclusions.",
            "Another possibility is to explore semi-supervised extensions to boosting (d\u2019Alch\u00b4e Buc et al., 2002).",
            "We contend that this is a result of our algorithm\u2019s ability to use the unlabelled test data to validate the patterns learned from the training data.",
            "It would be more elegant (and perhaps more effective) to incorporate the filtering heuristics directly into the position scoring mechanism.",
            "Second, most bootstrapping methods identify the highest quality fragments in the unlabelled data and then assume that they are as reliable as manually labelled data in subsequent iterations. contrast, scoring mechanism prevents errors from snowballing by recording the reliability of fragments extracted from unlabelled data.",
            "Compared to previous work, two novel features.",
            "Discussion",
            "Future work.",
            "The requirement for large labelled training corpora is widely recognized as a key bottleneck in the use of learning algorithms for extraction.",
            "First, position filtering is currently performed as a distinct post-processing step.",
            "The key idea is to exploit the following recursive definition: good patterns are those that extract good fragments, and good fragments are those that are extracted by good patterns.",
            "Transductive Pattern Learning For Information Extraction"
        ]
    },
    "D08-1094": {
        "input_sentences": [
            "A Structured Vector Space Model for Word Meaning in Context",
            "We argue that existing models for this task do not take syntactic structure sufficiently into account. present a novel vector space model that addresses these issues by incorporating the selectional preferences for words\u2019 argument positions.",
            "Abstract",
            "We have evaluated the SVS model on two datasets on the task of predicting the felicitousness of paraphrases in given contexts.",
            "This is especially interesting as the Lexical Substitution dataset, in contrast to the M&L data, uses \u201crealistic\u201d paraphrase candidates that are not necessarily maximally distinct.",
            "Conclusion",
            "In addition to a vector representing a word\u2019s lexical meaning, it contains vectors representing the word\u2019s selectional preferences.",
            "This task is a crucial step towards a robust, vector-based compositional account of sentence meaning.",
            "These selectional preferences play a central role in the computation of meaning in context.",
            "Our next step will be to integrate information from multiple relations (such as both the subject and object positions of a verb) into the computation of context-specific meaning.",
            "The most important limitation of the evaluation that we have given in this paper is that we have only considered single words as context.",
            "In this paper, we have considered semantic space models that can account for the meaning of word occurrences in context.",
            "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context.",
            "This makes it possible to integrate syntax into the computation of word meaning in context.",
            "Paraphrase-based inference rules play a large role in several recent approaches to Textual Entailment (e.g.",
            "Szpektor et al (2008)); appropriateness judgments of paraphrases in context, the task of Experiments 1 and 2 above, can be viewed as testing the applicability of these inferences rules.",
            "On the M&L dataset, SVS outperforms the state-of-the-art model of M&L, though the difference is not significant.",
            "In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.",
            "On the Lexical Substitution dataset, SVS significantly outperforms the state-of-the-art.",
            "We will explore the usability of vector space models of word meaning in NLP applications, formulated as the question of how to perform inferences on them in the context of the Textual Entailment task (Dagan et al., 2006).",
            "Arguing that existing models do not sufficiently take syntax into account, we have introduced the new structured vector space (SVS) model of word meaning.",
            "Our eventual aim is a model that can give a compositional account of a word\u2019s meaning in context, where all words in an expression disambiguate one another according to the relations between them."
        ]
    },
    "W09-1210": {
        "input_sentences": [
            "Our system combines and implements efficient parsing techniques to get a high accuracy as well as very good parsing and training time.",
            "Therefore, a possible further accuracy and parsing speed improvement would be to select different features sets for different languages or to leave out some features.",
            "Even if we traded off a lot of the speed improvement by using a more expensive decoder and more attributes to get a higher accuracy.",
            "For the applications of syntactic and semantic parsing, the parsing time and memory footprint are very important.",
            "The semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro F1 score) in the joint task for syntactic and semantic dependency parsing.",
            "We think that also the development of systems can profit from this since one can perform more experiments in the given time.",
            "Conclusion",
            "For some languages, features are not provided or the parser does not profit from using these features.",
            "For instance, the English parser does not profit from the lemmas and the Chinese as well as the Japanese corpus does not have lemmas different from the word forms, etc.",
            "Abstract",
            "For this task, our system has the highest accuracy for English with 89.88, German with 87.48 and the out-of-domain data in average with 78.79.",
            "For the subtask of syntactic dependency parsing, we could reach the second place with an accuracy in average of 85.68 which is only 0.09 points behind the first ranked system.",
            "We provided a fast implementation with good parsing time and memory footprint.",
            "Efficient Parsing of Syntactic and Semantic Dependency Structures",
            "In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple languages."
        ]
    },
    "C08-1081": {
        "input_sentences": [
            "Besides establishing a first benchmark for the SYNTAGRUS treebank, we have analyzed the influence of different kindsof features on parsing accuracy, showing conclu sively that both lexical and morphological features are crucial for obtaining good parsing accuracy.",
            "This will require notonly automatic morphological analysis and disambiguation but also a mechanism for inserting so called phantom tokens in elliptical constructions.",
            "Acknowledgments We want to thank Ivan Chardin for initiating this collaboration and Jens Nilsson for converting the SYNTAGRUS data to the CoNLL format.",
            "We aregrateful to the Russian Foundation of Basic Re search for partial support of this research (grant no. 07-06-00339).",
            "A feature analysis shows that high parsing accuracy is crucially dependent on the use of both lexical and morphological features.",
            "Conclusion",
            "We have presented the first results on parsing theSYNTAGRUS treebank of Russian using a data driven dependency parser.",
            "We hypothesize that this result can be generalized to other richly inflected languages, provided that sufficient amounts of data are available.Future work includes a deeper analysis of the in fluence of individual features, both morphological and lexical, as well as an evaluation of the parserunder more realistic conditions without gold stan dard annotation in the input.",
            "Abstract",
            "We present the first results on parsing the SYNTAGRUS treebank of Russian with a data-driven dependency parser, achieving a labeled attachment score of over 82% and an unlabeled attachment score of 89%.",
            "Parsing the SynTagRus Treebank of Russian",
            "We conjecture that the latter result can be generalized to richly inflected languages in general, provided that sufficient amounts of training data are available."
        ]
    },
    "D11-1132": {
        "input_sentences": [
            "Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations.",
            "First, we construct a large relation repository of more than 7,000 relations from Wikipedia.",
            "The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.",
            "Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors.",
            "Abstract",
            "Relation Extraction with Relation Topics",
            "This paper describes a novel approach to the semantic relation detection problem.",
            "Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations.",
            "Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations.",
            "Specifically, we detect a new semantic relation by projecting the new relation\u2019s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process."
        ]
    },
    "P14-1060": {
        "input_sentences": [
            "We believe that the approach has considerable theoretical as well as practical merits, and provides a simple and clean formulation for modeling phrasal and sentential semantics.",
            "The flexibility of having separate representations to model different semantic senses has considerable valuable, as compared with extant approaches that assign a single representation to each token, and are hence constrained to conflate several semantic senses into a common representation.",
            "Abstract",
            "Future work can focus on a more thorough quantitative evaluation of the paradigm, as well as extension to model non-contiguous motifs.",
            "Vector space semantics with frequency-driven motifs",
            "The approach also elegantly deals with the problematic issue of differential compositional and non-compositional usage of words.",
            "Conclusion",
            "While being deliberately vague in our working definition of motifs, we have presented simple efficient formulations to extract such motifs that uses both annotated as well as partially unannotated data.",
            "We have presented a new frequency-driven framework for distributional semantics of not only lexical items but also longer cohesive motifs.",
            "We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated.",
            "In particular, we believe that ours is the first method that can invoke different meaning representations for a token depending on textual context of the sentence.",
            "The approach depends on drawing features from frequency statistics, statistical correlations, and linguistic theories; and this work provides a computational framework to jointly model recurrence and semantic cohesiveness of motifs through compositional penalties and affinity scores in a data driven way.",
            "Such a framework for distributional models avoids the issue of data sparsity in learning of representations for larger linguistic structures.",
            "The theme of this work is a general paradigm of seeking motifs that are recurrent in common parlance, are semantically coherent, and are possibly noncompositional.",
            "Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items.",
            "Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.",
            "Finally, we obtain motif representations in form of low-dimensional vector-space embeddings, and our experimental findings indicate value of the learnt representations in downstream applications.",
            "The qualitative and quantitative analyis of results from our preliminary motif segmentation model indicate that such motifs can help to disambiguate contexts of single tokens, and provide cleaner, more interpretable representations.",
            "In this work, we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohelineal constituents, or The framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents, and circumvents some problems of data sparsity by design."
        ]
    },
    "W05-0104": {
        "input_sentences": [
            "In the fall term of 2004, I taught a new statistical NLP course focusing on core tools and machine-learning algorithms.",
            "A Core-Tools Statistical NLP Course",
            "This paper briefly outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way.",
            "However, I will certainly keep the substantial implementation component of the course, partially in response to very positive student feedback on the assignments, partially from my own reaction to the high quality of student work on those assignments, and partially from how easily students with so much handson experience seem to be able to jump into NLP research.",
            "Some students needed remedial linguistics sections and other students needed remedial math sections, and I would hold more such sessions, and ear3There was also verbose error reporting for assignment 4, which displayed each sentence\u2019s guessed and gold alignments in a grid, but since most students didn\u2019t speak French, this didn\u2019t have the same effect. lier in the term.",
            "The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system.",
            "Abstract",
            "There are certainly changes I will make when I teach this course again this fall.",
            "Using provided scaffolding, students built realistic tools with nearly state-of-theart performance in most cases.",
            "Conclusions",
            "I will likely shuffle the topics around so that word alignment comes earlier (closer to HMMs for tagging) and I will likely teach dynamic programming solutions to parsing and tagging in more depth than graph-search based methods."
        ]
    },
    "W12-2802": {
        "input_sentences": [
            "and a specific object in the environment.",
            "Inaddition, our approach learns grounded word mean ings or distributions corresponding to words in the language, that the system can use to follow novelcommands that it may have never encountered dur ing training.",
            "Conclusion",
            "Our prelimi nary evaluation demonstrates the effectivenessof the model on a corpus of ?pick up?",
            "Previous ap proaches to learning these grounded meaning representations require detailed annotations at training time.",
            "We presented a preliminary evaluation on a small corpus, demonstrating that the system isable to infer meanings for concrete noun phrases de spite having no direct supervision for these values.",
            "In order for robots to effectively understand natural language commands, they must be ableto acquire a large vocabulary of meaning rep resentations that can be mapped to perceptualfeatures in the external world.",
            "Abstract",
            "In this paper we described an approach for learningperceptually grounded word meanings from an un aligned parallel corpus of language paired with robotactions.",
            "The training algorithm jointly infers poli cies that correspond to natural language commands as well as alignments between noun phrases in the command and groundings in the external world.",
            "Our approach points the way towards a framework that can learn a large vocabulary of general groundedword meanings, enabling systems that flexibly respond to a wide variety of natural language com mands given by untrained users.",
            "com mands given to a robotic forklift by untrained users.",
            "We assume the action policy takes a parametric form that factors based on the structure of the language, based on the G3 framework and use stochastic gradient ascentto optimize policy parameters.",
            "There are many directions for improvement.",
            "Toward Learning Perceptually Grounded Word Meanings from Unaligned Parallel Data",
            "In this paper, we present an approach which is capable of jointly learninga policy for following natural language com mands such as ?Pick up the tire pallet,?",
            "as well as a mapping between specific phrases in the language and aspects of the external world; for example the mapping between the words ?the tire pallet?",
            "Weplan to train our system using a large dataset of language paired with robot actions in more complex en vironments, and on more than one robotic platform."
        ]
    },
    "W04-0837": {
        "input_sentences": [
            "We evaluate on the and English alldata.",
            "The method could be particularly useful when tailoring a WSD system to a particular domain.",
            "In this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text.",
            "In this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from SemCor many systems in",
            "We intend to experiment further using a wider variety of grammatical relations, which we hope will improve performance for verbs, and with data from larger corpora, such as the Gigaword corpus and the web, which should allow us to cover a great many more words which do not occur in manually created resources such as SemCor.",
            "Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently.",
            "Abstract",
            "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.",
            "We have demonstrated that it is possible to acquire predominant senses from raw textual corpora, and that these can be used as an unsupervised first sense heuristic that does not not rely on manually produced corpora such as SemCor.",
            "The first (or predominant) sense heuristic assumes the availability of handtagged data.",
            "Using Automatically Acquired Predominant Senses For Word Sense Disambiguation",
            "We also intend to apply our method to domain specific text.",
            "This approach is useful for words where there is no manually-tagged data available.",
            "Conclusions",
            "For accurate first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough.",
            "Our predominant senses have been used within a WSD system as a back-off method when data is not available from other resources (Villarejo et al., 2004)."
        ]
    },
    "W11-0610": {
        "input_sentences": [
            "We describe our data set, which consists of transcribed data from a widely used clinical diagnostic instrument (the ADOS) for children with autism, children with developmental language disorder, and typically developing children.",
            "The sentence He\u2019s flying in a lily-pond, for instance, could contain a developmental error (i.e., the child has not yet acquired the correct meaning of in) or a semantic error (i.e., the child is using the word flying instead of swimming).",
            "The seemingly large number of misclassifications of sentences like this indicates the need for further investigation of the existing coding procedure and in-depth classification error analysis.",
            "However, the superior performance of our automatically extracted language features suggests that perhaps it may not be the errors themselves that characterize the speech of children with ASD and DLD but rather a preference for certain structures and word sequences that sometimes manifest themselves as clear language errors.",
            "Abstract",
            "When we examined the output of our error-type classifier, we noticed that many of the misclassified examples could be construed, upon closer inspection, as belonging to multiple error classes.",
            "In summary, the methods explored in this paper show potential for improving diagnostic discrimination between typically developing children and those with these neurodevelopmental disorders.",
            "In this paper, we discuss previous work identifying language errors associated with atypical language in ASD and describe a procedure for reproducing those results.",
            "Our classifiers achieve results well above chance, demonstrating the potential for using NLP techniques to enhance neurodevelopmental diagnosis and atypical language analysis.",
            "In particular, we would like to explore semantic and lexical features that are less dependent on linear order and syntactic structure, such as Resnik similarity and features derived using latent semantic analysis.",
            "Our method of automatically identifying error type shows promise as a supplement to, or substitute for, the time-consuming and subjective manual coding process described in Volden and Lord (Volden and Lord, 1991).",
            "We expect further improvement with additional data, features, and classification techniques.",
            "Although our classifiers using automatically extracted features were generally robust, we expect that including additional classification techniques, subjects (especially ASD subjects without DLD), and features will further improve our results.",
            "Finally, we would like to investigate how informative the error types are and whether they can be reliably coded by multiple judges.",
            "Conclusions",
            "Further research is required, however, in finding the most reliable markers that can be derived from such spoken language samples.",
            "Atypical or idiosyncratic language is a characteristic of autism spectrum disorder (ASD).",
            "The CHILDES database of children\u2019s speech, although it is not large enough to be used on its own for our analysis and would require significant manual syntactic annotation, might provide enough data for us to adapt our models to the child language domain.",
            "Such variations in complexity and likelihood might be too subtle for humans to reliably observe.",
            "Classification of Atypical Language in Autism",
            "We also plan to expand the training input for the language model and parser to include children\u2019s speech.",
            "The Switchboard corpus is conversational speech, but it may fail to adequately model many linguistic features characteristic of small children.",
            "Future Work",
            "We then present methods for automatically extracting lexical and syntactic features from transcripts of children\u2019s speech to 1) identify certain syntactic and semantic errors that have previously been found to distinguish ASD language from that of children with typical development; and 2) perform diagnostic classification.",
            "Without knowing the context in which the sentence was uttered, it is not possible to determine the type of error through any manual or automatic means."
        ]
    },
    "I08-1012": {
        "input_sentences": [
            "Our proposed approach achieves an unlabeled at tachment score of 86.52, an absolute 1.24% improvement over the baseline system on the data set of Chinese Treebank.",
            "First, feature representationneeds to be improved.",
            "The unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words.",
            "There are many ways in which this research should be continued.",
            "We ex tract the information on short dependency relations 93in an automatically generated corpus parsed by a basic parser.",
            "Conclusion",
            "The new parser achieves an absolute im provement of 1.24% over the state-of-the-art parser on Chinese Treebank (from 85.28% to 86.52%).",
            "Second, we can try to select more accurately parsed sentences.",
            "Then we may collect more reliable information than the current one.",
            "We thentrain another parser which uses the informa tion on short dependency relations extractedfrom the output of the first parser.",
            "Abstract",
            "This paper presents an effective approach to improvedependency parsing by using unlabeled data.",
            "We then train a new parser with the information.",
            "This paper presents an effective dependencyparsing approach of incorporating short de pendency information from unlabeled data.",
            "Here, we use a simple fea ture representation on short dependency relations.We may use a combined representation to use the in formation from long dependency relations even they are not so reliable.",
            "Dependency Parsing with Short Dependency Relations in Unlabeled Data"
        ]
    },
    "P12-2006": {
        "input_sentences": [
            "In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions.",
            "Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2.",
            "In our experiments, the fastest settings of our decoder and Moses differ in translation speed by a factor of 22 on the WMT data and a factor of 19 on the IWSLT data.",
            "This work introduces two extensions to the wellknown beam search algorithm for phrase-based machine translation.",
            "Abstract",
            "Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation",
            "We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed.",
            "Both pre-sorting the phrase translation candidates with an LM score estimate and LM look-ahead during search are shown to have a positive effect on translation speed.",
            "We compare our decoder to Moses, reaching a similar highest BLEU score, but clearly outperforming it in terms of scalability with respect to the trade-off ratio between translation quality and speed.",
            "Conclusions",
            "At a speed of roughly 70 words per second, Moses 17.2% whereas our approach yields 20.0% with identical models.",
            "Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors.",
            "Our software is part of the open source toolkit Jane."
        ]
    },
    "W12-3706": {
        "input_sentences": [
            "Setting n for the ngram models depends on the size of the corpus but it would usually range from 4 to 6, 5 in our case.",
            "Abstract",
            "In this work we present experiments for Spanish in the financial domain but Opinum could easily be trained for a different language or domain.",
            "In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards.",
            "The next steps would be to study the possibility to classify in more than two classes by using several language models.",
            "Opinum: statistical sentiment analysis for opinion classification",
            "Its queries are very fast which makes it feasible for free on-line services.",
            "We present an accuracy above 81% for Spanish opinions in the financial products domain.",
            "Opinum is a sentiment analysis system designed for classifying customer opinions in positive and negative.",
            "Its approach based on morphological simplification, entity substitution and n-gram language models, makes it easily adaptable to other classification targets different from positive/negative.",
            "Regarding applications, Opinum could be trained for a given domain without expert knowledge.",
            "If several domains were available, then the same entities would have different scores depending on the domain, which would be a valuable analysis.",
            "It is necessary to perform a deeper analysis of the impact of lexical simplification on the accuracy of the language models.",
            "Conclusions and future work",
            "The classification of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach.",
            "Then the test opinions are compared to both models and a decision and confidence measure are calculated.",
            "Would it work for a general domain?",
            "The use of an external neutral corpus should also be considered in the future.",
            "It consists of building one probabilistic model for the positive and another one for the negative opinions.",
            "An interesting application would be to exploit the named entity recognition and associate positive/negative scores to the entities based on their surrounding text.",
            "Is it equally successful for a wider domain?",
            "There are other parameters which have to be experimentally tuned and they are not related to the positive or negative classification but to the subjective qualifier very/somewhat/little and to the confidence measure.",
            "For instance, trying to build the models from a mixed set of opinions of the financial domain and the IT domain.",
            "We propose an approach based on the order of the words without using any syntactic and semantic information.",
            "The classification performance of Opinum in our financial-domain experiments is 81,98% which would be difficult to improve because of the noise in the data and the subjectivity of the labeling in positive and negative.",
            "To this end an Apertium morphological analyser would be necessary (30 languages are currently available) as well as a labeled data set of opinions.",
            "It is also very important to establish the limitations of this approach for different domains."
        ]
    },
    "P13-1092": {
        "input_sentences": [
            "Grounded Unsupervised Semantic Parsing",
            "To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision.",
            "The resulting GUSP system is the first unsupervised approach to attain an accuracy comparable to the best supervised systems in translating complex natural-language questions to database queries.",
            "Directions for future work include: joint syntactic-semantic parsing, developing better features for learning; interactive learning in a dialog setting; generalizing distant supervision; application to knowledge extraction from database-rich domains such as biomedical sciences.",
            "Conclusion",
            "We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries.",
            "Abstract",
            "On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches.",
            "Grounded Unsupervised Semantic",
            "This paper introduces grounded unsupervised semantic parsing, which leverages available database for indirect supervision and uses a grounded meaning representation to account for syntax-semantics mismatch in dependency-based semantic parsing.",
            "Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM."
        ]
    },
    "W07-2214": {
        "input_sentences": [
            "We present an example from natural language which seems to require both types of context sensitivity, and introduce partially ordered multisets (pomsets) mcfgs as a formalism which succintly expresses both.",
            "Conclusion",
            "We introduced pomset mcfgs as a formalism for describing grammars with both types of context sensitivity, and outlined an informal proof of the its polynomialtime parsing complexity.",
            "Abstract",
            "In this paper we identified two types of context sensitivity, and provided a natural language example which exhibits both types of context sensitivity.",
            "This paper identifies two orthogonal dimensions of context sensitivity, the first being context sensitivity in concurrency and the second being structural context sensitivity.",
            "Pomset mcfgs"
        ]
    },
    "P05-1022": {
        "input_sentences": [
            "A discriminative reranker requires a source of candidate parses for each sentence.",
            "The reranker selects the best parse from this set of parses using a wide variety of features.",
            "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.",
            "The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.",
            "This result is only slightly higher than the highest reported result for this test-set, Bod\u2019s (.907) (Bod, 2003).",
            "Conclusion",
            "Because the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient.",
            "More to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the Charniak or Collins parsers.",
            "Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).",
            "Abstract",
            "Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking",
            "Acknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for providing an early draft of his paper and very useful comments on our paper.",
            "This method generates 50-best lists that are of substantially higher quality than previously obtainable.",
            "This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).",
            "A 91.0 f-score represents a 13% reduction in fmeasure error over the best of these parsers.2 Both the 50-best parser, and the reranking parser can be found at ftp://ftp.cs.brown.edu/pub/nlparser/, named parser and reranker respectively.",
            "This paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of parses.",
            "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).",
            "We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker."
        ]
    },
    "C08-1074": {
        "input_sentences": [
            "They also seem to show that starting point selection by random walk is slightly superiorto uniform random selection.",
            "Random Restarts in Minimum Error Rate Training for Statistical Machine Translation",
            "Abstract",
            "We compare several ways of perform ing random restarts with MERT.",
            "We findthat all of our random restart methods out perform MERT without random restarts,and we develop some refinements of ran dom restarts that are superior to the most common approach with regard to resulting model quality and training time.",
            "Finally, our exper iments suggest that time to carry out MERT canbe significantly reduced by using as few as 5 starting points per decoding iteration, performing post restart pruning of hypothesis sets, and cutting off training after a fixed number of decoding iterations (perhaps 7) rather than waiting for convergence.",
            "Och?s (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights instatistical machine translation (SMT) models.",
            "We believe that our results show very convincingly that using random restarts in MERT improves the BLEU scores produced by the result ing models.",
            "The use of multiple randomized start ing points in MERT is a well-established practice, although there seems to be nopublished systematic study of its benefits.",
            "Conclusions"
        ]
    },
    "P09-1065": {
        "input_sentences": [
            "We instead propose a method that combines multiple translation models in one decoder.",
            "Conclusion",
            "Abstract",
            "In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the latticebased MERT (Macherey et al., 2008).",
            "Current SMT systems usually decode with single translation models and cannot benefit from the strengths of other models in phase.",
            "We have presented a framework for including multiple translation models in one decoder.",
            "As our decoder accounts for multiple derivations, we extend the MERT algorithm to tune feature weights with respect to BLEU score for max-translation decoding.",
            "Joint Decoding",
            "Representing search space as a translation hypergraph, individual models are accessible to others via sharing nodes and even hyperedges.",
            "Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually.",
            "Therefore, one model can share translations and even derivations with other models.",
            "Joint Decoding with Multiple Translation Models",
            "Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding."
        ]
    },
    "D11-1094": {
        "input_sentences": [
            "The results might improve further if window-based context and dependency-based context are combined in an optimal way.",
            "Moreover, our approach scores well for both paraphrase ranking and paraphrase induction, whereas previous approaches only seem capable of improving performance on the former task at the expense of the latter.",
            "Abstract",
            "In this paper, we presented a novel method for the modeling of word meaning in context.",
            "Conclusion",
            "This paper presents a novel method for the computation of word meaning in context.",
            "Latent Vector Weighting for Word Meaning in Context",
            "During our research, a number of topics surfaced that we consider worth exploring in the future.",
            "Our evaluation shows that the approach presented here is able to improve upon the state-of-the art performance on paraphrase ranking.",
            "And thirdly, we would like to transfer the general idea of the approach presented in this paper to a tensorbased framework (which is able to capture the multiway co-occurrences of words, together with their window-based and dependency-based context features, in a natural way) and investigate whether such a framework proves beneficial for the modeling of word meaning in context.",
            "The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly.",
            "We make use of a factorization model based on non-negative matrix factorization, in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions.",
            "First of all, we would like to further investigate the optimal configuration for combining window-based and dependency-based contexts.",
            "A key feature of the algorithm is that we adapt the original dependency-based feature vector of the target word through the latent semantic space.",
            "By doing so, our model is able to make accurate similarity calculations for word meaning in context across the whole word space.",
            "The evaluation on a lexical substitution task \u2013 carried out for both English and French \u2013 indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations.",
            "The factorization model allows us to determine which particular dimensions are important for a target word in a particular context.",
            "We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions.",
            "Secondly, we would like to subject our approach to further evaluation, in particular on a number of different evaluation tasks, such as semantic compositionality.",
            "At the moment, the performance of the combined model does not yield a uniform picture."
        ]
    },
    "P14-2093": {
        "input_sentences": [
            "Effective Selection of Translation Model Training Data",
            "The results show that our methods outperform previous methods.",
            "In the future, we are interested in applying our methods into domain adaptation task of statistical machine translation in model level.",
            "When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points.",
            "Conclusion",
            "By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model.",
            "We present three novel methods for translation model training data selection, which are based on the translation model and language model.",
            "Compared with the methods which only employ language model for data selection, we observe that our methods are able to select high-quality domain-relevant sentence pairs and improve the translation performance by nearly 3 BLEU points.",
            "Abstract",
            "In addition, our methods make full use of the limited in-domain data and are easily implemented.",
            "Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest.",
            "Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus.",
            "In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection."
        ]
    },
    "P05-1063": {
        "input_sentences": [
            "The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.",
            "The syntactic features provide an additional 0.3% reduction in test\u2013set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (signifiat < which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system.",
            "Abstract",
            "We follow where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second \u201creranking\u201d model is then used to choose an utterance from these 1000-best lists.",
            "We describe a method for discriminative training of a language model that makes use of syntactic features.",
            "Discriminative Syntactic Language Modeling For Speech Recognition",
            "We describe experiments on the Switchboard speech recognition task."
        ]
    },
    "W11-2123": {
        "input_sentences": [
            "The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.",
            "Beyond optimizing the memory size of TRIE, there are alternative data structures such as those in Guthrie and Hepple (2010).",
            "Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.",
            "Finally, other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.",
            "If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram, then three or even fewer words could be kept in the backward state.",
            "Abstract",
            "Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.",
            "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "Exposing this information to the decoder will lead to better hypothesis recombination.",
            "Quantization can be improved by jointly encoding probability and backoff.",
            "There any many techniques for improving language model speed and reducing memory consumption.",
            "We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.",
            "Conclusion",
            "We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.",
            "KenLM: Faster and Smaller Language Model Queries",
            "For speed, we plan to implement the direct-mapped cache from BerkeleyLM.",
            "For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility.",
            "The PROBING model is 2.4 times as fast as the fastest alternative, SRILM, and uses less memory too.",
            "While we have minimized forward-looking state in Section 4.1, machine translation systems could also benefit by minimizing backward-looking state.",
            "Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.",
            "The code is opensource, has minimal dependencies, and offers both C++ and Java interfaces for integration.",
            "This paper describes the several performance techniques used and presents benchmarks against alternative implementations.",
            "These performance gains transfer to improved system runtime performance; though we focused on Moses, our code is the best lossless option with cdec and Joshua.",
            "This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.",
            "This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.",
            "Generalizing state minimization, the model could also provide explicit bounds on probability for both backward and forward extension.",
            "The structure uses linear probing hash tables and is designed for speed.",
            "For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.",
            "Future Work",
            "Much could be done to further reduce memory consumption."
        ]
    },
    "D09-1092": {
        "input_sentences": [
            "Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.",
            "We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.",
            "We explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.",
            "Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.",
            "Polylingual Topic Models",
            "Abstract",
            "When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
            "Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.",
            "Conclusions",
            "We introduce a polylingual topic model that discovers topics aligned across multiple languages.",
            "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages."
        ]
    },
    "P13-1155": {
        "input_sentences": [
            "The baseline system did not get it right; it got a wrong normalization with shorter edit distance.",
            "Abstract",
            "Finally, we have used the system as a preprocessing step for a machine translation system which improved the translation quality by 6%.",
            "We show that the proposed unsupervised approach provides a normalization system with very high precision and a reasonable recall.",
            "We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4).",
            "When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%.",
            "Social Text Normalization using Contextual Graph Random Walks",
            "Example(2) shows the same effect by getting \u201dcuz\u201d normalized to \u201dbecause\u201d.",
            "Table 5 lists a number of examples and their normalization using both Baseline1 and RW3.",
            "As an extension to this work, we will extend the approach to handle many-to-many normalization pairs; also we plan to apply the approach to more languages.",
            "The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text.",
            "Furthermore, the approach can be easily extended to handle similar problems such as accent restoration and generic entity normalization. dancing and should not be corrected to \u201d\u2018tweeting\u201d\u2019.",
            "The proposed approach is very scalable, adaptive to any domain and language.",
            "The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus.",
            "At the first example, RW3 got the correct normalization as \u201dinteresting\u201d which apparently is not the one with the shortest edit distance, though it is the most frequent candidate at the generated lexicon.",
            "We compared the system with conventional correction approaches and with recent previous work; and we showed that it highly outperforms other systems.",
            "At Example(3), both the baseline and RW3 did not get the correct normalization of \u201dyur\u201d to \u201dyou are\u201d which is currently a limitation in our system since we only allow one-to-one word mapping in the generated lexicons not one-to-many or many-tomany.",
            "This shows a characteristic of the proposed approach; it is very conservative in proposing normalization which is desirable as a preprocessing step for NLP applications.",
            "At Example(4), RW3 did not normalize \u201ddure\u201d to \u201dsure\u201d ; however the baseline normalized it by mistake to \u201ddare\u201d.",
            "Conclusion and Future Work",
            "Finally, Example 4 shows also that the system normalize \u201dgr8\u201d which is mainly due to having a flexible similarity cost during the normalization lexicon construction.",
            "This limitation can be marginalized by providing more data for generating the lexicon.",
            "We introduced a social media text normalization system that can be deployed as a preprocessor for MT and various NLP applications to handle social media text.",
            "The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.",
            "We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text."
        ]
    },
    "P08-1043": {
        "input_sentences": [
            "A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing",
            "Better grammars are shown here to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones.",
            "Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.",
            "In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go.",
            "Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.",
            "(Levinger et al., 1995; Goldberg et al., ; Adler et al., 2008)) will make the parser more robust and suitable for use in more realistic scenarios.",
            "Discussion and Conclusion",
            "Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.",
            "Abstract",
            "The work of the second author as well as collaboration visits to Israel was financed by NWO, grant number 017.001.271.",
            "We conjecture that this trend may continue by incorporating additional information, e.g., three-dimensional models as proposed by Tsarfaty and Sima\u2019an (2007).",
            "We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.",
            "The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.",
            "Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.",
            "The work of the first author was supported by the Lynn and William Frankel Center for Computer Sciences.",
            "These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.",
            "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity."
        ]
    },
    "N12-1052": {
        "input_sentences": [
            "The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering.",
            "When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.",
            "Abstract",
            "Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.",
            "We then showed that by using these cross-lingual word clusters, we can significantly improve on direct transfer of discriminative models for both parsing and NER.",
            "Conclusion",
            "The first method works by projecting word clusters, induced from monolingual data, from a source language to a target language directly via word alignments.",
            "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.",
            "In the second part, we provided two simple methods for inducing cross-lingual word clusters.",
            "Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure",
            "Although the performance of transfer systems is still substantially below that of supervised systems, this research provides one step towards bridging this gap.",
            "In the first part of this study, we showed that word clusters induced from a simple class-based language model can be used to significantly improve on stateof-the-art supervised dependency parsing and NER for a wide range of languages and even across language families.",
            "This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models.",
            "However, to our knowledge this is the first study to apply the same type of word cluster features across languages and tasks.",
            "Further, we believe that it opens up an avenue for future work on multilingual clustering methods, cross-lingual feature projection and domain adaptation for direct transfer of linguistic structure.",
            "Given previous work on word clusters for various linguistic structure prediction tasks, these results are not too surprising.",
            "First, we show that these results hold true for a number of languages across families.",
            "Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.",
            "Although the improvements vary between languages, the addition of word cluster features never has a negative impact on performance.",
            "As in the monolingual case, both types of cross-lingual word cluster features yield improvements across the board, with the more complex method providing a significantly larger improvement for NER.",
            "While previous work has focused primarily on English, we extend these results to other languages along two dimensions."
        ]
    },
    "P07-1051": {
        "input_sentences": [
            "Is the End of Supervised Parsing in Sight?",
            "We present a new algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees.",
            "Abstract",
            "Since supervised parsers seem to stick to categorical notions of constituent, we believe that in the field of syntax-based language models the end of supervised parsing has come in sight.",
            "However, except for the simple treebank PCFG, U-DOP* scores worse than supervised parsers if evaluated on hand-annotated data.",
            "If we only want to mimick a treebank or implement a linguistically motivated grammar, then supervised, grammar-based parsers are preferred to unsupervised parsers.",
            "While U-DOP* performs worse than state-of-the-art supervised parsers on handannotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl.",
            "A parser which does not allow for this fluidity may be of limited use as a language model.",
            "The problem with most supervised (and semi-supervised) parsers is their rigid notion of constituent which excludes \u2018constituents\u2019 like the German Ich m\u00f6chte or the French Il y a.",
            "Any sequence of words can be a unit of combination, including noncontiguous word sequences like closest X to Y.",
            "Conclusion: future of supervised parsing",
            "We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight.",
            "We believe that parsing, when separated from a task-based application, is mainly an academic exercise.",
            "At the same time U-DOP* significantly outperforms the supervised DOP* if evaluated in a practical application like MT.",
            "Instead, it has become increasingly clear that the notion of constituent is a fluid which may sometimes be in agreement with traditional syntax, but which may just as well be in opposition to it.",
            "We train both on Penn\u2019s WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set.",
            "What should we learn from these results?",
            "In this paper we have shown that the accuracy of unsupervised parsing under U-DOP* continues to grow when enlarging the training set with additional data.",
            "We argued that this can be explained by the fact that U-DOP learns both constituents and (non-syntactic) phrases while supervised parsers learn constituents only.",
            "How far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?",
            "But if we want to improve a practical application with a syntaxbased language model, then an unsupervised parser like U-DOP* might be superior."
        ]
    },
    "P13-2112": {
        "input_sentences": [
            "Figure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of known tokens for Italian (left) and Dutch (right).",
            "Interestingly, our method achieves higher accuracies on Germanic languages \u2014 the family of our source language, English \u2014 while Das and Petrov perform better on Romance languages.",
            "Table 2 shows results for our seed model, self training and revision, and the results reported by Das and Petrov.",
            "Abstract",
            "6 Conclusion We have proposed a method for unsupervised POS tagging that performs on par with the current stateof-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM). complexity of our algorithm is to that of Das and Petrov 637 where the size of training We our code are available for In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy.",
            "Dipanjan Das and Slav Petrov.",
            "This might be because our model relies on alignments, which might be more accurate for more-related languages, whereas Das and Petrov additionally rely on label propagation.",
            "Moreover, on average for the final model, approximately 10% of the test data tokens are unknown.",
            "Seattle, Washington, USA.",
            "Using our final model with unsupervised HMM methods might improve the final performance too, i.e. use our final model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging.",
            "Compared to Das and Petrov, our model performs poorest on Italian, in terms of percentage point difference in accuracy.",
            "Simpler unsupervised POS tagging with bilingual projections",
            "We find that for all languages, accuracy rises quickly in the first 5\u20136 iterations, and then subsequently improves only slightly.",
            "Although accuracy does not increase much in later iterations, they may still have some benefit as the vocabulary size continues to grow.",
            "One way to improve the performance of our tagger might be to reduce the proportion of unknown words by using a larger training corpus, as Das and Petrov did.",
            "2000.",
            "Conclusion",
            "The average accuracy of self training and revision is on par with that reported by Das and Petrov.",
            "In of the sixth conference on Applied natural language processing pages 224\u2013231.",
            "References Thorsten Brants.",
            "Figure 1 (left panel) shows accuracy, accuracy on known words, accuracy on unknown words, and proportion of known tokens for each iteration of our model for Italian; iteration 0 is the seed model, and iteration 31 is the final model.",
            "In of re-implemented label propagation from Das and Petrov (2011).",
            "In of the 23rd Pacific Asia Conference on Language, Information",
            "Self training and revision improve the accuracy for every language over the seed model, and gives an average improvement of roughly two percentage points.",
            "7 Acknowledgements This work is funded by Erasmus Mundus European Masters Program in Language and Communication Technologies (EM-LCT) and by the Czech Science Foundation (grant no.",
            "P103/12/G084).",
            "Our model performs poorly on unknown words as indicated by the low accuracy on unknown words, and high accuracy on known words compared to the overall accuracy.",
            "The overall performance dropped slightly.",
            "The complexity of our algorithm is O(nlogn) compared to O(n2) for that of Das and Petrov (2011) where n is the size of training data.3 We made our code are available for download.4 In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy.",
            "This might be because selftraining with revision already found the local maximal point. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language - Volume 1 (ACL pages 600\u2013609.",
            "(Findings are similar for other languages.)",
            "Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less effort.",
            "Using our final model with unsupervised HMM methods might improve the final performance too, i.e. use our final model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs sampling (GS).5 Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging.",
            "Given the improvements of our model over that of Das and Petrov on languages from the same family as our source language, and the observation of Snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language.",
            "On individual languages, self training and revision and the method of Das and Petrov are split \u2014 each performs better on half of the cases.",
            "We exemplify this in Figure 1 (right panel) for Dutch.",
            "TnT: A statistical part-oftagger.",
            "We have proposed a method for unsupervised POS tagging that performs on par with the current stateof-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM).",
            "Finally, we thank Siva Reddy and Spandana Gella for many discussions and suggestions.",
            "2009.",
            "The poor performance on unknown words is expected because we do not use any language-specific rules to handle this case.",
            "We examine the impact of self-training and revision over training iterations.",
            "In many cases, GS outperformed other methods, thus we would like to try GS first for our model.",
            "2011.",
            "It took over a day to complete this step on an eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, but only 15 minutes for our model. in fact have tried EM, but it did not help.",
            "We would like to thank Prokopis Prokopidis for providing us the Greek Treebank and Antonia Marti for the Spanish CoNLL 06 dataset.",
            "Portland, Oregon, USA.",
            "Unsupervised part-of-speech tagging with bilingual projections.",
            "Pascal Denis and Benoit Sagot."
        ]
    },
    "W06-3120": {
        "input_sentences": [
            "Although local reordering is supposed to be included in the phrase structure, performing local reordering improves the translation quality.",
            "Reordering is important when using a PhraseBased system.",
            "This paper reports translation results for the \u201cExploiting Parallel Texts for Statistical Machine Translation\u201d (HLT-NAACL Workshop on Parallel Texts 2006).",
            "In fact, local reordering, provided by the reordering approaches, allows for those generalizations which phrases could not achieve.",
            "We have studied different techniques to improve the standard Phrase-Based translation system.",
            "Abstract",
            "Reordering in the DeEn task is left as further work.",
            "TALP Phrase-Based Statistical Translation System For European Language Pairs",
            "Conclusions",
            "Mainly we introduce two reordering approaches and add morphological information."
        ]
    },
    "W05-1518": {
        "input_sentences": [
            "At the same time, we found that employing context to this task is very difficult even with a wellknown and widely used machine-learning approach.",
            "The unbalanced voting lead to the precision as high as 90.2 %, while the F-measure of 87.3 % outperforms the best result of balanced voting (87.0).",
            "We differ from them in exploring context features more deeply.",
            "Conclusion",
            "Improving Parsing Accuracy By Combining Diverse Dependency Parsers",
            "To some extent, we are porting the ideas of Henderson and Brill (1999) to the world of dependency structures.",
            "Abstract",
            "Also alternatively called unrestricted",
            "We were able to significantly improve over the best parsing result for the given setting, known so far.",
            "rate reduction).",
            "All our experiments were conducted on Czech but the method is language-independent.",
            "The methods are language independent, though the amount of accuracy improvement may vary according to the performance of the available parsers.",
            "We have tested several approaches to combining of dependency parsers.",
            "Moreover, our experiments show that even parsers far below the state of the art can contribute to the total improvement.",
            "This paper explores the possibilities of improving parsing results by combining outputs of several parsers.",
            "Although voting methods are themselves not new, as far as we know we are the first to propose and evaluate their usage in full dependency parsing.",
            "Accuracy-aware voting of the four best parsers turned out to be the best method, as it significantly improved the accuracy of the best component from 85.0 to 87.0 % (13 % error"
        ]
    },
    "W10-1401": {
        "input_sentences": [
            "Thereis ample evidence that the application of read ily available statistical parsing models to suchlanguages is susceptible to serious performance degradation.",
            "The overarching analysis suggests itself as a source of directions for future investigations.",
            "The term Morphologically Rich Languages(MRLs) refers to languages in which signif icant information concerning syntactic units and relations is expressed at word-level.",
            "We have shown that architectural, representational, and estimation issuesassociated with parsing MRLs are found to be chal lenging across languages and parsing frameworks.",
            "Such insights may be gained, among other things, in the context of more morphologically-aware shared parsing tasks.",
            "Furthermore, sound statistical esti mation methods for morphologically rich, complexlexica, turn out to be crucial for obtaining good pars ing accuracy when using general-purpose models and algorithms.",
            "Acknowledgements The program committee would like to thank NAACL for hosting the workshop and SIGPARSEfor their sponsorship.",
            "Statistical Parsing of Morphologically Rich Languages (SPMRL) What How and Whither",
            "Conclusion",
            "In the future we hope to gain better understanding of the common pitfalls in, and novel solutions for, parsing morphologically ambiguousinput, and to arrive at principled guidelines for selecting the model and features to include when pars ing different kinds of languages.",
            "We further thank INRIA Al page team for their generous sponsorship.",
            "Abstract",
            "We synthesize the contributions of re searchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages.",
            "The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite languagespecific idiosyncrasies, the problems associ ated with parsing MRLs cut across languagesand parsing frameworks.",
            "We are finally grateful to our reviewers and authors for their dedicated work and individual contributions.",
            "In this paper we re view the current state-of-affairs with respectto parsing MRLs and point out central challenges.",
            "This paper presents the synthesis of 11 contributionsto the first workshop on statistical parsing for mor phologically rich languages.",
            "The use of morphological information in the nongold-tagged input scenario is found to cause sub stantial differences in parsing performance, and inthe kind of morphological features that lead to per formance improvements.Whether or not morphological features help pars ing also depends on the kind of model in which they are embedded, and the different ways they aretreated within."
        ]
    },
    "P13-1007": {
        "input_sentences": [
            "This work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision.",
            "We also present a general model for learning to build partial orders from a set of pairpreferences.",
            "Finally, we significantly improve the performance of the previous model using a rich set of automatically generated features.",
            "We give an algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice.",
            "Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation",
            "Summary and future work",
            "No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation.",
            "Abstract",
            "In this paper we report early, though promising, results for automatic QSD when handling both phenomena.",
            "We develop the first statistical QSD model addressing the interaction of quantifiers with negation and the implicit universal of plurals, defining a baseline for this task on QuanText data (Manshadi et al., 2012).",
            "Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases.",
            "In addition, our work improves upon Manshadi and Allen (2011a)\u2019s work by (approximately) optimizing a well justified criterion, by using automatically generated features instead of hand-annotated dependencies, and by boosting the performance by a large margin with the help of a rich feature vector."
        ]
    },
    "P04-1013": {
        "input_sentences": [
            "This article has investigated the application of discriminative methods to broad coverage natural language parsing.",
            "We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model.",
            "We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.",
            "One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem.",
            "Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing.",
            "Abstract",
            "The latter model outperforms the previous two, achieving state-ofthe-art levels of performance (90.1% F-measure on constituents).",
            "Discriminative Training Of A Neural Network Statistical Parser",
            "This training method successfully satisfies the conflicting constraints that it be computationally tractable and that it be a good approximation to the theoretically optimal method.",
            "We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance.",
            "This approach contrasts with previous approaches to scaling up discriminative methods to broad coverage natural language parsing, which have parameterizations which depart substantially from the successful previous generative models of parsing.",
            "This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model.",
            "Conclusions",
            "Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003).",
            "We distinguish between two different ways to apply discriminative methods, one where the probability model is changed to a discriminative one, and one where the probability model remains generative but the training method optimizes a discriminative criteria."
        ]
    },
    "W12-3160": {
        "input_sentences": [
            "We showed that although thereare two competing strategies with comparable per formance, one is an unstable learner, and before weunderstand more regarding the nature of the insta bility, the preferred alternative is to use M?C as the hypothesis pair in optimization.",
            "Although the performance of the two strategies iscompetitive on the evaluation sets, this does not re lay the entire story.",
            "tion selection methods, PB and M+C significantly underperforming it.Given that the translation performance of optimiz ing the loss functions represented by LU/MC and M?C selection is comparable on the evaluation sets for fr-en and cs-en, it may be premature to make a general recommendation for one over the other.",
            "The author is supported by the Department of Defense through the NationalDefense Science and Engineering Graduate Fellow 487ship.",
            "On the contrary, the range of BLEU scores for the M?C optimizer is on the order of 2 BLEU points, leading to more gradual changes.",
            "Abstract",
            "Acknowledgments We would like to thank the anonymous reviewers for their comments.",
            "In all three cases, the oracle BLEU score is in about the same range,as expected, since all are using the same oracle se lection strategy.",
            "In both cs-en and fr-en, we can observe that M?C performs the best.",
            "However, these methods have not yet met with wide-spread adoption.",
            "M?C optimization is once again smooth, and converges quickly, with a small range for the oracle and prediction scores around the model best.",
            "As can be seen in Figure 1, while optimizing with M?C is stable and smooth, where we converge on our optimum after several iterations, optimizing with LU/MC is highly unstable.",
            "LU/MC remains unstable, oscillating up to 2 BLEU points between iterations.",
            "Thismay be partly due to the perceived complex ity of implementation, and partly due to the lack of standard methodology for applying these methods to MT. This papers aims to shedlight on large-margin learning for MT, explic itly presenting the simple passive-aggressivealgorithm which underlies many previous ap proaches, with direct application to MT, andempirically comparing several widespread op timization strategies.",
            "Conclusion",
            "In this paper, we strove to elucidate aspects of large margin structured learning with concrete application to the MT setting.",
            "Large-Margin Learning",
            "Optimization Strategies for Online Large-Margin Learning in Machine Translation",
            "Towards this goal, we presented the MIRA passive-aggressive algorithm, which can be used directly to effectively tune a statistical MT system with millions of parameters, in the hope that some confusion surrounding MIRA-based methods may be cleared, and more MT researchers can adoptit for their own use.",
            "Figures 3-6 compare the different optimization strategies further.",
            "On the other hand, MC selection also stands out, since it is the only one with a large drop in prediction BLEU score.",
            "This is at least in part due to the wide range in BLEU scores for the oracle and prediction, which are in the range of 10 BLEU points higher or lower than the current model best.",
            "The introduction of large-margin based dis criminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process.",
            "0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 1 2 3 4 5 6 7 8 9 10 BLE U Iteration Figure 6: Comparison of performance on development set for fr-en of the three prediction selection strategies when using LU selection as oracle.",
            "By removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features.",
            "For a more complete view of the differences between optimization strategies, we turn to Figures 1-6.",
            "In Figures 4 and 6, we use LU as the oracle, andshow performance using the three prediction selec tion strategies, with each line representing the same strategy as described above.",
            "Error bars indicate the oracle and prediction BLEU scores for each pairing as before.",
            "The major difference, which is immediately evident, is that the optimizers are highly unstable.",
            "0.05 0.1 0.15 0.2 0.25 0.3 0.35 1 2 3 4 5 6 7 8 9 10 BLE U Iteration Figure 4: Comparison of performance on development set for cs-en of the three prediction selection strategies when using LU selection as oracle.",
            "In Figures 3 and 5, we use M-Cas the oracle, and show performance on the develop ment set while using the three prediction selection strategies, M+C with a solid blue line, PB with a dotted green line, and MC with a dashed red line.",
            "We can immediately observe that PB has no error bars going down, indicating that the PB method for selecting the prediction keeps pace with the model best at each iteration.",
            "The only pairing which showssome stability is LU/MC, with both the other predic 486 0.05 0.07 0.09 0.11 0.13 0.15 0.17 0.19 0.21 0.23 0.25 1 2 3 4 5 6 7 8 9 10 BLE U Iteration Figure 3: Comparison of performance on development set for cs-en of the three prediction selection strategies when using M-C selection as oracle.",
            "Crucially, all learners are stable, and move toward convergence smoothly, which serves to validate our earlier observation that M-C oracle selection can bepaired with any prediction selection strategy and op timize effectively.",
            "Discussion",
            "M?C is indicated with a solid black line, while LU/MC is a dotted red line.",
            "We see a similar, albeit slightly less pronounced behavior on fr-en in Figure 2.",
            "We then used the presented al gorithm to empirically compare several widespreadloss functions and strategies for selecting hypothe ses for optimization.",
            "The corpus-level oracle and prediction BLEU scores at each iteration are indicated with error bars around each point, using solid lines for M?C and dotted lines for LU/MC.",
            "Figure 1 and 2 present thecomparison of performance on the NT08 development set for cs-en and fr-en, respectively, when using LU/MC to select the oracle and prediction ver sus M?C selection.",
            "Any opinions, findings, conclusions, or rec ommendations expressed are the author?s and do not necessarily reflect those of the sponsors.",
            "However, taking the unstable nature of LU/MC intoaccount, the extent of which may depend on the tun ing set, as well as other factors which need to befurther examined, the current more prudent alterna tive is selecting the oracle and prediction pair based on M?C.",
            "0.05 0.1 0.15 0.2 0.25 0.3 1 2 3 4 5 6 7 8 9 10 BLE U Iteration Figure 5: Comparison of performance on development set for fr-en of the three prediction selection strategies when using M-C selection as oracle."
        ]
    },
    "W04-1505": {
        "input_sentences": [
            "We have discussed how DG allows the expression of the majority of LDDs in a contextfree way and shown that DG allows for simple but powerful statistical models.",
            "Abstract",
            "Its parsing speed of about 300,000 words per hour is very good for a deep-linguistic parser and makes it fast enough for unlimited application.",
            "Fast Deep-Linguistic Statistical Dependency Parsing",
            "We present and evaluate an implemented statistical minimal parsing strategy exploiting DG charateristics to permit fast, robust, deeplinguistic analysis of unrestricted text, and compare its probability model to (Collins, 1999) and an adaptation, (Dubey and Keller, 2003).",
            "Conclusions",
            "We show that DG allows for the expression of the majority of English LDDs in a context-free way and offers simple yet powerful statistical models.",
            "An evaluation shows that the performance of its implementation is state-of-the-art10."
        ]
    },
    "N06-1045": {
        "input_sentences": [
            "Further advances in determinization will provide additional benefit to the community.",
            "Users of MT systems are generally interested in the string yield of those trees, and not the trees per se.",
            "This redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes. is due to nondeterminism in the weighted automata that produce the results.",
            "Conclusion",
            "Ranked lists of output trees from syntactic statistical NLP applications frequently contain multiple repeated entries.",
            "We have shown that weighted determinization is useful for recovering -best unique trees from a weighted forest.",
            "As summarized in Figure 9, the number of repeated trees prior to determinization was typically very large, and thus determinization is critical to recovering true tree weight.",
            "The translation system detailed here is a string-to-tree system, and the determinization algorithm returns the -best unique trees from a packed forest.",
            "Abstract",
            "We plan for our weighted determinization algorithm to be one component in a generally available tree automata package for intersection, composition, training, recognition, and generation of weighted and unweighted tree automata for research tasks such as the ones described above.",
            "We introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees.",
            "We also demonstrate our algorithm\u2019s effectiveness on two large-scale tasks.",
            "Thus, an algorithm that can return the -best unique strings from a packed forest would be a useful extension.",
            "We have improved evaluation scores by incorporating the presented algorithm into our MT work and we believe that other NLP researchers working with trees can similarly benefit from this algorithm.",
            "A Better N-Best List: Practical Determinization Of Weighted Finite Tree Automata"
        ]
    },
    "E09-1034": {
        "input_sentences": [
            "An interesting line of future work would be to find an equivalent characterisation of mildly ill-nested structures which is more grammar-oriented and would provide a more linguistic insight into these structures.",
            "Additionally, we have defined a set of structures for any gap degree k which we call mildly ill-nested.",
            "Conclusions and future work",
            "We have defined a parsing algorithm for wellnested dependency structures with bounded gap degree.",
            "The practical interest of mildly ill-nested structures can be seen in the data obtained from several dependency treebanks, showing that all of the ill-nested structures in them are mildly ill-nested for their corresponding gap degree.",
            "When the gap degree is greater than 1, the time complexity goes up by a factor of n2 for each extra unit of gap degree, as in parsers for coupled context-free grammars.",
            "Most of the non-projective sentences appearing in treebanks are well-nested and have a small gap degree, so this algorithm directly parses the vast majority of the non-projective constructions present in natural languages, without requiring the construction of a constituency grammar as an intermediate step.",
            "Parsing Mildly Non-Projective Dependency Structures",
            "Abstract",
            "This set includes ill-nested structures verifying certain conditions, and can be parsed in O(n3k+4) with a variant of the parser for wellnested structures.",
            "Therefore, our O(n3k+4) parser can analyse all the gap degree k structures in these treebanks.",
            "Another research direction, which we are currently working on, is exploring how variants of the MGk parser\u2019s strategy can be applied to the problem of binarising LCFRS (G\u00b4omezRodriguez et al., 2009).",
            "The third case includes all the degree in a number of dependency treebanks.",
            "In particular, algorithms are presented for: all well-nested structures of degree at most with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power; all well-nested structures with degree bounded by any constant and a new class of structures with gap deup to includes some ill-nested structures.",
            "We present parsing algorithms for various mildly non-projective dependency formalisms.",
            "In terms of computational complexity, this algorithm is comparable to the best parsers for related constituency-based formalisms: when the gap degree is at most 1, it runs in O(n7), like the fastest known parsers for LTAG, and can be made O(n6) if we use unlexicalised dependencies.",
            "The set of mildly ill-nested structures for gap degree k is defined as the set of structures that have a binarisation of gap degree at most k. This definition is directly related to the way the MGk parser works, since it implicitly finds such a binarisation."
        ]
    },
    "P10-1044": {
        "input_sentences": [
            "This approach is capable of producing human interpretable classes, however, avoids the drawbacks of traditional class-based approaches (poor lexical coverage and ambiguity).",
            "We have presented an application of topic modeling to the problem of automatically computing selectional preferences.",
            "We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences.",
            "By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation\u2019s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007).",
            "Because LDA-SP generates a complete probabilistic model for our relation data, its results are easily applicable to many other tasks such as identifying similar relations, ranking inference rules, etc.",
            "Abstract",
            "computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability.",
            "LDA-SP achieves state-of-the-art performance on predictive tasks such as pseudo-disambiguation, and filtering incorrect inferences.",
            "Our method, LDA-SP, learns a distribution over topics for each relation while simultaneously grouping related words into these topics.",
            "Finally, our repository of selectional preferences for 10,000 relations is available at http://www.cs.washington.edu/ research/ldasp.",
            "Conclusions and Future Work",
            "A Latent Dirichlet Allocation Method for Selectional Preferences",
            "In the future, we wish to apply our model to automatically discover new inference rules and paraphrases.",
            "We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al.,"
        ]
    },
    "P06-1109": {
        "input_sentences": [
            "The main problem is how to quantitatively compare these different parsers, as any evaluation on handannotated data (like the Penn treebank) will unreasonably favor semi-supervised parsers.",
            "Abstract",
            "There is a final question as to how far the DOP approach to unsupervised parsing can be stretched.",
            "There is thus is a quest for designing an annotationindependent evaluation scheme.",
            "Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.",
            "We investigate generalizations of the allsubtrees &quot;DOP&quot; approach to unsupervised parsing.",
            "We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.",
            "Yet we should neither rule out the possibility that entirely unsupervised methods will in fact surpass semi-supervised methods.",
            "Since parsers are becoming increasingly important in applications like syntax-based machine translation and structural language models for speech recognition, one way to go would be to compare these different parsing methods by isolating their contribution in improving a concrete NLP system, rather than by testing them against gold standard annotations which are inherently theory-dependent.",
            "The initially disappointing results of inducing trees entirely from raw text was not so much due to the difficulty of the bootstrapping problem per se, but to (1) the poverty of the initial models and (2) the difficulty of finding theoryindependent evaluation criteria.",
            "7 Conclusion: Is the end of supervised parsing in sight?",
            "McClosky et al. 2006).",
            "We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.",
            "Whether such a massively maximalist approach is feasible can only be answered by empirical investigation in due time.",
            "To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank PCFG).",
            "The time has come to fully reappraise unsupervised parsing models which should be trained on massive amounts of data, and be evaluated in a concrete application.",
            "Thus if we modify our question as: does the exclusively supervised approach to parsing come to an end, we believe that the answer is certainly yes.",
            "Now that we have outperformed a well-known supervised parser by an unsupervised one, we may raise the question as to whether the end of supervised NLP comes in sight.",
            "In principle we can assign all possible syntactic categories, semantic roles, argument structures etc. to a set of given sentences and let the statistics decide which assignments are most useful in parsing new sentences.",
            "An All-Subtrees Approach To Unsupervised Parsing",
            "All supervised parsers are reaching an asymptote and further improvement does not seem to come from more hand-annotated data but by adding unsupervised or semi-unsupervised techniques (cf."
        ]
    },
    "P11-2121": {
        "input_sentences": [
            "The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-the-art performance with respect to other parsing approaches.",
            "The new addition to the algorithm shows a clear advantage in parsing speed.",
            "Abstract",
            "Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features.",
            "The additional transition gives improvements to both parsing speed and accuracy, showing a linear time parsing speed with respect to sentence length.",
            "In the future, we will test the robustness of these approaches in more languages.",
            "Getting the Most out of Transition-based Dependency Parsing",
            "We present two ways of improving transition-based, non-projective dependency parsing.",
            "This paper suggests two ways of improving transition-based, non-projective dependency parsing.",
            "Conclusion and future work",
            "The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set.",
            "First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed."
        ]
    },
    "N12-1090": {
        "input_sentences": [
            "We explored the under-investigated yet challenging task of performing coreference resolution for a language for which we have no coreference-annotated data and no linguistic knowledge of the language.",
            "To gain additional insights into our approach, we plan to pursue several directions.",
            "Third, rather than translate from the target to the source language, we will examine whether it is better to translate all the coreference-annotated data available in the source language to the target language, and train a coreference model for the target language on the translated data.",
            "We believe that this approach has the potential to allow coreference technologies to be deployed across a larger number of languages than is currently possible, and that this is just the beginning of a new line of work.",
            "Translation-Based Projection for Multilingual Coreference Resolution",
            "To build a coreference resolver for a new language, the typical approach is to first coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques.",
            "Abstract",
            "Experimental results on two target languages demonstrate the promise of our approach.",
            "Conclusions and Future Work",
            "To alleviate this corpus annotation bottleneck, we examine a translation-based projection approach to multilingual coreference resolution.",
            "In experiments with Spanish and Italian, we obtained promising results: our approach achieved around 90% of the performance of a supervised resolver when only a mention extractor for the target language was available.",
            "Our translation-based projection approach has the flexibility to exploit any available knowledge about the target language.",
            "Fourth, since the success of our projection approach depends heavily on the accuracies of machine translation as well as coreference resolution in the source language, we will determine whether their accuracies can be improved via an ensemble approach, where we employ multiple MT engines and multiple coreference resolvers.",
            "Second, we will perform an empirical comparison of two approaches to projecting coreference annotations, our translation-based approach and Camargo de Souza and Orasan\u2019s (2011) approach, where annotations are projected via a parallel corpus.",
            "First, we will isolate the impact of each factor that adversely affects its performance, including errors in projection, translation, and coreference resolution in the resource-rich language.",
            "Finally, we plan to employ our approach to alleviate the corpus-annotation bottleneck, specifically by using the annotated data it produces to augment the manual coreference annotations that capture the specific properties of the target language.",
            "However, the high cost associated with manually coreference-annotating documents needed by a supervised approach makes it difficult to deploy coreference technologies across a large number of natural languages."
        ]
    },
    "E09-1018": {
        "input_sentences": [
            "We have presented a generative model of pronounanaphora in which virtually all of the parameters are learned by expectation maximization.",
            "Abstract",
            "Perhaps the largest limitation is the programs inability to recognize the speaker of a quoted segment.",
            "The current system has several obvious limitation.",
            "We have compared it to several systems available on the web (all we have found so far).",
            "Our program significantly outperforms all of them.",
            "EM Works for Pronoun Anaphora Resolution",
            "To down-load it, go to (to be announced).",
            "Conclusion",
            "We find it of interest first as an example of one of the few tasks for which EM has been shown to be effective, and second as a useful program to be put in general use.",
            "The algorithm is fast and robust, and has been made publically available for downloading.",
            "It is, to the best of our knowledge, the best-performing system available on the web.",
            "The result is a very large fraction of first person pronouns are given incorrect antecedents.",
            "It does not handle cataphora (antecedents occurring after the pronoun), only allows antecedents to be at most two sentences back, does not recognize that a conjoined NP can be the antecedent of a plural pronoun, and has a very limited grasp of pronominal syntax.",
            "We present an algorithm for pronounanaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion.",
            "Besides being of the greatest importance in its own right, it would also allow us to add one piece of information we currently neglect in our pronominal system \u2014 the more times a document refers to an entity the more likely it is to do so again.",
            "While EM frequently fails to find good models for the tasks to which it is set, in this case it works quite well.",
            "Fixing these problems would no doubt push the system\u2019s performance up several percent.",
            "However the most critical direction for future research is to push the approach to handle full NP anaphora."
        ]
    },
    "P12-1099": {
        "input_sentences": [
            "Future work includes extending this approach to use multiple translation models with multiple language models in ensemble decoding.",
            "In addition, we can extend our approach by applying some of the techniques used in other system combination approaches such as consensus decoding, using n-gram features, tuning using forest-based MERT, among other possible extensions.",
            "In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain.",
            "Mixing Multiple Translation Models in Statistical Machine Translation",
            "Conclusion & Future Work",
            "In this approach a number of MT systems are combined at decoding time in order to form an ensemble model.",
            "Abstract",
            "We showed that this approach can gain up to 2.2 BLEU points over its concatenation baseline and 0.39 BLEU points over a powerful mixture model.",
            "Different mixture operations can be investigated and the behaviour of each operation can be studied in more details.",
            "The model combination can be done using various mixture operations.",
            "Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain.",
            "In this paper, we presented a new approach for domain adaptation using ensemble decoding.",
            "Furthermore, ensemble decoding can be applied on domain mixing settings in which development sets and test sets include sentences from different domains and genres, and this is a very suitable setting for an ensemble model which can adapt to new domains at test time.",
            "Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.",
            "We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step.",
            "We will also add capability of supporting syntax-based ensemble decoding and experiment how a phrase-based system can benefit from syntax information present in a syntax-aware MT system."
        ]
    },
    "W99-0613": {
        "input_sentences": [
            "In addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98).",
            "The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.",
            "The contextual rules are restricted and may not be applicable to every example, but the spelling rules are generally applicable and should have good coverage.",
            "Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them.",
            "Abstract",
            "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.",
            "The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.",
            "The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).",
            "Unsupervised Models for Named Entity Classification Collins",
            "The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98).",
            "We are currently exploring other methods that employ similar ideas and their formal properties.",
            "We present two algorithms.",
            "Conclusions",
            "A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.",
            "The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed."
        ]
    },
    "E03-1005": {
        "input_sentences": [
            "Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.",
            "As our second experimental goal, we compared the models SL-DOP and LS-DOP explained in Section 3.2.",
            "Table 2 shows the results for sentences 100 words for various values of n. Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.",
            "This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).",
            "Conclusion",
            "Recall that for n=1, SL-DOP is equal to the PCFG-reduction of Bod (2001) (which we also called Likelihood-DOP) while LS-DOP is equal to Simplicity-DOP.",
            "An Efficient Implementation of a New DOP Model",
            "Abstract",
            "While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential processing time when the most probable parse tree is computed.",
            "While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.",
            "Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.",
            "This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.",
            "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.",
            "Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.",
            "This paper proposes an integration of the two models which outperforms each of them separately.",
            "But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP."
        ]
    },
    "P13-2083": {
        "input_sentences": [
            "A Structured Distributional Semantic Model for Event Co-reference",
            "We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models.",
            "We outlined an approach that introduces structure into distributed semantic representations gives us an ability to compare the identity of two representations derived from supposedly semantically identical phrases with different surface realizations.",
            "In the future, we would like to extend our model to other semantic tasks such as paraphrase detection, lexical substitution and recognizing textual entailment.",
            "Abstract",
            "Conclusion and Future Work",
            "We would also like to replace our syntactic relations to semantic relations and explore various ways of dimensionality reduction to solve this problem.",
            "We employed the task of event coreference to validate our representation and achieved significantly higher predictive accuracy than several baselines.",
            "In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods.",
            "We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature."
        ]
    },
    "N12-1086": {
        "input_sentences": [
            "Optimiza tion is also easy when there are additional terms in a graph objective suited to a specific problem; ourgeneric optimizer would simply require the compu tation of new partial derivatives, unlike prior workthat required specialized techniques for a novel ob jective function.",
            "Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach producessignificantly smaller lexicons and obtains bet ter predictive performance.",
            "Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties",
            "Finally, experiments on two natural language lexicon learning problems show that our methods produce better performance with respect to state-of-the-art graph-based SSL methods, and also result in much smaller lexicons.",
            "Conclusion",
            "Abstract",
            "To achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in Gaussian and entropic pairwise Markovnetworks constructed from labeled and unla beled data.",
            "Martins, Amar Subramanya, and Partha Talukdar for helpful discussion during the progress of thiswork and the three anonymous reviewers for their valuable feedback.",
            "Acknowledgments We thank Andre?",
            "We present novel methods to construct compact natural language lexicons within a graph based semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data.",
            "Ourmethods relax the oft-used assumption that the measures at each vertex form a normalized probabil ity distribution, making optimization and the use ofcomplex penalties easier than prior work.",
            "685",
            "This research was supported by Qatar Na tional Research Foundation grant NPRP 08-485-1-083, Google?s support of the Worldly Knowledge Project, andTeraGrid resources provided by the Pittsburgh Supercom puting Center under NSF grant number TG-DBS110003.",
            "We have presented a family of graph-based SSL objective functions that incorporate penalties encour aging sparse measures at each graph vertex.",
            "Sparse measures are desirable forhigh-dimensional multi-class learning problems such as the induction of labels on natu ral language types, which typically associate with only a few labels."
        ]
    },
    "W08-0406": {
        "input_sentences": [
            "Conclusion and Future Plans",
            "We have described a novel approach to word reordering in SMT, which successfully integrates syntactically motivated reordering in phrase-based SMT.",
            "Abstract",
            "We also want to examine the relation between word alignment method and the extracted rules and the relationship between reordering and word selection.",
            "Finally, a limitation of the current experiments is that they only allow rule-based external reorderings.",
            "We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT.",
            "We achieve an absolute improvement in translation quality of 1.1 % BLEU.",
            "Syntactic Reordering Integrated with Phrase-Based SMT",
            "On an English- Danish task, we achieve an absolute improvement in translation quality of 1.1 % BLEU.",
            "Manual evaluation supports the claim that the present approach is significantly superior to previous approaches.",
            "Since the SPTO scoring is not tied to a source reordering approach, we want to examine the effect of simply adding it as an additional parameter to the baseline PSMT system.",
            "This is achieved by reordering the input string, but scoring on the output string.",
            "This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules.",
            "Unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based SMT.",
            "As opposed to previous approaches, this neither biases against phrase internal nor external reorderings.",
            "In decoding, the alternatives are scored based on the output word order, not the order of the input.",
            "This way, all external reorderings are made possible, but only the rule-supported ones get promoted.",
            "In the future, we plan to apply this approach to English-Arabic translation.",
            "A result that is supported by manual evaluation, which shows that the SPTO approach is significantly superior to previous approaches.",
            "We expect greater gains, due to the higher need for reordering between these less-related languages."
        ]
    },
    "P09-2003": {
        "input_sentences": [
            "The crucial difference between previously proposed topdown RCG parsers and the new Earley-style algorithm is that while the former compute all clause instantiations during predict operations, the latter 4Of course, the use of constraints makes comparisons between items more complex and more expensive which means that for an efficient implementation, an integer-based representation of the constraints and adequate techniques for constraint solving are required.",
            "We have presented a new CYK and Earley parsing algorithms for the full class of RCG.",
            "A(x) \u2192 \u03b5 \u2208 P with an instantiation \u03c8 satisfying hp, Ci such that \u03c8(A(x)) = A(0) avoids this using a technique of dynamic updating of a set of constraints on range boundaries.",
            "Abstract",
            "The characteristic property of the Earley parser is that we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible.",
            "Experiments show that this significantly decreases the number of generated items, which confirms that range boundary constraint propagation is a viable method for a lazy computation of ranges.",
            "Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart.",
            "The Earley parser could be improved by allowing to process the predicates of the righthand sides of clauses in any order, not necessarily from left to right.",
            "We plan to include this strategy in future work.",
            "Conclusion and future work",
            "We present a CYK and an Earley-style algorithm for parsing Range Concatenation Grammar (RCG), using the deductive parsing framework.",
            "An Earley Parsing Algorithm for Range Concatenation Grammars",
            "This way, one could process predicates whose range boundaries are better known first."
        ]
    },
    "N12-1049": {
        "input_sentences": [
            "We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference.",
            "Furthermore, the system makes very few language-specific assumptions, and the algorithm could be adapted to domains beyond temporal resolution.",
            "FA8750-09-C-0181.",
            "Conclusion",
            "Acknowledgements The authors would like to thank Valentin Spitkovsky, David McClosky, and Angel Chang for valuable discussion and insights.",
            "Abstract",
            "We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.",
            "We also note that the approach is theoretically better adapted for phrases more complex than those found in TempEval-2.",
            "While most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a comof time This is used to construct a parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity.",
            "We hope to improve detection and explore system performance on multilingual and complex datasets in future work.",
            "Parsing Time: Learning to Interpret Time Expressions",
            "We achieve an accuracy of 72% on an adapted TempEval-2 task \u2013 comparable to state of the art systems.",
            "The system allows for output which captures uncertainty both with respect to the syntactic structure of the phrase and the pragmatic ambiguity of temporal utterances.",
            "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.",
            "We present a new approach to resolving temporal expressions, based on synchronous parsing of a fixed grammar with learned parameters and a compositional representation of time.",
            "In this way, we can employ a loosely supervised EM-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework."
        ]
    },
    "D07-1013": {
        "input_sentences": [
            "We have shown that these differences can be quantified and tied to theoretical expectations of each model, which may provide insights leading to better models in the future.",
            "Characterizing the Errors of Data-Driven Dependency Parsing Models",
            "Conclusion",
            "We have presented a thorough study of the dif ference in errors made between global exhaustivegraph-based parsing systems (MSTParser) and local greedy transition-based parsing systems (Malt Parser).",
            "Abstract",
            "We present a comparative error analysisof the two dominant approaches in datadriven dependency parsing: global, exhaus tive, graph-based models, and local, greedy, transition-based models.",
            "130",
            "This analysisleads to new directions for parser develop ment.",
            "We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models."
        ]
    },
    "C10-1045": {
        "input_sentences": [
            "First, we identify sources of syntactic ambiguity under studied in the existing parsing literature.",
            "This paper is based on work supported in part by DARPA through IBM.",
            "In this paper, we offer broad insightinto the underperformance of Arabic constituency parsing by analyzing the inter play of linguistic phenomena, annotationchoices, and model design.",
            "The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.",
            "The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship.",
            "We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation er rors.",
            "Conclusion",
            "Second, we show that although the PennArabic Treebank is similar to other tree banks in gross statistical terms, annotation consistency remains problematic.",
            "By establishing significantly higher parsing baselines, we have shown that Arabic parsing perfor mance is not as poor as previously thought, butremains much lower than English.",
            "Acknowledgments We thank Steven Bethard, Evan Rosen, and Karen Shiells for material contributions to this work.",
            "We are also grateful to Markus Dickinson, Ali Farghaly, Nizar Habash, Seth Kulick, David McCloskey, Claude Reichard, Ryan Roth, and Reut Tsarfaty for constructive discussions.",
            "Abstract",
            "Our results suggest that current parsing models would benefit frombetter annotation consistency and enriched anno tation in certain syntactic configurations.",
            "Third,we develop a human interpretable grammar that is competitive with a latent vari able PCFG.",
            "Fourth, we show how to build better models for three different parsers.Finally, we show that in application set tings, the absence of gold segmentation lowers parsing performance by 2?5% F1.",
            "401",
            "Better Arabic Parsing: Baselines Evaluations and Analysis",
            "With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus."
        ]
    },
    "H05-1094": {
        "input_sentences": [
            "Composition Of Conditional Random Fields For Transfer Learning",
            "AcknowledgmentsThis work was supported in part by the Center for Intelligent In formation Retrieval, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grants #IIS-0326249 and #IIS-0427594, and in part by the Defense Advanced Research Projects Agency (DARPA),through the Department of the Interior, NBC, Acquisition Ser vices Division, under contract number NBCHD030010.",
            "Our results suggest that off-the-shelf NLP tools will need not only to provide a single-best prediction, but also to be engineered so that they can easily communicate distributions over predictions to models for higher-level tasks.",
            "Many learning tasks have subtasks for which much training data exists.",
            "On two standard text data sets, we show that joint decoding outperforms cascaded decoding.",
            "Conclusion",
            "Specifically, we perform joint decoding ofseparately-trained sequence models, preserv ing uncertainty between the tasks and allowinginformation from the new task to affect predic tions on the old task.",
            "Abstract",
            "Therefore, we wantto transfer learning from the old, general purpose subtask to a more specific new task, for which there is often less data.",
            "This paper has implications for how such standard tools are pack aged.",
            "Anyopinions, findings and conclusions or recommendations ex pressed in this material are the author(s) and do not necessarily reflect those of the sponsor.",
            "In this paper we have shown that joint decoding improves transfer between interdependent NLP tasks, even when the old task is named-entity recognition, for which highly accurate systems exist.",
            "It is now common for researchers to publicly release trained models for standard tasks such as part-of-speechtagging, named-entity recognition, and parsing.",
            "The rich features afforded by aconditional model allow the new task to influence the pre 753 dictions of the old task, an effect that is only possible with joint decoding.",
            "While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old."
        ]
    },
    "E09-1038": {
        "input_sentences": [
            "For example parsing Arabic using the Arabic Treebank and the Buckwalter analyzer, or parsing English biomedical text using a biomedical treebank and the UMLS Specialist Lexicon.",
            "We show that using an external lexicon for dealing with rare lexical events greatly benefits a PCFG parser for Hebrew, and that results can be further improved by the incorporation of lexical probabilities estimated in a semi-supervised manner using a wide-coverage lexicon and a large unannotated corpus.",
            "This is achieved by defining a stochastic mapping layer between the two resources.",
            "Abstract",
            "Conclusions",
            "We present a framework for interfacing a parser with an external lexicon following a different annotation scheme.",
            "Lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora.",
            "In the future, we plan to integrate this framework with a parsing model that is specifically crafted to cope with morphologically rich, free-word order languages, as proposed in (Tsarfaty and Sima\u2019an, 2008).",
            "We present a framework for interfacing a PCFG parser with lexical information from an external resource following a different tagging scheme than the treebank.",
            "Unlike other studies (Yang Huang et al., 2005; Szolovits, 2003) in which such interfacing is achieved by a restricted heuristic mapping, we propose a novel, stochastic approach, based on a layered representation.",
            "Apart from Hebrew, our method is applicable in any setting in which there exist a small treebank and a wide-coverage lexical resource.",
            "We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens.",
            "Enhancing Unlexicalized Parsing Performance Using a Wide Coverage Lexicon Fuzzy Tag-Set Mapping and EM-HMM-Based Lexical Probabilities"
        ]
    },
    "C10-2096": {
        "input_sentences": [
            "Medium-scale experiments show an improvement of +0.9 BLEU points over a state-of-the-art forest-based baseline.",
            "Abstract",
            "In this paper, we have proposed a lattice-forestbased model to alleviate the problem of error propagation in traditional single-best pipeline frame work.",
            "Unlike previous works, which only focus onone module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-to string system.",
            "Machine Translation with Lattices and Forests",
            "We integrate both lat tice and forest into a single tree-to-stringsystem, and explore the algorithms of lattice parsing, lattice-forest-based rule ex traction and decoding.",
            "HR0011-06-C-0022, and DARPA under DOI-NBC Grant N10AP20031 (L. H and H. M).",
            "The work is sup ported by National Natural Science Foundation of China, Contracts 90920004 and 60736014, and 863 State Key Project No. 2006AA010108 (H. M and Q. L.), and in part by DARPA GALE Contract No.",
            "Traditional 1-best translation pipelinessuffer a major drawback: the errors of 1 best outputs, inevitably introduced by each module, will propagate and accumulate along the pipeline.",
            "The experimental results showthat our lattice-forest approach achieves an abso lute improvement of +0.9 points in term of BLEU score over a state-of-the-art forest-based model.",
            "For future work, we would like to pay more attention to word alignment between lattice pairs and forest pairs, which would be more principledthan our current method of word alignment be tween most-refined segmentation and string.",
            "In order to alleviate this problem, we use compact structures, lattice and forest, in each module insteadof 1-best results.",
            "844",
            "AcknowledgementWe thank Steve DeNeefe and the three anonymous reviewers for comments.",
            "More importantly,our model takes into account all the probabilities of different steps, such as segmen tation, parsing, and translation.",
            "We have explored the algorithms of lattice parsing, rule extraction and decoding.",
            "Conclusion and Future Work",
            "The main advantage of our model is that we can make global decision to search for the best segmentation, parse-tree and translation in one step.",
            "Ourmodel postpones the disambiguition of segmenta tion and parsing into the final translation step, so that we can make a more global decision to searchfor the best segmentation, parse-tree and transla tion in one step."
        ]
    },
    "E12-1047": {
        "input_sentences": [
            "While they provide some efficiency gains, they do not help with the main problem of longer sentences.",
            "Linear Context-Free Rewriting Systems",
            "We have presented a new technique for largescale parsing with LCFRS, which makes it possible to parse sentences of any length, with favorable accuracies.",
            "Our results show that optimal binarizations are clearly not the answer to parsing LCFRS efficiently, as they do not significantly reduce parsing complexity in our experiments.",
            "Conclusion",
            "Instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy.",
            "Abstract",
            "Efficient parsing with Linear Context-Free Rewriting Systems",
            "Previous work on treebank parsing with discontinuous constituents using Linear Rewriting systems has been limited to sentences of up to 30 words, for reasons of computational complexity.",
            "There have been some results on an a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible.",
            "The resulting parser has been applied to a discontinuous treebank with favorable results.",
            "The availability of this technique may lead to a wider acceptance of LCFRS as a syntactic backbone in computational linguistics."
        ]
    },
    "P05-1039": {
        "input_sentences": [
            "The high performance was due to three factors: (i) treebank transformations (ii) an integrated model of morphology in the form of a suffix analyzer and (iii) the use of smoothing in an unlexicalized grammar.",
            "Abstract",
            "There are two primary results: first, although LP/ID rules have been suggested as suitable for German\u2019s flexible word order, it appears that Markov rules actually perform better.",
            "To test if this was indeed the case, we re-ran the final experiment, but excluded the SBAR transformation.",
            "In this paper, we presented the best-performing parser for German, as measured by labelled bracket scores.",
            "While the SBAR transformation slightly reduces performance, recall that we argued the S GF transformation only made sense if the SBAR transformation is already in place.",
            "While we only presented results on the German NEGRA corpus, there is reason to believe that the techniques we presented here are also important to other languages where lexicalization provides little benefit: smoothing is a broadly-applicable technique, and if difficulties with lexicalization are due to sparse lexical data, then suffix analysis provides a useful way to get more information from lexical elements which were unseen while training.",
            "In addition to our primary results, we also provided a detailed error analysis which shows that PP attachment and co-ordination are problematic for our parser.",
            "Indeed, the suffix analyzer could well be of value in a lexicalized model.",
            "In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results.",
            "In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve labelled bracket of 76.2, higher than previously reported results on the NEGRA corpus.",
            "Moreover, there are possible paths for improvement: lexicalization could be added to the model, as could some of the treebank transformations suggested by Schiehlen (2004).",
            "5 Discussion",
            "Conclusions",
            "We did indeed find that applying S GF without the SBAR transformation reduced performance.",
            "Furthermore, while POS tagging is highly accurate, the error analysis also shows it does have surprisingly large effect on parsing errors.",
            "Because of the strong impact of POS tagging on parsing results, we conjecture that increasing POS tagging accuracy may be another fruitful area for future parsing research.",
            "What To Do When Lexicalization Fails: Parsing German With Suffix Analysis And Smoothing",
            "Second, adding suffix analysis provides a clear benefit, but only after the inclusion of the Coord GF transformation."
        ]
    },
    "A00-2005": {
        "input_sentences": [
            "Bagging and boosting, two effective machine learning techniques, are applied to natural language parsing.",
            "Abstract",
            "A semi-automated technique for doing this as well as examples from the Treebank that are inconsistently annotated were presented.",
            "This gain compares favorably with a bound on potential gain from increasing the corpus size.",
            "The economic basis for using ensemble methods will continue to improve with the increasing value (performance per price) of modern hardware.",
            "We have shown how to exploit the distribution created as a side-effect of the boosting algorithm to uncover inconsistencies in the training corpus.",
            "Conclusion",
            "Experiments using these techniques with a trainable statistical parser are described.",
            "Even though it is computationally expensive to create and evaluate a small (15-30) ensemble of parsers, the cost is far outweighed by the opportunity cost of hiring humans to annotate 40000 more sentences.",
            "Bagging And Boosting A Treebank Parser",
            "Neither of the algorithms exploit any specialized knowledge of the underlying parser induction algorithm, and the data used in creating the ensembles has been restricted to a single common training set to avoid issues of training data quantity affecting the outcome.",
            "That baseline system is the best known Treebank parser.",
            "The best resulting system provides roughly as large of a gain in F-measure as doubling the corpus size.",
            "Perhaps the biggest advantage of this technique is that it requires no a priori notion of how the inconsistencies can be characterized.",
            "Error analysis of the result of the boosting technique reveals some inconsistent annotations in the Penn Treebank, suggesting a semi-automatic method for finding inconsistent treebank annotations.",
            "It resulted in a statistically significant Fmeasure gain of 0.6 over the performance of the baseline parser.",
            "We have shown two methods, bagging and boosting, for automatically creating ensembles of parsers that produce better parses than any individual in the ensemble.",
            "Our best bagging system performed consistently well on all metrics, including exact sentence accuracy.",
            "Our boosting system, although dominated by the bagging system, also performed significantly better than the best previously known individual parsing result."
        ]
    },
    "D07-1091": {
        "input_sentences": [
            "Moreover, we expect to overcome the constraints of the currently implemented synchronous factored models by developing a more general asynchronous framework, where multiple translation steps mayoperate on different phrase segmentations (for instance a part-of-speech model for large scale re ordering).",
            "We are currently exploringthese possibilities, for instance use of syntactic in formation in reordering and models with augmented input information.We have not addressed all computational problems of factored translation models.",
            "Factored Translation Models",
            "We presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes.",
            "These experiments demonstrate that within the frameworkof factored translation models additional informa tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase based statistical approach.",
            "In fact, compu tational problems hold back experiments with morecomplex factored models that are theoretically pos sible but too computationally expensive to carry out.Our current focus is to develop a more efficient im plementation that will enable these experiments.",
            "Conclusion and Future Work",
            "Abstract",
            "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.",
            "The framework of factored translation models is very general.",
            "In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.",
            "Many more models that incorporatedifferent factors can be quickly built using the ex isting implementation.",
            "Acknowledgments This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No NR0011-06-C-0022 and inpart under the EuroMatrix project funded by the Eu ropean Commission (6th Framework Programme).We also benefited greatly from a 2006 summer workshop hosted by the Johns Hopkins Uni versity and would like thank the other workshop participants for their support and insights, namelyNicola Bertoldi, Ondrej Bojar, Chris Callison Burch, Alexandra Constantin, Brooke Cowan, Chris Dyer, Marcello Federico, Evan Herbst Christine Moran, Wade Shen, and Richard Zens.",
            "We reported on experiments that showed gains over standard phrase-based models, both in terms of automatic scores (gains of up to 2% BLEU), as well as a measure of grammatical coherence."
        ]
    },
    "E06-1010": {
        "input_sentences": [
            "We investigate a series of graph-theoretic constraints on non-projective dependency and their effect on i.e. whether they allow naturally occurring syntactic constructions to be adequately and i.e. whether they reduce the search space for the parser.",
            "The results indicate that, whereas complete linguistic coverage in principle requires unrestricted non-projective dependency graphs, limiting the degree of non-projectivity to at most 2 can reduce average running time from quadratic to linear, while excluding less than 0.5% of the dependency graphs found in the two treebanks.",
            "The constraints are evaluated experimentally using data from the Prague Dependency Treebank and the Danish Dependency Treebank.",
            "Conclusion",
            "Empirical experiments based on data from two treebanks, from different languages and with different annotation schemes, have shown that limiting the degree d of non-projectivity to 1 or 2 gives an average case running time that is linear in practice and allows us to capture about 98% of the dependency graphs actually found in the treebanks with d < 1, and about 99.5% with d < 2.",
            "This suggests that the integration of such constraints into non-projective parsing algorithms will improve both accuracy and efficiency, but we have to leave the corroboration of this hypothesis as a topic for future research.",
            "Abstract",
            "This is a substantial improvement over the commonly used projective approximation (degree 0), which excludes 15\u201325% of the graphs.",
            "In particular, we define a new measure the non-projectivity in an acyclic dependency graph obeying the single-head constraint.",
            "We have investigated a series of graph-theoretic constraints on dependency structures, aiming to find a better approximation than PROJECTIVITY for the structures found in naturally occurring data, while maintaining good parsing efficiency.",
            "This is a substantial improvement over the projective approximation, which only allows 75\u201385% of the dependency graphs to be captured exactly.",
            "In particular, we have defined the degree of nonprojectivity in terms of the maximum number of connected components that occur under a dependency arc without being dominated by the head of that arc.",
            "Constraints On Non-Projective Dependency Parsing"
        ]
    },
    "W12-3117": {
        "input_sentences": [
            "In total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data.",
            "We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques.",
            "Unfortunately, we did not submit our best configuration for the shared task.",
            "Conclusion",
            "The best results are obtained by our unconstrained system containing all features and using an E-SVR regression method with a Radial Basis Function kernel.",
            "Abstract",
            "In this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the Latent Dirichlet Allocation approach, and features based on source and target language syntax extracted using part-of-speech (POS) taggers and parsers.",
            "We also presented further experiments using different machine learning techniques and we evaluated the impact of two sets of features - one set which is based on linguistic features extracted using POS tagging and parsing, and a second set which is based on topic modelling.",
            "This setup leads to a Mean Average Error of 0.62 and a Root Mean Squared Error of 0.78.",
            "Two sets features are proposed: one i.e. respecting the data limitation suggested by the organisers, and one i.e. using data or tools trained on data that was not provided by the workshop organisers.",
            "We presented in this paper our submission for the WMT12 Quality Estimation shared task.",
            "Our immediate next steps are to continue to investigate the contribution of individual features, to explore feature selection in a more detailed fashion and to apply our best system to other types of data including sentences taken from an online discussion forum.",
            "This paper describes the features and the machine learning methods used by Dublin City (DCU) and the WMT 2012 quality estimation task.",
            "DCU-Symantec Submission for the WMT 2012 Quality Estimation Task",
            "We plan to continue working on the task of machine translation quality estimation."
        ]
    },
    "N01-1023": {
        "input_sentences": [
            "The algorithm iteratively labels the entire data set with parse trees.",
            "These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.",
            "We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilistic treatment.",
            "Abstract",
            "It is important to note that unlike previous studies, our method of moving towards unsupervised parsing can be directly compared to the output of supervised parsers.",
            "It uses a Co-Training method where a pair of models attempt to increase their agreement on labeling the data.",
            "The algorithm presented iteratively labels the unlabeled data set with parse trees.",
            "Unlike previous approaches to unsupervised parsing our method can be trained and tested on the kind of representations and the complexity of sentences that are found in the Penn Treebank.",
            "Conclusion",
            "The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text.",
            "We obtained 80.02% and 79.64% labeled bracketing precision and recall respectively.",
            "We then train a statistical parser on the combined set of labeled and unlabeled data.",
            "The baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and recall.",
            "We propose a novel Co-Training method for statistical parsing.",
            "Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.",
            "In this paper, we proposed a new approach for training a statistical parser that combines labeled with unlabeled data.",
            "Applying Co-Training Methods To Statistical Parsing",
            "The algorithm takes as input a small corpus of 9695 sentences (234467 word tokens) of bracketed data, a large pool of unlabeled text and a tag dictionary of lexicalized structures for each word in this training set (based on the LTAG formalism).",
            "In addition, as a byproduct of our representation we obtain more than the phrase structure of each sentence."
        ]
    },
    "W09-1207": {
        "input_sentences": [
            "In EMNLP/CoNLL- Chang and Chih-Jen Lin, 2001. a for support vector Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li.",
            "In the future, we will study how to solve the domain adaptive problem and how to do joint learning between syntactic and semantic parsing.",
            "The additional in-domain (devel) SRL data can help the in-domain test.",
            "Our CoNLL 2009 Shared Task system is composed of three cascaded components.",
            "dependency parser.",
            "In Jason Eisner.",
            "2000.",
            "The pseudoprojective high-order syntactic dependency model outperforms our CoNLL 2008 model (in English).",
            "Abstract",
            "Multilingual Dependency-based Syntactic and Semantic Parsing",
            "Conclusion and Future Work",
            "A cascaded syntactic and semantic dependency parsing system.",
            "Bilexical grammars and their cubicparsing algorithms.",
            "However, it is harmful to the ood test.",
            "Our final system achieves promising results.",
            "2008.",
            "In in Probabilistic"
        ]
    },
    "N09-1061": {
        "input_sentences": [
            "Discussion",
            "Our results generalize previous work on Synchronous Context-Free Grammar, and are particularly relevant for machine translation from or to languages that require syntactic analyses with discontinuous constituents.",
            "Abstract",
            "To conclude this paper, we now discuss a number of aspects of the results that we have presented, including various other pieces of research that are particularly relevant to this paper.",
            "The parsing complexity of an is exponential in both the of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which productions have rank at most and has minimal fan-out.",
            "Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation.",
            "Optimal Reduction of Rule Length in Linear Context-Free Rewriting Systems"
        ]
    },
    "E09-1055": {
        "input_sentences": [
            "As a final remark, we would like to point out that an alternative technique for efficient non-projective dependency parsing, developed by G\u00f3mez Rodr\u00edguez et al. independently of this work, is presented elsewhere in this volume.",
            "The obvious next step is the evaluation of our technique in the context of an actual parser.",
            "It is interesting to compare our approach with techniques for well-nested dependency trees (Kuhlmann and Nivre, 2006).",
            "Abstract",
            "Nevertheless, the coverage of our technique is actually higher than that of an approach that relies on well-nestedness, at least on the CoNLL 2006 data (see again Table 1).",
            "We have shown how to extract mildly contextsensitive grammars from dependency treebanks, and presented an efficient algorithm that attempts to convert these grammars into an efficiently parseable binary form.",
            "Due to previous results (Rambow and Satta, 1999), we know that this is not always possible.",
            "Out of these, only 24 (0.02%) could not be binarized using our technique.",
            "The results are given in Table 1.",
            "For efficient parsing, the extracted grammars need to be transformed in order to minimize the number of nonterminal symbols per production.",
            "2007TJNZRE_002.",
            "We take this number as an indicator for the usefulness of our result.",
            "Since it is easy to see that our algorithm always succeeds on context-free productions (productions where each nonterminal has fan-out 1), we evaluated our algorithm on the 102 687 productions with a higher fan-out.",
            "An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures.",
            "We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms.",
            "The work of the first author was funded by the Swedish Research Council.",
            "This raises the question about the practical relevance of our technique.",
            "Well-nestedness is a property that implies the binarizability of the extracted grammar; however, the classes of wellnested trees and those whose corresponding productions can be binarized using our algorithm are incomparable\u2014in particular, there are well-nested productions that cannot be binarized in our framework.",
            "Acknowledgements We would like to thank Ryan McDonald, Joakim Nivre, and the anonymous reviewers for useful comments on drafts of this paper, and Carlos G\u00f3mez Rodr\u00edguez and David J. Weir for making a preliminary version of their paper available to us.",
            "However, our algorithm may fail even in cases where a binarization exists\u2014our notion of adjacency is not strong enough to capture all binarizable cases.",
            "Our second contribution is an algorithm that computes this transformation for a large, empirically relevant class of grammars.",
            "Discussion",
            "Treebank Grammar Techniques for Non-Projective Dependency Parsing",
            "In order to get at least a preliminary answer to this question, we extracted LCFRS productions from the data used in the 2006 CoNLL shared task on data-driven dependency parsing (Buchholz and Marsi, 2006), and evaluated how large a portion of these productions could be binarized using our algorithm.",
            "First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks.",
            "The second author was partially supported by MIUR under project PRIN No.",
            "We see our results as promising first steps in a thorough exploration of the connections between non-projective and mildly context-sensitive parsing.",
            "In this paper, we provide two key tools for this approach."
        ]
    },
    "W11-2138": {
        "input_sentences": [
            "We also documented our experiments with several back-off techniques for English to Czech translation.",
            "We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words.",
            "We carried out experiments showing improvements in BLEU when using our method for translating into Czech, Finnish, German and Slovak with small parallel data.",
            "Finally, we described our primary submissions to the WMT 2011 Shared Translation Task.",
            "Conclusion",
            "We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation.",
            "We showed that gains in BLEU score increase with growing size of monolingual data.",
            "Abstract",
            "We also provide a description of the systems we",
            "On the other hand, growing parallel data size diminishes the effect of our method quite rapidly.",
            "This method called \u201creverse self-training\u201d improves the decoder\u2019s ability to produce grammatically correct translations into languages with morphology richer than the source language esp. in small-data setting.",
            "Improving Translation Model by Monolingual Data",
            "We introduced a technique for exploiting monolingual data to improve the quality of translation into morphologically rich languages.",
            "We discussed the issues of including similar translation models as separate components in MERT."
        ]
    },
    "D12-1127": {
        "input_sentences": [
            "The methods outlined in the paper are standard and easy to replicate, yet highly accurate and should serve as baselines for more complex proposals.",
            "Despite significant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks.",
            "However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps.",
            "Conclusion",
            "Wiki-ly Supervised Part-of-Speech Tagging",
            "We achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised Penn",
            "We have shown that the Wiktionary can be used to train a very simple model to achieve state-ofart weakly-supervised and out-of-domain POS taggers.",
            "Abstract",
            "Use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy.",
            "These encouraging results show that using free, collaborative NLP resources can in fact produce results of the same level or better than using expensive annotations for many languages.",
            "Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods.",
            "Furthermore, the Wiktionary contains other possibly useful information, such as glosses and translations.",
            "It would be very interesting and perhaps necessary to incorporate this additional data in order to tackle challenges that arise across a larger number of language types, specifically non-European languages.",
            "In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary."
        ]
    },
    "W11-2133": {
        "input_sentences": [
            "Further investigation is needed to determine if there is a suitable approximation that avoids computing probabilities across all n-grams.",
            "One downside to the MDI adaptation approach is that the computation of the normalization term z(h) is expensive and potentially prohibitive during continuous speech translation tasks.",
            "During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language\u2019s vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM).",
            "We also note that, unlike previous works involving topic modeling, we did not remove stop words and punctuation, but rather assumed that these features would have a relatively uniform topic distribution.",
            "Bilingual Latent Semantic Models",
            "We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task.",
            "Our topic modeling approach is simpler to construct than its counterparts.",
            "Abstract",
            "An alternative approach to bilingual topic modeling has been presented that integrates the PLSA framework with MDI adaptation that can effectively adapt a background language model when given a document in the source language.",
            "Preliminary experiments show a reduction in perplexity and an overall improvement in BLEU and NIST scores on speech translation.",
            "This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training.",
            "Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models",
            "Rather than training two topic models and enforcing a one-to-one correspondence for translation, we use the assumption that parallel texts refer to the same topics and have a very similar topic distribution.",
            "Conclusions"
        ]
    },
    "D12-1069": {
        "input_sentences": [
            "Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base of facts, and syntactic supervision in the form of a standard dependency parser.",
            "Experimental results show that our trained semantic parser extracts binary relations as well as a state-of-the-art weakly supervised relation extractor (Hoffmann et al., 2011).",
            "Weakly Supervised Training of Semantic Parsers",
            "Abstract",
            "On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.",
            "We presented an algorithm for training a semantic parser in the form of a probabilistic Combinatory Categorial Grammar, using these two types of weak supervision.",
            "Although these patterns capture a variety of linguistic phenomena, they require manual engineering and may miss important relations.",
            "This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments.",
            "To test this capability, we applied the trained parser to natural language queries against Freebase.",
            "An elegant aspect of semantic parsing is that it is easily extensible to include more complex linguistic phenomena, such as quantification and events (multi-argument relations).",
            "Together, these two experimental analyses suggest that the combination of syntactic and semantic weak supervision is indeed a sufficient basis for training semantic parsers for a diverse range of corpora and predicate ontologies.",
            "Further experiments tested our trained parser\u2019s ability to extract more complex meanings from sentences, including logical forms involving conjunctions of multiple relation and category predicates with shared arguments (e.g., Ax.MUSICIAN(x) \u2227 PERSONBORNIN(x, LONDON) \u2227 CITYINCOUNTRY(LONDON, ENGLAND)).",
            "We used this algorithm to train a semantic parser for an ontology of 77 Freebase predicates, using Freebase itself as the weak semantic supervision.",
            "In the future, we plan to increase the expressivity of our parser\u2019s meaning representation to capture more linguistic and semantic phenomena.",
            "In this fashion, we can make progress toward broad coverage semantic parsing, and thus natural language understanding.",
            "We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase.",
            "Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: supervision a knowledge base, supervision dependencyparsed sentences.",
            "This paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences.",
            "We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation.",
            "Discussion",
            "One limitation of our method is the reliance on hand-built dependency parse patterns for lexicon initialization.",
            "The semantic parser correctly interpreted 56% of these queries, despite the broad domain and never having seen an annotated logical form.",
            "An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al., 2010) to the weakly supervised setting.",
            "Such an algorithm seems especially important if one wishes to model phenomena such as adjectives, which are difficult to initialize heuristically without generating large numbers of lexical entries.",
            "We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences."
        ]
    },
    "P08-1028": {
        "input_sentences": [
            "We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.",
            "Abstract",
            "The applications of the framework discussed here are many and varied both for cognitive science and NLP.",
            "NLP tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (Coccaro and Jurafsky, 1998).",
            "This paper proposes a framework for representing the meaning of phrases and sentences in vector space.",
            "In this paper we presented a general framework for vector-based semantic composition.",
            "The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.",
            "Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.",
            "Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.",
            "Future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets.",
            "Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.",
            "Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.",
            "Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.",
            "Vector-based Models of Semantic Composition",
            "We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here.",
            "Further research is needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures (e.g., adjectivenoun, noun-noun) and also in terms of exploring the space of models more fully.",
            "We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "In particular, the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore.",
            "Discussion",
            "Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.",
            "We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al., 1988) and inductive inference (Heit and Rubinstein, 1994)."
        ]
    },
    "W05-0636": {
        "input_sentences": [
            "So far, we have not been able to show improvement over selecting the 1-best parse tree.",
            "Joint Parsing And Semantic Role Labeling",
            "Our results with Collins-style reranking are too preliminary to draw definite conclusions, but the potential improvement does not appear to be great.",
            "To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser.",
            "Abstract",
            "In this paper, we have considered several methods for reranking parse trees using information from semantic role labeling.",
            "In this paper, we confirm these results with a MaxEnt-trained SRL model, and we extend them to show that weighting the probabilities does not help either.",
            "In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses.",
            "Conclusion and Related Work",
            "Gildea and Jurafsky (Gildea and Jurafsky, 2002) also report results on reranking parses using an SRL system, with negative results.",
            "Our current results are negative, because a locallytrained SRL model can return inaccurate probability estimates.",
            "A striking feature of human syntactic prois that it is that is, it seems to take into account semantic information from the discourse context and world knowledge.",
            "In future work, we will explore the max-sum approach, which has promise to avoid the pitfalls of max-max reranking approaches."
        ]
    },
    "A00-2018": {
        "input_sentences": [
            "Abstract",
            "Indeed, it may be that adding this new parser to the mix may yield still higher results.",
            "This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].",
            "The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.",
            "That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.",
            "Conclusion",
            "Two aspects of this model deserve some comment.",
            "Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.",
            "As noted above, the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.",
            "This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].",
            "More important in our eyes, though, is the flexibility of the maximum-entropy-inspired model.",
            "The results reported here disprove this conjecture.",
            "A Maximum-Entropy-Inspired Parser *",
            "The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.",
            "We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.",
            "It is to this project that our future parsing work will be devoted.",
            "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.",
            "Though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing.",
            "The first is the slight, but important, improvement achieved by using this model over conventional deleted interpolation, as indicated in Figure 2.",
            "From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.",
            "Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming.",
            "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "Neither of these results were anticipated at the start of this research.",
            "We expect that as we experiment with other, more semantic conditioning information, the importance of this aspect of the model will increase."
        ]
    },
    "N06-1039": {
        "input_sentences": [
            "Preemptive Information Extraction Using Unrestricted Relation Discovery",
            "Sep. and Nov. (from Wikipedia).",
            "As its key technique, we presented Unrestricted Relation Discovery that tries to find parallel correspondences between multiple entities in a document, and perform clustering using basic patterns as features.",
            "Conclusion",
            "We presented the implementation of our preliminary system and its outputs.",
            "surface text patterns for a question answering system. of the 40th Annual Meeting of the As",
            "We obtained dozens of usable tables.",
            "Abstract",
            "The number of the source articles that contained a mention of the hurricane is shown in the right column.",
            "In this paper we proposed Preemptive Information Extraction as a new direction of IE research.",
            "To increase the number of basic patterns, we used a cluster of comparable articles instead of a single document.",
            "Rows with a star (*) were actually extracted."
        ]
    },
    "W06-2920": {
        "input_sentences": [
            "In this respect, it would be interesting to repeat experiments with the recently released new version of the TIGER treebank which now contains this information.",
            "The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.",
            "Abstract",
            "Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?",
            "Each year the Conference on Computational Natural Language Learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.",
            "Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing and helping with the also to Alexander Yeh for additional help with the paper reviews.",
            "Future research",
            "In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.",
            "CoNLL-X Shared Task On Multilingual Dependency Parsing",
            "We also hope that by combining parsers we can achieve even better performance, which in turn would facilitate the semi-automatic enlargement of existing treebanks and possibly the detection of remaining errors.",
            "One line of research that does not require additional annotation effort is defining or improving the mapping from coarse-grained to finegrained PoS tags.34 Another is harvesting and using large-scale distributional data from the internet.",
            "There are many directions for interesting research building on the work done in this shared task.",
            "This would create a positive feedback loop.",
            "We also give an overview of the parsing approaches that participants took and the results that they achieved.",
            "Finally one must not forget that almost all of the LEMMA, (C)POSTAG and FEATS values and even part of the FORM column (the multiword tokens used in many data sets and basically all tokenization for Chinese and Japanese, where words are normally not delimited by spaces) have been manually created or corrected and that the general parsing task has to integrate automatic tokenization, morphological analysis and tagging.",
            "Another is finding out how much of parsing performance depends on annotations such as the lemma and morphological features, which are not yet routinely part of treebanking efforts.",
            "His work was made possible by the MITRE Cor",
            "One is the question which factors make data sets \u201ceasy\u201d or difficult.",
            "We hope that the resources created and lessons learned during this shared task will be valuable for many years to come but also that they will be extended and improved by others in the future, and that the shared task website will grow into an informational hub on multilingual dependency parsing."
        ]
    },
    "P05-1065": {
        "input_sentences": [
            "Reading Level Assessment Using Support Vector Machines And Statistical Language Models",
            "Combining information from statistical LMs with other features using support vector machines provided the best results.",
            "In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level.",
            "Statistical LMs were used to classify texts based on reading level, with trigram models being noticeably more accurate than bigrams and unigrams.",
            "Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.",
            "This task can be addressed with natural language processing technology to assess reading level.",
            "Reading proficiency is a fundamental component of language competency.",
            "However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers.",
            "Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models.",
            "Abstract",
            "Conclusions and Future Work",
            "We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.",
            "Future work includes testing additional classifier features, e.g. parser likelihood scores and features obtained using a syntax-based language model such as Chelba and Jelinek (2000) or Roark (2001)."
        ]
    },
    "C04-1074": {
        "input_sentences": [
            "The paper aims at a deeper understanding of sev eral well-known algorithms and proposes ways to optimize them.",
            "The factors used in the algorithms and the algorithms themselves are evaluated on a Germancorpus annotated with syntactic and coreference in formation (Negra) (Skut et al, 1997).",
            "Abstract",
            "Optimizing Algorithms For Pronoun Resolution",
            "It describes and discusses factorsand strategies of factor interaction used in the algo rithms.",
            "A commonformat for pronoun resolution algorithms with sev eral open parameters is proposed, and the parameter settings optimal on the evaluation data are given."
        ]
    },
    "I05-6010": {
        "input_sentences": [
            "The information gained from cor pus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and extraction tool for German text corpora.",
            "Moreover,some examples are given that under line the necessity of integrating somekind of information other than gram mar sensu stricto into the treebank.",
            "Abstract",
            "This article is devoted to the problem of quantifying noun groups in German.After a thorough description of the phenom ena, the results of corpus-based in vestigations are described.",
            "We argue that a more sophisticatedand fine-grained annotation in the tree bank would have very positve effects onstochastic parsers trained on the tree bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source ofdata for theoretical linguistic investigations.",
            "Some remarks on the Annotation of Quantifying Noun Groups in Treebanks"
        ]
    },
    "W04-0305": {
        "input_sentences": [
            "When accuracy is plotted as a function of k (figure 1), we found that there is a large increase in accuracy when the first word of lookahead is added (only 2.7% F-measure below non-deterministic search).",
            "Given the pervasive ambiguity in natural language, it is not surprising that this drastic pruning strategy results in a large reduction in accuracy.",
            "Abstract",
            "If the family of deterministic parsers had more flexibility in this ordering, then the optimal deterministic parser could use an ordering which was tailored to the statistics of the data, thereby avoiding being forced to make decisions before sufficient information is available.",
            "When this pruning is applied directly after each word, there is a large reduction in accuracy (8.3% F-measure) as compared to the non-deterministic search.",
            "This suggests that one word lookahead is sufficient, but that other modifications to our left-corner parsing model could make deterministic parsing more effective.",
            "Deterministic parsing takes the extreme position that there can only be one analysis for any sentence prefix.",
            "One method which has been extensively used to address the difficulty of deterministic parsing is lookahead, where information about a bounded number of subsequent words is used to decide which analyses to pursue.",
            "To support incremental interpretation, any model of human sentence processing must not only process the sentence incrementally, it must to some degree restrict the number of analyses which it produces for any sentence prefix.",
            "The smooth curve of increasing performance as the lookahead is increased above one word is the type of results we would expect if the lookahead were simply correcting mistakes in this way.",
            "In the limit as lookahead increases, the performance of \u00a2Note that when the lookahead length is longer than the longest sentence, the deterministic and nondeterministic parsers become equivalent. the deterministic and non-deterministic parsers will become the same, no matter what family of deterministic parsers has been specified.",
            "The left-corner ordering completely determines when each decision about the phrase structure tree must be made.",
            "Lookahead In Deterministic Left-Corner Parsing",
            "Deterministic parsing is simulated by allowing the sequence of decisions between two words to be combined into a single parser action, and choosing the best single combined action based on the probability calculated using the basic left-corner probability model.",
            "We conclude that the first word of lookahead is necessary for the success of any deterministic parser, but that additional lookahead is probably not necessary.",
            "The large gain provided by the first word of lookahead indicates that this lookahead is necessary for deterministic parsing.",
            "Experiments with an incremental statistical parser show that performance is severely degraded when the search for the most probable parse is pruned to only the most probable analysis after each prefix.",
            "We simulate the optimal use of k-word lookahead by summing over all partial parses which continue the given partial parse to the point where all k words in the lookahead have been generated.",
            "When expressed in terms of search, this means that the deterministic pruning is done k words behind a non-deterministic search for the best parse, based on a sum over the partial parses found by the non-deterministic search.",
            "Examples of possible limitations to the family of deterministic parsers assumed here include the choice of the left-corner ordering of parser decisions.",
            "As the lookahead increases, some previously mistaken choices will become disambiguated by the additional lookahead information, thereby improving performance.",
            "Further increases in the lookahead length have much less of an impact.",
            "We simulate the effects of lookahead by summing probabilities over possible parses for the lookahead words and using this sum to choose which parse to pursue.",
            "We find that a large improvement is achieved with one word lookahead, but that more lookahead results in relatively small additional improvements.",
            "Using an unconstrained search procedure to try to find the most probable parse according to this probability model (i.e. nondeterministic parsing) results in state-of-the-art accuracy.",
            "We believe that one word of lookahead is both necessary and sufficient for a model of deterministic parsing.",
            "Conclusions",
            "We use a neural network to estimate the probabilities for an incremental history-based probability model based on leftcorner parsing.",
            "For this reason, deterministic parsers usually use some form of lookahead.",
            "The remaining error created by this model of deterministic parsing is probably best dealt with by investigating other aspect of the model of deterministic parsing assumed here, in particular the strict adherence to the left-corner parsing order.",
            "The performance curves in figure 1 also suggest that one word of lookahead is sufficient.",
            "Discussion",
            "terministic parsing by characterizing these issues in terms of the search procedure used by a statistical parser.",
            "Given the fact the with one word of lookahead the F-measure of the deterministic parser is only 2.7% below the maximum possible, it is unlikely that the family of deterministic parsers assumed here is so sub-optimal that the entire 5.6% improvement gained with one word lookahead is simply the result of compensating for limitations in the choice of this family.",
            "This fact is important in making the non-deterministic search strategy used with this parser tractable.",
            "Any limitations in this family will result in the deterministic search making choices before the necessary disambiguating information is available, thereby leading to additional errors.",
            "The observations made in this paper could lead to more sophisticated search strategies which further increase the speed of this or similar parsers without significant reductions in accuracy.",
            "We believe the gain provided by more than one word of lookahead is the result of compensating for limitations in the family of deterministic parsers assumed here.",
            "Despite the need to consider alternatives to the left-corner parsing order, these results do demonstrate that the left-corner parsing strategy proposed is surprisingly good at supporting deterministic parsing.",
            "The large improvement in performance which results from adding the first word of lookahead, as compared to adding the subsequent words, indicates that the first word of lookahead has a qualitatively different effect on deterministic parsing.",
            "Lookahead gives the parser more information about the sentence at the point when the choice of the next parser action takes place.",
            "All parses which do not use this chosen action are then pruned from the search."
        ]
    },
    "D11-1006": {
        "input_sentences": [
            "The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.",
            "Acknowledgements: We would like to thank Kuzman Ganchev, Valentin Spitkovsky and Dipanjan Das for numerous discussions on this topic and comments on earlier drafts of this paper.",
            "Multi-Source Transfer of Delexicalized Dependency Parsers",
            "This suggests that even better transfer models can be produced by separately weighting each of the sources depending on the target language \u2013 either weighting by hand, if we know the language group of the target language, or automatically, if we do not.",
            "Abstract",
            "This advantage does not come simply from having more data.",
            "Preliminary experiments for Arabic (ar), Chinese (zh), and Japanese (ja) suggest similar direct transfer methods are applicable.",
            "As previously mentioned, the latter has been explored in both S\u00f8gaard (2011) and Cohen et al. (2011).",
            "In this setting only Indo-European languages are used as source data.",
            "This observation should lead to a new baseline in unsupervised and projected grammar induction \u2013 the UAS of a delexicalized English parser.",
            "We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.",
            "However, this is not necessary as treebanks are available for a number of language groups, e.g., Indo-European, Altaic, Semitic, and Sino-Tibetan.",
            "In particular, if one can transfer part-of-speech tags, then a large part of transferring unlabeled dependencies has been solved.",
            "We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.",
            "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.",
            "We then use a constraint driven learning algorithm to adapt the transferred parsers to the respective target language, obtaining an additional 16% error reduction on average in a multi-source setting.",
            "We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.",
            "Of course, our experiments focus strictly on IndoEuropean languages.",
            "Our final parsers achieve state-of-the-art accuracies on eight Indo-European languages, significantly outperforming previous unsupervised and projected systems.",
            "Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.",
            "For example, on the CoNLL test sets, a DMV model obtains UAS of 28.7/41.8/34.6% for ar/zh/ja respectively, whereas an English direct transfer parser obtains 32.1/53.8/32.2% and a multi-source direct transfer parser obtains 39.9/41.7/43.3%.",
            "Conclusions",
            "Thus, even across language groups direct transfer is a reasonable baseline.",
            "In fact, if we randomly sampled from the multi-source data until the training set size was equivalent to the size of the English data, then the results still hold (and in fact go up slightly for some languages).",
            "Discussion",
            "We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders S\u00f8gaard for sharing early drafts of their recent related work.",
            "One fundamental point the above experiments illustrate is that even for languages for which no resources exist, simple methods for transferring parsers work remarkably well.",
            "Central to our approach is the idea of delexicalizing the models, which combined with a standardized part-of-speech tagset allows us to directly transfer models between languages.",
            "Multi-Source Transfer",
            "The second fundamental observation is that when available, multiple sources should be used.",
            "Even through naive multi-source methods (concatenating data), it is possible to build a system that has comparable accuracy to the single-best source for all languages."
        ]
    },
    "P03-1013": {
        "input_sentences": [
            "The results show that the baseline model achieves a performance of up to 73% recall and 70% precision.",
            "However, the learning curves for the three models failed to produce any evidence that they suffer from sparse data.",
            "In Experiment 2, we therefore investigated an alternative hypothesis: the poor performance of the lexicalized models is due to the fact that the rules in Negra are flatter than in the Penn Treebank, which makes lexical head-head dependencies less useful for correctly determining constituent boundaries.",
            "The results show that sister-head dependencies improve parsing performance not only for NPs (which is well-known for English), but also for PPs, VPs, Ss, and coordinate categories.",
            "This finding is at odds with what has been reported for parsing models trained on the Penn Treebank.",
            "Abstract",
            "It can be hypothesized that this finding carries over to other treebanks that are annotated with flat structures.",
            "We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.",
            "Both lexicalized models perform substantially worse.",
            "Such annotation schemes are often used for languages that (unlike English) have a free or semi-free wordorder.",
            "This model uses lexical sisterhead dependencies, which makes it particularly suitable for parsing Negra\u2019s flat structures.",
            "We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.",
            "Learning curves show that this effect is not due to lack of training data.",
            "This model achieves up to 74% recall and precision, thus outperforming the unlexicalized baseline model.",
            "We present a probabilistic parsing model for German trained on the Negra treebank.",
            "As a possible explanation we considered lack of training data: Negra is about half the size of the Penn Treebank.",
            "This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.",
            "We presented the first probabilistic full parsing model for German trained on Negra, a syntactically annotated corpus.",
            "The best performance was obtained for a model that uses sister-head dependencies for all categories.",
            "Testing our sister-head model on these languages is a topic for future research.",
            "Conclusions",
            "Based on this assumption, we proposed an alternative model hat replaces lexical head-head dependencies with lexical sister-head dependencies.",
            "This can the thought of as a way of binarizing the flat rules in Negra.",
            "Probabilistic Parsing For German Using Sister-Head Dependencies",
            "This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.",
            "The flatness of the Negra annotation reflects the syntactic properties of German, in particular its semi-free wordorder.",
            "In Experiment 1, we applied three standard parsing models from the literature to Negra: an unlexicalized PCFG model (the baseline), Carroll and Rooth\u2019s (1998) head-lexicalized model, and Collins\u2019s (1997) model based on head-head dependencies."
        ]
    },
    "W08-1007": {
        "input_sentences": [
            "We report a labeled attachment score close to 90% for dependency versions of the TIGER and T\u00a8uBa- D/Z treebanks.",
            "We can report state-of-the-art results for parsing the dependency versions of two German treebanks, and we have demonstrated, with promising results, how a dependency parser can parse full constituent structures by encoding the inverse mapping in complex arc labels of the dependency graph.",
            "Conclusion",
            "A Dependency-Driven Parser for German Dependency and Constituency Representations",
            "Abstract",
            "Moreover, the parser is able to recover both constituent labels and grammatical functions with an F-Score over 75% for T\u00a8uBa-D/Z and over 65% for TIGER.",
            "We believe that this method can be improved by using, for example, head-finding rules.",
            "We have shown that a transition-based dependencydriven parser can be used for parsing German with both dependency and constituent representations.",
            "We present a dependency-driven parser that parses both dependency structures and constituent structures.",
            "Constituency representations are automatically transformed into dependency representations with complex arc labels, which makes it possible to recover the constituent structure with both constituent labels and grammatical functions."
        ]
    },
    "P11-1086": {
        "input_sentences": [
            "We hope that by modeling context in both axes, we will be able to completely replace composed-rule grammars with smaller minimal-rule grammars.",
            "First, our rule Markov models dramatically improve a grammar of minimal rules, giving an improvement of 2.3 B .",
            "Conclusion",
            "In this paper, we have investigated whether we can eliminate composed rules without any loss in translation quality.",
            "Most statistical machine translation systems on rules that can be formed out of smaller rules in the grammar).",
            "Abstract",
            "Finally, when we compare against full composed rules, we find that we can reach the same level of performance under some conditions, but in order to do so consistently, we believe we need to extend our model to condition on horizontal context in addition to vertical context.",
            "We draw three main conclusions from our experiments.",
            "Rule Markov Models for Fast Tree-to-String Translation",
            "Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both trainand decoding Here, we take the approach, where we only use minrules that cannot be formed out other rules), and instead rely on a model the derivation history to capture dependencies between minimal rules.",
            "Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.",
            "We have developed a rule Markov model that captures vertical bigrams and trigrams of minimal rules, and tested it in the framework of treeto-string translation.",
            "Second, when we compare against vertically composed rules we are able to get about the same B score, but our model is much smaller and decoding with our model is faster."
        ]
    },
    "N06-2033": {
        "input_sentences": [
            "Discussion",
            "We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.",
            "We have presented a reparsing scheme that produces results with accuracy higher than the best individual parsers available by combining their results.",
            "Abstract",
            "By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.",
            "Parser Combination By Reparsing",
            "In constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score.",
            "We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.",
            "We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers."
        ]
    },
    "W10-2924": {
        "input_sentences": [
            "We give an efficient labeling algorithm that is analogous to parsing using dynamic programming.",
            "There are several possible directions for extending the current approach.",
            "Also, it will be interesting to see if a kernel that computes the similarity between subcard-pyramids could be developed and used for relation classification.",
            "Abstract",
            "In that case, instead of using multiple probabilistic classifiers, one could employ a single jointlytrained probabilistic model, which is theoretically more appealing and might give better results.",
            "For example, by using dependency-based kernels (Bunescu and Mooney, 2005a; Kate, 2008) or syntactic kernels (Qian et al., 2008; Moschitti, 2009) or by including the word categories and their POS tags in the subsequences.",
            "In that case, it should be possible to binarize those relations and then use card-pyramid parsing.",
            "Experimental results show improved results for our joint extraction method compared to a pipelined approach.",
            "We present a new method for joint entity and relation extraction using a graph we call a \u201ccard-pyramid.\u201d This graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes.",
            "If the relations are between relations instead of between entities, then card-pyramid parsing can handle it by considering the labels of the immediate children as RHS nonterminals instead of the labels of the left-most and the right-most leaves beneath it.",
            "We presented an efficient parsing algorithm for jointly labeling a card-pyramid using dynamic programming and beam search.",
            "The experiments demonstrated the benefit of our joint extraction method over a pipelined approach.",
            "Both entity and relation extraction can benefit from being performed jointly, allowing each task to correct the errors of the other.",
            "We introduced a card-pyramid graph structure and presented a new method for jointly extracting entities and their relations from a sentence using it.",
            "In this work, and in most previous work, relations are always considered between two entities.",
            "The card-pyramid structure could be used to perform other languageprocessing tasks jointly with entity and relation extraction.",
            "A card-pyramid compactly encodes the entities and relations in a sentence thus reducing the joint extraction task to jointly labeling its nodes.",
            "Joint Entity and Relation Extraction Using Card-Pyramid Parsing",
            "Conclusions",
            "However, there could be relations between more than two entities.",
            "Thus, it would be interesting to apply card-pyramid parsing to extract higher-order relations (such as causal or temporal relations).",
            "Finally, we note that a better relation classifier could be used in the current approach which makes more use of linguistic information.",
            "Future Work",
            "Given the regular graph structure of the cardpyramid, it would be interesting to investigate whether it can be modeled using a probabilistic graphical model (Koller and Friedman, 2009).",
            "For example, co-reference resolution between two entities within a sentence can be easily incorporated in card-pyramid parsing by introducing a production like coref \u2014* Person Person, indicating that the two person entities are the same."
        ]
    },
    "P11-1141": {
        "input_sentences": [
            "In Pro",
            "Abstract",
            "Chinese word segmentation as tagging.",
            "Lan- 11(2):207\u2013238.",
            "Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005).",
            "Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too.",
            "2003.",
            "We gave good reasons why this should be done, and we presented an effective method showing how this could be done.",
            "The most important reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6.",
            "Finally, we must note that the use of flat labels such as \u201cNNf\u201d is less than ideal.",
            "Linguistics and Language 8(1):29\u201348.",
            "In this paper we proposed a new paradigm for Chinese word segmentation in which not only flat words were identified but words with structures were also parsed.",
            "We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications.",
            "Chinese segmentawith a word-based perceptron algorithm.",
            "Yue Zhang and Stephen Clark.",
            "The problem with this approach is that word structures and phrase structures are then not treated in a truly unified way, and besides the 33 part-of-speech tags originally contained in Penn Chinese Treebank, another 33 tags ending with \u2018f\u2019 are introduced.",
            "Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006).",
            "Conclusion and Discussion",
            "Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation",
            "annotation of a large corpus.",
            "Nianwen Xue.",
            "We leave this problem open for now and plan to address it in future work.",
            "We showed that word structures can be recovered with high precision, though there\u2019s still much room for improvement, especially for higher level constituent parsing.",
            "2007.",
            "With the progress in statistical parsing technology and the development of large scale treebanks, the time has now come for this paradigm shift to happen."
        ]
    },
    "P12-1051": {
        "input_sentences": [
            "Drawing this connection facilitates a greater flow of ideas in the research community, allowing semantic parsing to leverage ideas from other work with tree automata, while making clearer how seemingly isolated efforts might relate to one another.",
            "Semantic Parsing with Bayesian Tree Transducers",
            "Conclusion",
            "However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata.",
            "In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.",
            "Abstract",
            "We have argued that tree transformation based semantic parsing can benefit from the literature on formal language theory and tree automata, and have taken a step in this direction by presenting a tree transducer based semantic parser.",
            "The new VB algorithm results in an overall performance improvement for the transducer over EM training, and the general effectiveness of the approach is further demonstrated by the Bayesian transducer achieving highest accuracy among other tree transformation based approaches.",
            "We demonstrate this by both building on previous work in training tree transducers using EM (Graehl et al., 2008), and describing a general purpose variational inference algorithm for adapting tree transducers to the Bayesian framework.",
            "Many semantic parsing models use tree transformations to map between natural language and meaning representation.",
            "This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions."
        ]
    },
    "E12-1083": {
        "input_sentences": [
            "Structural and Topical Dimensions in Multi-Task Patent Translation",
            "In this paper we analyze patents along the orthogonal dimensions of topic and textual structure.",
            "Conclusion",
            "The most straightforward approach to improve machine translation performance on patents is to enlarge the training set to include all available data.",
            "We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters.",
            "Abstract",
            "This question has been investigated by Tins",
            "Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents.",
            "A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs.",
            "We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance.",
            "We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning."
        ]
    },
    "P07-1108": {
        "input_sentences": [
            "And the translation quality is comparable with that of the model directly trained with 30,000 French-Spanish sentence pairs.",
            "Moreover, with small bilingual corpus available, our method can further improve the translaquality by using the additional bilingual corpora.",
            "Abstract",
            "With a small bilingual corpus available for Lf-Le, we built a translation model, and interpolated it with the pivot model trained with the large Lf-Lp and Lp-Le bilingual corpora.",
            "To conduct transbetween languages with a small bilingual corpus, we bring in a third which is named the lan- For and there exist bilingual corpora.",
            "The results indicate that using larger training corpora to train the pivot model leads to better translation quality.",
            "The less the Lf-Le bilingual corpus is, the bigger the improvement is.",
            "Conclusion",
            "The results on both the Europarl corpus and Chinese-Japanese translation indicate that the interpolated models achieve the best results.",
            "Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly with 5,000 sentence pairs for French-Spanish translation.",
            "Pivot Language Approach for Phrase-Based Statistical Machine Translation",
            "We also performed experiments using Lf-Lp and Lp-Le corpora of different sizes.",
            "Results also indicate that our pivot language approach is suitable for translation on language pairs with a small bilingual corpus.",
            "To perform translation between Lf and Le, we bring in a pivot language Lp, via which the large corpora of Lf-Lp and Lp-Le can be used to induce a translation model for Lf-Le.",
            "The results also indicate that using more pivot languages leads to better translation quality.",
            "This paper proposes a novel method for phrase-based statistical machine translation by using pivot language.",
            "Using only bilingual corpora, we can build a model for The advantage of this method lies in that we can perform between even if there is no bilingual corpus available for this language pair.",
            "This paper proposed a novel method for phrasebased SMT on language pairs with a small bilingual corpus by bringing in pivot languages.",
            "The advantage of this method is that it can perform translation between the language pair Lf-Le even if no bilingual corpus for this pair is available.",
            "Using BLEU as a metric, our method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 sentence pairs for FrenchSpanish translation."
        ]
    },
    "W03-1509": {
        "input_sentences": [
            "And fine-tune to a specific domain such as sports.",
            "Though many approaches have been tried, the result is still not satisfactory.",
            "The F-measure of person names, location names, and organization names on the newswire test data for the 1999 IEER evaluation in Mandarin is 86.84%, 84.40% and 76.22% respectively.",
            "So in the future we will shift our algorithm to other languages.",
            "Our lab was mainly devoted to cross-language information processing and its application.",
            "Unfortunately, Chinese Named Entity Recognition (NER) is more difficult for the lack of capitalization information and the uncertainty in word segmentation.",
            "Abstract",
            "In order to avoid data sparseness problem, we employ a back-off model and YI CI CI LIN a Chinese thesaurus, to smooth the parameters in the model.",
            "The main contribution of this paper is putting forward an approach which can make up for the limitation of using the statistical model or human knowledge purely by combining them organically.",
            "Chinese NER is a more difficult task than English NER.",
            "In this paper, we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well.",
            "Thus we only need a relative small-sized labeled corpus (onemonth\u2019s Chinese People\u2019s Daily tagged with NER tags at Peking University) and human knowledge, but can achieve better performance.",
            "Conclusions",
            "Chinese Named Entity Recognition Combining Statistical Model Wih Human Knowledge",
            "In this paper, we present a hybrid algorithm of incorporating human knowledge into statistical model.",
            "Named Entity Recognition is one of the key techniques in the fields of natural language processing, information retrieval, question answering and so on."
        ]
    },
    "P05-1053": {
        "input_sentences": [
            "Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance.",
            "Abstract",
            "This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.",
            "In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.",
            "Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.",
            "In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.",
            "While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.",
            "Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.",
            "Discussion and Conclusion",
            "Evaluation on the ACE corpus shows that base phrase chunking contributes to most of the performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.",
            "The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.",
            "Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.",
            "The effective incorporation of diverse features enables our system outperform previously bestreported systems on the ACE corpus.",
            "Last, effective ways need to be explored to incorporate information embedded in the full parse trees.",
            "Extracting semantic relationships between entities is challenging.",
            "Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.",
            "We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.",
            "Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.",
            "Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.",
            "Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.",
            "This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.",
            "Exploring Various Knowledge In Relation Extraction",
            "Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information first.",
            "Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.",
            "This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other."
        ]
    },
    "E12-3010": {
        "input_sentences": [
            "Abstract",
            "Secondly, an overt pronoun cannot be restored from a finite impersonal verb without making the sentence ungrammatical; therefore, our approach is not useful for treating impersonal sentences.",
            "Also, in order to correctly render the meaning of a preprocessed sentence, we plan to mark restored subject pronouns in such a way that the information about their absence/presence in the original text is preserved as a feature in parsing and translation.",
            "We then improved the rule-based system using a rule-based preprocessor to restore pro-drop as overt pronouns and a statistical post-editor to correct the translation.",
            "Finally, we would like to use a larger corpus to train the SPE component and compare the effects of utilizing machine translations on the target side versus human reference translations.",
            "As already mentioned, we have not found the solution to some problems yet.",
            "Next, we evaluate null subjects\u2019 translation into French, a \u201cnon prodrop\u201d language.",
            "As it is a rule-base system, some problems as the subject pronoun mistranslation in subordinated sentences can be fixed by means of more specific rules and heuristics.",
            "In particular, the system now generates more pronouns in French than before, confirming the advantage of using a combination of preprocessing and postediting with rule-based machine translation.",
            "Improving machine translation of null subjects in Italian and Spanish",
            "Conclusion",
            "A second evaluation of the improved Its-2 system shows an average increase of 15.46% in correct pro-drop translations for Italian-French and 12.80% for",
            "Results obtained from the second evaluation showed an improvement in the translation of both sorts of pronouns.",
            "As a consequence, we think that an annotation of the empty category, as done by Chung and Gildea (2010), could provide better results.",
            "Null subjects are non overtly expressed pronouns found in languages such as Italian and Spanish.",
            "Future work",
            "For example, binding theory does not contain any formalization on gender, reason why a specific statistical component could be a more ideal option in order to tackle aspects such as masculine/feminine pronouns.",
            "We also evaluated its translation into a \u201cnon pro-drop\u201d language, that is, French, obtaining better results for personal pro-drop than for impersonal pro-drop, for both Its-2 and Moses, the two MT systems we tested.",
            "We use the Europarl corpus to evaluate two MT systems on their performance regarding null subject translation: Its-2, a rule-based system developed at LATL, and a statistical system built using the Moses toolkit.",
            "Besides, we would like to further explore variations on the plain SPE technique, for example, by injecting Moses translation of sentences being translated into the phrase-table of the post-editor (Chen and Eisele, 2010).",
            "In this study we quantify and compare the occurrence of this phenomenon in these two languages.",
            "Then we add a rule-based preprocessor and a statistical post-editor to the Its-2 translation pipeline.",
            "First of all, we would like to include an AR module in our system.",
            "In this paper we measured and compared the occurrence of one syntactic feature \u2013 the null subject parameter \u2013 in Italian and Spanish.",
            "Besides, an approach based on binding theory (B\u00a8uring, 2005) could be effective as deep syntactic information is available, even though limited."
        ]
    },
    "W11-0314": {
        "input_sentences": [
            "ULISSE: an Unsupervised Algorithm for Detecting Reliable Dependency Parses",
            "ULISSE shows a promising performance against the output of two supervised parsers selected for their behavioral differences.",
            "Different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains.",
            "Conclusion",
            "In all cases, ULISSE appears to outperform the baseline algorithms.",
            "The fact of carrying out the task of reliable parse selection in a supervised scenario represents an important novelty: however, the unsupervised nature of ULISSE could also be used in an unsupervised scenario (Reichart and Rappoport, 2010).",
            "Abstract",
            "In all experiments, ULISSE outperforms all baselines, including dPUPA and Sentence Length (SL), the latter representing a very strong baseline selection method in a supervised scenario, where parsers have a very high performance with short sentences.",
            "Current direction of research include a careful study of a) the quality score function, in particular for what concerns the combination of individual feature weights, and b) the role and effectivess of the set of linguistic features.",
            "This is the case, for instance, of the domain adaptation task in a self\u2013training scenario (McClosky et al., 2006), of the treebank construction process by minimizing the human annotators\u2019 efforts (Reichart and Rappoport, 2009b), of n\u2013best ranking methods for machine translation (Zhang, 2006).",
            "In this paper we present ULISSE, an unsupervised linguistically\u2013driven algorithm to select reliable parses from the output of a dependency parser.",
            "To our knowledge, it represents the first unsupervised ranking algorithm operating on dependency representations which are more and more gaining in popularity and are arguably more useful for some applications than constituency parsers.",
            "This study is being carried out with a specific view to NLP tasks which might benefit from the ULISSE algorithm.",
            "ULISSE is an unsupervised linguistically\u2013driven method to select reliable parses from the output of dependency parsers."
        ]
    },
    "D07-1015": {
        "input_sentences": [
            "This paper provides an algorithmic framework for learning statistical models involv ing directed spanning trees, or equivalently non-projective dependency structures.",
            "Although we concentrate on loglinear and max-margin estimation, the inference al gorithms we present can serve as black-boxes in many other statistical modeling techniques.",
            "Abstract",
            "6/21/1998.",
            "Both results are significant with p ? 0.001.",
            "Out of 2,472 sentences total, log-linear models gave improved parses over the perceptron on 448 sentences, and worse parses on 343 sentences.",
            "Our experiments suggest that marginal-basedtraining produces more accurate models than per ceptron learning.",
            "Xavier Carreraswas supported by the Catalan Ministry of Innova tion, Universities and Enterprise, and a grant from NTT, Agmt.",
            "We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff?s Matrix-Tree Theorem.",
            "Michael Collins was funded by NSF grants 0347631 and DMS-0434222.",
            "149",
            "The new training methods give improvements in accuracy over perceptron-trained models.",
            "7We ran the sign test at the sentence level to measure the statistical significance of the results aggregated across the six languages.",
            "Notably, this is the first large-scale application of the EG algorithm, and shows that it is a promising approach for structured learning.",
            "In addition, the authors gratefully acknowledge the follow ing sources of support.",
            "This paper describes inference algorithms forspanning-tree distributions, focusing on the funda mental problems of computing partition functionsand marginals.",
            "AcknowledgmentsThe authors would like to thank the anonymous reviewers for their constructive comments.",
            "Dtd.",
            "To demonstrate an application of the method, we perform experiments which use the algorithm in training both log-linear and max-margin dependency parsers.",
            "Structured Prediction Models via the Matrix-Tree Theorem",
            "In line with McDonald et al (2005b), we confirmthat spanning-tree models are well-suited to depen dency parsing, especially for highly non-projective languages such as Dutch.",
            "Conclusions",
            "The max-margin method gave improved/worse parses for 500/383 sentences.",
            "Moreover, spanning-treemodels should be useful for a variety of other prob lems involving structured data.",
            "Terry Koo was funded by a grant from the NSF (DMS-0434222) and a grantfrom NTT, Agmt.",
            "Amir Globerson was supported by a fellowship from the Roth schild Foundation - Yad Hanadiv."
        ]
    },
    "P13-1109": {
        "input_sentences": [
            "Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data.",
            "Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.",
            "Conclusion",
            "However, oovs can be considered as n-grams (phrases) instead of unigrams.",
            "Our results showed improvement over the baselines both in intrinsic evaluations and on BLEU.",
            "Increasing the size of parallel corpus on one hand reduces the number of oovs.",
            "Future work includes studying the effect of size of parallel corpus on the induced oov translations.",
            "In this paper, we propose a novel approach to finding translations for oov words.",
            "Abstract",
            "We presented a novel approach for inducing oov translations from a monolingual corpus on the source side and a parallel data using graph propagation.",
            "Currently, we find paraphrases for oov words.",
            "We also plan to explore different graph propagation objective functions.",
            "Regularizing these objective functions appropriately might let us scale to much larger data sets with an order of magnitude more nodes in the graph.",
            "We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases.",
            "Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation",
            "In this scenario, we also can look for paraphrases and translations for phrases containing oovs and add them to the phrase-table as new translations along with the translations for unigram oovs.",
            "But, on the other hand, there will be more labeled paraphrases that increases the chance of finding the correct translation for oovs in the test set.",
            "Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations."
        ]
    },
    "P05-1013": {
        "input_sentences": [
            "We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.",
            "Pseudo-Projective Dependency Parsing",
            "In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures.",
            "Conclusion",
            "The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion, leading to the best reported performance for robust non-projective parsing of Czech.",
            "This leads to the best reported performance for robust non-projective parsing of Czech.",
            "Abstract",
            "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy."
        ]
    },
    "W05-1505": {
        "input_sentences": [
            "Our model, based on a MaxEnt classifier, improves overall dependency accuracy by .7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures.",
            "The goal is to recover non-projective dependency structures that are lost when using state-of-the-art constituencybased parsers; we show that our technique recovers over 50% of these dependencies.",
            "Our algorithm provides a simple framework for corrective modeling of dependency trees, making no prior assumptions about the trees.",
            "Corrective Modeling For Non-Projective Dependency Parsing",
            "However, in the current model, we focus on trees with local errors.",
            "Conclusion",
            "Corrective Modeling",
            "Abstract",
            "Analysis of the types of dependency errors made by these parsers on a Czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser.",
            "The continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees.",
            "We have presented a Maximum Entropy-based corrective model for dependency parsing.",
            "Overall, our technique improves dependency parsing and provides the necessary mechanism to recover non-projective structures.",
            "We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers."
        ]
    },
    "C10-1151": {
        "input_sentences": [
            "1351",
            "In this paper, we focus on the challenge of con stituent syntactic parsing with treebanksof different annotations and propose a collaborative decoding (or co-decoding) ap proach to improve parsing accuracy byleveraging bracket structure consensus be tween multiple parsing decoders trainedon individual treebanks.",
            "Experimental results show the effectiveness of the proposed approach, which outperforms state of-the-art baselines, especially on long sentences.",
            "This paper proposed a co-decoding approach tothe challenge of heterogeneous parsing.",
            "However, such corpora are generally used independently due to distinctions in annotation standards.",
            "Compared to previous work on this challenge, co decoding is able to directly utilize heterogeneous treebanks by incorporating consensus informationbetween partial output of individual parsers dur ing the decoding phase.",
            "Experiments demonstratethe effectiveness of the co-decoding approach, es pecially the effectiveness on long sentences.",
            "Abstract",
            "Acknowledgments This work was supported in part by the National Science Foundation of China (60873091).",
            "We would like to thank our anonymous reviewers for their comments.",
            "Heterogeneous Parsing via Collaborative Decoding",
            "Conclusions",
            "There often exist multiple corpora for the same natural language processing (NLP)tasks.",
            "For the purpose of full use of readily available human annotations, it is significant to simultaneously utilize multiple corpora of different annotation standards."
        ]
    },
    "D07-1111": {
        "input_sentences": [
            "For example, in section 3 we mention the use of different weighting schemes in dependency voting.",
            "In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.",
            "Acknowledgements We thank the shared task organizers and treebank providers.",
            "Abstract",
            "For example, using MSTParser (McDonald and Pereira, 2005), a large-margin all pairs parser, in our domain adaptation procedure results in significantly improved accuracy (83.2 LAS).",
            "Conclusion",
            "We list additional ideas that were not attempted due to time constraints, but that are likely to produce improved results.",
            "We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing.",
            "In addition, other learning approaches, such as memory-based lan guage processing (Daelemans and Van den Bosch, 2005), could be used.",
            "As mentioned in section 5, the addition of a backward SVM model did im prove accuracy on the Turkish set significantly, and it is likely that improvements would also be obtained in other languages.",
            "We apply this pars ing framework to both tracks of the CoNLL 2007 shared task, in each case taking ad vantage of multiple models trained with different learners.",
            "Of course, the use of different approaches used by different groups in the CoNLL 2006 and 2007 shared tasks represents great opportunity for parser ensembles.",
            "This work was supported in part by Grant-in-Aid for Specially Promoted Re search 18002007.",
            "There are several possible extensions and improvements to the approach we have described.",
            "We also thank the reviewers for their comments and suggestions, and Yusuke Miyao for insightful discussions.",
            "Parser actions are determined by a classifier, based on features that represent the current state of the parser.",
            "In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme.",
            "Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles",
            "One of the simplest improvements to our approach is simply to train more models with no oth er changes to our set-up.",
            "A similar idea that may be more effective, but requires more effort, is to add parsers based on dif ferent approaches.",
            "Our results demonstrate the effectiveness of even small ensembles of parsers that are relatively similar (using the same features and the same algorithm).",
            "A drawback of adding more models that became obvious in our experiments was the increased cost of both training (for example, the SVM parsers we used required significant ly longer to train than the MaxEnt parsers) and run-time (parsing with MBL models can be several times slower than with MaxEnt, or even SVM)."
        ]
    },
    "W99-0623": {
        "input_sentences": [
            "Both of the switching techniques, as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.",
            "Two general approaches are presented and two combination techniques are described for each approach.",
            "Combining multiple highly-accurate independent parsers yields promising results.",
            "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "We plan to explore more powerful techniques for exploiting the diversity of parsing methods.",
            "Exploiting Diversity in Natural Language Processing: Combining Parsers",
            "Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.",
            "Conclusion",
            "Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.",
            "Abstract",
            "Both parametric and non-parametric models are explored.",
            "For each experiment we gave an nonparametric and a parametric technique for combining parsers.",
            "The resulting parsers surpass the best previously published performance results for the Penn Treebank.",
            "All four of the techniques studied result in parsing systems that perform better than any previously reported."
        ]
    },
    "P09-1111": {
        "input_sentences": [
            "Discussion",
            "In case rb < r(p), we are not guaranteed to have achieved an optimal reduction in the rank, but we can still obtain an asymptotic improvement in parsing time if we use the new productions obtained in the transformation.",
            "An Optimal-Time Binarization Algorithm for Linear Context-Free Rewriting Systems with Fan-Out Two",
            "Abstract",
            "We have presented an algorithm for the binarization of a LCFRS with fan-out 2 that does not increase the fan-out, and have discussed how this can be applied to improve parsing efficiency in several practical applications.",
            "It still needs to be investigated whether the proposed technique, based on determinization of the choice of the reduction, can also be used for finding binarizations for LCFRS with fan-out larger than two, again without increasing the fan-out.",
            "Our algorithm has optimal time complexity, since it works in linear time with respect to the input production length.",
            "If we do this, when a binarization with fan-out < 2 does not exist the algorithm will still provide us with a list of reductions that can be converted into a set of productions equivalent to p with fan-out at most 2 and rank bounded by some rb, with 2 < rb < r(p).",
            "However, it seems unlikely that this can still be done in linear time, since the problem of binarization for LCFRS in general, i.e., without any bound on the fan-out, might not be solvable in polynomial time.",
            "This is still an open problem; see (G\u00b4omez-Rodr\u00b4\u0131guez et al., 2009) for discussion.",
            "In the algorithm of Figure 1, we can modify line 14 to return R even in case of failure."
        ]
    },
    "W11-0115": {
        "input_sentences": [
            "The method, Partial Least Squares Regression, is well known in other data-intensive fields of research, but to our knowledge had never been put to work in computational semantics.",
            "Abstract",
            "Finally, some approaches may be particularly successful with a specific distributional space architecture (like PLS and RI, and ADD and HAL).",
            "Second, different evaluation methods may favor competing approaches.",
            "In a second experimental setting based on Verb-Noun pairs, a comparatively much lower performance was obtained by all the models; however, the proposed approach gives the best results in combination with a Random Indexing semantic space.",
            "This work has intentionally left the data as raw as possible, in order to keep the noise present in the models at a realistic level.",
            "It is remarkable that PLS did not actually have to compete against any of the previously proposed approaches to compositionality, but only against the NOUN- and ADJ-baselines, and in particular against the former.",
            "In particular, the PLS model generates compositional predictions that are closer to the observed composed vectors than those of its rivals.",
            "In particular, the distributional semantic representations of the constituents are used to predict those of the complex structures.",
            "These representations are then used as feature vectors in a supervised learning model using multivariate multiple regression.",
            "This is an extremely promising result, indicating that it is possible to generalize linear transformation functions beyond single lexical items in Distributional Semantics\u2019 spaces.",
            "It remains to prove if this approach is able to model the symbolic, logic-inspired kind of compositionality that is common in Formal Semantics; being inherently based on functional items, it is at present time very difficult and computationally intensive to attain, but hopefully this will change in the near future.",
            "Our experiments also show that AN compositionality by regression performs nearly equally well in semantic spaces of very different nature (HAL and RI).",
            "First, not all syntactic relations may be equally &quot;easy&quot; to model.",
            "This article introduces and evaluates an approach to semantic compositionality in computational linguistics based on the combination of Distributional Semantics and supervised Machine Learning.",
            "PLS outperformed all the competing models in the reported experiments with AN pairs.",
            "This rather negative result may be due to its relatively smaller size, but this excuse may only be applied to PLS, the only model that relies on parameter estimation.",
            "Surprisingly, though, the gold-standard comparison of shared neighbours gave much better results, with ADD performing well in the HAL space and PLS performing very well in the RI space.",
            "The second dataset used in this paper contained VN pairs.",
            "Conclusions",
            "In brief, distributional semantic spaces containing representations for complex constructions such as Adjective-Noun and Verb-Noun pairs, as well as for their constituent parts, are built.",
            "Even if the VN dataset did not produce excellent results, it highlights some interesting issues.",
            "This fact is expected from a theoretical point of view: since the Noun is the head of the AN pair, it is likely that the complex expression and its head share much of their distributional properties.",
            "This paper proposes an improved framework to model the compositionality of meaning in Distributional Semantics.",
            "PLS nearly always outperformed the NOUN-baseline, but only by small margins, which indicates that there is a still plenty of space for improvement.",
            "The combination of Machine Learning and Distributional Semantics here advocated suggests a very promising perspective: transformation functions corresponding to different syntactic relations could be learned from suitably processed corpora and then combined to model larger, more complex structures, probably also recursive phenomena.",
            "Computing Semantic Compositionality in Distributional Semantics",
            "Generally, this dataset did not produce good results with any of the considered approaches to model compositionality.",
            "This approach outperforms the rivals in a series of experiments with Adjective-Noun pairs extracted from the BNC."
        ]
    },
    "P07-1021": {
        "input_sentences": [
            "In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages.",
            "In previous work, several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense; the best-known but also most restrictive of these is projectivity.",
            "Mildly Context-Sensitive Dependency Languages",
            "Most constraints are formulated on fully specified structures, which makes them hard to integrate into models where structures are composed from lexical information.",
            "Dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity.",
            "Abstract"
        ]
    },
    "P10-1074": {
        "input_sentences": [
            "While conventional wisdom says that adding more training data should always improve performance, this work is the first to our knowledge to incorporate singly-annotated data into a joint model, thereby providing a method for this additional data, which cannot be directly used by the non-hierarchical joint model, to help improve joint modeling performance.",
            "We performed experiments on joint parsing and named entity recognition, and found that our hierarchical joint model substantially outperformed a joint model which was trained on only the jointly annotated data.",
            "Experiments on joint parsing and named entity recognition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.",
            "In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model.",
            "Conclusion",
            "Future directions for this work include automatically learning the variances, Qm and Q* in the hierarchical model, so that the degree of information sharing between the models is optimized based on the training data available.",
            "Abstract",
            "We are also interested in ways to modify the objective function to place more emphasis on learning a good joint model, instead of equally weighting the learning of the joint and single-task models.",
            "Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data",
            "We built single-task models for the non-jointly labeled data, designing those single-task models so that they have features in common with the joint model, and then linked all of the different single-task and joint models via a hierarchical prior.",
            "One of the main obstacles to producing high quality joint models is the lack of jointly annotated data.",
            "In this paper we presented a novel method for improving joint modeling using additional data which has not been labeled with the entire joint structure.",
            "Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model.",
            "Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data."
        ]
    },
    "N06-1022": {
        "input_sentences": [
            "Many aspects of the current implementation that are far from optimal.",
            "Conclusion and Future Research",
            "The best-first search method of Charniak et al. (1998) estimates Equation 1.",
            "This was dictated by ease of implementation.",
            "Abstract",
            "There is no reason to think it is anywhere close to optimal.",
            "We use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level.",
            "This can be included only in an ad-hoc way when working bottom up, but could be easily added here.",
            "What we actually did is to propose all possible constituents at the next level, and immediately rule out those lacking a corresponding constituent remaining at the previous level.",
            "Before using mlctf parsing in a production parser, the other method should be evaluated to see if our intuitions of greater efficiency are correct.",
            "It is also possible to combine mlctf parsing with queue reordering methods.",
            "It should be possible to define \u201coptimal\u201d formally and search for the best mlctf constituent tree.",
            "Several aspects of the method recommend it.",
            "There is one more future-research topic to add before we stop, possibly the most interesting of all.",
            "Our approach requires the user to specify a sequence of nested partitions or equivalence classes of the PCFG nonterminals.",
            "Since our method computes the exact outside probability of constituents (albeit at a coarser level) all of the top down information is available to the system.",
            "This would be a clustering problem, and, fortunately, one thing statistical NLP researchers know how to do is cluster.",
            "We have presented a novel parsing algorithm based upon the coarse-to-fine processing model.",
            "We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results.",
            "All the cleverness goes into estimating the outside probability.",
            "Or again, another very useful feature in English parsing is the knowledge that a constituent ends at the right boundary (minus punctuation) of a string.",
            "We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG.",
            "We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse.",
            "It has been repeatedly shown to improve parsing accuracy (Johnson, 1998; Charniak, 2000; Klein and Manning, 2003b), but it is difficult if not impossible to integrate with best-first search in bottom-up chart-parsing (as in Charniak et al. (1998)).",
            "Multilevel Coarse-To-Fine PCFG Parsing",
            "We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG.",
            "The reason is that when working bottom up it is difficult to determine if, say, ssbar is any more or less likely than ss, as the evidence, working bottom up, is negligible.",
            "It seems clear to us that extracting the maximum benefit from our pruning would involve taking the unpruned constituents and specifying them in all possible ways allowed by the next level of granularity.",
            "The particular tree of coarser to finer constituents that governs our mlctf algorithm (Figure 1) was created by hand after about 15 minutes of reflection and survives, except for typos, with only two modifications.",
            "Quite clearly the current method could be used to provide a more accurate estimate of the outside probability, namely the outside probability at the coarser level of granularity.",
            "For example, consider the impact of parent labeling.",
            "First, unlike methods that depend on best-first search, the method is \u201cholistic\u201d in its evaluation of constituents.",
            "Working bottom up, estimating the inside probability is easy (we just sum the probability of all the trees found to build this constituent)."
        ]
    },
    "W12-2429": {
        "input_sentences": [
            "The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches).",
            "Analysis showed that the sense distributions extracted from the MBR were different from those observed in the evaluation data, providing an explanation for this result.",
            "The most accurate approaches to Word Sense Disambiguation (WSD) for biomedical documents are based on supervised learning.",
            "Three techniques for determining the number of examples to use for training are explored.",
            "Abstract",
            "Surprisingly it was also found that using information from the MBR did not improve performance.",
            "This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus.",
            "Conclusion",
            "Alternative techniques for generating labeled examples will also be explored.",
            "In addition, further evaluation of the WSD system will be carried out, such as applying it to an all words task and within applications.",
            "Evaluation showed that accurate information about the bias of training examples is useful for WSD systems and future work will explore other unsupervised ways of obtaining this information.",
            "Evaluation on the NLM-WSD and MSHWSD data sets demonstrates that the WSD system outperforms the PPR approach without making any use of labeled data.",
            "It is found that a supervised approach (which makes use of manually labeled data) provides the best results.",
            "This paper describes the development of a large scale WSD system based on automatically labeled examples.",
            "We find that these examples can be generated for the majority of CUIs in the UMLS Metathesaurus.",
            "An alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones.",
            "Scaling up WSD with Automatically Generated Examples",
            "The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus.",
            "However, these require manually labeled training examples which are expensive to create and consequently supervised WSD systems are normally limited to disambiguating a small set of ambiguous terms."
        ]
    },
    "E09-1053": {
        "input_sentences": [
            "All these results were technically restricted to the fragment of PF-CCG, and one focus of future work will be to extend them to as large a fragment of CCG as possible.",
            "Abstract",
            "For instance, the CRISP generation algorithm (Koller and Stone, 2007), while specified for TAG, could be generalized to arbitrary grammar formalisms that use regular tree languages\u2014 given our results, to CCG in particular.",
            "Indeed, the exact characterization of the class of CCG-inducable dependency languages is an open issue.",
            "It turns out that the valency trees generated by a PF-CCG grammar form regular tree languages, as in TAG and LCFRS; however, unlike these formalisms, the sets of dependency trees including word order are not regular, and in particular can be more non-projective than the other formalisms permit.",
            "In particular, we plan to extend the lambda notation used in Figure 3 to cover typeraising and higher-order categories.",
            "On the other hand, we find it striking that CCG and TAG generate the same string languages from the same tree languages by incomparable mechanisms for ordering the words in the tree.",
            "Conclusion",
            "We used our new dependency view to compare the strong generative capacity of PF-CCG with other mildly contextsensitive grammar formalisms.",
            "We propose a novel algorithm for extracting dependencies from the derivations of a large fragment of CCG.",
            "Dependency Trees and the Strong Generative Capacity of CCG",
            "We would then be set to compare the behavior of wide-coverage statistical parsers for CCG with statistical dependency parsers.",
            "Finally, we found new formal evidence for the importance of restricting rule schemata for describing non-context-free languages in CCG.",
            "Unlike previous proposals, our view on CCG dependencies is in line with the mainstream dependency parsing literature, which assumes tree-shaped dependency structures; while our dependency trees are less informative than the CCG derivations themselves, they contain sufficient information to reconstruct the semantic representation.",
            "We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees \u2013 but the mechanisms they use to bring the words in these trees into a linear order are incomparable.",
            "This constitutes a most interesting avenue of future research that is opened up by our results.",
            "Unlike earlier proposals, our dependency structures are always tree-shaped.",
            "We anticipate that our results about the strong generative capacity of PF-CCG will be useful to transfer algorithms and linguistic insights between formalisms.",
            "In this paper, we have shown how to read derivations of PF-CCG as dependency trees.",
            "This also has consequences for parsing complexity: We can understand why TAG and LCFRS can be parsed in polynomial time from the bounded block-degree of their dependency trees (Kuhlmann and M\u00f6hl, 2007), but CCG can be parsed in polynomial time (Vijay-Shanker and Weir, 1990) without being restricted in this way."
        ]
    },
    "W10-1407": {
        "input_sentences": [
            "Direct Parsing of Discontinuous Constituents in German",
            "In both treebanks, discontinuities are annotated with crossing branches.",
            "In future work, all of the presented evaluation methods will be investigated to greater detail.",
            "In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks.",
            "Discontinuities occur especially frequently in languages with a relatively free word order, such as German.",
            "In order to do this, we will parse our data sets with current state-of-the-art systems.",
            "Last, since an algorithm is available which extracts LCFRSs from dependency structures (Kuhlmann and Satta, 2009), the parser is instantly ready for parsing them.",
            "Especially a more elaborate dependency conversion should enable a more informative comparison between the output of PCFG parsers and the output of the PLCFRS parser.",
            "Our evaluation, which used different metrics, showed that a PLCFRS parser can achieve competitive results.",
            "Abstract",
            "Conclusion and Future Work",
            "We are currently performing the corresponding experiments.",
            "We have investigated the possibility of using Probabilistic Linear Context-Free Rewriting Systems for direct parsing of discontinuous constituents.",
            "Consequently, we have applied a PLCFRS parser on the German NeGra and TIGER treebanks.",
            "Generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser.",
            "Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems."
        ]
    },
    "P04-1042": {
        "input_sentences": [
            "By this new evaluation metric our algorithm achieves 60% error reduction on gold-standard input trees and 5% error reduction on state-ofthe-art machine-parsed input trees, when compared with the best previous work.",
            "We used customized features to compensate to some extent, but temporal annotation already exists in WSJ and could be used.",
            "The performance of our algorithm is also evident in the substantial contribution to typed dependency accuracy seen in Table 3.",
            "As the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches.",
            "Abstract",
            "Deep Dependencies From Context-Free Statistical Parsers: Correcting The Surface Dependency Approximation",
            "We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks.",
            "As can be seen in the right half of Table 4, performance falls off considerably on vanilla PCFGWSJ(full) WSJ(sm) NEGRA parsed data.",
            "The first was a widespread ambiguity of S and VP nodes within S and VP nodes; many true dislocations of all sorts are expressed at the S and VP levels in CFG parse trees, such as VP1 of Figure 2, but many adverbial and subordinate phrases of S or VP category are genuine dependents of the main clausal verb.",
            "Nevertheless, seemingly dismal performance here still provided a strong boost to typed dependency evaluation of parsed data, as seen in A \u25e6 P of Table 5.",
            "The WSJ results shown in Tables 2 and 3 suggest that discriminative models incorporating both nonlocal and local lexical and syntactic information can achieve good results on the task of non-local dependency identification.",
            "On the PARSEVAL metric, our algorithm performed particularly well on null complementizer and control locus insertion, and on S node relocation.",
            "This was the approach taken by Dienes and Dubey (2003a,b) and Dienes (2003); it is also practiced in recent work on broad-coverage CCG parsing (Hockenmaier, 2003).",
            "It seems that this degradation is not primarily due to noise in parse tree outputs reducing recall of nonlocal dependency identification: precision/recall splits were largely the same between gold and parsed data, and manual inspection revealed that incorrect nonlocal dependency choices often arose from syntactically reasonable yet incorrect input from the parser.",
            "Second, both control locus insertion and dislocated NP remapping must be sensitive to the presence of argument NPs under classified nodes.",
            "For example, the gold-standard parse right-wing whites ... will [VP step up [NP their threats [S [VP * to take matters into their own hands ]]]] has an unindexed control locus because Treebank annotation specifies that infinitival VPs inside NPs are not assigned controllers.",
            "Control loci were often under-annotated; the first five development-set false positive control loci we checked were all due to annotation error.",
            "The third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart.",
            "Results in the G column of Table 5, showing the accuracy of the context-free dependency approximation from gold-standard parse trees, quantitatively corroborates the intuition that nonlocal dependency is more prominent in German than in English.",
            "This approach was tentatively investigated by Plaehn (2000).",
            "But temporal NPs, indistinguishable by gross category, also appear under such nodes, creating a major confound.",
            "Our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order, the surface dependencies in context-free parse trees are a poorer approximation to underlying dependency structure.",
            "The second was the ambiguity that some matrix S-initial NPs are actually dependents of the VP head (in these cases, NEGRA annotates the finite verb as the head of S and the non-finite verb as the head of VP).",
            "We suspect this indicates that dislocated terminals are being usefully identified and mapped back to their proper governors, even if the syntactic projections of these terminals and governors are not being correctly identified by the parser.",
            "We find that our algorithm compares favorably with prior work on English using an existing evaluation metric, and also introduce and argue for a new dependency-based evaluation metric.",
            "Further Work",
            "In particular, Johnson noted that the proper insertion of control loci was a difficult issue involving lexical as well as structural sensitivity.",
            "Note that the dependency figures of Dienes lag behind even the parsed results for Johnson\u2019s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%.",
            "We found the loglinear paradigm a good one in which to model this feature combination; when run in isolation on gold-standard development trees, our model reached 96.4% F1 on control locus insertion, reducing error over the Johnson model\u2019s 89.3% 14Many head-dependent relations in NEGRA are explicitly marked, but for those that are not we used a Collins (1999)style head-finding algorithm independently developed for German PCFG parsing. by nearly two-thirds.",
            "We note that Klein and Manning (2003) independently found retention of temporal NP marking useful for PCFG parsing.",
            "And why-WHADVPs under SBAR, which are always dislocations, were not so annotated 20% of the time.",
            "Obviously, having access to reliable case marking would improve performance in this area; such information is in fact included in NEGRA\u2019s morphological annotation, another argument for the utility of involving enhanced annotation in CF parsing.",
            "Against the background of CFG as the standard approximation of dependency structure for broadcoverage parsing, there are essentially three options for the recovery of nonlocal dependency.",
            "Charniak\u2019s parser, however, attaches the infinitival VP into the higher step up ... VP.",
            "The English/German comparison shown in Tables 4 and 5 is suggestive, but caution is necessary in its interpretation due to the fact that differences in both language structure and treebank annotation may be involved.",
            "Manual investigation of errors made on German gold-standard data revealed two major sources of error beyond sparsity.",
            "We use an algorithm based on loglinear classifiers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation.",
            "These dislocations often require non-local information (such as identity of surface lexical governor) for identification and are thus especially susceptible to degradation in parsed data.",
            "This is not necessarily a genuine discontinuity per se, but rather corresponds to identification of the subject NP in a clause.",
            "Discussion",
            "As can be seen in Table 3, the absolute improvement in dependency recovery is smaller for both our and Johnson\u2019s postprocessing algorithms when applied to parsed input trees than when applied to gold-standard input trees.",
            "First, we noted that annotation inconsistency accounted for a large number of errors, particularly false positives.",
            "Infinitival VPs inside VPs generally do receive controllers for their null subjects, and our algorithm accordingly yet mistakenly assigns right-wing-whites as the antecedent.",
            "This fall-off seems more dramatic than that seen in Sections 4.1 and 4.2, no doubt partly due to the poorer performance of the vanilla PCFG, but likely also because only non-relativization dislocations are considered in Section 4.3.",
            "The second is to incorporate nonlocal dependency information into the category structure of CF trees.",
            "The first option is to postprocess CF parse trees, which we have closely investigated in this paper.",
            "For parsed input trees, our algorithm reduces dependency error by 23% over the baseline, and by 5% compared with Johnson\u2019s results.",
            "Manual investigation of errors on English goldstandard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size.",
            "VPs from which an S has been extracted ([SShut up,] he [VP said t]) are inconsistently given an empty SBAR daughter, suggesting the cross-model low-70\u2019s performance on null SBAR insertion models (see Table 2) may be a ceiling.",
            "We were able to find a number of features to distinguish some cases, such as the presence of certain unambiguous relativeclause introducing complementizers beginning an S node, but much ambiguity remained.",
            "For gold-standard input trees, our algorithm reduces error by over 80% from the surface-dependency baseline, and over 60% compared with Johnson\u2019s results.",
            "We also present the first results on nonlocal dependency reconstruction for a language other than English, comparing performance on English and German."
        ]
    },
    "W12-3131": {
        "input_sentences": [
            "We have finally discussed the impact of the availability of WMTscale data on system building decisions and provided comparative experimental results.",
            "We have presented the French-English translation system built for the NAACL WMT12 shared translation task, including descriptions of our data selection and text processing techniques.",
            "This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12).",
            "Abstract",
            "Experimental results have shown incremental improvement for each addition to our baseline system.",
            "Summary",
            "Translation System",
            "We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building.",
            "The CMU-Avenue French-English Translation System"
        ]
    },
    "P04-1015": {
        "input_sentences": [
            "Secondly, combining with the generative model can be done in several ways.",
            "In the current paper, the generative score was simply added as another feature.",
            "Overall, these results show much promise in the use of discriminative learning techniques such as the perceptron algorithm to help perform heuristic search in difficult domains such as statistical parsing.",
            "Abstract",
            "The improvement that was derived from the additional punctuation features demonstrates the flexibility of the approach in incorporating novel features in the model.",
            "When the training algorithm is provided the generative model scores as an additional feature, the resulting parser is quite competitive on this task.",
            "We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.",
            "Future research will look in two directions.",
            "A beam-search algorithm is used during both training and decoding phases of the method.",
            "Such variants deserve investigation.",
            "Much improvement could potentially be had by looking for other features that could improve the models.",
            "Conclusions",
            "This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm.",
            "Another approach might be to use the generative model to produce candidates at a word, then assign perceptron features for those candidates.",
            "This paper was intended to compare search with the generative model and the perceptron model with roughly similar feature sets.",
            "First, we will look to include more useful features that are difficult for a generative model to include.",
            "Some of the constraints on the search technique that were required in the absence of the generative model can be relaxed if the generative model score is included as another feature.",
            "Incremental Parsing With The Perceptron Algorithm",
            "In such an approach, the unnormalized discriminative parsing model can be applied without either cluding labeled precision (LP), labeled recall (LR), and F-measure an external model to present it with candidates, or potentially expensive dynamic programming.",
            "The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.",
            "In this paper we have presented a discriminative training approach, based on the perceptron algorithm with a couple of effective refinements, that provides a model capable of effective heuristic search over a very difficult search space."
        ]
    },
    "E99-1016": {
        "input_sentences": [
            "An em- pirical evaluation of the method yields very good results for NP/PP chunking of German ewspaper texts.",
            "Cascaded Markov Models",
            "Abstract",
            "This paper presents a new approach to partial parsing of context-free structures.",
            "Each layer of the resulting structure is represented byits own Markov Model, and output of a lower layer is passed as input to the next higher layer.",
            "The approach is based on Markov Mod- els."
        ]
    },
    "C08-1049": {
        "input_sentences": [
            "This paper describes a reranking strategy calledword lattice reranking.",
            "However, our rerank ing implementation is relative coarse, and it must have many chances for improvement.",
            "Conclusion",
            "Using word- and POS- gram information, this reranking technique achieves an error reduction of 16.3% on Joint S&T, and 11.9% on segmentation, over the baseline classifier, and it also outperformsreranking on n-best list.",
            "Word Lattice",
            "With aperceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local fea tures that can?t be easily incorporated intothe perceptron baseline.",
            "We will also investigate the featureselection strategy under the word lattice architec ture, for effective use of non-local information.",
            "Abstract",
            "As a derivation of the forest reranking of Huang (2008), it performs rerank ing on pruned word lattice, instead of on n-best list.",
            "We show our special thanks to Liang Huang for his valuable suggestions.",
            "It confirms that word lattice reranking can effectively use non-local infor mation to select the best candidate result, from a relative small representation structure while with aquite high oracle F-measure.",
            "Experimental results show that, this strategy achieves im provement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking.",
            "In futurework, we will develop more precise pruning al gorithm for word lattice generation, to further cutdown the search space while maintaining the ora cle F-measure.",
            "In this paper, we describe a new rerank ing strategy named word lattice reranking,for the task of joint Chinese word segmen tation and part-of-speech (POS) tagging.",
            "Word Lattice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging",
            "As a derivation of the forest reranking for parsing (Huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking.",
            "AcknowledgementThis work was supported by National Natural Sci ence Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No. 2006AA010108."
        ]
    },
    "P04-1054": {
        "input_sentences": [
            "Abstract",
            "We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a \u201cbag-of-words\u201d kernel.",
            "Another approach might be to calculate the information gain for each feature and use that as its weight.",
            "It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.",
            "The most immediate extension is to automatically learn the feature compatibility function C(vq, vr).",
            "We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.",
            "Detecting relations is a difficult task for a kernel method because the set of all non-relation instances is extremely heterogeneous, and is therefore difficult to characterize with a similarity metric.",
            "A more complex system might learn a weight for each pair of features; however this seems computationally infeasible for large numbers of features.",
            "A first approach might use tf-idf to weight each feature.",
            "Here, C(vq, vr) might return \u03b11 if vq = vr, and \u03b12 if vq and vr are in the same category, where \u03b11 > \u03b12 > 0.",
            "Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.",
            "Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.",
            "Dependency Tree Kernels For Relation Extraction",
            "An improved system might use a different method to detect candidate relations and then use this kernel method to classify the relations.",
            "These results contradict those given in Zelenko et al. (2003), where the sparse kernel achieves 2-3% better F1 performance than the contiguous kernel.",
            "One could also perform latent semantic indexing to collapse feature values into similar \u201ccategories\u201d \u2014 for example, the words \u201cfootball\u201d and \u201cbaseball\u201d might fall into the same category.",
            "Conclusions",
            "Any method that provides a \u201csoft\u201d match between feature values will sharpen the granularity of the kernel and enhance its modeling power.",
            "Future Work",
            "While the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low.",
            "We have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel."
        ]
    },
    "P08-1013": {
        "input_sentences": [
            "We have done a mum number of best hypotheses N. first step in this direction by estimating the probability of a parse tree.",
            "It would therefore be interesting to investigate ways of introducing word information into our grammar-based model.",
            "The language model is applied by means of an N-best rescoring step, which allows to directly measure the performance gains relative to the baseline system without rescoring.",
            "Conclusions and Outlook",
            "Language Model",
            "As N-grams and statistical parsers demonstrate, word information can be very valuable.",
            "To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task.",
            "Abstract",
            "We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree.",
            "This obviously reduces the benefit of pure grammaticality information.",
            "Applying a Grammar-Based Language Model to a Simplified Broadcast-News Transcription Task",
            "We report a significant reduction in word error rate compared to a state-of-the-art baseline system.",
            "It is a well-known fact that natural language is highly ambiguous: a correct and seemingly unambiguous sentence may have an enormous number of readings.",
            "We have presented a language model based on a precise, linguistically motivated grammar, and we have successfully applied it to a difficult broad-domain task.",
            "However, our model only looks at the structure of a parse tree and does not take the actual words into account.",
            "A solution is to use additional information to asses how \u201cnatural\u201d a reading of a word sequence is.",
            "A related \u2013 and for our approach even more relevant \u2013 phenomenon is that many weirdlooking and seemingly incorrect word sequences are in fact grammatical."
        ]
    },
    "P10-1097": {
        "input_sentences": [
            "Another direction for further study will be the generalization of our model to larger syntactic contexts, including more than only the direct neighbors in the dependency graph, ultimately incorporating context information from the whole sentence in a recursive fashion.",
            "Contextualizing Semantic Representations Using Syntactically Enriched Vector Models",
            "Also, our system is the first unsupervised method that has been applied to Erk and McCarthy\u2019s (2009) graded word sense assignment task, showing a substantial positive correlation with the gold standard.",
            "We solved the problems of data sparseness and incompatibility of dimensions which are inherent in this approach by modeling contextualization as an interplay between first- and second-order vectors.",
            "Conclusion",
            "We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion.",
            "Evaluating on the SemEval 2007 lexical substitution task dataset, our model performs substantially better than all earlier approaches, exceeding the state of the art by around 9% in terms of generalized average precision and around 7% in terms of precision out of ten.",
            "In contrast to earlier approaches, our model incorporates detailed syntactic information.",
            "It employs a systematic combination of firstand second-order context vectors.",
            "Abstract",
            "We have presented a novel method for adapting the vector representations of words according to their context.",
            "Alternatively, one can conceptualize it as semantic composition: in particular, the head of a phrase incorporates semantic information from its dependents, and the final result may to some extent reflect the meaning of the whole phrase.",
            "We further showed that a weakly supervised heuristic, making use of WordNet sense ranks, can be significantly improved by incorporating information from our system.",
            "We studied the effect that context has on target words in a series of experiments, which vary the target word and keep the context constant.",
            "A natural objective for further research is the influence of varying contexts on the meaning of target expressions.",
            "We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.",
            "This process can be considered one of mutual disambiguation, which is basically the view of E&P.",
            "This extension might also shed light on the status of the modelled semantic process, which we have been referring to in this paper as \u201ccontextualization\u201d."
        ]
    },
    "P07-1083": {
        "input_sentences": [
            "This is the first research to apply discriminative string similarity to the task of cognate identification.",
            "A character-based measure of similarity is an important component of many natural language processing systems, including approaches to transliteration, coreference, word alignment, spelling correction, and the identification of cognates in related vocabu- We propose an alignment-based disfor string similarity.",
            "Alignment-Based Discriminative String Similarity",
            "Phonetic, semantic, or syntactic features could be included within our discriminative infrastructure to aid in the identification of cognates in text.",
            "This approach achieves exceptional performance; on nine separate cognate identification experiments using six language pairs, we more than double the precision of traditional orthographic measures like Longest Common Subsequence Ratio and Dice\u2019s Coefficient.",
            "We have introduced and successfully applied an alignment-based framework for discriminative similarity that consistently demonstrates improved performance in both bitext and dictionary-based cognate identification on six language pairs.",
            "Conclusion",
            "We may also compare alignmentbased discriminative string similarity with a more complex discriminative model that learns the alignments as latent structure (McCallum et al., 2005).",
            "Abstract",
            "Our improved approach can be applied in any of the diverse applications where traditional similarity measures like Edit Distance and LCSR are prevalent.",
            "Furthermore, we have provided a natural framework for future cognate identification research.",
            "For example, researchers have automatically developed translation lexicons by seeing if words from each language have similar frequencies, contexts (Koehn and Knight, 2002), burstiness, inverse document frequencies, and date distributions (Schafer and Yarowsky, 2002).",
            "In particular, we plan to investigate approaches that do not require the bilingual dictionaries or bitexts to generate training data.",
            "Semantic and string similarity might be learned jointly with a co-training or bootstrapping approach (Klementiev and Roth, 2006).",
            "We have also made available our cognate identification data sets, which will be of interest to general string similarity researchers.",
            "We also show strong improvements over other recent discriminative and heuristic similarity functions.",
            "We gather features from substring pairs consistent with a character-based alignment of the two strings."
        ]
    },
    "W10-3504": {
        "input_sentences": [
            "In the future, we aim at releasing the annotated portion of the WIKI corpus to the community; we will also carry out further experiments and refine the feature spaces.",
            "This method can then be used to expand corpora such as the Fracas test-suite (Cooper et al., 1996) which is more oriented to specific semantic phenomena.",
            "In this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones.",
            "Even if the performance increase of the completely unsupervised cotraining method is not extremely high, this model can be used to semiautomatically expanding corpora by using active learning techniques (Cohn et al., 1996).",
            "Finally, as Wikipedia is a multilingual resource, we will use the WIKI methodology to semi-automatically build RTE corpora for other languages.",
            "In this paper we proposed a method for expanding existing textual entailment corpora that leverages Wikipedia.",
            "We report empirical evidence that our method successfully expands existing textual entailment corpora.",
            "Abstract",
            "The method is extremely promising as it allows building corpora homogeneous to existing ones.",
            "The initial increase of performances is an interesting starting point.",
            "The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones.",
            "Expanding textual entailment corpora fromWikipedia using co-training",
            "Conclusions",
            "The model we have presented is not strictly related to the RTE corpora."
        ]
    },
    "P11-2074": {
        "input_sentences": [
            "It was shown that the same model structure can be effectively applied to feature combination, alignment combination and domain adaptation.",
            "In this paper we present a novel discriminative mixture model for statistical machine translation (SMT).",
            "We partition the feature space into multiple regions.",
            "All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance.",
            "In this paper we presented a novel discriminative mixture model for bridging the gap between the maximum-likelihood training and the discriminative training in SMT.",
            "Conclusion",
            "The features in each region are tied together to share the same mixture weights that are optimized towards the maximum BLEU scores.",
            "Abstract",
            "It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications.",
            "The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task.",
            "This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT.",
            "For example, we can cluster the features based on both feature types and alignments.",
            "We model the feature space with a log-linear combination of multiple mixture components.",
            "Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation",
            "Each component contains a large set of features trained in a maximumentropy framework.",
            "We also point out that it is straightforward to combine any of these three.",
            "Further improvement may be achieved with other feature space partition approaches in the future."
        ]
    },
    "W10-1404": {
        "input_sentences": [
            "We have extended previous works, giving a finer grained description of morphosyntactic features on the learner\u2019s configuration, (FO: feature optimization; TP TC TS: Pseudo-projective, Coordination and Subordinated sentence transformations; SVG, SNP, SC: Stacking (feature passing) on Verb Groups, NPs and Coordination; SP, SCH, SGP: Stacking (category, features and dependency) on Parent, CHildren and GrandParent; *: statistically significant in McNemar's test, p < 0.005; **: statistically significant, p < 0.001) showing that it can significantly improve the results.",
            "Conclusions and future work",
            "Application of Different Techniques to Dependency Parsing of Basque",
            "We studied several proposals for improving a baseline system for parsing the Basque Treebank.",
            "Abstract",
            "In this respect, they can be viewed as alternative approaches to dealing with these phenomena.",
            "Some of the stacked features were languageindependent, as in (Nivre and McDonald.",
            "We present a set of experiments on dependency parsing of the Basque Dependency Treebank (BDT).",
            "In particular, differentiating case and the type of subordinated sentence gives the best LAS increase (+0.82%). the effect of a stacked learning scheme.",
            "We have obtained the following main results: \u2022 Using rich morphological features.",
            "2008), but we have also applied a generalization of the stacking mechanism to a morphologically rich language, as some of the stacked features are morphosyntactic features (such as case and number) which were propagated through a first stage dependency tree by the application of linguistic principles (noun phrases, verb groups and coordination). with respect to the individual systems, some others do not give a improvement over the basic systems.",
            "A careful study must be conducted to investigate whether the approaches are exclusive or complementary.",
            "For example, the transformation on subordinated sentences and feature propagation on verbal groups seem to be attacking the same problem, i. e., the relations between main and subordinated sentences.",
            "The results show that the application of these techniques can give noticeable results, getting an overall improvement of 1.90% (from 77.08% until 78.98%), which can be roughly comparable to the effect of doubling the size of the treebank (see the last two lines of Table 1).",
            "All the results were evaluated on the new version, BDT II, three times larger than the previous one.",
            "All the tests were conducted using MaltParser (Vivre et al., 2007a), a freely available and state of the art dependency parser generator.",
            "The present work has examined several directions that try to explore the rich set of morphosyntactic features in the BDT: i) experimenting the impact of morphological features, ii) application of dependency tree transformations, iii) application of a two-stage parsing scheme (stacking), and iv) combinations of the individual experiments."
        ]
    },
    "E06-2025": {
        "input_sentences": [
            "However, the only current estimation methods that are consistent in the frequency-distribution test, have the linguistically undesirable property of converging to a distribution with all probability mass in complete parse trees.",
            "Theoretical Evaluation Of Estimation Methods For Data-Oriented Parsing",
            "Estimation Methods",
            "Although these method fail the weight-distribution test for the whole class of STSGs, we argued earlier that this test is not the appropriate test either.",
            "Discussion & Conclusions",
            "The STSG formalism, however, allows for many different derivations of the same parse tree, and for many different grammars to generate the same frequency-distribution.",
            "We have shown that DOP1 and methods based on correction factors also fail the weaker frequency-distribution test.",
            "Abstract",
            "In forthcoming work we therefore study yet another estimator, and the linguistically motivated evaluation criterion of convergence to a maximally general STSG consistent with the training data5.",
            "A desideratum for parameter estimation methods is that they converge to the correct parameters with infinitely many data \u2013 that is, we like an estimator to be consistent.",
            "Both estimation methods for STSGs and the criteria for evaluating them, thus require thorough rethinking.",
            "Consistency in the weight-distribution test is therefore too stringent a criterion.",
            "We show that all current estimation methods are inconsistent in the \u201cweight-distribution test\u201d, and argue that these results force us to rethink both the methods proposed and the criteria used.",
            "We analyze estimation methods for Data- Oriented Parsing, as well as the theoretical criteria used to evaluate them."
        ]
    },
    "P08-1006": {
        "input_sentences": [
            "A major drawback of our methodology is that the evaluation is indirect and the results depend on a selected task and its settings.",
            "We also found that accuracy improvements vary when parsers are retrained with domainspecific data, indicating the importance of domain adaptation and the differences in the portability of parser training methods.",
            "This indicates that different results might be obtained with other tasks.",
            "Abstract",
            "The basic idea is to measure the accuracy improvements of the PPI extraction task by incorporating the parser output as statistical features of a machine learning classifier.",
            "We evaluate eight parsers (based on dependency parsing, phrase structure parsing, or deep parsing) using five different parse representations.",
            "Candidates include RASP (Briscoe and Carroll, 2006), the C&C parser (Clark and Curran, 2004), the XLE parser (Kaplan et al., 2004), MINIPAR (Lin, 1998), and Link Parser (Sleator and Temperley, 1993; Pyysalo et al., 2006), but the domain adaptation of these parsers is not straightforward.",
            "Experiments showed that state-of-theart parsers attain accuracy levels that are on par with each other, while parsing speed differs significantly.",
            "Our approach is to measure the impact of each parser when it is used as a component of an information extraction system that performs protein-protein interaction (PPI) identification in biomedical papers.",
            "It is also possible to evaluate unsupervised parsers, which is attractive since evaluation of such parsers with goldstandard data is extremely problematic.",
            "We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy.",
            "Task-oriented Evaluation of Syntactic Parsers and Their Representations",
            "This paper presents a comparative evaluation of several state-of-the-art English parsers based on different frameworks.",
            "Syntactic Parsers and Their Representations",
            "Although we restricted ourselves to parsers trainable with Penn Treebank-style treebanks, our methodology can be applied to any English parsers.",
            "In order to obtain general ideas on parser performance, experiments on other tasks are indispensable.",
            "Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data.",
            "Conclusion and Future Work",
            "Hence, we cannot conclude the superiority of parsers/representations only with our results.",
            "We have presented our attempts to evaluate syntactic parsers and their representations that are based on different frameworks; dependency parsing, phrase structure parsing, or deep parsing."
        ]
    },
    "D08-1008": {
        "input_sentences": [
            "However, the latter is difficult since no structure is available inside segments, and we had to resort to computing upper-bound results usinggold-standard input; despite this, the dependency based system clearly outperformed the upper bound of the performance of the segment-based system.",
            "VOICE.",
            "An example of a method to mitigate this shortcoming is the forest reranking byHuang (2008), in which complex features are evalu ated as early as possible.",
            "Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008.",
            "This holds regardless of whether a segmentbased or a dependency-based metric is used.",
            "Our system is the first semantic role labeler based only on syntactic dependency that achieves a competitive performance.",
            "Abstract",
            "These features represent the setof dependents of the predicate using combina tions of dependency labels, words, and parts of speech.DEPSUBCAT.",
            "For nouns, it is not defined.",
            "Features Used in Argument Identification and LabelingPREDLEMMASENSE.",
            "The grammatical function of the argument node.",
            "Form and part-of-speech tag of the parent node of the predicate.CHILDDEPSET, CHILDWORDSET, CHILDWORDDEPSET, CHILDPOSSET, CHILD POSDEPSET.",
            "To handle this situation fairly to both types of systems, we carried out a two-way evaluation: conversion of dependencies to segments for the dependency-basedsystem, and head-finding heuristics for segment based systems.",
            "PREDPOS.",
            "Dependency relation be tween the predicate and its parent.",
            "Subcategorization frame: the concatenation of the dependency labels of the pred icate dependents.PREDRELTOPARENT.",
            "The lexical form and lemma of the predicate.",
            "Dependency-based Semantic Role Labeling of PropBank",
            "POSITION.",
            "A representation of the complex gram matical relation between the predicate and theargument.",
            "Conclusion",
            "Position of the argument with respect to the predicate: Before, After, or On.ARGWORD and ARGPOS.",
            "Evaluation and comparison is a difficult issuesince the natural output of a dependency-based sys tem is a set of semantic links rather than segments, as is normally the case for traditional systems.",
            "We have described a dependency-based system1 for semantic role labeling of English in the PropBankframework.",
            "To tackle the problemof joint syntactic?semantic analysis, the system relies on a syntactic and a semantic subcomponent.",
            "It consists of the sequence of de pendency relation labels and link directions in the path between predicate and argument, e.g. IM?OPRD?OBJ?.",
            "In this work, we used a sim 1Our system is freely available for download at http://nlp.cs.lth.se/lth_srl.",
            "The syntactic model is a projective parser using pseudo-projective transfor mations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers.",
            "Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance.",
            "Form/part-of-speech tag of the left most/rightmost dependent of the argument.",
            "A Classifier Features Features Used in Predicate Disambiguation PREDWORD, PREDLEMMA.",
            "PREDPARENTWORD and PREDPARENTPOS.",
            "Part-of-speech tag of the predicate.RELPATH.",
            "The same criticisms that are often leveled at reranking-based models clearly apply here too: The set of tentative analyses from the submodules is too small, and the correct analysis is often pruned too early.",
            "Form/part-of-speech tag of the left sibling of the argument.",
            "Binary feature that is set to true if the predicate verb chain has a subject.",
            "Lexical form and part of-speech tag of the argument node.LEFTWORD, LEFTPOS, RIGHTWORD, RIGHTPOS.",
            "The complete syntactic?semanticoutput is selected from a candidate pool gen erated by the subsystems.We evaluate the system on the CoNLL 2005 test sets using segment-based and dependency-based metrics.",
            "VERBCHAINHASSUBJ.",
            "76 plistic loose coupling by means of reranking a small set of complete structures.",
            "LEFTSIBLINGWORD, LEFTSIBLINGPOS.",
            "This feature is meant to resolve control ambiguity as in Figure 4.FUNCTION.",
            "I SBJ eat drinkyouand COORD SBJCONJ ROOT SBJ COORD ROOT drinkandeatI CONJ Figure 3: Coordination ambiguity: The subject I is in an ambiguous position with respect to drink.",
            "The comparison can also be slightly misleading since the dependency-based system was optimized for the dependency metric and previous systems for the segment metric.Our evaluations suggest that the dependency based SRL system is biased to finding argument heads, rather than argument text snippets, and thisis of course perfectly logical.",
            "For direct dependents of the predi cate, this is identical to the RELPATH.",
            "Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently.",
            "Binary feature that is true if the link between the predicate verb chain andits parent is OPRD, and the parent has an ob ject.",
            "The purpose of this feature is to resolve verb coordination ambiguity as in Figure 3.",
            "77",
            "Our evaluations show that the perfor mance of our system is close to the state of theart.",
            "The lemma and sense num ber of the predicate, e.g. give.01.",
            "In terestingly, our system has a complete proposition accuracy that surpasses other systems by nearly 3 percentage points.",
            "For verbs, this feature is Active or Passive.",
            "Whether this is an advantage or a drawback will depend on the applica tion ? for instance, a template-filling system might need complete segments, while an SRL-based vectorspace representation for text categorization, or a rea soning application, might prefer using heads only.",
            "CONTROLLERHASOBJ.",
            "We present a PropBank semantic role label ing system for English that is integrated with a dependency parser.",
            "In the future, we would like to further investigatewhether syntactic and semantic analysis could be integrated more tightly.",
            "I to IMSBJ want sleephim OBJOPRD ROOT IM sleepI SBJ want ROOT to OPRDFigure 4: Subject/object control ambiguity: I is in an am biguous position with respect to sleep."
        ]
    },
    "W08-2107": {
        "input_sentences": [
            "In addition, the use of clustering methods is an interesting possibility for automatically identifying clusters of productive classes of both verbs and of particles that combine well together.",
            "The results confirm that the use of statistical and linguistic information to automatically identify verb-particle constructions presents a reasonable way of improving coverage of existing lexical resources in a very simple and straightforward manner.",
            "In terms of grammar engineering, the information about compositional candidates belonging to productive classes provides us with the basis for constructing a family of fine-grained redundancy rules for these classes.",
            "The VPCs identified as idiomatic, on the other hand, need to be explicitly added to the lexicon, after their semantic is determined.",
            "Abstract",
            "Picking them up and Figuring them out: Verb-Particle Constructions Noise and Idiomaticity",
            "This paper investigates, in a first stage, some methods for the automatic acquisition of verb-particle constructions (VPCs) taking into account their statistical properties and some regular patterns found in productive combinations of verbs and particles.",
            "We investigated the identification of VPCs using a combination of statistical methods and linguistic information, and whether there is a correlation between the productivity of VPCs and their semantics that could help us detect if a VPC is idiomatic or compositional.",
            "Given the limited coverage provided by lexical resources, such as dictionaries, and the constantly growing number of VPCs, possible ways of automatically identifying them are crucial for any NLP task that requires some degree of semantic interpretation.",
            "These rules are applied in a constrained way to verbs already in the lexicon, according to their semantic classes.",
            "The results obtained show that such combination can successfully be used to detect VPCs and distinguish idiomatic from compositional cases.",
            "In a second stage we also study whether the combination of statistical and linguistic properties can provide some indication of the degree of idiomaticity of a given VPC.",
            "One of the important challenges for robust natural language processing systems is to be able to successfully deal with Multiword Expressions and related constructions.",
            "Conclusions",
            "This study can also be complemented with the results of investigations into the semantics of VPCs, as discussed by both Bannard (2005) and McCarthy et al. (2003)."
        ]
    },
    "P07-1055": {
        "input_sentences": [
            "It is realistic gression to train the local models yielded similar re- to imagine a training set where many examples do sults to those in Table 2. not have every level of sentiment annotated.",
            "Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions.",
            "4 Future Work Finally we should note that experiments using One important extension to this work is to augment CRFs to train the structured models and logistic re- the models for partially labeled data.",
            "Abstract",
            "Future Work",
            "Experiments show that this method can significantly reduce classification error relative to models trained in isolation.",
            "Structured Models for Fine-to-Coarse Sentiment Analysis",
            "In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.",
            "The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another."
        ]
    },
    "P08-1102": {
        "input_sentences": [
            "A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging",
            "However, can the perceptron incorporate all the knowledge used in the outside-layer linear model?",
            "Under this model, many knowledge sources that may be intractable to be incorporated into the perceptron directly, can be utilized effectively in the outside-layer linear model.",
            "Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.",
            "If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)?",
            "Abstract",
            "Cascaded Linear Model",
            "On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.",
            "This is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large.",
            "In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004).",
            "We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.",
            "We proposed a cascaded linear model for Chinese Joint S&T.",
            "With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.",
            "We will investigate these problems in the following work.",
            "Conclusions",
            "How can we utilize these knowledge sources effectively?"
        ]
    },
    "P11-2124": {
        "input_sentences": [
            "Many other uses of lattice parsing are possible.",
            "These include joint segmentation and parsing of Chinese, empty element prediction (see (Cai et al., 2011) for a successful application), and a principled handling of multiword-expressions, idioms and named-entities.",
            "We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task.",
            "The code of the lattice extension to the Berkeley parser is publicly available.10 Despite its strong performance, we observed that the Berkeley parser did not learn morphological agreement patterns.",
            "Agreement information could be very useful for disambiguating various constructions in Hebrew and other morphologically rich languages.",
            "We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser.",
            "We demonstrated that the combination of lattice parsing with the PCFG-LA Berkeley parser is highly effective.",
            "Abstract",
            "Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser",
            "We plan to address this point in future work.",
            "In this work, we applied the Berkeley+Lattice parser to the challenging task of joint segmentation and parsing of Hebrew text.",
            "Conclusions and Future Work",
            "Lattice parsing allows much needed flexibility in providing input to a parser when the yield of the tree is not known in advance, and the grammar refinement and estimation techniques of the Berkeley parser provide a strong disambiguation component.",
            "This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.",
            "The result is the first constituency parser which can parse naturally occurring unsegmented Hebrew text with an acceptable accuracy (an Fi score of 80%)."
        ]
    },
    "S12-1097": {
        "input_sentences": [
            "Results fromevaluation show that the supervised approach provides the best results on average but also that per formance of the unsupervised approach is better forsome data sets.",
            "Abstract",
            "This approach also makes use of information from WordNet.",
            "The first is an unsupervised technique based on the widely used vector space model and information from WordNet.The second method relies on supervised ma chine learning and represents each sentence as a set of n-grams.",
            "AcknowledgementsThis research has been supported by a Google Re search Award.",
            "Two approaches for computing semantic similaritybetween sentences were explored.",
            "Resultsfrom the formal evaluation show that both approaches are useful for determining the simi larity in meaning between pairs of sentences with the best performance being obtained bythe supervised approach.",
            "In future, we would like to explore whether performance can be improved by applying deeper analysis to provide information about the structure and semantics of the sentences being compared.",
            "This paper describes the University of Sheffield?s submission to SemEval-2012 Task 6: Semantic Text Similarity.",
            "Incorporating information from WordNet alo improves perfor mance for both approaches.",
            "Both approaches used WordNet to provide information about similarity between lexical items.",
            "The best overall results for the SemEval evaluation were obtained using a hybrid system that attempts to choose the most suitable ap proach for each data set.",
            "University_Of_Sheffield: Two Approaches to Semantic Text Similarity",
            "The results reported here show that the semantic text similarity task can be successfully approached using lexical overlap techniques augmented withlimited semantic information derived from Word Net.",
            "The first, unsu pervised approach, uses a vector space model andcomputes similarity between sentences by comparing vectors while the second is supervised and rep resents the sentences as sets of n-grams.",
            "For example, parsing the input sentences would provide more information about their structure than can be obtained by representing them as a bag of words orset of n-grams.",
            "Conclusion and Future Work",
            "Two approaches were developed.",
            "We would also like to explore methods for improving performance of the n-gram over lap approach and making it more robust to different data sets."
        ]
    },
    "W09-1218": {
        "input_sentences": [
            "Dependency Parsing",
            "In the closed challenge of the CoNLL-2009 Shared Task (Haji\u02c7c et al., 2009), our system achieved the 3rd best performances for English and Czech, and the 4th best performance for Japanese.",
            "Although our system is not a joint approach but a pipeline approach, the system is comparable to the top system for some of the 7 languages.",
            "We believe that there is still room for improvements since we used only two types of global features for the argument classifier.",
            "For semantic dependency parsing, we explore use of global features.",
            "In this paper, we have described our system for syntactic and semantic dependency analysis in multilingual.",
            "A further research direction we are investigating is the application of various types of global features.",
            "Another research direction is investigating joint approaches.",
            "Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage Approximate Max-Margin Linear Models",
            "Conclusion",
            "We would like to investigate syntactic-semantic joint approaches with reasonable time complexities.",
            "Abstract",
            "We attempted to perform Nbest based joint approach, however, the expensive computational cost of the 2nd-order projective parser discouraged it.",
            "All components are trained with an approximate max-margin learning algorithm.",
            "To the best of our knowledge, three types of joint approaches have been proposed: N-best based approach (Johansson and Nugues, 2008), synchronous joint approach (Henderson et al., 2008), and a joint approach where parsing and SRL are performed simultaneously (Llu\u00b4\u0131s and M`arquez, 2008).",
            "The system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classifier, and an argument classifier for semantic dependency parsing.",
            "This paper describes a system for syntacticsemantic dependency parsing for multiple languages."
        ]
    },
    "W09-2208": {
        "input_sentences": [
            "It can be applied to different types of NEs as long as independent evidence exists, which is usually available.",
            "It is simple and, we believe not limited by the choice of the classifier.",
            "We also show that our algorithm achieves high accuracy when the training and test sets are from different domains.",
            "A Simple Semi-supervised Algorithm For Named Entity Recognition",
            "Conclusion",
            "In addition we proposed using high precision label features to improve classification accuracy as well as to reduce training and test time.",
            "It is domain and data independent.",
            "Although it requires a small amount of labeled training data, the data is not required to be from the same domain as the one in which are interested to tag NEs.",
            "Abstract",
            "Such independent evidence is used to automatically extract highaccuracy and non-redundant data, leading to a much improved classifier at the next iteration.",
            "With only a small amount of training data, our algorithm can achieve a better NE tagging accuracy than a supervised algorithm with a large amount of training data.",
            "We presented a simple semi-supervised learning algorithm for NER using conditional random fields (CRFs).",
            "The algorithm is based on exploiting evidence that is independent from the features used for a classifier, which provides high-precision labels to unlabeled data.",
            "Compared to other semi-supervised learning algorithm, our proposed algorithm has several advantages.",
            "Named Entity Recognition",
            "Although we used CRFs in our framework, other models can be easily incorporated to our framework as long as they provide accurate confidence scores.",
            "We show that our algorithm achieves an averimprovement of recall and precision compared to the supervised algorithm.",
            "We present a simple semi-supervised learning algorithm for named entity recognition (NER) using conditional random fields (CRFs)."
        ]
    },
    "D11-1086": {
        "input_sentences": [
            "Another seemingly relevant but different direction is the sparse covariance matrix selection research (Banerjee et al, 2005).",
            "The objective in this workis to find matrices such that the inverse of the co variance matrix is sparse which has applications in Gaussian processes.In this paper, we tried sparsification in the con text of CCA only but our technique is general andcan be applied to its variants like OPCA.",
            "Our experimental results on the task of aligning compa rable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.",
            "Abstract",
            "Mapping documents into an interlingual representation can help bridge the language bar rier of cross-lingual corpora.",
            "Our work is different from the sparse CCA (Hardoon and Shawe-Taylor, 2011; Rai and Daume?",
            "Many existing approaches are based on word co-occurrencesextracted from aligned training data, repre sented as a covariance matrix.",
            "In theory, sucha covariance matrix should represent seman tic equivalence, and should be highly sparse.",
            "We are not aware of any NLP research that attempts to recover the sparseness of the covariance matrices to improve the projection directions.",
            "Later, we explore different selection strategies to remove the noisy pairsbased on the association scores.",
            "Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations.",
            "In this paper, we have proposed the idea of sparsifyng covariance matrices to improve bilingual pro 938 jection directions.",
            "In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways.",
            "Improving Bilingual Projections via Sparse Covariance Matrices",
            "This is cer tainly encouraging and in future we would like to explore more sophisticated techniques to recover the sparsity based on the training data itself.",
            "III, 2009) proposed in the Machine Learningliterature.",
            "First, we explore word association measures and bilingual dictionaries to weigh the word pairs.",
            "Discussion",
            "Their objective is to find projection directions such that the original documents are repre sented as a sparse vectors in the common sub-space.",
            "Moreover, we also observe that computingword pair association measures from the same train ing data along with an appropriate selection criteriacan also yield significant improvements.",
            "Our experimental results show that using external informa tion such as bilingual dictionaries which is gleanedfrom cleaner resources brings significant improve ments."
        ]
    },
    "P12-1065": {
        "input_sentences": [
            "It also achieves accuracy comparable to DL-CoTrain, but does not require the features to be split into two independent views.",
            "As future work, we would like to apply our algorithm to a structured task such as part of speech tagging.",
            "Abstract",
            "This paper introduces a novel variant of the Yarowsky algorithm based on this view.",
            "We also believe that our method for adapting Collins and Singer (1999)\u2019s cautiousness to Yarowsky-prop can be applied to similar algorithms with other underlying classifiers, even to structured output models such as conditional random fields.",
            "Bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them.",
            "Bootstrapping via Graph Propagation",
            "The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets.",
            "Conclusions",
            "It is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function.",
            "N Our novel algorithm achieves accuracy comparable to Yarowsky-cautious, but is better theoretically motivated by combining ideas from Haffari and Sarkar (2007) and Subramanya et al. (2010)."
        ]
    },
    "N03-2024": {
        "input_sentences": [
            "As has been seen, a major improvement of summary readability can be achieved by using the simple set of rewrite rules that realize the highest probability path in the derived Markov model.",
            "This approach will make better use of the Markov model, but it also requires work towards deeper semantic processing of the input.",
            "Referring expressions can be generated by recombining different pieces of the input rather than the currently used extraction of full NPs.",
            "Abstract",
            "One possible usage of the model which is not discussed in the paper but is the focus of current and ongoing work, is to generate realizations \u201con demand\u201d.",
            "Semantic information is needed in order to prevent the combination of almost synonymous premodifiers in the same NP and also for the identification of properties that are more central for the enity with respect to the focus of the input cluster.",
            "Conclusion and Future work",
            "References To Named Entities: A Corpus Study",
            "References included in multi-document summaries are often problematic.",
            "The interpretation of the probabilistic data helps us gain insight on how extractive summaries can be rewritten in an efficient manner to produce more fluent and easy-to-read text.",
            "In this paper, we present a corpus study performed to derive a statistical model for the syntactic realization of referential expressions."
        ]
    },
    "W07-0738": {
        "input_sentences": [
            "Experimental results also show that metrics requiring linguistic analysis seem very robust against parsing errors committed by automatic linguistic processors, at least at the document level.",
            "The reason is that, while MT quality aspects are its scope to the lexical dimension.",
            "Abstract",
            "That is very interesting, taking into account that, while reference translations are supposedly well formed, that is not always the case of automatic translations.",
            "Preliminary results, based on the QUEEN measure inside the QARLA Framework (Amig\u00b4o et al., 2005), indicate that metrics at different linguistic levels may be robustly combined.",
            "First, these tools are not available for all languages.",
            "We believe that, in order to perform \u2018global\u2019 evaluations, different quality dimensions should be integrated into a single measure of quality.",
            "Indeed, all these metrics focus on \u2018partial\u2019 aspects of quality.",
            "We have shown, through empirical evidence, that linguistic features at more abstract levels may provide more reliable system rankings, specially when the systems under evaluation do not share the same lexicon.",
            "In the future, we plan to incorporate more accurate, and possibly faster, linguistic processors, also for languages other than English, as they become publicly available.",
            "Second, usually they are too slow to allow for massive evaluations, as required, for instance, in the case of system development.",
            "For instance, the following set could be used: { \u2018DP-HWCr-4\u2019, \u2018DP-Oc-*\u2019, \u2018DP-Ol-*\u2019, \u2018DP-Or-*\u2019, \u2018CPSTM-9\u2019, \u2018SR-Or-*\u2019, \u2018SR-Orv\u2019 } All these metrics are among the top-scoring in all the translation tasks studied.",
            "We strongly believe that future MT evaluation campaigns should benefit from these results, by including metrics at different linguistic levels.",
            "However, it remains pending to test the behaviour at the sentence level, which could be very useful for error analysis.",
            "Moreover, relying on automatic processors implies two other important limitations.",
            "However, none of these metrics provides, in isolation, a \u2018global\u2019 measure of quality.",
            "We have presented a comparative study on the behavior of a wide set of metrics for automatic MT evaluation at different linguistic levels (lexical, shallow-syntactic, syntactic, and shallow-semantic) under different scenarios.",
            "Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, may not be a reliable MT quality indicator.",
            "Conclusions",
            "We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature.",
            "This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon.",
            "Linguistic Features for Automatic Evaluation of Heterogenous MT Systems",
            "With that purpose, we are currently exploring several metric combination strategies.",
            "In this work, we suggest using metrics which take into account linguistic features at more abstract levels."
        ]
    },
    "P12-1045": {
        "input_sentences": [
            "Learning the semantics of language from the perceptual context in which it is uttered is a useful approach because only minimal human supervision is required.",
            "Fast Online Lexicon Learning for Grounded Language Acquisition",
            "While the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets.",
            "In this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results.",
            "Conclusion",
            "It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context.",
            "Learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language.",
            "Abstract",
            "Recent work by Chen and Mooney (2011) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs.",
            "In addition to being more scalable, SGOLL also performed better on the task of interpreting navigation instructions.",
            "In contrast to the previous approach that computed common subgraphs between different contexts in which an n-gram appeared, we instead focus on small, connected subgraphs and introduce an algorithm, SGOLL, that is an order of magnitude faster.",
            "We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon\u2019s Mechanical Turk we can further improve the results.",
            "Finally, we demonstrated the generality of the system by using it to learn Chinese navigation instructions and achieved similar results.",
            "In addition, we showed that changing the MRG and collecting additional training data from Mechanical Turk further improve the performance of the overall navigation system.",
            "We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach.",
            "In this paper we presented a novel online algorithm for building a lexicon from ambiguously supervised relational data."
        ]
    },
    "P12-2002": {
        "input_sentences": [
            "Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios.",
            "Joint Evaluation of Morphological Segmentation and Syntactic Parsing",
            "Conclusion",
            "Abstract",
            "Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags).",
            "Our contribution is both technical, providing an evaluation tool that can be straightforwardly applied for parsing scenarios involving trees over lattices,4 and methodological, suggesting to evaluate parsers in all possible scenarios in order to get a realistic indication of parser performance.",
            "We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance.",
            "We presented distance-based metrics defined for trees over lattices and applied them to evaluating parsers on joint morphological and syntactic disambiguation.",
            "The protocol uses distance-based metrics defined for the space of trees over lattices."
        ]
    },
    "W07-0718": {
        "input_sentences": [
            "Similar to last year\u2019s workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from four European languages into English, and vice versa.",
            "Understanding the exact causes of those differences still remains an important issue for future research.",
            "Abstract",
            "On the other hand, comparing systems by ranking them manually (constituents or entire sentences), resulted in much higher inter-annotator agreement.",
            "They were: semantic role overlap (newly introduced in this workshop) ParaEval-recall and Meteor.",
            "We measured the correlation of automatic evaluation metrics with human judgments.",
            "There were substantial differences in the results results of the human and automatic evaluations.",
            "This year we substantially increased the number of automatic evaluation metrics and were also able to nearly double the efforts of producing the human judgments.",
            "This meta-evaluation reveals surprising facts about the most commonly used methodologies.",
            "One striking observation is that inter-annotator agreement for fluency and adequacy can be called \u2018fair\u2019 at best.",
            "j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.",
            "This year\u2019s evaluation also measured the agreement between human assessors by computing the Kappa coefficient.",
            "Although we do not claim that our observations are indisputably conclusive, they again indicate that the choice of automatic metric can have a significant impact on comparing systems.",
            "We measured timing and intraand inter-annotator agreement for three types of subjective evaluation.",
            "Conclusions",
            "We take the human judgments to be authoritative, and used them to evaluate the automatic metrics.",
            "(Meta-) Evaluation of Machine Translation",
            "We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process.",
            "We measured correlation using Spearman\u2019s coefficient and found that three less frequently used metrics were stronger predictors of human judgments than Bleu."
        ]
    },
    "W10-2009": {
        "input_sentences": [
            "Modeling the Noun Phrase versus Sentence Coordination Ambiguity in Dutch: Evidence from Surprisal Theory",
            "The former only has one topic, whereas the latter has two.",
            "The outcome of our surprisal model is also compatible with the results of Hoeks et al. (2006) who found that thematic information can strongly reduce but not completely eliminate the NP-coordination preference.",
            "And finally, our approach is consistent with a large body of evidence indicating that language comprehension is incremental and makes use of expectation-driven word prediction (Pickering and Garrod, 2007).",
            "A word\u2019s difficulty is related to the expectations the language processor forms, given the structural and lexical material that precedes it.",
            "Frazier\u2019s explanation is based on a metric of syntactic complexity which in turn depends on quite specific syntactic representations of a language\u2019s phrase structure.",
            "Surprisal theory is explicitly built on the assumption that multiple sources of information can interact in parallel at any point in time during sentence processing.",
            "Surprisal theory allows us to build a formally precise computational model of reading time data which generates testable, quantitative predictions about the differential processing of individual test items.",
            "Abstract",
            "In addition, syntactic expectations were modified by lexical expectations.",
            "Accordingly, we suggest here that the residual preference for NPcoordination found in the study of Hoeks et al. (2006) might be explained in terms of syntactic and lexical expectation.",
            "We find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987).",
            "Surprisal theory models processing difficulty on a word level.",
            "Hoeks et al. (2002) argue that having more than one topic is unexpected.",
            "Some Scoordination rules could have higher probability than NP-coordination rules.",
            "Sentences with more than one topic will therefore cause more processing difficulty.",
            "It is further modulated by the immediate context from which upcoming words are predicted during processing.",
            "Consequently, the model\u2019s preference for one structural type can vary across sentence tokens and even be reversed on occasion.",
            "Thus, it may very well be the case that the NP-coordination preference that was present in our training corpus may have had a pragmatic origin related to topic-structure.",
            "Surprisal is estimated using a Probabilistic Context-Free Grammar (PCFG), which is induced from an automatically annotated corpus.",
            "Thus, even when NP-coordination was structurally favored over S-coordination, highly unexpected lexical material could lead to more processing difficulty for NP-coordination than for S-coordination.",
            "This preference for simple topic-structure that was evident in language comprehension may also be present in language production, and hence in language corpora.",
            "Surprisal Theory",
            "The minimal attachment principle postulates that the bias towards NP-coordination is an initial processing primitive.",
            "We argued that our grammar showed an overall preference for NP-coordination but this preference was not necessarily reflected on each and every rule that dealt with coordinations.",
            "In contrast, the bias in our simulations is a function of the model\u2019s input history and linguistic experience from which the grammar is induced.",
            "We found these results to be robust for a critical model parameter (beam size), which suggests that syntactic processing in human comprehension might be based on limited parallelism only.",
            "Both these aspects make surprisal theory a parsimonious explanatory framework.",
            "Our results are consistent with the findings of Hoeks et al. (2002), who also found evidence for an NP-coordination preference in a self-paced reading experiment as well as in an eye-tracking experiment.",
            "It remains to be tested whether our model can explain behavioral data from the processing of ambiguities other than the Dutch NP- versus S-coordination case.",
            "In this paper we have shown that a model of lexicalized surprisal, based on an automatically induced PCFG, can account for the NP-/S-ambiguity reading time data of Frazier (1987).",
            "We argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference.",
            "The model showed a clear preference for NP-coordination which suggests that structural and lexical expectations as estimated from a corpus might be sufficient to explain the NP-coordination bias in human sentence processing.",
            "These predictions (Figure 5) indicate that mean reading times for a set of NP/S-coordination sentences may not be adequate to tap the origin of differential processing difficulty.",
            "Discussion",
            "This paper investigates whether surprisal theory can account for differential processing difficulty in the NP-/S-coordination ambiguity in Dutch.",
            "They suggested that NP-coordination might be easier to process because it has a simpler topic structure than S-coordination.",
            "Surprisal theory, on the other hand, is largely neutral with respect to the form syntactic representations take in the human mind.4 Moreover, differential processing in surprisal-based models does not require the specification of a notion of syntactic complexity.",
            "Our account of this bias differs considerably from the original account proposed by Frazier (minimal attachment principle) in a number of ways."
        ]
    },
    "P06-1012": {
        "input_sentences": [
            "In this paper, we show that using well calibrated probabilities to estimate sense priors is important.",
            "NB-EM represents our earlier approach in (Chan and Ng, 2005b).",
            "These results are shown in Table 3 and the column EM shows that using the predictions of logistic regression to estimate sense priors consistently gives the lowest KL divergence.",
            "However, the WSD accuracies of these unadjusted logistic regression classifiers are on average about 4% lower than those of the unadjusted naive Bayes classifiers.",
            "Abstract",
            "As an example, NBcal-EM specifies that the sense priors estimated by logistic regression is used to adjust the predictions of the calibrated naive Bayes classifier, and corresponds to accuracies in column EM under NBcal in Table 1.",
            "Corresponding results when the predictions of the multiclass naive Bayes is used in Equation (4), are given in the column EM under NB.",
            "Although the accuracy of logistic regression as a basic classifier is lower than that of naive Bayes, its predictions may still be suitable for estimating 'Though not shown, we also calculated the accuracies of these binary classifiers without calibration, and found them to be similar to the accuracies of the multiclass naive Bayes shown in the column L under NB in Table 1. sense priors.",
            "Using sense priors estimated by logistic regression further improves performance.",
            "This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains.",
            "These estimates and the predictions of the calibrated naive Bayes classifier are then used in Equation (4) to obtain the adjusted predictions.",
            "In the rest of this section, we first conduct experiments to estimate sense priors using the predictions of logistic regression.",
            "The resulting WSD accuracy is shown in the column EM under NBcal in Table 1.",
            "Based on the significance tests, the adjusted accuracies of EM and EM in Table 1 are significantly better than their respective unadjusted L accuracies, indicating that estimating the sense priors of a new domain via the EM approach presented in this paper significantly improves WSD accuracy compared to just using the sense priors from the old domain.",
            "For example, row 1 of Table 4 shows that adjusting the predictions of multiclass naive Bayes classifiers by sense priors estimated by logistic regression (NBEM ) performs significantly better than using sense priors estimated by multiclass naive Bayes (NB-EM ).",
            "Estimating Class Priors In Domain Adaptation For Word Sense Disambiguation",
            "The results of significance tests for the various methods on the 4 datasets are given in Table 4, where the symbols (0.01, 0.05], and 0.01 respectively.",
            "We found that the WSD accuracies obtained with the method of (McCarthy et al., 2004) are on average 1.9% lower than our NBcal-EM method, and the difference is statistically significant.",
            "The significance tests show that our current approach of using calibrated naive Bayes probabilities to estimate sense priors, and then adjusting the calibrated probabilities by these estimates (NBcal-EM ) performs significantly better than NB-EM (refer to row 2 of Table 4).",
            "Conclusion",
            "By using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy.",
            "In the case of DSO nouns, this improvement is especially significant.",
            "The t statistic of the difference between each test instance pair is computed, giving rise to a p value.",
            "To gauge how well the sense priors are estimated, we measure the KL divergence between the true sense priors and the sense priors estimated by using the predictions of (uncalibrated) multiclass naive Bayes, calibrated naive Bayes, and logistic regression.",
            "However, using a learning algorithm which already gives well calibrated posterior probabilities may be more effective in estimating the sense priors.",
            "By calibrating the probabilities of the naive Bayes algorithm, and using the probabilities given by logistic regression (which is already well calibrated), we achieved significant improvements in WSD accuracy over previous approaches.",
            "One possible reason is that being a discriminative learner, logistic regression requires more training examples for its performance to catch up to, and possibly overtake the generative naive Bayes learner (Ng and Jordan, 2001).",
            "The methods in Table 4 are represented in the form a1-a2, where a1 denotes adjusting the predictions of which classifier, and a2 denotes how the sense priors are estimated.",
            "Results of the KL divergence test motivate us to use sense priors estimated by logistic regression on the predictions of the naive Bayes classifiers.",
            "This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations.",
            "We trained logistic regression classifiers and evaluated them on the 4 datasets.",
            "To elaborate, we first use the probability estimates of logistic regression in Equations (2) and (3) to estimate the sense priors .",
            "As in our earlier work (Chan and Ng, 2005b), we normalized the prevalence score of each sense to obtain estimated sense priors for each word, which we then used to adjust the predictions of our naive Bayes classifiers.",
            "Finally, using sense priors estimated by logistic regression to adjust the predictions of calibrated naive Bayes (NBcal-EM ) in general performs significantly better than most other methods, achieving the best overall performance.",
            "The results show that the sense priors provided by logistic regression are in general effective in further improving the results.",
            "The relative improvements against using the true sense priors, based on the calibrated probabilities, are given in the column EM L in Table 2.",
            "Paired t-tests were conducted to see if one method is significantly better than another.",
            "Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word).",
            "Discussion",
            "Then, we perform significance tests to compare the various methods.",
            "Differences in sense priors between training and target domain datasets will result in a loss of WSD accuracy.",
            "One possible algorithm is logistic regression, which directly optimizes for getting approximations of the posterior probabilities.",
            "Hence, its probability estimates are already well calibrated (Zhang and Yang, 2004; NiculescuMizil and Caruana, 2005).",
            "In addition, we implemented the unsupervised method of (McCarthy et al., 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense.",
            "For DSO nouns, though the results are similar, the p value is a relatively low 0.06.",
            "The experimental results show that the sense priors estimated using the calibrated probabilities of naive Bayes are effective in increasing the WSD accuracy."
        ]
    },
    "J01-2004": {
        "input_sentences": [
            " Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model",
            "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition",
            " A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers",
            " A small recognition experiment also demonstrates the utility of the model",
            " A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity",
            " The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling"
        ]
    },
    "N07-1050": {
        "input_sentences": [
            "An open issue in data-driven dependency parsing is how to handle non-projective dependencies, which seem to be required by linguistically adequate representations, but which pose problems in parsing with respect to both accuracy and efficiency.",
            "In this paper, we have investigated a data-driven approach to dependency parsing that combines a deterministic incremental parsing algorithm with historybased SVM classifiers for predicting the next parser action.",
            "We have shown that, for languages with a non-negligible proportion of non-projective structures, parsing accuracy can be improved significantly by allowing non-projective structures to be derived.",
            "We have also shown that the parsing time can be reduced substantially, with only a marginal loss in accuracy, by limiting the degree of nonprojectivity allowed during parsing.",
            "Using data from five different languages, we evaluate an incremental deterministic parser that derives non-projective depenstructures in supported by SVM classifiers for predicting the next parser action.",
            "Conclusion",
            "Moreover, by restricting the class of permissible structures to limited degrees of non-projectivity, the parsing time can be reduced by up to 50% without a significant decrease in accuracy.",
            "The experiments show that unrestricted non-projective parsing gives a significant improvement in accuracy, compared to a strictly projective baseline, with up to 35% error reduction, leading to state-of-the-art results for the given data sets.",
            "Abstract",
            "Incremental Non-Projective Dependency Parsing",
            "A comparison with results from the CoNLL-X shared task shows that the parsing accuracy is comparable to that of the best available systems, which means that incremental non-projective dependency parsing is a viable alternative to approaches based on post-processing of projective approximations."
        ]
    },
    "P12-1053": {
        "input_sentences": [
            "Strong Lexicalization of Tree Adjoining Grammars",
            "A more powerful model, the simple context-free tree grammar, admits such a normal form.",
            "It can be effectively constructed and the maximal rank of the nononly increases by Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.",
            "For k \u2208 ICY, let CFTG(k) be the set of those CFTG whose nonterminals have rank at most k. Since the normal form constructions preserve the nonterminal rank, the proof of Theorem 17 shows that CFTG(k) are strongly lexicalized by CFTG(k+1).",
            "Kepser and Rogers (2011) show that non-strict TAG are strongly equivalent to CFTG(1).",
            "Conclusion",
            "Abstract",
            "G\u00b4omez-Rodriguez et al. (2010) show that wellnested LCFRS of maximal fan-out k can be parsed in time O(n2k+2), where n is the length of the input string w \u2208 A\u2217.",
            "Linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol.",
            "Hence, non-strict TAG are strongly lexicalized by CFTG(2).",
            "From this result we conclude that CFTG(k) can be parsed in time O(n2k+4), in the sense that we can produce a parse tree t that is generated by the CFTG with yd\u0394(t) = w. It is not clear yet whether lexicalized CFTG(k) can be parsed more efficiently in practice.",
            "it was shown Tree-adjoining grammars are not closed unstrong Comput.",
            "It follows from Section 6 of Engelfriet et al. (1980) that the classes CFTG(k) with k \u2208 ICY induce an infinite hierarchy of string languages, but it remains an open problem whether the rank increase in our lexicalization construction is necessary."
        ]
    },
    "W05-0602": {
        "input_sentences": [
            "A Statistical Semantic Parser That Integrates Syntax And Semantics",
            "It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label.",
            "We present experimentalresults demonstrating that SCISSOR produces more accurate semantic representa tions than several previous approaches.",
            "A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation.",
            "Abstract",
            "We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer.",
            "We introduce a learning semantic parser,SCISSOR, that maps natural-language sentences to a detailed, formal, meaning representation language."
        ]
    },
    "P13-1126": {
        "input_sentences": [
            "This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM).",
            "The approach is simple, easy to implement, and computationally cheap.",
            "An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.",
            "This paper proposed a new approach to domain adaptation in statistical machine translation, based on vector space models (VSMs).",
            "Abstract",
            "Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set.",
            "This approach measures the similarity between a vector representing a particular phrase pair in the phrase table and a vector representing the dev set, yielding a feature associated with that phrase pair that will be used by the decoder.",
            "For the two language pairs we looked at, it provided a large performance improvement over a non-adaptive baseline, and also compared favourably with linear mixture adaptation techniques.",
            "An obvious and appealing one, which we intend to try in future, is a vector space based on a bag-of-words topic model.",
            "Thus, we obtain a decoding feature whose value represents the phrase pair\u2019s closeness to the dev.",
            "This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set.",
            "In our experiments, we based the vector space on subcorpora defined by the nature of the training data.",
            "Conclusions and future work",
            "Experiments on large scale NIST evaluation data show improvements over strong baselines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation.",
            "This was done purely out of convenience: there are many, many ways to define a vector space in this situation.",
            "Furthermore, VSM adaptation can be exploited in a number of different ways, which we have only begun to explore.",
            "The general idea is first to create a vector profile for the in-domain development (\u201cdev\u201d) set.",
            "A feature derived from this topicrelated vector space might complement some features derived from the subcorpora which we explored in the experiments above, and which seem to exploit information related to genre and style.",
            "This is a simple, computationally cheap form of instance weighting for phrase pairs.",
            "Vector Space Model for Adaptation in Statistical Machine Translation"
        ]
    },
    "P13-2073": {
        "input_sentences": [
            "We presented an experiment showing the effect of using two language independent features, source connectivity score and target connectivity score, to improve the quality of pivot-based SMT.",
            "Abstract",
            "Conclusion and Future Work",
            "We showed that these features help improving the overall translation quality.",
            "In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT.",
            "We also plan to explore language specific features which could be extracted from some seed parallel data, e.g., syntactic and morphological compatibility of the source and target phrase pairs.",
            "One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages.",
            "Although pivoting is a robust technique, it introduces some low quality translations.",
            "Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation",
            "We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.",
            "In the future, we plan to explore other features, e.g., the number of the pivot phases used in connecting the source and target phrase pair and the similarity between these pivot phrases.",
            "The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table.",
            "An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs."
        ]
    },
    "N06-1037": {
        "input_sentences": [
            "Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes.",
            "We may integrate more features (such as head words or WordNet semantics) into nodes of parse trees.",
            "The most immediate extension of our work is to improve the accuracy of relation detection.",
            "Exploring Syntactic Features For Relation Extraction Using A Convolution Tree Kernel",
            "We can also benefit from the learning algorithm to study how to solve the data imbalance and sparseness issues from the learning algorithm viewpoint.",
            "This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.",
            "Abstract",
            "Conclusion and Future Work",
            "It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types.",
            "In this paper, we explore the syntactic features using convolution tree kernels for relation extraction.",
            "We may adopt a two-step method (Culotta and Sorensen, 2004) to separately model the relation detection and characterization issues.",
            "We also would like to thank the three anonymous reviewers for their invaluable suggestions.",
            "Acknowledgements: We would like to thank Dr. Alessandro Moschitti for his great help in using his Tree Kernel Toolkits and fine-tuning the system.",
            "In the future, we would like to test our algorithm on the other version of the ACE corpus and to develop fast algorithm (Vishwanathan and Smola, 2002) to speed up the training and testing process of convolution kernels.",
            "Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.",
            "We conclude that: 1) the relations between entities can be well represented by parse trees with carefully calibrating effective portions of parse trees; 2) the syntactic features embedded in a parse tree are particularly effective for relation extraction; 3) the convolution tree kernel can effectively capture the syntactic features for relation extraction."
        ]
    },
    "W12-3407": {
        "input_sentences": [
            "We have performed three experiments for each statistical parser, trying with the chunks provided by the chunker, the partial dependency parser, and both.",
            "The rule-based dependency analyzer (RBDA, Aranzabe et al., 2004) uses a set of 505 CG rules that try to assign dependency relations to wordforms.",
            "This paper presents the results of a set of preliminary experiments combining two knowledge-based partial dependency analyzers with two statistical parsers, applied to the Basque Dependency Treebank.",
            "The general idea will be to apply a stacked scheme where the output of the rule-based partial parsers will be given as input to MaltParser and MST, two state of the art statistical parsers.",
            "We performed an additional test using the partial dependency analyzer\u2019s gold dependency relations as input to MaltParser.",
            "The first one is to mark the beginning of the phrase (B-VP if it is a verb phrase and B-NP whether it's a noun phrase) and the other one to mark the continuation of the phrase (I-NP or I-VP, meaning that the word is inside an NP or VP).",
            "For that reason, the results can serve as an upper bound on the real results.",
            "In our work we will study the use of two partial rule-based syntactic analyzers together with two data-driven parsers: set of predefined tags to each word, where each tag gives both the name of a dependency relation (e.g. subject) together with the direction of its head (left or right).",
            "The table shows modest gains, suggesting that the rule-based analyzers help the statistical ones, giving slight increases over the baseline, which are statistically significant when applying MaltParser to the output of the rule-based dependency parser and a combination of the chunker and rule-based parsers.",
            "This means that there is room for improvement in the first-stage knowledge-based parsers, which will have, at least in theory, a positive effect on the second-phase statistical parsers, allowing us to test whether knowledge-based and machine learningbased systems can be successfully combined.",
            "As it was successfully done on part of speech (POS) tagging, where the use of rule-based POS taggers (Tapanainen and Voutilainen, 1994) or a combination of a rulebased POS tagger with a statistical one (Aduriz et al., 1997, Ezeiza et al., 1998) outperformed purely statistical taggers, we think that exploring the combination of knowledge-based and data-driven systems in syntactic processing can be an interesting line of research.",
            "As each type of syntactic information can have an important influence on the results on specific relations, their study can shed light on novel schemes of parser combination.",
            "Each word contains its form, lemma, category or coarse part of speech (CPOS), POS, morphosyntactic features such as case, number of subordinate relations, and the dependency relation (headword + dependency).",
            "For evaluation, we divided the treebank in three sets, corresponding to training, development, and test (80%, 10%, and 10%, respectively).",
            "McDonald and Nivre (2007) examined the types of errors made by the two data-driven parsers used in this work, showing how the greedy algorithm of MaltParser performed better with local dependency relations, while the graph-based algorithm of MST was more accurate for global relations.",
            "When performing this task, we found the problem of matching the treebank tokens with those obtained from the analyzers, as there were divergences on the treatment of multiword units, mostly coming from Named Entities, verb compounds and complex postpositions (formed with morphemes appearing at two different words).",
            "The chunker delimits the chunks with three tags, using a standard IOB marking style (see figure 1).",
            "We use the freely available version of MSTParser1.",
            "The evaluation of the chunker on the BDT gave a result of 87% precision and 85% recall over all chunks.",
            "For that reason, MaltParser, seems to mostly benefit of the local nature of the stacked features, while MST does not get a significant improvement, except for some local dependency relations, such as ncobj and ncsubj.",
            "2 f-score = 2 * precision * recall / (precision + recall) (ncmod = non-clausal modifier, ncobj = non-clausal object, ncpred = non-clausal predicate, ncsubj = non-clausal subject, nciobj = non-clausal indirect object) Table 2 shows how the addition of the rule-based parsers\u2019 tags performs in accord with this behavior, as MaltParser gets f-score improvements for the local relations.",
            "In our experiments, we will use the StackLazy algorithm with the liblinear classifier.",
            "For example, this analyzer assigns tags of the form &NCSUBJ> (see figure 1), meaning that the corresponding wordform is a non-clausal syntactic subject and that its head is situated to its right (the \u201c>\u201d or \u201c<\u201d symbols mark the direction of the head).",
            "This algorithm finds the highest scoring directed spanning tree in a dependency graph forming a valid dependency tree.",
            "As table 1 shows, the parser type is relevant, as MaltParser seems to be sensitive when using the stacked features, while the partial parsers do not seem to give any significant improvement to MST.",
            "Abstract",
            "In this paper we will experiment the use of the stacking technique, giving the tags obtained by the rulebased syntactic partial parsers as input to the statistical parsers.",
            "As both the chunker and the partial dependency analyzer are based on a set of local rules in the CG formalism, we could expect that the stacked parsers could benefit mostly on the local dependency relations.",
            "In the last years, many attempts have been performed trying to combine different parsers (Surdeanu and Manning, 2010), with significant improvements over the best individual parser\u2019s baseline.",
            "Finally, we must also take into account that the rule-based analyzers were developed mainly having linguistic principles in mind, such as coverage of diverse linguistic phenomena or the treatment of specific syntactic constructions (Aranzabe et al., 2004), instead of performanceoriented measures, such as precision and recall.",
            "We will experiment the effect of using the output of the knowledge-based analyzers as input to the data-driven parsers in a stacked learning scheme.",
            "The two most successful approaches have been stacking (Martins et al., 2008) and voting (Sagae and Lavie, 2006, Nivre and McDonald, 2008, McDonald and Nivre, 2011).",
            "In the rest of this paper, section 2 will first present the corpus and the different parsers we will combine, followed by the experimental results in section 3, and the main conclusions of the work.",
            "This section will describe the main resources that have been used in the experiments.",
            "If only categorial (POS) ambiguity is taken into account, there is an average of 1.55 interpretations per wordform, which rises to 2.65 when the full morphosyntactic information is taken into account, giving an overall 64% of ambiguous word-forms. can pose an important problem, as determining the correct interpretation for each word-form requires in many cases the inspection of local contexts, and in some others, as the agreement of verbs with subject, object or indirect object, it could also suppose the examination of elements which can be far from each other, added to the free constituent order of the main sentence elements in Basque.",
            "For example the non-clausal modifier (ncmod) relation\u2019s f-score increases 3.25 points, while the dependency relation for clausal subordinate sentences functioning as indirect object decreases 0.46 points, which is surprising in principle.",
            "Figure 1 shows how the two last lines of the example sentence contain the tags assigned by the rule-based chunker (B-NP, I-NP, B-VP and I-VP) and the rule-based partial dependency analyzer (&NCSUBJ, &<NCMOD, &<AUXMOD, &CCOMP_OBJ and &MAINV) .",
            "For all those reasons, the relation between the input dependency tags and the obtained results seems to be intricate, and we think that it deserves new experiments in order to determine their nature.",
            "The experiments were performed on the development set, leaving the best system for the final test.",
            "The rule-based analyzers are based on the Contraint Grammar (CG) formalism (Karlsson et al., 1995), based on the assignment of morphosyntactic tags to words using a formalism that has the capabilities of finite state automata or regular expressions, by means of a set of rules that examine mainly local contexts of words to determine the correct tag assignment.",
            "The system was evaluated on the BDT, obtaining f-scores between 90% for the auxmod dependency relation between the auxiliary and the main verb and 52% for the subject dependency relation, giving a (macro) average of 65%.",
            "First, subsection 2.1 will describe the Basque Dependency Treebank, and then subsection 2.2 will explain the main details of the analyzers that have been employed.",
            "The last two lines of the sentence in figure 1 do not properly correspond to the treebank, but are the result of the rule-based partial syntactic analyzers (see subsection 2.2).",
            "Looking with more detail at the errors made by the different versions of the parsers, we observe significant differences in the results for different dependency relations, seeing that the statistical parsers behave in a different manner regarding to each relation, as shown in table 2.",
            "The MST Parser can be considered a representative of global, exhaustive graph-based parsing (McDonald et al., 2005, 2006).",
            "The experiments have been performed on the Basque Dependency Treebank (Aduriz et al., 2003).",
            "Although the potential gain is in theory high, the experiments have shown very modest improvements, which seem to happen in the set of local dependency relations.",
            "Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish (Nivre et al., 2007a).",
            "This subsection will present the four types of analyzers that have been used.",
            "This means that the result of this analysis is on the one hand a partial analysis and, on the other hand, it does not define a dependency tree, and can also be seen as a set of constraints on the shape of the tree.",
            "As the analyzers are applied after morphological processing, the errors can be propagated and augmented.",
            "Regarding the data-driven parsers, they are trained using two kinds of tags as input: syntactic analyzers (two last lines of the example in figure 1).",
            "Although not shown in Table 2, we also inspected the results on the long distance relations, where we did not observe noticeable improvements with respect to the baseline on any parser.",
            "In this paper we present a set of preliminary experiments on the combination of two knowledge-based partial syntactic analyzers with two state of the art data-driven statistical parsers.",
            "Yeh (2000) used the output of several baseline diverse parsers to increase the performance of a second transformation-based parser.",
            "Figure 1 presents an example of a syntactically annotated sentence.",
            "The erroneous assignment of incorrect part of speech or morphological features can difficult the work of the parser.",
            "The first step consisted in applying the complete set of text processing tools for Basque, including: properties, such as case, number, tense, or different types of subordination for verbs.",
            "The information in figure 1 has been simplified due to space reasons, as typically each word contains many morphosyntactic features (case, number, type of subordinated sentence, ...), which are relevant for parsing.",
            "Combining Rule-Based and Statistical Syntactic Analyzers",
            "Most of the experiments on combined parsers have relied on different types of statistical parsers (Sagae and Lavie, 2006, Martins et al., 2008, McDonald and Nivre, 2011), trained on an automatically annotated treebank.",
            "The rule-based chunker (RBC henceforth, Aranzabe et al., 2009) uses 560 rules, where 479 of the rules deal with noun phrases and the rest with verb phrases.",
            "To determine the best action at each step, the parser uses history-based feature models and discriminative machine learning.",
            "The results show a modest improvement over the baseline, although they also present interesting lines for further research.",
            "The analyzers are a rulebased chunker, a rule-based shallow dependency parser and two state of the art data-driven dependency parsers, MaltParser and MST.",
            "The learning configuration can include any kind of information (such as word-form, lemma, category, subcategory or morphological features).",
            "As could be expected, the gold tags gave a noticeable improvement to the parser\u2019s results, reaching 95% LAS.",
            "The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, and not just over single arc attachments.",
            "We can point out some avenues for further research: schemes, such as voting, trying to get the best from each type of parser.",
            "The table shows the differences in f-score2 corresponding to five local dependency relations, (determination of verbal modifiers, such as subject, object and indirect object).",
            "Table 1 shows the results of using the output of the knowledge-based analyzers as input to the statistical parsers.",
            "As the CG formalism only allows the assignment of tags, the rules only aim at marking the name of the dependency relation together with the direction of the head (left or right).",
            "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al., 2007b) and MST Parser (McDonald et al., 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (McDonald and Nivre, 2007).",
            "We have presented a preliminary effort to integrate different syntactic analyzers, with the objective of getting the best from each system.",
            "To learn arc scores, it uses large-margin structured learning algorithms, which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set.",
            "Consequently, the morphological analyzer for Basque (Aduriz et al. 2000) gives a high ambiguity.",
            "However, when examining the scores for the output dependency relations, we noticed that the gold partial dependency tags are beneficial for some relations, although negative for some others.",
            "We must take into account that this evaluation was performed on the gold POS tags, rather than on automatically assigned POS tasks, as in the present experiment.",
            "In the following experiments we will make use of the second order non-projective algorithm.",
            "These tags contain errors of the CG-based syntactic taggers.",
            "Our work will make use the second version of the Basque dependency Treebank (BDT II, Aduriz et al., 2003), containing 150,000 tokens (11,225 sentences).",
            "The last tag marks words that are outside a chunk.",
            "For that reason, we performed a matching process trying to link the multiword units given by the morphological analysis module and the treebank, obtaining a correct match for 99% of the sentences.",
            "MaltParser (Nivre, 2006) is a representative of local, greedy, transition-based dependency parsing models, where the parser obtains deterministically a dependency tree in a single pass over the input using two data structures: a stack of partially analyzed items and the remaining input sequence.",
            "Several variants of the parser have been implemented, and we will use one of its standard versions (MaltParser version 1.4).",
            "This is in contrast to the local but richer contexts used by transition-based parsers."
        ]
    },
    "P87-1015": {
        "input_sentences": [
            "Discussion",
            "The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar, Klein, Pulluna, and Sag, 1985), and GB (as described by Berwick, 1984) with those underlying LFG and FUG.",
            "We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems, and classified these formalisms on the basis of two features: path complexity; and path independence.",
            "We address the question of whether or not a formalism can generate only structural descriptions with independent paths.",
            "We contrasted formalisms such as CFG's, HG's, TAG's and MCTAG's, with formalisms such as IG's and unificational systems such as LFG's and FUG's.",
            "In order to observe the similarity between these constrained systems, it is crucial to abstract away from the details of the structures and operations used by the system.",
            "In a grammar which generates independent paths the derivations of sibling constituents can not share an unbounded amount of information.",
            "The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.",
            "As suggested in Section 4.3.2, a derivation with independent paths can be divided into subcomputations with limited sharing of information.",
            "Abstract",
            "In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.",
            "We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.",
            "As illustrated by MCTAG's, it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's, HG's, and TAG's.",
            "Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.",
            "It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.",
            "This property reflects an important aspect of the underlying linguistic theory associated with the formalism.",
            "CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*"
        ]
    },
    "D09-1034": {
        "input_sentences": [
            "This work contributes to the important, developing enterprise of leveraging data-driven NLP approaches to derive new measures of high utility for psycholinguistic and neuropsychological studies.",
            "We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size.",
            "In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG.",
            "We have presented novel methods for teasing apart syntactic and lexical surprisal from a fully lexicalized parser, as well as for extending the operation of a predictive parser to capture novel entropy measures that are also shown to be relevant to psycholinguistic modeling.",
            "Empirical results demonstrate the utility of our methods in predicting human reading times.",
            "Abstract",
            "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing",
            "A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006).",
            "Our approach to calculating syntactic surprisal, based on fully lexicalized parsing, provided significant effects, while the POS-tag based (unlexicalized) surprisal \u2013 of the sort used in Boston et al. (2008a) and Demberg and Keller (2008) \u2013 did not provide a significant effect in our trials.",
            "Further, we showed an effect of lexical surprisal for closed class words even when combined with unigram and bigram probabilities in the same model.",
            "Summary",
            "Such automatic methods provide psycholinguistically relevant measures that are intractable to calculate by hand.",
            "The empirical validation presented here demonstrated that the new measures \u2013 particularly syntactic entropy and syntactic surprisal \u2013 have high utility for modeling human reading time data."
        ]
    },
    "W05-0638": {
        "input_sentences": [
            "The experimental results show that full parsing information not only increases the F-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies, which increases the F-score by 0.64%.",
            "Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ.",
            "Finally, our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ.",
            "Conclusion",
            "In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs.",
            "Abstract",
            "In this paper, we add more full parsing features to argument classification models, and represent full parsing information as constraints in ILPs to resolve labeling inconsistencies.",
            "In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs.",
            "We also integrate two argument classification models, ME and SVM, by combining their argument classification results and applying them to the above-mentioned ILPs.",
            "Exploiting Full Parsing Information To Label Semantic Roles Using An Ensemble Of ME And SVM Via Integer Linear Programming",
            "The results show full parsing information increases the total F-score by 1.34%.",
            "The ensemble of SVM and ME also boosts the F-score by 0.77%."
        ]
    },
    "W12-3145": {
        "input_sentences": [
            "In of the Sixth on Statistical Machine pages 187\u2013197.",
            "We obtained a moderate gain of 0.4 BLEU points with the ensemble decoding over the baseline system in newstest-2011.",
            "Conclusion",
            "In French-English, we experimented the ensemble decoding framework that effectively utilizes the small amount of news genre data to improve the performance in the testset belonging to the same genre.",
            "We submitted systems in two language pairs FrenchEnglish and English-Czech for WMT-12 shared task.",
            "For newstest-2012, it performs comparably to that of the baseline and we are presently investigating the lack of improvement in newstest-2012.",
            "2011.",
            "models for morpheme segmentation and morphology Transactions on Speech and Language 4(1):3:1\u20133:34, February.",
            "Abstract",
            "For Cz-En, We found that the BLEU scores do not substantially differ from each other and also the minor differences are not consistent for Test-11 and Test-12.",
            "Kriya - The SFU System for Translation Task at WMT-12",
            "KenLM: Faster and smaller model queries.",
            "Kenneth Heafield."
        ]
    },
    "P04-1036": {
        "input_sentences": [
            "Abstract",
            "In the future, we will perform a large scale evaluation on domain specific corpora.",
            "Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration.",
            "We use an automatically acquired thesaurus and a WordNet Similarity measure.",
            "Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.",
            "There is plenty of scope for further work.",
            "This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.",
            "Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.",
            "Indeed, the merit of our technique is the very possibility of obtaining predominant senses from the data at hand.",
            "Whilst we have used WordNet as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbours and the senses.",
            "We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.",
            "We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.",
            "In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.",
            "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.",
            "The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.",
            "We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.",
            "Conclusions",
            "This is a very promising result given that our method does not require any hand-tagged text, such as SemCor.",
            "Finding Predominant Word Senses in Untagged Text",
            "In particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora.",
            "The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data.",
            "The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.",
            "We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al., 2004).",
            "The lesk measure for example, can be used with definitions in any standard machine readable dictionary."
        ]
    },
    "N10-1091": {
        "input_sentences": [
            "In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models.",
            "First, we showed that the diversity of base parsers is more important than complex learning models for parser combination, i.e., (a) ensemble models that combine several base parsers at runtime performs significantly better than a state-ofthe-art model that combines two parsers at learning time, and (b) meta-classification does not outperform unsupervised voting schemes for the re-parsing of candidate dependencies when six base models are available.",
            "And lastly, our analysis indicates that unweighted voting performs as well as weighted voting for the re-parsing of candidate dependencies.",
            "Considering that different base models are easy to generate, this work proves that ensemble parsers that are both accurate and fast can be rapidly developed with minimal effort.",
            "Second, we showed that well-formed dependency trees can be guaranteed without significant performance loss by linear-time approximate re-parsing algorithms.",
            "Abstract",
            "Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking.",
            "This study unearthed several non-intuitive yet important observations about ensemble models for dependency parsing.",
            "Ensemble Models for Dependency Parsing: Cheap and Good?",
            "This study proves that fast and accurate ensemble parsers can be built with minimal effort.",
            "Conclusions"
        ]
    },
    "E12-1055": {
        "input_sentences": [
            "Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation",
            "We expand on work by (Foster et al., 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research.",
            "While techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately.",
            "We also show that we can separately optimize the four variable features in the Moses translation model through perplexity optimization.",
            "This paper contributes to SMT domain adaptation research in several ways.",
            "Conclusion",
            "This is not only useful for domain adaptation, but for various tasks that profit from mixture mod15The source code is available in the Moses repository http://github.com/moses-smt/mosesdecoder elling.",
            "We also explore adapting multiple (4\u201310) data sets with no priori between in-domain and out-of-domain data except for an in-domain development set.",
            "Abstract",
            "We envision that a weighted combination could be useful to deal with noisy datasets, or applied after a clustering of training data.",
            "We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT).",
            "We demonstrate that perplexity minimization scales well to a higher number of translation models.",
            "We break with prior domain adaptation research in that we do not rely on a binary clustering of in-domain and out-of-domain training data."
        ]
    },
    "D11-1033": {
        "input_sentences": [
            "The maximum size of a useful general-domain corpus is now limited only by the availability of data, rather than by how large a translation model can be fit into memory at once.",
            "Domain Adaptation via Pseudo In-Domain Data Selection",
            "Abstract",
            "Besides improving translation performance, this work also provides a way to mine very large corpora in a computationally-limited environment, such as on an ordinary computer or perhaps a mobile device.",
            "In particular, the new bilingual Moore-Lewis method, which is specifically tailored to the machine translation scenario, is shown to be more efficient and stable for MT domain adaptation.",
            "Nonetheless, we have shown that relatively tiny amounts of this pseudo in-domain data can prove more useful than the entire general-domain corpus for the purposes of domain-targeted translation tasks.",
            "A translation model trained on any of these subcorpora can be comparable \u2013 or substantially better \u2013 than a translation system trained on the entire corpus.",
            "Translation models trained on data selected in this way consistently outperformed the general-domain baseline while using as few as 35k out of 12 million sentences.",
            "Performance is further improved when we use these domain-adapted models in combination with a true in-domain model.",
            "This paper has also explored three simple yet effective methods for extracting these pseudo indomain sentences from a general-domain corpus.",
            "We have also shown in passing that the linear interpolation of translation models may work less well for translation model adaptation than the multiple paths decoding technique of (Birch et al., 2007).",
            "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain.",
            "The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding.",
            "Sentence pairs from a general-domain corpus that seem similar to an in-domain corpus may not actually represent the same distribution of language, as measured by language model perplexity.",
            "These approaches of data selection and model combination can be stacked, resulting in a compact, two phrase-table, translation system trained on 1% of the available data that again outperforms a state-of-theart translation system trained on all the data.",
            "Conclusions",
            "As these sentences are not themselves identical the in-domain data, we call them These subcorpora \u2013 1% the size of the original \u2013 can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus.",
            "This fast and simple technique for discarding over 99% of the general-domain training corpus resulted in an increase of 1.8 BLEU points.",
            "These sentences may be selected with simple cross-entropy based methods, of which we present three."
        ]
    },
    "P10-1021": {
        "input_sentences": [
            "In the psycholinguistic literature, various types of semantic information have been investigated: lexical semantics (word senses, selectional restrictions, thematic roles), sentential semantics (scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion.",
            "For example, we could envisage a parser that uses semantic representations to guide its search, e.g., by pruning syntactic analyses that have a low semantic probability.",
            "The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load.",
            "We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words).",
            "Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors.",
            "However, we are not able to investigate this hypothesis: our approach to testing the significance of factors requires nested models; the log-likelihood test (see Section 4) is only able to establish whether adding a factor to a model improves its fit; it cannot compare models with disjunct sets of factors (such as a model containing the integrated surprisal measure and one containing the three separate ones).",
            "Moreover, comparisons of the individual contributions of syntactic and semantic factors were generally absent from the literature.",
            "Abstract",
            "This means that semantic costs are a significant predictor of reading time in addition to the well-known syntactic surprisal.",
            "In doing so, we were able to compare different syntactic and semantic costs on the same footing.",
            "At the same time, the semantic model should have access to syntactic information, i.e., the composition of word representations should take their syntactic relationships into account, rather than just linear order.",
            "It is conceptually simpler (as it is more parsimonious), and is also easier to use in applications (such as readability prediction).",
            "An interesting question is which aspects of semantics our model is able to capture, i.e., why does the combination of LSA or LDA representations with an incremental parser yield a better fit of the behavioral data.",
            "There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated.",
            "Our work departs from previous approaches in that we propose a single measure which integrates syntactic and semantic factors.",
            "Finally, an integrated measure requires less parameters; our definition of surprisal in 12 is simply the sum of the trigram, syntactic, and semantic components.",
            "However, we would argue that a single, integrated measure that captures human predictive processing is preferable over a collection of separate measures.",
            "Crucially, we were able to show that the semantic component of our measure improves reading time predictions over and above a model that includes syntactic measures (based on a trigram model and incremental parser).",
            "Previous analyses of semantic constraint have been conducted on different eye-tracking corpora (Dundee and Embra Corpus) and on different languages (English, French).",
            "In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.",
            "In evaluating our model, we adopted a broad coverage approach using the reading time data from a naturalistic corpus rather than artificially constructed experimental materials.",
            "However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference).",
            "An LME model containing separate factors, on the other hand, requires a coefficient for each of them, and thus has more parameters.",
            "A key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly.",
            "Discussion",
            "Evaluation on an eye-tracking corpus shows that our measure predicts reading time better than a baseline model that captures low-level factors in reading (word length, landing position, etc.).",
            "Our analysis showed that both of these factors can be captured by our integrated surprisal measure which is uniformly probabilistic and thus preferable to modeling semantic and syntactic costs disjointly using a mixture of probabilistic and nonprobabilistic measures.",
            "An open issue is whether a single, integrated measure (as evaluated in Table 4) fits the eyemovement data significantly better than separate measures for trigram, syntactic, and semantic surprisal (as evaluated in Table 3.",
            "Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure",
            "In this paper we investigated the contributions of syntactic and semantic constraint in modeling processing difficulty."
        ]
    },
    "E12-1014": {
        "input_sentences": [
            "We propose a novel algorithm to estimate reordering probabilities from monolingual data.",
            "We estimate the parameters of a phrasebased statistical machine translation sysfrom instead of a corpus.",
            "We also showed that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated features.",
            "We extend existing research on bilingual lexicon induction estimate and phrasal translation probabilities for MT-scale phrasetables.",
            "Conclusion",
            "In this paper, we examine an idealization where a phrase-table is given.",
            "Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover over 82% of BLEU loss that resulted from removing the bilingual-corpus-derived phrase-table probabilities.",
            "Abstract",
            "Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights.",
            "We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone.",
            "We evaluated the performance of our algorithms in a full end-to-end translation system.",
            "Thus our techniques have stand-alone efficacy when large bilingual corpora are not available and also make a significant contribution to combined ensemble performance when they are.",
            "This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains.",
            "We report translation results for an end-to-end translation system using these monolingual features alone.",
            "We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features.",
            "Toward Statistical Machine Translation without Parallel Corpora"
        ]
    },
    "P14-2022": {
        "input_sentences": [
            "We contribute a faster decoding algorithm for phrase-based machine translation.",
            "This leads to two contributions: hiding irrelevant state from features and an incremental refinement algorithm to find high-scoring combinations.",
            "Our key contribution avoids this behavior by placing hypotheses into equivalence classes, masking the parts of state that matter least to the score.",
            "Conclusion",
            "Faster Phrase-Based Decoding by Refining Feature State",
            "Most features depend upon only part of the state, but traditional algorithms, including cube pruning, handle state atomically.",
            "Abstract",
            "Since our algorithm and cube pruning are both approximate, improvement can be used to increase speed or accuracy.",
            "When tuned to attain the same accuracy, our algorithm is 4.0\u20137.7 times as fast as the Moses decoder with cube pruning.",
            "We have contributed a new phrase-based search algorithm based on the principle that the language model cares the most about boundary words.",
            "Translation hypotheses keep track of state, such as context for the language model and coverage of words in the source sentence.",
            "For example, cube pruning will repeatedly query the language model with hypotheses that differ only in source coverage, despite the fact that source coverage is irrelevant to the language model.",
            "This algorithm is implemented in a new fast phrase-based decoder, which we release as open-source under the LGPL at kheafield.com/code/.",
            "Moreover, we exploit shared words in hypotheses to iteratively refine language model scores rather than handling language model state atomically."
        ]
    },
    "N07-2041": {
        "input_sentences": [
            "Simultaneous Identification of Biomedical Named-Entity and Functional Relation Using Statistical Parsing Techniques",
            "In this paper we introduced a statistical parsingbased method to extract biomedical relations from MEDLINE articles.",
            "We made use of a large unlabeled data set to train our relation extraction model.",
            "Abstract",
            "We also plan to train a graphical model based on all extracted BP, PL and BPL relations to infer relations from multiple sentences and documents.",
            "We build a parser that derives both syntactic and domain-dependent semantic information achieves an F-score of the relation extraction task.",
            "In this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles.",
            "We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research.",
            "Experiments show that the semi-supervised method significantly outperforms the fully supervised method with F-score increasing from 48.4% to 83.2%.",
            "We have implemented a discriminative model (Liu et al., 2007) which takes as input the examples with gold named entities and identifies BPL relations on them.",
            "Discussion and Future Work",
            "In future work, we plan to let the discriminative model take the output of our parser and refine our current results further."
        ]
    },
    "P13-2009": {
        "input_sentences": [
            "Discussion",
            "Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance.",
            "Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser.",
            "The parser performs comparably to several recent purpose-built semantic parsers on the GeoQuery dataset, while training considerably faster than state-of-the-art systems.",
            "The first is the incorporation of a language model (or comparable long-distance structure-scoring model) to assign scores to predicted parses independent of the transformation model.",
            "Semantic Parsing as Machine Translation",
            "These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.",
            "Abstract",
            "In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems.",
            "The second is the use of large, composed rules (rather than rules which trigger on only one lexical item, or on tree portions of limited depth (Lu et al., 2008)) in order to \u201cmemorize\u201d frequently-occurring largescale structures.",
            "We have presented a semantic parser which uses techniques from machine translation to learn mappings from natural language to variable-free meaning representations.",
            "Our experiments demonstrate the usefulness of several techniques which might be broadly applied to other semantic parsers, and provides an informative basis for future work.",
            "Our results validate the hypothesis that it is possible to adapt an ordinary MT system into a working semantic parser.",
            "Conclusions",
            "The results also demonstrate the usefulness of two techniques which are crucial for successful MT, but which are not widely used in semantic parsing.",
            "For this reason, we argue for the use of a machine translation baseline as a point of comparison for new methods.",
            "In spite of the comparative simplicity of the approach, it achieves scores comparable to (and sometimes better than) many state-of-the-art systems."
        ]
    },
    "W11-2147": {
        "input_sentences": [
            "For English?German we attempted to improve the trans lation tables with a combination of standard statistical word alignments and phrase-basedword alignments.",
            "For German?English trans lation we tried to make the German text moresimilar to the English text by normalizing Ger man morphology and performing rule-basedclause reordering of the German text.",
            "This resulted in small improvements for both transla tion directions.",
            "Abstract",
            "Experiments with word alignment normalization and clause reordering for SMT between English and German",
            "This paper presents the LIU system for theWMT 2011 shared task for translation be tween German and English."
        ]
    },
    "P14-1008": {
        "input_sentences": [
            "An inference engine is built to achieve inference on abstract denotations.",
            "In this paper, we equip the DCS framework logical inference, by defining abdenotations an abstraction of the computing process of denotations in original DCS.",
            "Abstract",
            "Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics.",
            "Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation.",
            "Experiments on FraCaS and PASCAL RTE datasets show promising results.",
            "Logical Inference on Dependency-based Compositional Semantics"
        ]
    },
    "C10-1132": {
        "input_sentences": [
            "Abstract",
            "A simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches.",
            "Experiments on the Sec ond SIGHAN Bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one.",
            "Also, we thank Behavior Design Corporation for using their show special thanks t her helps with the Gen The research work has been partially funded by the Natural Science Foundation of China under Grant No. 60975053, 90820303 and 60736014, the National Key Technology R&D Program under Grant No. 2006BAH03B02, and also the Hi-Tech Research and Develop ent Program (?863?",
            "In addition, closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best F score in four out of five corpora.",
            "A Character-Based Joint Model for Chinese Word Segmentation",
            "Conclusion",
            "Ad MNLP, pages Hu ld Word Segmenter Ku d Keh-Yih Su, 2009.",
            "How ever, further study is required to find out the true reason for this strange but interesting ph 1180 A Generic-Beam-Search code and o Ms. Nanyan Kuo for eric-Beam-Search code.",
            "pages Hu Second Ru2006.",
            "Subword-based Tagging for Con Yi scores: How much im Yu f ACL, pages 840-847, cknowledgement The authors extend sincere thanks to Wenbing Jiang for his helps with our experiments.",
            "Last, it is found that weighting various features differently would give better result.",
            "Yi scriminative Ni ssing, 8 (1).",
            "Experiments on the Second SIGHAN Bakeoff show that the joint model achieves 21% error reduction over the dis criminative model (14% over the generative model).",
            "Program) of China under Grant No. 2006AA010108-4 as well.",
            "The character-based tagging approach is a dominant technique for Chinese word segmentation, and both discrimi native and generative models can be adopted in that framework.",
            "From the error analysis of the character-based generative model and the discriminative o we found that thes each other on hanwords.",
            "To take advantage of these two ap proaches, a joint model is thus proposed to combine them.",
            "Moreover, closed tests on the second SIGHAN Bakeoff corpora show that this joint model significantly outperforms all the state of-the-art systems reported in the literature.",
            "In Proceedings of GHAN Workshop on Chinese Lan St Th Jia W Jo Hw Fu ction using conditional random fields.",
            "m optimum entation.",
            "However, generative and discriminative charac ter-based approaches are significantly different and complement each other."
        ]
    },
    "P11-1089": {
        "input_sentences": [
            "However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other.",
            "In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures.",
            "This model may be refined by incorporating richer features and improved decoding.",
            "In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.",
            "We have proposed a discriminative model that jointly infers morphological properties and syntactic structures.",
            "A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing",
            "Abstract",
            "Further evaluation on other morphological systems would also be desirable.",
            "Conclusions and Future Work",
            "In particular, we would like to experiment with higher-order features (\u00a76), and with maximum a posteriori decoding, via max-product BP or (relaxed) integer linear programming.",
            "Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the \u201cpipeline\u201d approach, assuming that morphological information has been separately obtained.",
            "Most previous studies of morphological disambiguation and dependency parsing have been pursued independently."
        ]
    },
    "W06-0508": {
        "input_sentences": [
            "We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases.",
            "We suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text.",
            "Conclusions and future work",
            "Currently we are concluding the integration of the several modules composing our architecture.",
            "The relations extracted can be used for various tasks, including semantic web annotation and ontology learning.",
            "Abstract",
            "Subsequently, we will incorporate the architecture to a semantic web portal and accomplish an extrinsic evaluation in the context of that application.",
            "We will then carry experiments with our corpus of newsletters in order to evaluate the approach.",
            "A Hybrid Approach For Extracting Semantic Relations From Texts",
            "We presented a hybrid approach for the extraction of semantic relations from text.",
            "It was designed mainly to enrich the annotations produced by a semantic web portal, but can be used for other domains and applications, such as ontology population and development.",
            "Since the approach uses deep linguistic processing and corpus-based strategies not requiring any manual annotation, we expect it will accurately discover most of the relevant relations in the text."
        ]
    },
    "C10-1061": {
        "input_sentences": [
            "We evaluate our parser with a gram mar extracted from the German NeGratreebank.",
            "LCFRS, an extension of CFG, can de scribe discontinuities both in constituencyand dependency structures in a straight forward way and is therefore a naturalcandidate to be used for data-driven parsing.",
            "Conclusion",
            "2Note that these results were obtained on sentences with a length of ? 40 words and that those parser possibly would deliver better results if tested on our test set.",
            "To speed up parsing, we use context summary estimates for parse items.",
            "Therefore, our approach demon strates convincingly that PLCFRS is a natural and tractable alternative for data-driven parsing which takes non-local dependencies into consideration.",
            "Abstract",
            "544",
            "Our experiments show that data driven LCFRS parsing is feasible with a reasonable speed and yields output of competitive quality.",
            "Data-Driven Parsing with Probabilistic Linear Context-Free Rewriting Systems",
            "This paper presents a first efficient imple mentation of a weighted deductive CYKparser for Probabilistic Linear ContextFree Rewriting Systems (PLCFRS), to gether with context-summary estimatesfor parse items used to speed up parsing.",
            "An evaluation on the NeGra treebank, both in terms of output quality and speed, showsthat data-driven parsing using PLCFRS is feasible.",
            "Already in this first attempt with a straight forward binarization, we obtain results that are comparable to state-of-the-art PCFG results in terms of F1, while yielding parse trees that are richer than context-free trees since they describediscontinuities.",
            "We have presented the first parser for unrestrictedProbabilistic Linear Context-Free Rewriting Sys tems (PLCFRS), implemented as a CYK parser with weighted deductive parsing."
        ]
    },
    "P12-1025": {
        "input_sentences": [
            "Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.",
            "The appropriate application of heterogeneous annotations leads to a significant improvement (a relative error reduction of 11%) over the best performance for this task.",
            "Our theoretical and empirical analysis of two representative popular corpora highlights two essential characteristics of heterogeneous annotations which are explored to reduce approximation and estimation errors for Chinese word segmentation and POS tagging.",
            "The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error.",
            "Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations",
            "Conclusion",
            "We employ stacking models to incorporate features derived from heterogeneous analysis and apply them to convert heterogeneous labeled data for re-training.",
            "Penn Chinese Treebank (CTB) and PKU\u2019s People\u2019s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible.",
            "Abstract",
            "We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging.",
            "We empirically analyze the diversity between two representative corpora, i.e.",
            "Although our discussion is for a specific task, the key idea to leverage heterogeneous annotations to reduce the approximation error with stacking models and the estimation error with automatically converted corpora is very general and applicable to other NLP tasks."
        ]
    },
    "P14-1012": {
        "input_sentences": [
            "Secondly, compared with the baseline phrase features Xi, our introduced input original phrase features X significantly improve the performance of not only our DAE features but also the DBN features.",
            "Firstly, our DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the DBN features and the baseline features, respectively.",
            "Using the unsupervised pre-trained deep belief net (DBN) to initialize DAE\u2019s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features.",
            "In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we have learned new features using the DAE for the phrase-based translation model.",
            "Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning.",
            "On two Chinese- English tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the baseline features, respectively.",
            "Lastly, to learn high dimensional feature representation, we introduce a natural horizontal composition of two DAEs for large hidden layers feature learning.",
            "Compared with the original features, DNN (DAE and HCDAE) features are learned from the non-linear combination of the original features, they strong capture high-order correlations between the activities of the original features, and we believe this deep learning paradigm induces the original features to further reach their potential for SMT.",
            "In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep autoencoder (DAE) paradigm for phrase-based translation model.",
            "Abstract",
            "Using the unsupervised pre-trained DBN to initialize DAE\u2019s parameters and using the input original phrase features as the \u201cteacher\u201d for semi-supervised backpropagation, our semi-supervised DAE features are more effective and stable than the unsupervised DBN features (Maskey and Zhou, 2012).",
            "Moreover, to further improve the performance, we introduce some simple but effective features as the input features for feature learning.",
            "On two Chinese-English translation tasks, the results demonstrate that our solutions solve the two aforementioned shortcomings successfully.",
            "Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation",
            "Conclusions",
            "The results also demonstrate that DNN (DAE and HCDAE) features are complementary to the original features for SMT, and adding them together obtain statistically significant improvements of 3.16 (IWSLT) and 2.06 (NIST) BLEU points over the baseline features."
        ]
    },
    "P14-2110": {
        "input_sentences": [
            "We Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis.",
            "Abstract",
            "Learning Polylingual Topic Models from Code-Switched Social Media Documents",
            "Code-switched documents are in social media, providing evidence polylingual topic models to infer aligned topics across languages.",
            "We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators."
        ]
    },
    "D11-1140": {
        "input_sentences": [
            "In the future, it will also be important to explore morphological models, to better model variation within the existing lexemes.",
            "Such lexicons can be inefficient when words appear repeatedly with closely related lexical content.",
            "1 Discussion",
            "Abstract",
            "Lexical Generalization in CCG Grammar Induction for Semantic Parsing",
            "Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content.",
            "The factored lexical representation also has significant potential for lexical transfer learning, where we would need to learn new lexemes for each target application, but much of the information in the templates could, potentially, be ported across domains.",
            "We argued that factored CCG lexicons, which include both lexemes and lexical templates, provide a compact representation of lexical knowledge that can have advantages for learning.",
            "Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring.",
            "We also described a complete approach for inducing factored, probabilistic CCGs for semantic parsing, and demonstrated strong performance across a wider range of benchmark datasets that any previous approach.",
            "In this paper, we introduce factored lexicons, which both model word meaning model systematic variation in word usage.",
            "We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations.",
            "We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model."
        ]
    },
    "P01-1018": {
        "input_sentences": [
            "Discussion",
            "We have proposed a more constrained version of Joshi\u2019s question, \u201cHow much strong generative power can be squeezed out of a formal system without increasing its weak generative power,\u201d and shown that within these constraints, a variant of TAG called MMTAG characterizes the limit of how much strong generative power can be squeezed out of CFG.",
            "Moreover, using the notion of a meta-level grammar, this result is extended to formalisms beyond CFG.",
            "This way of approaching Joshi\u2019s question is by no means the only way, but we hope that this work will contribute to a better understanding of the strong generative capacity of constrained grammar formalisms as well as reveal more powerful formalisms for linguistic analysis and natural language processing.",
            "Finally, we generalize this result to formalisms beyond CFG.",
            "Abstract",
            "It remains to be seen whether RF-MMTAG, whether used directly or for specifying meta-level grammars, provides further practical benefits on top of existing \u201csqueezed\u201d grammar formalisms like tree-local MCTAG, tree insertion grammar, or regular form TAG.",
            "We consider the question \u201cHow much strong generative power can be squeezed out of a formal system without increasing its weak generative power?\u201d and propose some theoretical and practical constraints on this problem.",
            "We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar.",
            "Constraints On Strong Generative Power"
        ]
    },
    "D12-1046": {
        "input_sentences": [
            "Joint Chinese Word Segmentation, POS Tagging and Parsing",
            "Abstract",
            "In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing.",
            "Previous work often used a pipeline method \u2013 Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components.",
            "Acknowledgments The authors thank Zhongqiang Huang for his help with experiments.",
            "Conclusion",
            "\u2193 and \u2191 mean the error number reduced or increased by the joint model. parsing method.",
            "Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system.",
            "Our experiments demonstrate the advantage of the joint models.",
            "This work is partly supported by DARPA under Contract No.",
            "We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features.",
            "The sub-models are independently trained for the three tasks to reduce model complexity and optimize individual sub-models.",
            "Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.",
            "We will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm.",
            "As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing.",
            "In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding.",
            "HR0011-12-C-0016.",
            "In this paper, we proposed a new algorithm for joint Chinese word segmentation, POS tagging, and parsing.",
            "In the future work, we will compare this joint model to the pipeline approach that uses multiple candidates or soft decisions in the early modules.",
            "Our algorithm is an extension of the CYK number of the corresponding pattern made by the pipeline tagging model."
        ]
    },
    "P12-2058": {
        "input_sentences": [
            "Heuristic Cube Pruning in Linear Time",
            "We propose a novel heuristic algorithm for Cube Pruning running in linear time in the beam size.",
            "Abstract",
            "Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy."
        ]
    },
    "W10-2803": {
        "input_sentences": [
            "They are often based on feature vectors, and there are many interesting ideas in these models that have not yet been used (much) in computational models of word meaning.",
            "While dimensions of distributional models are usually not individually interpretable, there are some first models (Almuhareb and Poesio, 2005; Baroni et al., 2010) that use patterns to extract meaningful dimensions from corpus data.",
            "Will they do better than dictionary-based models?",
            "Abstract",
            "One of the most exciting ones, perhaps, is that cognitive models often have interpretable dimensions.",
            "Can interpretable dimensions be used for inferences?",
            "More specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees.",
            "In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation.",
            "We have argued that it makes sense to look to cognitive models of mental concept representation.",
            "Or move away from dictionary senses completely, and only model similarities between individual word usages.",
            "The third hypothesis then departs from dictionary senses, suggesting (C) focusing on individual word usages and their similarities instead.",
            "What Is Word Meaning Really? (And How Can Distributional Models Help Us Describe It?)",
            "The current evaluations, testing paraphrase applicability in context, are a step in the right direction, but more task-oriented evaluation schemes have to follow.",
            "We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models.",
            "The first two involve dictionary senses, suggesting that (A) senses may best be viewed as applying to a certain degree, rather than in a binary fashion, and (B) that it may make sense to describe an occurrence through multiple senses as a default rather than an exception.",
            "In this paper, we have argued that it may be time to consider alternative computational models of word meaning, given that word sense disambiguation, after all this time, is still a tough problem for humans as well as machines.",
            "This offers many new perspectives: For which tasks can we improve performance by selecting dimensions that are meaningful specifically for that task (as in Mitchell et al. (2008))?",
            "One big question is, of course, about the usability of these alternative models of word meaning in NLP applications.",
            "We have followed three hypotheses.",
            "Conclusion and outlook",
            "And, when we are computing vector space representations for word meaning in context, is it possible to select meaningful dimensions that are appropriate for a given context?",
            "We have argued that distributional models are a good match for word meaning models following hypotheses (A)(C): They can represent individual word usages as points in vector space, and they can also represent dictionary senses in a way that allows for graded membership and overlapping senses, and we have discussed some existing models, both prototypebased and exemplar-based."
        ]
    },
    "P12-1048": {
        "input_sentences": [
            "In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation.",
            "This paper presents a novel method for SMT system adaptation by making use of the monolingual corpora in new domains.",
            "Finally, the reasonable estimation of topic number for better translation model adaptation will also become our study emphasis.",
            "Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.",
            "Abstract",
            "Experimental results show that our method achieves better performance than the baseline system, without increasing the burden of the translation system.",
            "To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily.",
            "Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora.",
            "Our approach first estimates the translation probabilities from the out-ofdomain bilingual corpus given the topic information, and then rescores the phrase pairs via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora.",
            "Furthermore, since the in-domain phrase-topic distribution is currently estimated with simple smoothing interpolations, we expect that the translation system could benefit from other sophisticated smoothing methods.",
            "Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information",
            "Conclusion and future work",
            "In the future, we will verify our method on other language pairs, for example, Chinese to Japanese."
        ]
    },
    "P04-1040": {
        "input_sentences": [
            "For all subtasks we used the same settings for TiMBL: simple feature overlap measure, 5 nearest neighbours with majority voting.",
            "Abstract",
            "Thus, we are able to provide virtually all information available in the corpus, without modifying the parser, viewing it, indeed, as a black box.",
            "It has been successfully applied to the identification of semantic relations (Ahn et al., 2004), using FrameNet as the training corpus.",
            "This is very close to the performance reported by Carroll et al. (2003) for the parser specifically designed for the extraction of grammatical relations.",
            "The method is based on graph rewriting using memorybased learning, applied to dependency structures.",
            "The experiments described in the previous sections indicate that although statistical parsers do not explicitly output some information available in the corpus they were trained on (grammatical and semantic tags, empty nodes, non-local dependencies), this information can be recovered with reasonably high accuracy, using pattern matching and machine learning methods.",
            "Our method is largely independent of the choice of parser and corpus, and shows state of the art performance.",
            "Our method allows us to perform a meaningful dependency-based comparison of phrase structure parsers.",
            "First, after converting both the treebank trees and parsers\u2019 outputs to graphs with head\u2013modifier relations, our method needs very little information about the linguistic nature of the data, and thus is largely corpusand parser-independent.",
            "Indeed, after the conversion, the only linguistically informed operation is the straightforward extraction of features indicating the presence of subject and object dependents, and finiteness of verb groups.",
            "0 Conclusions",
            "This general framework allows us to accurately recover both grammatical and semantic information as well as non-local dependencies.",
            "Our preliminary experiments with Collins\u2019 parser and the corpus annotated with grammatical relations (Carroll et al., 2003) are promising: the system achieves 76% precision/recall fscore, after the parser\u2019s output is enriched with our method and transformed to grammatical relations using a set of 40 simple rules.",
            "Finally, our method for enriching the output of a parser is, to a large extent, independent of a specific parser and corpus, and can be used with other syntactic and semantic resources.",
            "We presented a method to automatically enrich the output of a parser with information that is not provided by the parser itself, but is available in a treebank.",
            "The evaluation on a dependency corpus derived from the Penn Treebank showed that, after our post-processing, two state of the art statistical parsers achieve 84% accuracy on a fine-grained set of dependency labels.",
            "Furthermore, with the fine-grained set of dependency labels that our system provides, it is possible to map the resulting structures to other dependency formalisms, either automatically in case annotated corpora exist, or with a manually developed set of rules.",
            "Adding such semantic relations to syntactic dependency graphs was simply an additional graph transformation step.",
            "For our task, using dependency structures rather than phrase trees has several advantages.",
            "It is not clear whether the PARSEVAL evaluation can be easily extended to take non-local relations into account (see (Johnson, 2002) for examples of such extension).",
            "Enriching The Output Of A Parser Using Memory-Based Learning",
            "Finally, the independence from the details of the parser and the corpus suggests that our method can be applied to systems based on other formalisms, e.g., (Hockenmaier, 2003), to allow a meaningful dependency-based comparison of very different parsers.",
            "For this task, we viewed semantic relations (e.g., Speaker, Topic, Addressee) as dependencies between a predicate and its arguments.",
            "Discussion",
            "It also facilitates dependency-based evaluation of phrase structure parsers.",
            "During further experiments with our method on different corpora, we found that quite different settings led to a better performance.",
            "We describe a method for enriching the output of a parser with information available in a corpus.",
            "It is clear that more careful and systematic parameter tuning and the analysis of the contribution of different features have to be addressed.",
            "Despite the high-dimensional feature spaces, the large number of lexical features, and the lack of independence between features, we achieved high accuracy using a memory-based learner.",
            "Second, using a dependency formalism facilitates a very straightforward evaluation of the systems that produce structures more complex than trees.",
            "Finally, our method is not restricted to syntactic structures.",
            "Using the method with two state of the art statistical parsers and the Penn Treebank allowed us to recover functional tags (grammatical and semantic), empty nodes and traces.",
            "TiMBL performed well on tasks where structured, more complicated and task-specific statistical models have been used previously (Blaheta and Charniak, 2000)."
        ]
    },
    "P14-1004": {
        "input_sentences": [
            "We have shown our models yield superior performance both qualitatively and quantitatively.",
            "We apply them to two real, non-trivial datasets: human-computer spoken dialogues in bus query service, and humanhuman text-based chats from a live technical support service.",
            "We evaluated on two very different corpora\u2014logs from spoken, human-computer dialogues about bus time, and logs of textual, humanhuman dialogues about technical support.",
            "One possible avenue for future work is scalability.",
            "We show that our models extract meaningful state representations and dialogue structures consistent with human annotations.",
            "We have presented three new unsupervised models to discover latent structures in task-oriented dialogues.",
            "Abstract",
            "A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue, since it provides a basis for analysing, evaluating, and building conversational systems.",
            "Conclusion and Future Work",
            "In addition to MCMC, another class of inference method is variational Bayesian analysis (Blei et al., 2003; Beal, 2003), which is inherently easier to distribute (Zhai et al., 2012) and online update (Hoffman et al., 2010).",
            "We propose three new unsupervised models to discover latent structures in task-oriented dialogues.",
            "Quantitatively, we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task.",
            "Discovering Latent Structure in Task-Oriented Dialogues",
            "Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states).",
            "Parallelization (Asuncion et al., 2012) or online learning (Doucet et al., 2001) could significantly speed up inference."
        ]
    },
    "D10-1004": {
        "input_sentences": [
            "Other parsing formalisms can be handled with the inventory of factors shown here\u2014 among them, phrase-structure parsing.",
            "The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs.",
            "We presented a unified view of two recent approximate dependency parsers, by stating their underlying factor graphs and by deriving the variational problems that they address.",
            "Experiments show state-of-the-art performance for 14 languages.",
            "Conclusion",
            "Turbo Parsers: Dependency Parsing by Approximate Variational Inference",
            "We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009).",
            "By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method.",
            "Dependency Parsing.",
            "Abstract",
            "We provided an aggressive online algorithm for training the models with a broad family of losses.",
            "We introduced new hard constraint factors, along with formulae for their messages, local belief constraints, and entropies.",
            "There are several possible directions for future work.",
            "We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation.",
            "Recent progress in message-passing algorithms yield \u201cconvexified\u201d Bethe approximations that can be used for marginal inference (Wainwright et al., 2005), and provably convergent max-product variants that solve the relaxed LP (Globerson and Jaakkola, 2008)."
        ]
    },
    "P12-3029": {
        "input_sentences": [
            "This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodifier relationships are recorded.",
            "We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published.",
            "The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.",
            "Syntactic Annotations for the Google Books NGram Corpus",
            "Abstract",
            "The annotations are produced automatically with statistical models that are specifically adapted to historical text.",
            "Syntactic Annotations",
            "Conclusions"
        ]
    },
    "P01-1005": {
        "input_sentences": [
            "In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.",
            "We have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation.",
            "In this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available.",
            "Abstract",
            "Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.",
            "We are fortunate that for this particular application, correctly labeled training data is free.",
            "Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.",
            "While it is encouraging that there is a vast amount of on-line text, much work remains to be done if we are to learn how best to exploit this resource to improve natural language processing.",
            "The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.",
            "We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets.",
            "Conclusions",
            "We propose that a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections, while deemphasizing the focus on comparing different learning techniques trained only on small training corpora.",
            "Scaling To Very Very Large Corpora For Natural Language Disambiguation"
        ]
    },
    "D12-1108": {
        "input_sentences": [
            "We start by describing the decoding algorithm and the state operations used by our decoder, then we present empirical results demonstrating the effectiveness of our approach and its usability with a document-level semantic language model, and finally we discuss some related work.",
            "Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time.",
            "Moreover, by optionally initialising the state with the output of a traditional DP decoder, we can ensure that the final hypothesis is no worse than what would have been found by DP search alone.",
            "We have presented preliminary results on a cross-sentence semantic language model addressing the problem of lexical cohesion to demonstrate that this kind of models is worth exploring further.",
            "Abstract",
            "Our algorithm can be combined with DP beam search to leverage the quality of the traditional approach with increased flexibility for models at the discourse level.",
            "In this paper, we present a method for decoding complete documents in phrase-based SMT.",
            "In this paper, we have presented a decoding procedure for phrase-based SMT that makes it possible to define feature models with cross-sentence dependencies.",
            "However, we argue that the popular DP beam search algorithm, which delivers excellent decoding performance, but imposes a particular kind of local dependency structure on the feature models, has also had its share in driving researchers away from discourse-level problems.",
            "In the last twenty years of SMT research, there has been a strong assumption that sentences in a text are independent of one another, and discourse context has been largely neglected.",
            "Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems.",
            "Developing good discourse-level models is difficult, and considering the modest translation quality that has long been achieved by SMT, there have been more pressing problems to solve and lower hanging fruit to pick.",
            "Conclusion",
            "Several factors have contributed to this.",
            "The initial state is improved by the application of a series of operations using a hill climbing strategy to find a (local) maximum of the score function.",
            "Document-Wide Decoding for Phrase-Based Statistical Machine Translation",
            "Besides lexical cohesion, cross-sentence models are relevant for other linguistic phenomena such as pronominal anaphora or verb tense selection.",
            "Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences.",
            "We believe that SMT research has reached a point of maturity where discourse phenomena should not be ignored any longer, and we consider our decoder to be a step towards this goal.",
            "any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required.",
            "This setup gives us complete freedom to define scoring functions over the entire document."
        ]
    },
    "P08-1108": {
        "input_sentences": [
            "Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.",
            "Conclusion",
            "Abstract",
            "By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.",
            "In this paper, we have demonstrated how the two dominant approaches to data-driven dependency parsing, graph-based models and transition-based models, can be integrated by letting one model learn from features generated by the other.",
            "Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing.",
            "Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.",
            "In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.",
            "Moreover, a comparative error analysis reveals that the improvements are largely predictable from theoretical properties of the two models, in particular the tradeoff between global learning and inference, on the one hand, and rich feature representations, on the other.",
            "Integrating Graph-Based and Transition-Based Dependency Parsers"
        ]
    },
    "P12-1002": {
        "input_sentences": [
            "Since inference for SMT (unlike many other learning problems) is very expensive, especially on large training sets, good parallelization is key.",
            "Discussion",
            "Our approach is made feasible and effective by applying joint feature selection across distributed stochastic learning processes.",
            "We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.",
            "We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learnalgorithm that applies regularization for joint feature selection over distributed stochastic learning processes.",
            "In future work, we would like to investigate more sophisticated features, better learners, and in general improve the components of our system that have been neglected in the current investigation of relative improvements by scaling the size of data and feature sets.",
            "Abstract",
            "Furthermore, our local features are efficiently computable at runtime.",
            "For example, patent data can be characterized along the dimensions of patent classes and patent text fields (W\u00a8aschle and Riezler, 2012) and thus are well suited for multi-task translation.",
            "Evidence from machine learning indicates that increasing the training sample size results in better prediction.",
            "Joint Feature Selection in Distributed",
            "We presented an approach to scaling discriminative learning for SMT not only to large feature sets but also to large sets of parallel training data.",
            "Ultimately, since our algorithms are inspired by multi-task learning, we would like to apply them to scenarios where a natural definition of tasks is given.",
            "The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT.",
            "Our algorithms and features are generic and can easily be reimplemented and make our results relevant across datasets and language pairs.",
            "Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT",
            "With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data."
        ]
    },
    "N03-1004": {
        "input_sentences": [
            "In particular, we focused on two answering agents, one adopting a knowledge-based approach and one using statistical methods.",
            "Best performance using the % correct metric was achieved by the three-level algorithm that combines after each stage, while highest average precision was obtained by a two-level algorithm merging at the question and answer levels, supporting a tightly-coupled design for multi-agent question answering.",
            "In Question Answering Two Heads Are Better Than One",
            "The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques.",
            "Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric.",
            "In this paper, we introduced a multi-strategy and multisource approach to question answering that enables combination of answering agents adopting different strategies and consulting multiple knowledge sources.",
            "We discussed our answer resolution component which employs a multi-level combination algorithm that allows for resolution at the question, passage, and answer levels.",
            "Abstract",
            "Our experiments showed that our best performing algorithms achieved a 35.0% relative improvement in the number of correct answers and a 32.8% improvement in average precision on a previously unseen test set.",
            "We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels.",
            "Conclusions",
            "Motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multistrategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora."
        ]
    },
    "P12-1110": {
        "input_sentences": [
            "Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese",
            "Acknowledgement We are grateful to the anonymous reviewers for their comments and suggestions, and to Xianchao Wu, Kun Yu, Pontus Stenetorp, and Shinsuke Mori for their helpful feedback.",
            "The model demonstrated substantial improvements on the three tasks over the pipeline combination of the state-of-the-art joint segmentation and POS tagging model, and dependency parser.",
            "We conducted some comparison experiments of the partially joint and full joint models.",
            "Conclusion",
            "In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing.",
            "Compared to SegTagDep, SegTag+TagDep performs reasonably well in terms of dependency parsing accuracy, whereas the POS tagging accuracies are more than 0.5% lower.",
            "In this paper, we proposed the first joint model for word segmentation, POS tagging, and dependency parsing in Chinese.",
            "Abstract",
            "We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese.",
            "Particularly, results showed that the accuracies of POS tagging and dependency parsing were remarkably improved by 0.6% and 2.4%, respectively corresponding to 8.3% and 10.2% error reduction.",
            "We also perform comparison experiments with the partially joint models.",
            "More efficient decoding would also allow the use of the look-ahead features (Hatori et al., 2011) and richer parsing features (Zhang and Nivre, 2011).",
            "In future work, probabilistic pruning techniques such as the one based on a maximum entropy model are expected to improve the efficiency of the joint model further because the accuracies are apparently still improved if a larger beam can be used.",
            "Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models.",
            "We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework.",
            "For word segmentation, although the overall improvement was only around 0.1%, greater than 1% improvements was observed for OOV words."
        ]
    },
    "D10-1069": {
        "input_sentences": [
            "In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set.",
            "Most notably, deterministic shift-reduce parsers have difficulty dealing with the modified word order and lose more than 20% in accuracy.",
            "We propose an in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies).",
            "What is less known is that some parsers suffer more from domain shifts than others.",
            "We presented a method for domain adaptation of deterministic shift-reduce parsers.",
            "Uptraining with large amounts of unlabeled data gives similar improvements as having access to 2,000 labeled sentences from the target domain.",
            "Abstract",
            "We evaluated multiple state-of-the-art parsers on a question corpus and showed that parsing accuracies degrade substantially on this out-of-domain task.",
            "With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.",
            "Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training.",
            "We then proposed a simple, yet very effective uptraining method for domainadaptation.",
            "In a nutshell, we trained a deterministic shift-reduce parser on the output of a more accurate, but slower parser.",
            "With 2,000 labeled questions and a large amount of unlabeled questions, uptraining is able to close the gap between in-domain and out-of-domain accuracy.",
            "It is well known that parsing accuracies drop significantly on out-of-domain data.",
            "Uptraining for Accurate Deterministic Question Parsing",
            "Conclusions",
            "We show that dependency parsers have more difficulty parsing questions than constituency parsers."
        ]
    },
    "P11-2084": {
        "input_sentences": [
            "In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources.",
            "Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space.",
            "We have proven that topical knowledge is useful and improves the quality of word translations.",
            "Our next steps involve experiments with other topic models and other corpora, and combining this unsupervised approach with other tools for lexicon extraction and synonymy detection from unrelated and comparable corpora.",
            "Identifying Word Translations from Comparable Corpora Using Latent Topic Models",
            "A topic model outputs a set of multinomial distributions over words for each topic.",
            "The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.",
            "Conclusion",
            "Abstract",
            "We have presented a generic, language-independent framework for mining translations of words from latent topic models.",
            "The quality of translations depends only on the quality of a topic model and its ability to find latent relations between words."
        ]
    },
    "N09-2064": {
        "input_sentences": [
            "First, we propose a method of parse hybridization that recomproductions of conthereby preserving the structure of the output of the individual parsers to a greater extent.",
            "Third, we extend these parser combination from multiple outputs to muloutputs.",
            "While constituent recombination results in the highest f-score of the methods explored, contextfree production recombination produces trees which better preserve the syntactic structure of the individual parses.",
            "Discussion & Conclusion",
            "the output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (Henderson and Brill, 1999; Sagae and Lavie, 2006).",
            "Increasing the n-best list size from 1 to 10 improves parse selection and context-free production recombination, though further increasing n does not, in general, help.4 Chinese-English test set f-score gets a bigger boost from combination than WSJ test set f-score, perhaps because the best individual parser\u2019s baseline f-score is lower on the out-of-domain data.",
            "Abstract",
            "Second, we propose an efficient lineartime algorithm for computing expected f-score using Minimum Bayes Risk parse selection.",
            "Combining Constituent Parsers",
            "We propose three ways to improve upon existing methods for parser combination.",
            "We present results on WSJ section 23 and also on the English side of a Chinese-English parallel corpus.",
            "We have presented an algorithm for parse hybridization by recombining context-free productions.",
            "We have also presented an efficient linear-time algorithm for selecting the parse with maximum expected f-score.",
            "On both test sets, constituent recombination achieves the best f-score (1.0 points on WSJ test and 2.3 points on Chinese-English test), followed by context-free production combination, then parse selection, though the differences in f-score among the combination methods are not statistically significant.",
            "Results are shown in Tables 2, 3, and 4."
        ]
    },
    "W03-1022": {
        "input_sentences": [
            "This framework has a number of practical advantages.",
            "We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns.",
            "Supersense Tagging Of Unknown Nouns In WordNet",
            "Within this framework it is possible to use the information contained in WordNet to improve classification and define a more realistic evaluation than standard cross-validation.",
            "We used a fixed set 26 semantic labels, which we called su- These are the labels used by lexicographers developing WordNet.",
            "Conclusion",
            "We also define a more realistic evaluation procedure than cross-validation.",
            "Abstract",
            "We present a new framework for classifying common nouns that extends namedentity classification.",
            "We presented a new framework for word sense classification, based on the WordNet lexicographer classes, that extends named-entity classification.",
            "Directions for future research include the following topics: disambiguation of the training data, e.g. during training as in cotraining; learning unknown ambiguous nouns, e.g., studying the distribution of the labels the classifier guessed for the individual tokens of the new word."
        ]
    },
    "D11-1022": {
        "input_sentences": [
            "DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011).",
            "Dual Decomposition with Many Overlapping Components",
            "We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.",
            "Conclusion",
            "We have introduced new feature-rich turbo parsers.",
            "We defer this for future work.",
            "Abstract",
            "DD-ADMM compares favourably against the subgradient method in several aspects: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions.",
            "Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011).",
            "Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power.",
            "We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes.",
            "Since exact decoding is intractable, we solve an LP relaxation through a recently proposed consensus algorithm, DD-ADMM, which is suitable for problems with many overlapping components.",
            "While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors.",
            "We study the empirical runtime and convergence properties of DD-ADMM, complementing the theoretical treatment in Martins et al. (2011).",
            "Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011).",
            "However, in cases where lightweight decomare not readily available due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient."
        ]
    },
    "D07-1092": {
        "input_sentences": [
            "Translating Unknown Words by Analogical Learning",
            "Recently, Stroppa and Yvon (2005) have shown how analogical learning alone deals nicely with morphology in differentlanguages.",
            "This work is currently being developed in severaldirections.",
            "This would relieve usfrom requiring the training material while translat ing, and would allow us to compare our approachwith other methods proposed for unsupervised mor phology acquisition.Acknowledgement We are grateful to the anony mous reviewers for their useful suggestions and to Pierre Poulin for his fruitful comments.",
            "In particular, they drastically impact machine transla tion quality.",
            "In this study we show that ana logical learning offers as well an elegant andeffective solution to the problem of identify ing potential translations of unknown words.",
            "On the contrary to several lines of work, our approach does not rely on massive additional resources but capitalizes instead on an information which is inherently pertaining tothe language.",
            "This study has been partially funded by NSERC.",
            "885",
            "We measured that roughly 80% of or dinary unknown French words can receive a valid translation into English with our approach.",
            "Last, we want to investigate the training of a model that can learn regularities from the analogies we are making.",
            "Abstract",
            "First, we are investigating why our ap proach remains silent for some words or phrases.This will allow us to better characterize the limitations of ANALOG and will hopefully lead us to de sign a better strategy for identifying the stems of agiven word or phrase.",
            "Unknown words are a well-known hindranceto natural language applications.",
            "An easy way out commercial translation systems usually offer their users is the possibility to add unknown wordsand their translations into a dedicated lex icon.",
            "In this paper, we have investigated the appropri ateness of analogical learning to handle unknown words in machine translation.",
            "Second, we are investigat ing how a systematic enrichment of a phrase-transfer table will impact a phrase-based statistical machine translation engine.",
            "Discussion and Future Work"
        ]
    },
    "W06-3114": {
        "input_sentences": [
            " Evaluation was done automatically using the BLEU score and manually on fluency and adequacy",
            "Replacing this with an ranked evaluation seems to be more suitable.",
            "The manual evaluation of scoring translation on a graded scale from 1\u20135 seems to be very hard to perform.",
            "While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems.",
            "Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.",
            "The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.",
            "Human judges also pointed out difficulties with the evaluation of long sentences.",
            "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back",
            "Conclusions"
        ]
    },
    "D09-1161": {
        "input_sentences": [
            "For feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm.",
            "Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively.",
            "In this paper, we propose a linear model-based general framework for multiple parser combination.",
            "Our experiments are carried out on both the Chinese and English Penn Treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model.",
            "As a result, it is able to fully utilize both the logarithm of the probability of each k-best parse tree from each individual parser and any additional useful features.",
            "We verify our method by combining the two representative parsing models, lexicalized model and un-lexicalized model, on both Chinese and English.",
            "Experimental results show our method is very effective and advance the state-of-the-art results on both Chinese and English syntax parsing.",
            "Abstract",
            "In this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers.",
            "K-Best Combination of Syntactic Parsers",
            "The proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model.",
            "Compared with previous methods, our method is able to use diverse features, including logarithm of the parse tree probability calculated by the individual systems.",
            "Conclusions",
            "In the future, we will explore more features and study the forest-based combination methods for syntactic parsing."
        ]
    },
    "W12-3401": {
        "input_sentences": [
            "Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features.",
            "Abstract",
            "The standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class.",
            "We tested a standard approach to lexical generalization for parsing that has been previously explored, where a word is mapped to a single cluster or synset.",
            "However, the probabilistic approach also has the downside of a slower running time.",
            "However, we did not see statistically significant improvements over the baseline when parsing in-domain text or out-of-domain parliamentary text.",
            "This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis.",
            "Probabilistic Lexical Generalization for French Dependency Parsing",
            "Conclusion",
            "Probabilities for the lemma space were calculated using the distributional thesaurus, and probabilities for the WordNet synset space were calculated using ASR sense prevalence scores, with probabilistic clusters left for future work.",
            "Our experiments with an arc-eager transitionbased dependency parser resulted in modest but significant improvements in LAS over the baseline when parsing out-of-domain medical text.",
            "We use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets.",
            "We also introduced a novel probabilistic lexical generalization approach, where a lemma is represented by a categorical distribution over the space of lemmas, clusters, or synsets.",
            "We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.",
            "Comparing the standard single-mapping approach to the probabilistic generalization approach, we found a slightly (though not significantly) better performance for probabilistic generalization across different parsing configurations and evaluation sets.",
            "An explanation for this result is that the French Treebank training set vocabulary has a very high lexical coverage over the evaluation sets in these domains, suggesting that lexical generalization does not provide much additional benefit.",
            "We built a distributional thesaurus from an automatically-parsed large text corpus, using it to generate word clusters and perform WordNet ASR.",
            "Based on the findings in this paper, our focus for future work on lexical generalization for dependency parsing is to continue improving parsing performance on out-of-domain text, specifically for those domains where lexical variation is high with respect to the training set.",
            "A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking.",
            "One possibility is to experiment with building a distributional thesaurus that uses text from both the source and target domains, similar to what Candito et al. (2011) did with Brown clustering, which may lead to a stronger bridging effect across domains for probabilistic lexical generalization methods.",
            "We have investigated the use of probabilistic lexical target spaces for reducing lexical data sparseness in a transition-based dependency parser for French."
        ]
    },
    "W11-1310": {
        "input_sentences": [
            "Exemplar-Based Word-Space Model for Compositionality Detection: Shared Task System Description",
            "This model is tested for compositionality detection and it is found to outperform existing prototype-based models.",
            "Also, we use multiple evidences for compositionality detection rather than basing our judgement on a single evidence.",
            "In this paper, we examined the effect of polysemy in word space models for compositionality detection.",
            "We have participated in the shared task (Biemann and Giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations.",
            "Abstract",
            "We showed exemplar-based WSM is effective in dealing with polysemy.",
            "Most models represent each word as a single prototype-based vector without addressing polysemy.",
            "In this paper, we highlight the problems of polysemy in word space models of compositionality detection.",
            "We propose an exemplar-based model which is designed to handle polysemy.",
            "Conclusions",
            "Overall, performance of the Exemplar-based models of compositionality detection is found to be superior to prototype-based models."
        ]
    },
    "A00-2030": {
        "input_sentences": [
            "1 Conclusions",
            "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.",
            "A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.",
            "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.",
            "The semantic training corpus was produced by students according to a simple set of guidelines.",
            "Abstract",
            "This simple semantic annotation was the only source of task knowledge used to configure the model.",
            "In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
            "We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.",
            "A Novel Use of Statistical Parsing to Extract Information from Text"
        ]
    },
    "W12-3134": {
        "input_sentences": [
            "Acknowledgements This research was supported by in part by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme), and by the NSF under grant IIS-0713448.",
            "Opinions, interpretations, and conclusions are the authors\u2019 alone.",
            "We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation.",
            "Our system has been extended towards efficiently supporting large-scale experiments in parsing-based machine translation and text-to-text generation: Joshua 4.0 supports compactly represented large grammars with its packed grammars, as well as large language models via KenLM and BerkeleyLM.We include an implementation of PRO, allowing for stable and fast tuning of large feature sets, and extend our toolkit beyond pure translation applications by extending Thrax with a large-scale paraphrase extraction module.",
            "Conclusion",
            "We present a new iteration of the Joshua machine translation toolkit.",
            "Abstract",
            "Joshua 4.0: Packing, PRO, and Paraphrases",
            "The main con"
        ]
    },
    "W04-1506": {
        "input_sentences": [
            "We have evaluated the application of the grammar to corpus, measuring the linking of the verb with its dependents, with satisfactory results.",
            "Previous to the completion of the grammar for the dependency parsing, the design of the Dependency Structure-based Scheme had to be accomplished; we concentrated on issues that must be resolved by any practical system that uses such models.",
            "We have presented the application of the dependency grammar parser for the processing of Basque, which can serve as a representative of agglutinative languages with free order of constituents.",
            "For instance, all kinds of constructions without a clear syntactic head are difficult to analyse: ellipses, sentences without a verb (e.g., copula-less predicative), and coordination.",
            "All these aspects have been treated in our manually annotated Corpus; our efforts now are oriented to deal with them automatically.",
            "However, the development of a full dependency syntactic analyser is still a matter of research.",
            "We have also evaluated the application of the grammar to corpus, measuring the linking of the verb with its dependents, with satisfactory results.",
            "We have shown how dependency grammar approach provides a good solution for deeper syntactic analysis, being at this moment the best alternative for morphologically complex languages.",
            "The Dependency syntactic model is applied to establish the dependency-based grammatical relations between the components within the clause.",
            "The manually tagged corpus has been used to evaluate the accuracy of the parser.",
            "Abstract",
            "We present the Dependency Parser, for the linguistic processing of Basque, which can serve as a representative of agglutinative languages that are also characterized by the free order of its constituents.",
            "Such a deep analysis is used to improve the output of the shallow parsing where syntactic structure ambiguity is not fully and explicitly resolved.",
            "This scheme was used both to the manual tagging of the corpus and to develop the parser.",
            "Conclusions",
            "Towards A Dependency Parser For Basque"
        ]
    },
    "P04-1006": {
        "input_sentences": [
            "We presented a parsing technique that shifts the attention of a word-lattice parser in order to ensure syntactic analyses for all lattice paths.",
            "Conclusion",
            "The parser\u2019s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited.",
            "Abstract",
            "We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling.",
            "This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model.",
            "Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice.",
            "Attention Shifting For Parsing Speech"
        ]
    },
    "P10-1155": {
        "input_sentences": [
            "Accuracy figures close to self domain training lend credence to the viability of our approach.",
            "Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD.",
            "Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation.",
            "Many supervised WSD systems have been built, but the effort of creatthe training corpus sense corpora has always been a matter of concern.",
            "Based on our study of WSD in 4 domain adaptation scenarios, we make the following conclusions: ing finding with the following implication: as long as one has sense marked corpus - be it from a mixed or specific domain - simply injecting ANY small amount of data from the target domain suffices to beget good accuracy.",
            "We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus.",
            "Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.",
            "As future work, we would like to test our work on the Environment domain data which was released as part of the SEMEVAL 2010 shared task on \u201cAllwords Word Sense Disambiguation on a Specific Domain\u201d.",
            "Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora.",
            "Abstract",
            "Conclusion and Future Work",
            "However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy.",
            "We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain.",
            "All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision",
            "In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal."
        ]
    },
    "N12-1007": {
        "input_sentences": [
            "With more data, we might be able to relax the constraints and use an exchangeable DPMM, which might be more effective.",
            "In this paper, we have set baselines and proposed models that significantly exceeded those baselines.",
            "The instance-level constraints represent tendencies that could be learned from larger amounts of data.",
            "Abstract",
            "When co-referent text mentions appear in different languages, these techniques cannot be easily applied.",
            "Cross-lingual entity clustering is a natural step toward more robust natural language understanding.",
            "More importantly, our techniques can be used to extend existing cross-document entity clustering systems for the increasingly multilingual web.",
            "Conclusion",
            "Although MT may achieve more accurate results for some language pairs, the PLTM training resources (e.g., Wikipedia) are readily available for many languages.",
            "Crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown.",
            "This work was started during the SCALE 2010 summer workshop at Johns Hopkins.",
            "As a result, we plan a model that clusters the mentions directly, thus removing the dependence on within-document coreference resolution.",
            "Finally, we have shown that significant quantities of within-document errors cascade into the cross-lingual clustering phase.",
            "As for the clustering algorithms, HAC appears to perform better than the DPMM on our dataset, but this may be due to the small corpus size.",
            "Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures.",
            "Consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters.",
            "Acknowledgments We thank Jason Eisner, David Mimno, Scott Miller, Jim Mayfield, and Paul McNamee for helpful discussions.",
            "Standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like English WordNet.",
            "Entity Clustering Across Languages",
            "This result was achieved without a knowledge base, which is required by previous approaches to cross-lingual entity linking.",
            "The first author is supported by a National Science Foundation Graduate Fellowship.",
            "We proposed pipeline models that make clustering decisions based on cross-lingual similarity.",
            "The best model improved upon the cross-lingual entity baseline by 24.3% F1.",
            "We investigated two methods for mapping documents in different languages to a common representation: MT and the PLTM.",
            "On an Arabic-English corpus that contains seven different text genres, our best model yields a 24.3% F1 gain over the baseline."
        ]
    },
    "D07-1026": {
        "input_sentences": [
            "The basic assumption is that if a word is entailed by another in a given context, then some of the contexts of the entailed word should be similar tothat of the word to be disambiguated.",
            "Alfio Gliozzo is sup ported by the FIRB-Israel research project N. RBIN045PXH.",
            "Our tech nique is effective, as it largely surpasses both the random and most frequent baselines.",
            "The approach is fully unsupervised and based on kernel methods.",
            "Claudio Giuliano is sup ported by the X-Media project (http://www.x-media-project.org), sponsored by the European Commission as part of the Information So ciety Technologies (IST) program under EC grantnumber IST-FP6-026978.",
            "Abstract",
            "We demonstrate the effectiveness of our technique largelysurpassing both the random and most fre quent baselines and outperforming current state-of-the-art unsupervised approaches ona benchmark ontology available in the liter ature.",
            "Acknowledgments The authors would like to thank Bernardo Magnini and Hristo Tanev for providing the benchmark, Ido Dagan for useful discussions and commentsregarding the connections between lexical entail ment and ontology population, and Alberto Lavellifor his thorough review.",
            "In addition, we plan to exploit our lexical entailment asa subcomponent of a more complex system to rec ognize textual entailment.",
            "In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text.",
            "Instance Based Lexical Entailment for Ontology Population",
            "Conclusions and Future Work",
            "In this paper, we presented a novel unsupervised technique for recognizing lexical entailment in texts, namely instance based lexical entailment, and we exploited it to approach an ontology population task.",
            "Finally, we are going toexplore more elaborated kernel functions to recog nize lexical entailment and more efficient learning strategies to apply our method to web-size corpora.",
            "Instance Based Lexical Entailment",
            "In addition, it improves over the state-of-the-art for unsupervisedapproaches, achieving performances close to the su pervised rivaling techniques requiring hundreds of examples for each class.",
            "For the future, we plan to apply our instance based approach to a wide variety of tasks, e.g., lexical substitution, word sense disambiguation and information retrieval.",
            "Ontology population is only one of the possible applications of lexical entailment."
        ]
    },
    "P05-1061": {
        "input_sentences": [
            "The binary relation classifier we employ is quite simplistic and most likely can be improved by using features over a deeper representation of the data such as parse trees.",
            "We presented a method for complex relation extraction, the core of which was to factorize complex relations into sets of binary relations, learn to identify binary relations and then reconstruct the complex relations by finding maximal cliques in graphs that represent relations between pairs of entities.",
            "However, their model relied on the transitive property inherent in the co-reference relation.",
            "Abstract",
            "Letting the edge probabilities in the graphs represent a distance in some space, it may be possible to learn how to cluster vertices into relational groups.",
            "We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text.",
            "The primary advantage of this method is that it allows for the use of almost any binary relation classifier, which have been well studied and are often accurate.",
            "All that is required is to remove the constraint that maximal cliques must be consistent with the structure of the relation.",
            "The first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances.",
            "Hence, it is reasonable to assume that our binary factorization method will perform well when binary relations can be learnt with high accuracy.",
            "We mentioned earlier that the only restriction of our complex relation definition is that the arity of the relation must be known in advance.",
            "Simple Algorithms For Complex Relation Extraction With Applications To Biomedical IE",
            "A distinction may be made between the factored system presented here and one that attempts to classify complex relations without factorization.",
            "Conclusions and Future Work",
            "It turns out that the algorithms we described can actually handle dynamic arity relations.",
            "We also plan on running these algorithms on more data sets to test if the algorithms empirically generalize to different domains.",
            "complex relation is any relation in which some of the arguments may be be unspecified.",
            "Our system can be seen as an instance of a local learner.",
            "In fact, this approach is both significantly quicker and more accurate then enumerating and classifying all possible instances.",
            "This is related to the distinction between methods that learn local classifiers that are combined with global constraints after training and methods that incorporate the global constraints into the learning process.",
            "One possibility is recent work on supervised clustering.",
            "However, since a vertex/entity can participate in one or more relation, any clustering algorithm would be required to produce non-disjoint clusters.",
            "We believe this work provides a good starting point for continued research in this area.",
            "We showed that such a method can be successful with an empirical evaluation on a large set of biomedical data annotated with genomic variation relations.",
            "Other more powerful binary classifiers should be tried such as those based on tree kernels (Zelenko et al., 2003).",
            "As for future work, there are many things that we plan to look at.",
            "Perhaps the most interesting open problem is how to learn the complex reconstruction phase.",
            "Punyakanok et al. (2004) argued that local learning actually outperforms global learning in cases when local decisions can easily be learnt by the classifier.",
            "We present here a simple two-stage method for extracting complex relations between named entities in text.",
            "McCallum and Wellner (2003) showed that learning binary co-reference relations globally improves performance over learning relations in isolation.",
            "This represents another advantage of binary factorization over enumeration, since it would be infeasible to enumerate all possible instances for dynamic arity relations."
        ]
    },
    "P11-2067": {
        "input_sentences": [
            "Speculations as to cause have suggested the parser, the data, or other factors.",
            "Our results show that choices in the PSMT system can completely erode potential gains of the reordering preprocessing step, with the largest effect due to simple choice of data.",
            "Clause Restructuring For SMT Not Absolutely Helpful",
            "Conclusion",
            "We have shown that a lack of overall improvement using reordering-aspreprocessing need not be due to the usual suspects, language pair and reordering process.",
            "We have systematically varied several aspects of the Howlett and Dras (2010) system and reproduced results close to both papers, plus a full range in between.",
            "Abstract",
            "We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT?",
            "The reimplementation of this system by Howlett and Dras (2010) came to the opposite conclusion.",
            "An early work proposing this idea showed improved translation performance, but subsequent work has had mixed results.",
            "We conclude that effort is best directed at determining for which sentences the improvement will appear.",
            "There are a number of systems that use a syntax-based reordering step prior to phrasebased statistical MT.",
            "Significantly, our oracle experiments show that in all cases the reordering system does produce better translations for some sentences.",
            "Collins et al. (2005) reported that a reorderingas-preprocessing approach improved overall performance in German-to-English translation."
        ]
    },
    "N10-1035": {
        "input_sentences": [
            "Acknowledgments G\u00f3mez-Rodr\u00edguez has been supported by MEC/FEDER (HUM2007-66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR, Redes Galegas de PL e RI e de Ling. de Corpus, Bolsas Estad\u00edas INCITE/FSE cofinanced).",
            "Kuhlmann has been supported by the Swedish Research Council.",
            "In this paper, we have presented an efficient parsing algorithm for well-nested linear context-free rewriting systems, based on a new normal form for this formalism.",
            "Linear Context-Free Rewriting Systems",
            "The normal form takes up linear space with respect to grammar size, and the algorithm is based on a bottom-up process that can be applied to any LCFRS, achieving O(cp \u00b7 |G |\u00b7 |w|2cp+2) time complexity when applied to LCFRS of fan-out cp in our normal form.",
            "The class of well-nested LCFRS is an interesting syntactic formalism for languages with discontinuous constituents, providing a good balance between coverage of linguistic phenomena in natural language treebanks (Kuhlmann and Nivre, 2006; Maier and Lichte, 2009) and desirable formal properties (Kanazawa, 2009).",
            "Conclusion",
            "Our results offer a further argument in support of well-nested LCFRS: while the complexity of parsing general LCFRS depends on two dimensions (rank and fan-out), this bidimensional hierarchy collapses into a single dimension in the well-nested case, where complexity is only conditioned by the fan-out.",
            "Abstract",
            "This complexity is an asymptotic improvement over existing results for this class, both from parsers specifically geared to well-nested LCFRS or equivalent formalisms (Hotz and Pitsch, 1996) and from applying general LCFRS parsing techniques to the well-nested case (Seki et al., 1991).",
            "The use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order.",
            "Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems",
            "Our result is obtained through a linear space construction of a binary normal form for the grammar at hand.",
            "We present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems."
        ]
    },
    "D07-1122": {
        "input_sentences": [
            "Then we present evaluation results and error analyses focusing on Chinese.",
            "We used Nivre?smethod to produce the dependency arcs and the se quence labeler to produce the dependency labels.",
            "Conclusion",
            "The experimental results showed that our system can provide good performance for all languages.",
            "For four languages with different values of ROOT, we design some spe cial features for the ROOT labeler.",
            "Abstract",
            "We describe the features used ineach stage.",
            "In this paper, we presented our two-stage depen dency parsing system submitted to the Multilingual Track of CoNLL-2007 shared task.",
            "A Two-Stage Parser for Multilingual Dependency Parsing",
            "The parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem.",
            "We present a two-stage multilingual de pendency parsing system submitted to the Multilingual Track of CoNLL-2007."
        ]
    },
    "P14-1078": {
        "input_sentences": [
            "Our model is built upon a medical corpus containing 80M sentences (11 gigabyte text) and designed to accurately and efficiently detect the key medical relations that can facilitate clinical decision making.",
            "Relation Extraction with Manifold Models",
            "To provide users with more flexibility, we also take label weight into consideration.",
            "We also present a new manifold model to efficiently extract these relations from text.",
            "It further provides users with the flexibility to take label weight into consideration.",
            "Abstract",
            "In this paper, we present a manifold model for medical relation extraction.",
            "Our model is developed to utilize both labeled and unlabeled examples.",
            "We apply the new model to construct a relation knowledge base (KB), and use it as a complement to the existing manually created KBs.",
            "Medical Relation Extraction with Manifold Models",
            "In this paper, we identify a list of key relations that can facilitate clinical decision making.",
            "Our approach integrates domain specific parsing and typing systems, and can utilize labeled as well as unlabeled examples.",
            "Effectiveness of the new model is demonstrated both theoretically and experimentally.",
            "Effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments.",
            "Conclusions"
        ]
    },
    "C10-1135": {
        "input_sentences": [
            "Conclusion",
            "As tokenization is usually ambiguous for many natural languages such as Chineseand Korean, tokenization errors might po tentially introduce translation mistakes fortranslation systems that rely on 1-best tokenizations.",
            "Abstract",
            "By integrat ing tokenization and translation features in a discriminative framework, our jointdecoder outperforms the baseline trans lation systems using 1-best tokenizationsand lattices significantly on both ChineseEnglish and Korean-Chinese tasks.",
            "Acknowledgement The authors were supported by SK Telecom CIBusiness, and National Natural Science Founda tion of China, Contracts 60736014 and 60903138.We thank the anonymous reviewers for their insightful comments.",
            "While using lattices to offer more alternatives to translation systems have elegantly alleviated this prob lem, we take a further step to tokenize and translate jointly.",
            "Joint Tokenization and Translation",
            "We have presented a novel method for joint tok enization and translation which directly combines the tokenization model into the decoding phase.Allowing tokenization and translation to collaborate with each other, tokenization can be opti mized for translation, while translation also makes contribution to tokenization performance under a supervised way.",
            "We believe that our approach can be applied to other string-based model such asphrase-based model (Koehn et al, 2003), stringto-tree model (Galley et al, 2006) and string-to dependency model (Shen et al, 2008).",
            "We are also grateful to Wen bin Jiang, Zhiyang Wang and Zongcheng Ji for their helpful feedback.",
            "1207",
            "Taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on thetarget side simultaneously.",
            "Interestingly, as a tokenizer, our joint de coder achieves significant improvements over monolingual Chinese tokenizers."
        ]
    },
    "D07-1030": {
        "input_sentences": [
            "We presented a method using the existing RBMT system as a black box to produce synthetic bilin gual corpus, which was used as training data for the SMT system.",
            "In our experi ments using BLEU as the metric, such a system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the syn thetic bilingual corpora.",
            "We also interpolated the model trained on a real bilingual corpus and the models trained on the syn thetic bilingual corpora, the interpolated model achieves an absolute improvement of 0.0245 BLEU score (13.1% relative) as compared with the individual model trained on the real bilingual cor 293 pus.",
            "The newly added phrase pairs from the synthetic models can assist to improve the translation results of the in terpolated model.",
            "(shouhoufuwu)\" is added; (2) The prob ability distribution of the phrase pairs is changed.",
            "For example, the phrase pair \"after-sale service <-> ????",
            "Abstract",
            "6.1 Model Interpolation vs. Corpus Merge.",
            "And Figure 3 shows the phrase pairs used for translation.",
            "The interpolated model achieves an abso lute improvement of 0.0245 BLEU score (13.1% relative) as compared with the in dividual model trained on the real bilingual corpus.",
            "After add ing the synthetic corpus produced by the RBMT systems, the interpolated model outperforms the standard models mainly for the following two rea sons: (1) some new phrase pairs are added into the interpolated model.",
            "It indicates that using the existing RBMT systems to produce a synthetic bi lingual corpus, we can build an SMT system that outperforms the existing RBMT systems.",
            "(he chuangzao)\" increase.",
            "Another available way is directly combining these two kinds of corpora to train a translation model, namely corpus merge.",
            "Table 5 shows an English sentence and its Chinese translations produced by different methods.",
            "In section 5, we make use of the real bilingual corpus and the synthetic bilingual corpora by perform ing model interpolation.",
            "As discussed in Section 5.5, the number of the overlapped phrase pairs among the standard model and the synthetic models is very small.",
            "The probabilities of the other two pairs \"brand name < > ??",
            "In the future work, we will investigate the possi bility of training a reverse SMT system with the RBMT systems.",
            "For example, we will investigate to train Chinese-to-English SMT system based on natural English and RBMT-generated synthetic Chinese.",
            "37.6% phrase pairs (1993 out of 5306) are newly learned and used for translation.",
            "(pinpai)\" and \"and the creation of a <-> ? ??",
            "According to (Koehn and Monz, 2006; Callison Burch et al, 2006), the RBMT systems are usually not adequately appreciated by BLEU.",
            "Then we train an SMT system with the combination of this synthetic bilingual corpus and the real bilingual corpus.",
            "Statistics of Phrase Pairs Further analysis is shown in Table 6.",
            "6.2 Result Analysis.",
            "Phrase Pairs Phrase Pairs Used New Pairs Used Standard Model 6,105,260 5,509 ? Interpolated Model 73,221,525 5,306 1993 Table 6.",
            "The BLEU score of such system is 0.1887, while that of the model in terpolation system is 0.2020.",
            "The results show that imperfect translations of RBMT systems can be also used to boost the performance of an SMT sys tem.",
            "For example, the probabilities of the two pairs \"a brand name <-> ??",
            "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006).",
            "(pinpai)\" and \"and the crea tion of <-> ? ??",
            "Using RBMT Systems to Produce Bilingual Corpus for SMT",
            "6.3 Human Evaluation.",
            "With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus.",
            "Discussion",
            "(he jianli)\" decrease.",
            "We used the existing RBMT system to translate the monolingual corpus into a syn thetic bilingual corpus.",
            "Conclusion and Future Work",
            "It indicates that we can build a better SMT system by leveraging the real and the synthetic bi lingual corpus.",
            "We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus.",
            "This paper proposes a method using the ex isting Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Ma chine Translation (SMT) system.",
            "In our experi ments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora.",
            "We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora.",
            "It indicates that the model interpolation method is significantly better than the corpus merge method.",
            "We found that 930 phrase pairs, which are also in the phrase table of the standard model, are used by the interpolated model for translation but not used by the standard model.",
            "In order to compare these two methods, we use RBMT system 1 to translate the 1,087,651 monolingual English sentences to produce synthetic bi lingual corpus.",
            "In this section, we will use an example to further discuss the reason behind the improvement of the SMT system by using syn thetic bilingual corpus.",
            "The evalua tion results show that the SMT system with the interpolated model, which achieves the highest BLEU scores in Table 2, achieves slightly better adequacy and fluency scores than the two RBMT systems.",
            "Further result analysis shows that after adding the synthetic corpus produced by the RBMT systems, the interpolated model outperforms the stan dard models mainly because of two reasons: (1) some new phrase pairs are added to the interpo lated model; (2) the probability distribution of the phrase pairs is changed.",
            "With the synthetic bilingual corpus, we could build an SMT system even if there is no real bilingual corpus."
        ]
    },
    "P11-1061": {
        "input_sentences": [
            "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).",
            "Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.",
            "Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language.",
            "Conclusion",
            "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections",
            "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "Abstract",
            "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.",
            "Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.",
            "Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.",
            "Because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs."
        ]
    },
    "W06-2932": {
        "input_sentences": [
            "Second, we plan on integrating any available morphological features in a more principled manner.",
            "First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.",
            "The current system simply includes all morphological bi-gram features.",
            "The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.",
            "Abstract",
            "We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.",
            "Multilingual Dependency Analysis with a Two-Stage Discriminative Parser",
            "It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.",
            "The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.",
            "We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.",
            "In the future we plan to extend these models in two ways.",
            "present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.",
            "Conclusions",
            "It is our hypothesis that for languages with fine-grained label sets, joint parsing and labeling will improve performance."
        ]
    },
    "D12-1129": {
        "input_sentences": [
            "A New Minimally-Supervised Framework for Domain Word Sense Disambiguation",
            "We have here presented a new framework for domain Word Sense Disambiguation.",
            "Conclusion",
            "Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique.",
            "While we selected 30 domains for this study, nothing would prevent us from using a smaller or larger set of these domains, or a set of completely different domains.",
            "Abstract",
            "We depart from the use of general-purpose sense inventories like WordNet and propose a bootstrapping approach to the acquisition of sense inventories for virtually any domain.",
            "Our work provides three main contributions: i) we propose a new, flexible approach to glossary bootstrapping which harvests hundreds of thousands of term/gloss pairs; the resulting multidomain glossary is shown to have wide coverage across domains and to include a large amount of terms not available in WordNet; ii) we propose a novel framework for fullyunsupervised domain WSD which uses the multi-domain glossary as our sense inventory; iii) we show that high performance can be achieved by means of simple, unsupervised WSD algorithms (around 80% and 69% in a coarse- and fine-grained setting, respectively).",
            "The multi-domain glossary (and sense inventory) together with the seeds used for bootstrapping are available from http://lcl.uniroma1. it/dwsd.",
            "We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD).",
            "Note that our aim here has not been to determine which system performs best, but rather to show that a reliable, full-fledged framework for domain WSD can be set up with minimal supervision.",
            "Additionally, our framework can be applied to any language of interest, provided enough glossaries are available online, by simply translating the keywords used for our queries.",
            "The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD.",
            "Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level."
        ]
    },
    "D11-1039": {
        "input_sentences": [
            "When something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals.",
            "Bootstrapping Semantic Parsers from Conversations",
            "This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system.",
            "In this paper, we present an approach for using conversational interactions of this type to induce semantic parsers.",
            "Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations.",
            "Conversations provide rich opportunities for interactive, continuous learning.",
            "The loss was defined over the conversational context, without requiring annotation of user utterances meaning.",
            "The overall approach assumes that, in aggregate, the conversations contain sufficient signal (remediations such as clarification, etc.) to learn effectively.",
            "1 Discussion",
            "Abstract",
            "In this paper, we satisfied this requirement by using logs from automated systems that deployed reasonably effective recovery strategies.",
            "An important area for future work is to consider how this learning can be best integrated into a complete dialog system.",
            "We demonstrate learning without any explicit annotation of the meanings of user utterances.",
            "We presented a loss-driven learning approach that induces the lexicon and parameters of a CCG parser for mapping sentences to logical forms.",
            "Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation.",
            "This would include designing remediation strategies that allow for the most effective learning and considering how similar techniques could be used simultaneously for other dialog subproblems."
        ]
    },
    "P11-1060": {
        "input_sentences": [
            "In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.",
            "Learning Dependency-Based Compositional Semantics",
            "Abstract",
            "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.",
            "On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.",
            "In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms."
        ]
    },
    "W07-1712": {
        "input_sentences": [
            "We show that it is feasible to boost performance by considering several heuristics and patterns acquired from the Web data.",
            "In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999).",
            "From the language-oriented perspective, it would be useful to determine to which extent stemming and morphological analysis would boost performance.",
            "The approach we follow uses a restricted number of features.",
            "To improve the results that we get by employing orthographic and contextual features, we add patterns extracted from the Web and use a similarity measure to find the named entities similar to the NEs in the training set.",
            "Conclusions and Future work",
            "Abstract",
            "Kruislaan 419, 1098VA the katrenko@science.uva.nl Pieter HCSL, University of Kruislaan 419, 1098VA the pitera@science.uva.nl Abstract Named entity recognition (NER) is a subtask of information extraction (IE) which can be used further on for different purposes.",
            "As Ukrainian language is a language with a rich morphology, there are several directions we would like to explore in the future.",
            "The other problem which we have not considered up to now is the ambiguity of some named entities.",
            "For example, a word \u2019Ukraine\u2019 can belong to the category LOC as well as to the category ORG (as it is a part of a complex named entity).",
            "In this paper, we discuss named entity recognition for Ukrainian language, which is a Slavonic language with a rich morphology.",
            "The former might be explained by the size of the corpus we use and by the characteristics of the language.",
            "The results we received are, in general, lower than the performance of NER systems in other languages but higher than both baselines.",
            "In this paper, we focused on standard features used for the named entity recognition on the newswire data which have been used on many languages.",
            "Named Entity Recognition for Ukrainian: A Resource-Light Approach"
        ]
    },
    "D10-1025": {
        "input_sentences": [
            "This paper presents two different methods for creating discriminative projections: OPCA and CPLSA.",
            "Both of these methods avoid the use of artificial concatenated documents.",
            "When compared to other techniques, OPCA had the highest accuracy while still having a run-time that allowed scaling to large data sets.",
            "Instead, they model documents in multiple languages, with the constraint that comparable documents should map to similar locations in the projected space.",
            "We therefore recommend the use of OPCA as a pre-processing step for large-scale comparable document retrieval or cross-language text categorization.",
            "The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines.",
            "The OPCA method is shown to perform best.",
            "Abstract",
            "We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA).",
            "Both of these variants start with a basic model of documents (PCA and PLSA).",
            "Each model is then made discriminative by encouraging comparable document pairs to have similar vector representations.",
            "Translingual Document Representations from Discriminative Projections",
            "We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space.",
            "We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters.",
            "Conclusions",
            "Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization.",
            "The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel."
        ]
    },
    "D07-1066": {
        "input_sentences": [
            "The results of the ex periments show that, contrary to Ku?bler etal.",
            "Comparing PARSEVAL based parsing results for a parser trained on the Tu?Ba-D/Z or TIGER to results achieved by a parser trained on the English Penn-II treebank (Marcus et al, 1994) does not provide conclusive evidenceabout the parsability of a particular language, be cause the results show a bias introduced by thecombined effect of annotation scheme and evalua tion metric.",
            "Abstract",
            "test andtraining sets and cross-treebank and -language con trolled error insertion experiments.",
            "In this paper we present new ex periments to test this claim.",
            "A possible way forward is perhaps a dependency-based evaluation of TIGER/Tu?Ba-D/Z with Penn-II trained grammars for ?similar?",
            "This means that the question whether German is harder to parse than English, is still undecided.",
            "test/training sets across languages.",
            "Compar ing two different annotation schemes, PARSEVALconsistently favours the one with the higher node ra tio.",
            "This claim is based on the assumption that PARSEVAL metrics fully re flect parse quality across treebank encodingschemes.",
            "Therefore the question whether a particular language is harder to parse than another language or not, can not be answered by comparing parsingresults for parsers trained on treebanks with different annotation schemes.",
            "638 Acknowledgements We would like to thank the anomymous reviewers for many helpful comments.",
            "Even this is not entirely straightforward as it is not completely clear what constitutes ?similar?",
            "We have shown that different treebank annotation schemes have a strong impact on parsing results forsimilar input data with similar (simulated) parser er rors.",
            "(2006), the question whether or not Ger man is harder to parse than English remains undecided.",
            "Recent studies focussed on the question whether less-configurational languages like German are harder to parse than English, or whether the lower parsing scores are an artefact of treebank encoding schemes and data structures, as claimed by Ku?bler et al(2006).",
            "We showed that PAR SEVAL cannot be used to compare the output ofPCFG parsers trained on different treebank anno tation schemes, because the results correlate withthe ratio of non-terminal/terminal nodes.",
            "We examined the influence of treebank annotationschemes on unlexicalised PCFG parsing, and re jected the claim that the German Tu?Ba-D/Z treebankis more appropriate for PCFG parsing than the Ger man TIGER treebank and showed that converting the Tu?Ba-D/Z trained parser output to a TIGER-like format leads to PARSEVAL results slightly worse than the ones for the TIGER treebank trained parser.Additional evidence comes from a dependency based evaluation, showing that, for the output of the parser trained on the TIGER treebank, the mapping from the CFG trees to dependency relations yields better results than for the grammar trained on theTu?Ba-D/Z annotation scheme, even though PARSE VAL scores suggest that the TIGER-based parseroutput trees are substantial worse than Tu?Ba-D/Z based parser output trees.",
            "We use thePARSEVAL metric, the Leaf-Ancestor metric as well as a dependency-based evaluation, and present novel approaches measur ing the effect of controlled error insertion on treebank trees and parser output.",
            "Treebank Annotation Schemes and Parser Evaluation for German",
            "Wealso provide extensive past-parsing crosstreebank conversion.",
            "Conclusions",
            "We will attempt to pursue this in further research.",
            "This research has been supported by a Science Foundation Ireland grant 04|IN|I527.",
            "In this paper we presented novel experiments assess ing the validity of parsing results measured along different dimensions: the tree-based PARSEVAL metric, the string-based Leaf-Ancestor metric anda dependency-based evaluation.",
            "By inserting con trolled errors into gold treebank trees and measuring the effects on parser evaluation results we gave new evidence for the downsides of PARSEVAL which,despite severe criticism, is still the standard measure for parser evaluation."
        ]
    },
    "W06-3119": {
        "input_sentences": [
            "Our translation system is available open-source under the GNU General",
            "We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence.",
            "Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar.",
            "We present results on the French-to-English task for this workshop, representing significant improvements over the workshop\u2019s baseline system.",
            "While no improvements were available at submission time, our subsequent performance highlights the importance of tight integration of n-gram language modeling within the syntax driven parsing environment.",
            "Abstract",
            "In this work we applied syntax based resources (the target language parser) to annotate and generalize phrase translation tables extracted via existing phrase extraction techniques.",
            "Syntax Augmented Machine Translation Via Chart Parsing",
            "We present translation results on the shared task \u201dExploiting Parallel Texts for Statistical Machine Translation\u201d generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories.",
            "Our work reaffirms the feasibility of parsing approaches to machine translation in a large data setting, and illustrates the impact of adding syntactic categories to drive and constrain the structured search space.",
            "Conclusions",
            "Our translation system is available opensource under the GNU General Public License at: www.cs.cmu.edu/\u02dczollmann/samt"
        ]
    },
    "D07-1119": {
        "input_sentences": [
            "For the multi lingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language.",
            "DeSR implements an incre mental deterministic Shift/Reduce parsing algorithm, using specific rules to handle non-projective dependencies.",
            "Abstract",
            "We describe our experiments using the DeSR parser in the multilingual and do main adaptation tracks of the CoNLL 2007 shared task.",
            "We used a second order averaged perceptron as classifier and achieved accuracy scores quite above the average in all lan guages.",
            "For the domain adaptation track we applied a tree revision method which learns how to correct the mistakes made by the base parser on the adaptation domain.",
            "The technique achieved quite promising results and it also offers the interesting possibility of being iterated, allowing the parser to incorporate language knowledge from additional domains.",
            "For the adaptation track we used a novel ap proach, based on the technique of tree revision, applied to a parser trained on a corpus combining sentences from both the training and the adaptation domain.",
            "For proper comparison with other approaches, one should take into account that the parser is incremental and deterministic; hence it is typically faster than other non linear algorithms.",
            "Since the technique is applicable to any parser, we plan to test it also with more accurate English parsers.",
            "Multilingual Dependency Parsing and Domain Adaptation using DeSR",
            "For performing multilingual parsing in the CoNLL 2007 shared task we employed DeSR, a classifier based Shift/Reduce parser.",
            "Conclusions"
        ]
    },
    "C02-1154": {
        "input_sentences": [
            "These names exhibitcertain properties that make their identi ca tion more complex than that of regular propernames.",
            "Nomen uses a novel form of bootstrap ping to grow sets of textual instances and of their contextual patterns.",
            "The algorithm makes use of competing evidence to boost the learning of several categories of names simultaneously.",
            "We present an algorithm, Nomen, for learning generalized names in text.",
            "We also investigate the relative merits of several evaluation strategies.",
            "Abstract",
            "Unsupervised Learning Of Generalized Names",
            "We present results of the algorithm on a large corpus.",
            "Examples of these are names of diseases and infectious agents, such as bacteria and viruses."
        ]
    },
    "W11-0131": {
        "input_sentences": [
            "Distributed models of semantics assume that word meanings can be discovered from \u201cthe company they keep.\u201d Many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a docu- In contrast, this paper proposes a semantic framework, in which semantic vectors are defined and composed in syntactic context.",
            "This paper has introduced a structured vectorial semantic (SVS) framework in which vector composition and syntactic parsing are a single, interactive process.",
            "As such, syntax and semantics are fully interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse.",
            "It was found that relationallyclustered SVS outperformed the simpler lexicalized model and syntax-only models, and that additional clusters had a mildly positive effect.",
            "Additionally, perplexity results showed that the integration of distributed semantics in relationally-clustered SVS improved the model over a non-interactive baseline.",
            "Conclusion",
            "Abstract",
            "Two standard parsing techniques were defined within SVS and evaluated: headword-lexicalization SVS (bilexical parsing) and relational-clustering SVS (latent annotations).",
            "It is hoped that this flexible framework will enable new generations of interactive interpretation models that deal with the syntax\u2013semantics interface in a plausible manner.",
            "The framework thus fully integrates distributional semantics with traditional syntactic models of language.",
            "Evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.",
            "Structured Composition of Semantic Vectors"
        ]
    },
    "A97-1014": {
        "input_sentences": [
            "Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.",
            "Abstract",
            "In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "Since the combinatorics of syntactic constructions creates a. demand for very large corpora., efficiency of annotation is an important. criterion for the success of the developed methodology and tools.",
            "We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.",
            "Conclusion",
            "In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.",
            "Its extension is subject to further investigations.",
            "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme.",
            "The following features of our formalism are then of particular importance: The current tagset comprises only 16 node labels and 34 function tags, yet a. finely grinned classification will take place in the near future.",
            "Syntactically annotated corpora of German have been missing until now.",
            "An Annotation Scheme for Free Word Order Languages",
            "As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data., interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.",
            "As the annotation scheme described in this paper focusses on annotating argument structure rather than constituent trees, it differs from existing treebanks in several aspects.",
            "We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.",
            "The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- \u2022lar representational strata.",
            "We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.",
            "In general, the resulting interpreted data also are closer to semantic annotation and more neutral with respect to particular syntactic theories.",
            "Partial automation included in the current version significantly reduces the manna.1 effort."
        ]
    },
    "W10-1403": {
        "input_sentences": [
            "Results show that POS, case, suffix, root, along with local morphosyntactic features help dependency parsing.",
            "Abstract",
            "These methods can be thought as different paradigms of modularity.",
            "We systematically explored the effect of various linguistic features in Hindi dependency parsing.",
            "There have been attempts at using minimal semantic information in dependency parsing for Hindi (Bharati et al., 2008).",
            "Further, we compared the results of various experiments based on various criterions and did some error analysis.",
            "Some attempts at using clause information in dependency parsing for Hindi (Gadde et al., 2010) have also been made.",
            "For practical reasons (i.e. given the POS tagger/chunker accuracies), it is wiser to use this information as features rather than dividing the task into two stages.",
            "Further, we compare the results of various experiments based on various criterions and do some error analysis.",
            "Conclusion",
            "Inter-chunk dependencies are syntacto-semantic in nature.",
            "We then investigated the best way to incorporate this information during dependency parsing.",
            "We then described 2 methods to incorporate such features during the parsing process.",
            "As mentioned earlier, this is the first attempt at complete sentence level parsing for Hindi.",
            "All the experiments were done with two data-driven parsers, MaltParser and MSTParser, on a part of multi-layered and multi-representational Hindi Treebank which is under development.",
            "Discussion and Future Work",
            "In this paper we explored two strategies to incorporate local morphosyntactic features in Hindi dependency parsing.",
            "These features are obtained using a shallow parser.",
            "We then investigate the best way to incorporate this information during dependency parsing.",
            "This paper is also the first attempt at complete sentence level parsing for Hindi.",
            "These features were obtained using a shallow parser.",
            "This paper was also the first attempt at complete sentence level parsing for Hindi.",
            "Using gold-standard semantic features, they showed considerable improvement in the core inter-chunk dependency accuracy.",
            "But syntactic information alone is always not sufficient, either due to unavailability or due to ambiguity.",
            "These attempts were at inter-chunk dependency parsing using gold-standard POS tags and chunks.",
            "We plan to see their effect in complete sentence parsing using automatic shallow parser information also.",
            "In this paper we explore two strategies to incorporate local morphosyntactic features in Hindi dependency parsing.",
            "Recently, Ambati et al. (2009b) used six semantic features namely, human, non-human, in-animate, time, place, and abstract for Hindi dependency parsing.",
            "We first explored which information provided by the shallow parser is useful and showed that local morphosyntactic features in the form of chunk type, head/non-head info, chunk boundary info, distance to the end of the chunk and suffix concatenation are very crucial for Hindi dependency parsing.",
            "We first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in Hindi dependency parsing.",
            "In such cases, providing some semantic information can help in improving the inter-chunk dependency accuracy.",
            "The parser depends on surface syntactic cues to identify such relations.",
            "Two Methods to Incorporate &rsquo;Local Morphosyntactic&rsquo; Features in Hindi Dependency Parsing",
            "So, we cannot compare our results with previous attempts at Hindi dependency parsing, due to, (a) The data used here is different and (b) we produce complete sentence parses rather than chunk level parses.",
            "As mentioned in section 5.1, accuracies of intrachunk dependencies are very high compared to inter-chunk dependencies."
        ]
    },
    "W06-2207": {
        "input_sentences": [
            "Our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an Expectation Maximization clustering algorithm that uses the text words as attributes.",
            "In this paper we present a hybrid approach for the acquisition of syntacticosemantic patterns from raw text.",
            "A Hybrid Approach For The Acquisition Of Information Extraction Patterns",
            "We show that the combination of the two methods always outperforms the decision list learner alone.",
            "Abstract",
            "For the same recall point, the proposed method increases the precision of the generated models up to 35% from the previous state of the art.",
            "The direct evaluation of the acquired patterns by the human experts validates these results.",
            "Furthermore, using a modular architecture we investigate several algorithms for pattern ranking, the most important component of the decision list learner.",
            "Our results indicate that co-training the Expectation Maximization algorithm with the decision list learner tailored to acquire only high precision patterns is by far the best solution.",
            "Furthermore, the combination of the two feature spaces (words and patterns) also increases the coverage of the acquired patterns.",
            "For the evaluation of the proposed approach we have used both an indirect evaluation based on Text Categorization and a direct evaluation where human experts evaluated the quality of the generated patterns.",
            "Conclusions",
            "This paper introduces a hybrid, lightly-supervised method for the acquisition of syntactico-semantic patterns for Information Extraction.",
            "Furthermore, we customize the decision list learner with up to four criteria for pattern selection, which is the most important component of the acquisition algorithm."
        ]
    },
    "W09-0402": {
        "input_sentences": [
            "The results also showed that the syntax-oriented metrics are competitive with the widely used evaluation measures BLEU, METEOR and TER.",
            "We carried out an extensive analysis of the Spearman\u2019s rank correlation coefficients between the syntactic evaluation metrics and the human judgments.",
            "Especially promising are the POSBLEU and the POSF score.",
            "Syntax-Oriented Evaluation Measures for Machine Translation Output",
            "We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the the on the detailed tags as well as the precision, recall and F-measure obtained We also introduced Fbased on both word and grams.",
            "Abstract",
            "The correlations of the WPF score are slightly lower than those of the purely POS based metrics \u2013 however, this metric has advantage of taking both syntactic and lexical aspect into account.",
            "We proposed several syntax-oriented evaluation metrics based on the detailed POS tags: the POSBLEU score and POS-n-gram precision, recall and F-measure, i.e. the POSP, POSR, and POSF score.",
            "The results presented in this article suggest that the syntactic information has the potential to strenghten automatic evaluation metrics, and there are many possible directions for future work.",
            "Conclusions",
            "In addition, we introduced a measure which takes into account both POS tags and words: the WPF score.",
            "The obtained results showed that the new metrics correlate well with human judgments, namely the adequacy and fluency scores, as well as the sentence ranking."
        ]
    },
    "C04-1180": {
        "input_sentences": [
            "Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.",
            "This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.",
            "Abstract",
            "We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.",
            "Wide-Coverage Semantic Representations From A CCG Parser"
        ]
    },
    "D12-1133": {
        "input_sentences": [
            "We have presented the first system for joint partof-speech tagging and labeled dependency parsing with non-projective dependency trees.",
            "The error analysis reveals improvements in tagging accuracy for syntactically central categories, mainly verbs, with improvement in syntactic accuracy for core grammatical functions as a result.",
            "Conclusion",
            "Abstract",
            "Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.",
            "Evaluation on four languages shows consistent improvements in both tagging and parsing accuracy over a pipeline system with state-of-the-art results across the board.",
            "In future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features.",
            "A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing",
            "We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees.",
            "Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins."
        ]
    },
    "P14-1126": {
        "input_sentences": [
            "We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resourcerich language.",
            "In this paper, we propose an unsupervised projective dependency parsing approach for resourcepoor languages, using existing resources from a resource-rich source language.",
            "We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization.",
            "Our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages.",
            "Conclusion",
            "By presenting a model training framework, our approach can utilize parallel text to estimate transferring distribution with the help of a well-developed resourcerich language dependency parser, and use unlabeled data as entropy regularization.",
            "We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems.",
            "Abstract",
            "The experimental results on three data sets across ten target languages show that our approach achieves significant improvement over previous studies.",
            "We perform experiments on three Data sets \u2014 Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages.",
            "Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization"
        ]
    },
    "P12-3012": {
        "input_sentences": [
            "In this paper we present an API for programmatic access to BabelNet \u2013 a wide-coverage multilingual lexical knowledge base \u2013 and multilingual knowledge-rich Word Sense Disambiguation (WSD).",
            "Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction.",
            "Multilingual WSD with Just a Few Lines of Code: the BabelNet API",
            "Abstract"
        ]
    },
    "W11-2205": {
        "input_sentences": [
            "Instead, we argue that it is more appropriate to evaluate unsupervised methods in context, either as a pre-processing step for a downstream task or as a tool for data exploration.",
            "Following this, we proposed that future work should focus on adapting to and evaluating unsupervised learning methods in the context in which they are intended to be used and that a shared task would facilitate research in this direction.",
            "Using PoS tagging as our case study, we examined recent attempts of evaluating unsupervised approaches and showed that a lot of confusion is caused due to evaluating their output against a labeled gold standard.",
            "Using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods.",
            "Inwe argue that the rarely used evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied.",
            "However, this advantage makes them difficult to evaluate against a manually labeled gold standard.",
            "Finally, we hope that the adoption of in-context evaluation will result in the development of improved unsupervised learning methods for NLP tasks, so that researchers and practitioners can exploit the large amounts of textual data available.",
            "Abstract",
            "The primary advantage of these methods is that they do not require annotated data to learn a model.",
            "Finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing.",
            "The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research.",
            "Conclusions",
            "Evaluating unsupervised learning for natural language processing tasks",
            "In this position paper, we discussed the issue of evaluation of unsupervised learning methods for NLP tasks."
        ]
    },
    "W12-3154": {
        "input_sentences": [
            "In this paper, we first try to relate the effect of the outof-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the outof-domain data at different parts of the training pipeline (alignment, phrase extraction, and phrase scoring).",
            "\u201cfill in\u201d) the phrase-table, and using the scores provided by out-of-domain data has a tendency to be harmful to translation quality.",
            "Nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data.",
            "This explains why approaches which try to weight the outof-domain data in some way (e.g. corpus weighting or instance weighting) can be more successful than simply concatenating data sets.",
            "Analysing the Effect of Out-of-Domain Data on SMT Systems",
            "However, the experiments of Sections 3.4 and 3.5 show some common themes emerging in the two domains.",
            "In both cases, the out-of-domain data helps most when it is just allowed to add entries (i.e.",
            "The precision results of Section 3.5 show out-of-domain data (when it is simply added to the training set) mainly helping with the low frequency words, and having a neutral or harmful effect for higher frequency words.",
            "It also suggests that the way forward is to look for methods that use the out-of-domain data mainly for rarer words, and not to change translations which have a lot of evidence in the in-domain data.",
            "Abstract",
            "Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words.",
            "In this paper we have attempted to give an indepth analysis of the domain adaptation problem for two different domain adaptation problems in phrasebased MT.",
            "This can be detected by the differences in word distribution and out-ofvocabulary rates observed in Figure 2, and is reflected by the differing translation results in Figure 3.",
            "Conclusions",
            "The differences between the two problems are clearly illustrated by the results in Figures 2 and 3, where we see that the difference between the in-domain and out-of-domain data are larger for the OpenSubtitles domain than for the News-Commentary domain.",
            "In statistical machine translation (SMT), it is known that performance declines when the training data is in a different domain from the test data."
        ]
    },
    "P11-1144": {
        "input_sentences": [
            "We showed that graph-based label propagation and resulting smoothed frame distributions over unseen targets significantly improved the coverage of a state-of-the-art semantic frame disambiguation model to previously unseen predicates, also improving the quality of full framesemantic parses.",
            "Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework.",
            "Semi-Supervised Frame-Semantic Parsing for Unknown Predicates",
            "Conclusion",
            "Abstract",
            "The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement full frame-semantic parsing on a blind test set, over a state-of-the-art supervised baseline.",
            "We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones.",
            "The improved parser is available at http://www.ark.cs.cmu.edu/SEMAFOR.",
            "We have presented a semi-supervised strategy to improve the coverage of a frame-semantic parsing model.",
            "We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data."
        ]
    },
    "W09-1116": {
        "input_sentences": [
            "pronouns like reflect the gender and number of the entities to which they refer.",
            "A further name-matching post-processor reduced error even further, resulting in 96.7% accuracy on the test data.",
            "Conclusion",
            "We have shown how noun-pronoun co-occurrence counts can be used to automatically annotate the gender of millions of nouns in unlabeled text.",
            "While this provides useful statistics for frequent nouns, many infrequent nouns cannot be classified using this method.",
            "Abstract",
            "Rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model.",
            "Our model collectively classifies all occurrences of a noun in a document using a wide variety of contextual, morphological, and categorical gender features.",
            "Training from these examples produced a classifier that clearly exceeds the state-of-the-art in gender classification.",
            "By leveraging large volumes of unlabeled data, our full semi-supervised system reduces error by 50% over the existing stateof-the-art in gender classification.",
            "We incorporated thousands of useful but previously unexplored indicators of noun gender as features in our classifier.",
            "Our final system is the broadest and most accurate gender model yet created, and should be of value to many pronoun and coreference resolution systems.",
            "Glen Glenda or Glendale: Unsupervised and Semi-supervised Learning of English Noun Gender",
            "By combining the predictions of this classifier with the original gender counts, we were able to produce a gender predictor that achieves 95.5% classification accuracy on 2596 test nouns, a 50% reduction in error over the current state-of-the-art.",
            "Pronoun resolution systems can use this fact to filter noun candidates that do not agree with the pronoun gender.",
            "Indeed, broad-coverage models of noun gender have proved to be the most important source of world knowledge in automatic pronoun resolution systems.",
            "Previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class."
        ]
    },
    "W11-1002": {
        "input_sentences": [
            "Conclusion",
            "Our analysis finds that both properly structured and flattened representations fail to adequately account for the contribution of each semantic frame to the overall sentence.",
            "We then show that the correlation of HMEANT, the human variant of MEANT, can be greatly improved by introducing a simple length-based weighting scheme that approximates the degree of contribution of each semantic frame to the overall sentence.",
            "Abstract",
            "Structured vs. Flat Semantic Role Representations for Machine Translation Evaluation",
            "The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame\u2019s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER.",
            "We argue that failing to capture the degree of contribution of each semantic frame in a sentence explains puzzling results in recent work on the MEANT family of semantic MT evaluation metrics, which have disturbingly indicated that dissociating semantic roles and fillers from their predicates actually improves correlation with human adequacy judgments even though, intuitively, properly segregating event frames should more accurately reflect the preservation of meaning."
        ]
    },
    "W06-2905": {
        "input_sentences": [
            "Another concern is the size of the grammar the estimation procedure produces, and hence the time and space efficiency of the resulting parser.",
            "For each subtree t, we will need to consider the set of all its derivations (exponential in the size of t), and for each derivation the set of supertwigs of the first elementary trees and, for incompletely lexicalized subtrees, the set of superprunes of all elementary trees in their derivations.",
            "Abstract",
            "Equation (8) remains valid under various restrictions on the elementary trees that we are willing to consider as productive units.",
            "In table 2 we show, for the d = 4 grammar of figure 1, that a 10-fold reduction of the grammar size by pruning elementary trees with low scores, leads only to a small decrease in the LP and LR measures.",
            "The latter two sets, however, need not be constructed for every time the expected frequency E[f(t)] is calculated.",
            "CFG rules), equation (8) collapses to the product of inside and outside probabilities, which can be calculated using dynamical programming in polynomial time (Lari and Young, 1990).",
            "We explore a novel computational approach to identifying \u201cconstructions\u201d or \u201cmulti-word expressions\u201d (MWEs) in an annotated corpus.",
            "One will typically want to calculate this value for all subtrees, the number of which is exponential in the size of the trees in the training data.",
            "Table 1 already showed that push-n-pull leads to a more concise grammar.",
            "Finally, given that the current approach is dependent on the availability of a large annotated corpus, an important question is if and how it can be extended to work with unlabeled data.",
            "Theoretical linguistics has long strived to account for the unbounded productivity of natural language syntax with as few units and rules of combination as possible.",
            "How can we recover those signatures?",
            "Given our best guess of the STSG that generated the data, we can start to ask questions like: which subtrees are overrepresented in the corpus?",
            "Listed are the elementary trees of the induced STSG with for each tree the score, the weight and the frequency with which it occurs in the training set. corpus is generated by an STSG, and by inferring the properties of that underlying STSG.",
            "What Are The Productive Units Of Natural Language Grammar? A DOP Approach To The Automatic Identification Of Constructions",
            "Instead, we can, as we do in the current implementation, keep track of the two sums for every change of the weights.",
            "In this paper we have presented an approach to identifying the relevant statistical correlations in a corpus based on the assumption that the (a) The \u201cshow me NP PP\u201d frame, (b) The complete parse tree (c) The frame for \u201cflights from NP to which occurs very frequently in for the sentence \u201cWhich of NP\u201d the training data and is repre- these flights\u201d, which occurs sented in several elementary trees 16 times in training data. with high weight. using the push-n-pull algorithm on the ATIS3 corpus.",
            "That is, can we transform the push-npull algorithm to perform the unsupervised learning of STSGs?",
            "Some of these will remove the exponential dependence on the size of the trees in the training data.",
            "Calculating E[f(t)] using equation (8) can be extremely expensive in computational terms.",
            "We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank.",
            "With the added operation of adjunction, equation (8) is not valid anymore.",
            "Given the computational complexities that it already gives rise to, however, it seems that issue of linguistically motivated restrictions (other than lexicalization) should be considered first.",
            "In this MWEs have no special status, but emerge in a general procedure for finding the best statistical grammar to describe the training corpus.",
            "More generally, push-n-pull generates extremely tilted score distributions, which allows for even more compact but highly accurate approximations.",
            "If this view is correct, we expect to see statistical signatures of these constructions in the distributional information that can be derived from corpora of natural language utterances.",
            "Although most work on unsupervised grammar learning concerns SCFGs (including some of our own (Zuidema, 2003)) it is interesting to note that much of the evidence for construction grammar in fact comes from the language acquisition literature (Tomasello, 2000).",
            "A major topic for future research is to define linguistically motivated restrictions that allow for efficient computation.",
            "Which correlations are so strong that it is reasonable to think of the correlated phrases as a single unit?",
            "The statistical grammar formalism used is that of stochastic tree substitution grammars (STSGs), such as used in Data-Oriented Parsing.",
            "We presented a new algorithm for estimating weights of an STSG from a corpus, and reported promising empirical results on a small corpus.",
            "Conclusions",
            "For use in the current implementation, the parse trees have been converted to Chomsky Normal Form (all occurrences of A \u2192 B, B \u2192 w are replaced by A \u2192 w; all occurrences of A \u2192 BCw are replaced by A \u2192 BA\u2217, A\u2217 \u2192 Cw), all non-terminal labels are made unique for a particular parse tree (address labeling not shown) and all top nodes are replaced by the non-terminal \u201cTOP\u201d.",
            "The reason is that many potential elementary trees receive a score (and weight) 0.",
            "Discussion",
            "For instance, in the case where we restrict the productive units (with nonzero weights) to depth-1 trees (i.e.",
            "We report quantitative results on the ATIS corpus of phrase-structure annotated sentences, and give examples of the MWEs extracted from this corpus.",
            "Another interesting question is if and how the current algorithm can be extended to the full class of Stochastic Tree-Adjoining Grammars (Schabes, 1992; Resnik, 1992).",
            "In contrast, construction grammar and related theories of grammar postulate a heterogeneous and redundant storage of \u201cconstructions\u201d.",
            "However, there are many further possibilities for improving the efficiency of the algorithm that are currently not implemented."
        ]
    },
    "D10-1044": {
        "input_sentences": [
            "These estimates are in turn combined linearly with relative-frequency estimates from an in-domain phrase table.",
            "In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.",
            "Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.",
            "We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora.",
            "Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation",
            "The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).",
            "Conclusion",
            "Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.",
            "In both cases, the instanceweighting approach improved over a wide range of baselines, giving gains of over 2 BLEU points over the best non-adapted baseline, and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).",
            "We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.",
            "Abstract",
            "Instance Weighting",
            "Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.",
            "This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure.",
            "We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.",
            "We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not."
        ]
    },
    "I08-2105": {
        "input_sentences": [
            "The best configurationuses the syntactically-constrained graph, se lectional preferences computed from thecorpus and a PageRank tie-breaking algo rithm.",
            "We present experiments that analyze the necessity of using a highly interconnectedword/sense graph for unsupervised all words word sense disambiguation.",
            "Compared with graph methods, the approach we described is computationally lighter, while performing at the same level on Senseval-2and Senseval-3 all-words tasks test data.",
            "We especially note good performancewhen disambiguating verbs with grammati cally constrained links.",
            "Unsupervised All-words Word Sense Disambiguation with Grammatical Dependencies",
            "We show that allowing only grammatically related words to influence each other?s senses leads to disambiguation results on a par with thebest graph-based systems, while greatly reducing the computation load.",
            "Abstract",
            "6As opposed to other unsupervised approaches, the sense frequency information from WordNet was not used.",
            "We also com pare two methods for computing selectional preferences between the senses of every two grammatically related words: one using a Lesk-based measure on WordNet, the other using dependency relations from the British National Corpus.",
            "Grammat ical constraints serve both to limit the number of word-senses pair similarities necessary, and also to estimate selectional preferences from an untagged corpus.",
            "Using only grammatically motivated connections leads to better disambiguation of verbs for both Senseval-2 and Senseval-3 test data, but while thedifference is consistent (1.4%, 1.9%) it is not statis tically significant.",
            "Conclusions",
            "We have studied the impact of grammatical in formation for constraining and guiding the word sense disambiguation process in an unsupervised all-words setup.",
            "This indicates that grammatical relations and automatically derived sense association preference scores from a corpus have high potential for unsupervised all-word sense disambigua tion.",
            "We explored a new method for estimating sense association strength from a sense-untagged corpus.Disambiguation when using sense relatedness com puted from WordNet is very close in performance with disambiguation based on sense association strength computed from the British National Corpus,and on a par with state-of-the-art unsupervised systems on Senseval-2."
        ]
    },
    "N10-1002": {
        "input_sentences": [
            "Our results showed that simple unlexicalised features mined from the chart can be used to effectively extract VPCs, and that the model outperforms a probabilistic baseline and the Charniak parser at VPC extraction.",
            "Abstract",
            "We have proposed a chart mining technique for lexical acquisition based on partial parsing with precision grammars.",
            "The main advantage, though, of chart mining is that parsing with precision grammars does not any longer have to assume complete coverage, as has traditionally been the case.",
            "Conclusion",
            "The general chart mining technique can easily be adapted to learn other challenging linguistic phenomena, such as the countability of nouns (Baldwin and Bond, 2003), subcategorization properties of verbs or nouns (Korhonen, 2002), and general multiword expression (MWE) extraction (Baldwin and Kim, 2009).",
            "We applied the proposed method to the task of extracting English verb particle constructions from a prescribed set of corpus instances.",
            "The chart mining approach we propose in this paper is couched in the bottom-up chart parsing paradigm, based exclusively on passive edges.",
            "With MWE extraction, e.g., even though some MWEs are fixed and have no internal syntactic variability, such as ad hoc, there is a very large proportion of idioms that allow various degrees of internal variability, and with a variable number of elements.",
            "Moreover, it would be interesting to investigate the applicability of the technique in other parsing strategies, e.g., head-corner or left-corner parsing.",
            "Discussion and Future Work",
            "The chart mining technique we propose here, which makes use of partial parse results, may facilitate the automatic recognition task of even more flexible idioms, based on the encouraging results for VPCs.",
            "As an immediate consequence, the possibility of applying our chart mining technique to evolving medium-sized grammars makes it especially interesting for lexical acquisition over low-density languages, for instance, where there is a real need for rapid-prototyping of language resources.",
            "The general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks, and is particularly suited to domain-specific lexical tuning and lexical acquisition using lowcoverage grammars.",
            "In general, however, the exact degree of variability of an idiom is difficult to predict (Riehemann, 2001).",
            "Finally, it would also be interesting to investigate whether by using the features we acquire from chart mining enhanced with information on the prevalence of certain patterns, we could achieve performance improvements over broader-coverage treebank parsers such as the Charniak parser.",
            "As future work, we would also like to look into the top-level active edges (those active edges that are never completed), as an indication of failed assumptions.",
            "As an illustration of the functionality of our proposed technique, we develop a lexical acquisition model for English verb particle constructions which operates over unlexicalised features mined from a partial parsing chart.",
            "For example, the idiom spill the beans allows internal modification (spill mountains of beans), passivisation (The beans were spilled in the latest edition of the report), topicalisation (The beans, the opposition spilled), and so forth (Sag et al., 2002).",
            "The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features.",
            "The inventory of features we propose for VPC extraction is just one illustration of how partial parse results can be used in lexical acquisition tasks.",
            "Chart Mining-based Lexical Acquisition with Precision Grammars",
            "In this paper, we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars."
        ]
    }
}