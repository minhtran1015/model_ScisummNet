{
    "P87-1015": [
        "This paper talks about characterizing structural descriptions produced by various grammatical formalisms. We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. We consider properties of the tree sets generated by CFG's, Tree Adjoining Grammars (TAG's), Head Grammars (HG's), Categorial Grammars (CG's), and IG's. In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semi linearity and polynomial recognition of these systems follows. In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs. To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labeled by a pair to include the composition operations. Our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.",
        "In this paper the author aims at to outline how such family of formalisms can be defined, and show that like CFG's, each member possesses a number of desirable linguistic and computational properties: in particular, the constant growth property and polynomial recognisability. In considering the relationship between formalisms, the author shows that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties of their derivation trees. Little attention has been paid to the structural descriptions that these formalisms can assign to strings, i.e. their strong generative capacity. This aspect of the formalism is both linguistically and computationally important. We consider properties of the tree sets generated by CFG's, Tree Adjoining Grammars (TAG's), Head Grammars (HG's), Categorial Grammars (CG's), and IG's. They examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths. These two properties of the tree sets are not only linguistically relevant, but also have computational importance. LCFRS's have only been loosely defined in this paper; they have to provide a complete set of formal properties associated with members of this class. The authors\u2019 goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of the strong generative capacity.",
        "Vijay-Shankar et all considered the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. They showed that it was useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees, find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars. On the basis of that observation, they described a class of formalisms which they called Linear Context- Free Rewriting Systems (LCFRs), and showed they were recognizable in polynomial time and generated only semilinear languages."
    ],
    "W06-2932": [
        "In this paper the author aims at presenting a two-stage multilingual dependency parser and evaluates it on 13 diverse languages. With the availability of resources such as the Penn WSJ Treebank, much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure. However, recently there has been a revived interest in parsing models that produce dependency graph representations of sentences, which model words and their arguments through directed edges. Dependency graphs also encode much of the deep syntactic information needed for further processing. In the future, the author plans to extend these models in two ways. First, they plan on examining the performance difference between two-staged dependency parsing and joint parsing plus labelling. Second, they plan on integrating any available morphological features in a more principled manner. The current system simply includes all morphological bi-gram features. It is the hope that a better morphological feature set will help with both unlabeled parsing and labelling for highly inflected languages.",
        "\tThis paper talks about Multilingual Dependency Analysis with a Two-Stage Discriminative Parser. Here we present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages. The first stage of our system creates an unlabeled parse y for an input sentence x. The second stage takes the output parse y for sentence x and classifies each edge (i, j) E y with a particular label l (i,j). Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph. The current system simply includes all morphological bi-gram features. It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.",
        "McDonald et all presented a two-stage multilingual pendency parser and evaluate it on 13 diverse languages. The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira was augmented with morphological features for a subset of the languages. The second stage took the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. They reported results on the CoNLL-X shared task data sets and presented an error analysis and presented results showing that the spanning tree dependency parsing framework of theirs generalized well to languages other than English."
    ],
    "P11-1060": [
        "The paper 'Learning Dependency-Based Compositional Semantics' by Percy Liang,Michael I. Jordan and Dan Klein propose a new way to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive. The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees,which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.The logical forms in DCS are called DCS trees,where nodes are labeled with predicates, and edges are labeled with relations.DCS trees are learned from Question-Answer pairs and database entries.They trained their model using an EM-like algorithm on two benchmarks, GEO and JOBS. Their system outperforms all existing systems despite using no annotated logical forms.\n",
        "In this paper, the author aims to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, he introduces a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner. CCG is one instantiation, which is used by many semantic parsers. The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive. The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient. Free from the burden of annotating logical forms, the author hopes to use their techniques in developing even more accurate and broader-coverage language understanding systems.",
        " In this paper, the authors mapped questions to answers via latent logical forms, which are induced automatically from question-answer pairs. They introduced a new semantic representation which highlighted a parallel between dependency syntax and efficient evaluation of logical forms. On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms. They built a system that interpreted natural language borders which specified only that border can be used, but utterances much more accurately than existing not how. The combination rules are encoded in the systems, despite using no annotated logical forms. Their features were soft preferences. This yielded a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offered a simple and expressive alternative to lambda calculus search through and parametrize using features. Free from the burden, it also allowed them to easily add new lexical triggers of annotating logical forms."
    ],
    "E03-1005": [
        "Rens Bod in his paper 'An Efficient Implementation of a New DOP Model' presents a DOP approach which is based on two distinctive features:first,the use of corpus fragments rather than grammar rules, and second,the use of arbitrarily large fragments rather than restricted ones.Together with a PCFG-reduction of DOP he obtains improved accuracy and efficiency on the Wall Street Journal treebank.The results show an 11%\nrelative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.The highest accuracy is obtained by SL-DOP(Simplicity-DOP) at 12 smaller than or equal to n smaller than or equal to 14: an LP(Label precision) of 90.8% and an LR(Label Recall) of 90.7%.The accuracy of SL-DOP decreases after n=14 and converges to Simplicity-DOP but the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.\n\n\n",
        "In this paper, Ren Bods proposes an integration of two existing DOP models (one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank)which outperforms each of them separately. They showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree resulted in fast processing times and very competitive accuracy on the Wall Street Journal treebank. They also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic dependencies. The results showed an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.",
        "In this paper the author aims at proposing an integration of the two models which outperforms each of them separately. Together with a PCFG reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal Treebank.  It also presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. They show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996). In the second part, the author extends his experiments with a new notion of the best parse tree. The DOP approach is based on two distinctive features as the first being the use of corpus fragments rather than grammar rules, and the other as the use of arbitrarily large fragments rather than restricted ones. Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence."
    ],
    "W99-0613": [
        "This paper discusses the use of unlabeled examples for the problem of named entity classification.The task is to learn a function from an in-\nput string (proper name) to its type.The approach uses both spelling and contextual rules. A spelling rule might be a simple look-up for the string or a rule that looks at words within a string.They introduced a new algorithm for learning from unlabeled examples, which they called DL-\nCo'Train (DL stands for decision list, the term Cot r a i n is taken from (Blum and Mitchell 98)).It also presents a boosting-like framework that builds on ideas from (Blum and Mitchell 98). The method uses a \"soft\" measure of the agreement between two classifiers as an objective function;it describes an algorithm which directly optimizes this function.\n\n\n",
        "In this paper the author aims at discussing the use of unlabeled examples for the problem of named entity classification. Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labelled training examples. The task is to learn a function from an input string to its type, which we will assume to be one of the categories Person, Organization, or Location. Supervised methods have been applied quite successfully to the full MUC named-entity task. The algorithm can be viewed as heuristically optimizing an objective function; empirically it is shown to be quite successful in optimizing this criterion. The AdaBoost algorithm was developed for supervised learning. They are currently exploring other methods that employ similar ideas and their formal properties. Future work should also extend the approach to build a complete named entity extractor, a method that pulls proper names from text and then classifies them.",
        "Collins and Singer, in this paper, discussed the use of unlabeled examples for the problem of named entity classification. They showed that the use of data could reduce the requirements for supervision to just 7 simple rules. The approach gained leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appeared were sufficient to determine its type. They presented two algorithms. The first method used a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extended ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98) and boosted the objective functions.",
        "This paper talks about Unsupervised Models for Named Entity Classification. A large number of rules are needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. Here we present two algorithms. The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98). The second algorithm builds on a boosting algorithm called AdaBoost. The AdaBoost algorithm was developed for supervised learning. We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories. The contextual rules are restricted and may not be applicable to every example, but the spelling rules are generally applicable and should have good coverage. "
    ],
    "A00-2030": [
        "The paper reports adapting a lexicalized,probabilistic context-free parser with head rules to information extraction.the approach is evaluated on two tasks:The Template Element (TE) task which identifies elements like organizations,persons, locations, and some artifacts and The Template Relations (TR) task which involves identifying instances of relations between elements in the text.TR builds on TE in that TR reports binary relations between elements of TE.The model can limit the propagation of errors by making all decisions jointly. For this reason,The authors designed an integrated model in which tagging, name finding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.The integrated model represents syntax and semantics jointly using augmented parse trees.In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.The system finished in second place among all entrants.Disabling the cross-sentence model entirely reduced overall F-Score by 2 points.\n",
        "In this paper the author aimed at reporting, adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate the new technique on MUC-7 template elements and template relations. The author is able to integrate both syntactic and semantic information into the parsing process, thus avoiding potential errors of syntax first followed by semantics. Their parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation. They were able to specify relatively simple guidelines that students with no training in computational linguistics could annotate. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model.",
        "This paper talks about A Novel Use of Statistical Parsing to Extract Information from Text. We report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations. We describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machine generated syntactic parse trees. We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model."
    ],
    "P08-1043": [
        "In this paper they propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a Treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique their model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of space delimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Using a wide-coverage morphological analyzer based on should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus will make the parser more robust and suitable for use in more realistic scenarios.",
        "Yoav Goldberg and Reut Tsarfaty in their paper'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing' propose a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.The data was taken from the Hebrew Treebank.They employed PCFG and used BitPar, an efficient general purpose parser,together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis and a data-driven morphological analyzer .Their model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.\n",
        "They proposed a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. They employed a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions which were not only theoretically clean and linguistically justified but also probabilistically appropriate and empirically sound. The overall performance of their joint framework demonstrated that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperformed upper bounds proposed by previous joint disambiguation systems and achieved segmentation and parsing results on a par with state-of-the-art standalone applications results. They showed better grammars to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones. Using a treebank grammar, the data-driven lexicon, and a linguistically motivated unknown-tokens handling technique their model outperformed previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far."
    ],
    "A00-2018": [
        "This paper talks about a Maximum-Entropy-Inspired Parser. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that is to successfully to test and combine many different conditioning events. Here we present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1~ average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established \"standard\" sections of the Wall Street Journal tree-bank. In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. The generative model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c). Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming. Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.",
        "In this paper the author aims at the major technical innovation that is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let successfully to test and combines many different conditioning events. They also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head. They also talk about the new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision or recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established  \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus. Indeed, they initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail. It is to this project that their future parsing work will be devoted.",
        "A maximum-entropy-inspired parser is a novel approach for parsing.the authors propose to improve the accuracy of parsing by employing probablistic generative method.The parser is inspired by maximum-entropy model for conditioning and smoothing.Feature sets are used for conditioning and deleted interpolation method is used for smoothing.the model uses a Markov-grammar approach.The lexicalized Markov grammar parsing model achieves an average accuracy of 91.1% on sentences of length less than or equal to 40 and 89.5% on sentences of length less than or equal to 100 which corresponds to an error reduction of 13%.the major achievement along with the accuracy is the strategy of guessing pre-terminal before guessing head,it improved the performance by 2%.This suggests that the flexibility offered by maximum-entropy model may give better results in future if conditioned with different grammars. \n"
    ],
    "W11-2123": [
        "This paper talks about KenLM: Faster and Smaller Language Model Queries.  The PROBING data structure uses linear probing hash tables and is designed for speed. This paper presents methods to query N-gram language models, minimizing time and space costs. Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed. The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM. The code is open source, has minimal dependencies, and offers both C++ and Java interfaces for integration.",
        " The authors presented, in this paper, KenLM, a library that implemented two data structures for efficient language model queries, reducing both time and costs. The structure used linear probing hash tables and was designed for speed. The PROBING model was 2.4 times as fast as the fastest alternative, SRILM, and used less memory too. The TRIE model used less memory than the smallest lossless alternative and was still faster than SRILM. These performance gained transfer to improved system run time performance. Though they focused on Moses, their code was the best lossless option with cdec and Joshua. They attained these results using several optimizations: hashing, custom look up tables, bit-level packing, and state for left-to-right query patterns. The code was open source, had minimal dependencies, and offered both C++ and Java interfaces for integration.",
        "This paper presents KenLM, a library that implements two data structures for efficient language model queries.The paper explains methods to query N-gram language models, minimizing time and space costs.It implements two data structures: PROBING , designed for speed, and TRIE , optimized for memory.It attains results using several optimizations: hashing,custom lookup tables, bit-level packing, and state for left-to-right query patterns.The PROBING model is 2.4 times as fast as the fastest alternative, SRILM, and uses less memory too. The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.\n"
    ],
    "A97-1014": [
        "This paper talks about an annotation scheme for free word order languages. The main key words annotated in this paper are tree bank, corpus, and free word order. It aims at providing syntactically annotated corpora ('tree banks') for stochastic grammar induction. The requirements for such formalism differ from those posited for configurational languages; several features have been added, influencing the architecture of the scheme. In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and refinements. This paper focuses on annotating argument structure rather than constituent trees; it differs from existing tree banks in several aspects. These differences are illustrated by a comparison with the Penn Treebank annotation scheme. Our annotation tool supplies efficient manipulation and immediate visualization of argument structures. ",
        "The author aims at describing an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. It also aims at providing syntactically annotated corpora ('treebanks') for stochastic grammar induction. In particular, they focus on several methodological issues concerning the annotation of non-configurational languages. They will closely coordinate the further development of the corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation. Since the combinatorics of syntactic constructions creates a demand for very large corpora, efficiency of annotation is an important criterion for the success of the developed methodology and tools. Their annotation tool supplies efficient manipulation and immediate visualization of argument structures. Partial automation included it, the current version significantly reduces the manual effort. Its extension is subject to further investigations.",
        "In this paper, Skut et all described an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. The annotation scheme described in this paper focused on annotating argument structure rather than constituent trees. It differed from existing treebanks in several aspects which could be illustrated by a comparison with the Penn Treebank annotation scheme. They argued that their selected approach was better suited for producing high quality interpreted corpora in languages exhibiting free constituent order and provided empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linearizations etc. Their annotation tool supplied efficient manipulation and immediate visualization of argument structures. Partial automation included in the current version significantly reduced the manna.1 effort. Its extension was subject to further investigations according to them."
    ],
    "D09-1092": [
        "In this paper the author aims at introducing a polylingual topic model that discovers topics aligned across multiple languages. They explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. Statistical topic models have emerged as an increasingly useful analysis tool for large text collections. Topic models have been used for analyzing topic trends in research literature, inferring captions for images, social network analysis in email, and expanding queries with topically related words in information retrieval. The author argues that topic modelling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages. By linking topics across languages, polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
        "This paper talks about Polylingual Topic Models. The main keywords annotated in this paper are Polylingual Topic Model, languages and parallel documents. This paper introduces a Polylingual topic model that discovers topics aligned across multiple languages. The Polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) for modeling Polylingual document tuples. First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents. This property is useful for building machine translation systems as well as for human readers. Second, because comparable texts may not use exactly the same topics, it becomes crucially important to be able to characterize differences in topic prevalence at the document level and at the language-wide level. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
        "Authors Mimno et all introduced a poly-lingual topic model that discovered topics aligned across multiple languages. They explored the models characteristics using two large corpora, each with over ten different languages, and demonstrated its usefulness in supporting machine translation and tracking topic trends across languages. They analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora. When applied to comparable document collections such as Wikipedia, PLTM supported data-driven analysis of differences and similarities across all languages for readers who understand any one language."
    ],
    "D10-1044": [
        "'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation' by George Foster, Cyril Goutte and Roland Kuhn study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material is availalable.They aim to explicitly characterize examples from OUT as belonging to general language or not and to apply instance weighting at the level of phrase pairs.Finally, they make some improvements to baseline approaches. they train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.they used a very simple phrase-based system in two different adaptation settings:The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus as OUT,for English/French translation in both directions.The second setting uses the news-related sub-corpora for the NIST09 MT Chinese to English evaluation 8 as IN, and the remaining NIST parallel Chinese/English corpora (UN, Hong Kong Laws,and Hong Kong Hansard) as OUT.They obtained positive results from the approach.\n",
        "Foster et all describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. They extended previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and used simpler training procedure. They incorporated instance-weighting into a mixture-model framework, and found that it yielded consistent improvements over a wide range of baselines.\n\nIn this paper, the authors proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair was characterized by a set of simple features intended to reflect how useful it would be. The features were weighted within a logistic model that gave an overall weight that was applied to the phrase pair and MAP-smoothed relative-frequency estimates which were combined linearly with relative-frequency estimates from an in-domain phrase table. Instance-weighting approach improved over a wide range of baselines, giving gains of over 2 BLEU points over the best non-adapted baseline. They plan on extending instance-weighting to other standard SMT components and \ncapture the degree of generality of phrase pairs.\n",
        "In this paper the author describes a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. Domain adaptation is a common concern when optimizing empirical NLP applications. For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components, which precludes a single universal approach to adaptation. The author also studies the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material. First, they aim to explicitly characterize examples from OUT as belonging to general language or not. Their second contribution is to apply instance weighting at the level of phrase pairs. Finally, they make some improvements to baseline approaches. In future work the author plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion. They will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences. Finally, they intend to explore more sophisticated instance weighting features for capturing the degree of generality of phrase pairs."
    ],
    "P05-1013": [
        "Authors Nilsson and Nivre showed how a data-driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank showed that the combined system can handle non-projective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. The combined system could recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion. This led to the best reported performance for robust non-projective parsing of Czech.",
        "Joakim Nivre and Jens Nilsson in their paper'Pseudo-Projective Dependency Parsing'show that how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of lifting operations and encoding information about these lifts in arc labels.When the parser is trained on the transformed data,it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts.The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a\nsignificant improvement in overall parsing accuracy.\n",
        "This paper talks about Pseudo-Projective Dependency Parsing. In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective tree banks is often neglected because of the relative scarcity of problematic constructions. Here we show how non-projective dependency parsing can be achieved by combining a data driven projective parser with special graph transformation techniques. The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers. The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion, leading to the best reported performance for robust non-projective parsing of Czech."
    ],
    "P11-1061": [
        "In this paper the author aims at describing a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labelled training data, but have translated text in a resource-rich language. They use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model. Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. Unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for training models. To bridge this gap, the author considers a practically motivated scenario, in which he wants to leverage existing resources from a resource-rich language when building tools for resource-poor foreign languages. Their final average POS tagging accuracy of 83.4% compares very favourably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).Their results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language. Their results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.",
        "Das and Petrov, in this paper, approached inducing unsupervised part-of-speech taggers for languages that had no labeled training data, but had translated text in a resource-rich language. Their method did not assume any knowledge about the target language, making it applicable to a wide array of resource-poor languages. They used graph-based label propagation for cross-lingual knowledge transfer and used the projected labels as features in an unsupervised model. Across eight European languages, their approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm. They showed the efficacy of graph-based label propagation for projecting part-of-speech information across languages. Their results suggested that it was possible to learn accurate POS taggers for languages which did not have any annotated data, but have translations into a resource-rich language. It outperformed strong unsupervised baselines as well as approaches that relied on direct projections, and bridged the gap between purely supervised and unsupervised POS tagging models.",
        "Dipanjan Das and Slav Petrov in their paper 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections' propose a new approach for developing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.They use a novel graph-based framework for projecting syntactic information across language boundaries.They construct a bilingual graph\nover word types to establish a connection between the two languages, and then use graph label propagation to project syntactic information from\nEnglish to the foreign language. They treat the projected labels as features in an unsupervised model, rather than using them directly for supervised training. To make the projection practical,they used the twelve universal part-of-speech tags.The focus of this work is on building POS taggers for foreign languages, including an English POS tagger and some parallel text between the two languages.Their results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised\nPOS tagging models.\n"
    ],
    "P08-1028": [
        "In this paper the author aims at proposing a framework for representing the meaning of phrases and sentences in vector space. Central to the approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, they introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Vector-based models of word meaning have become increasingly popular in natural language processing (NLP) and cognitive science. Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature. In fact, the commonest method for combining the vectors is to average them. Vector averaging is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary. Further research is needed to gain a deeper understanding of vector composition, both in terms of modelling a wider range of structures and also in terms of exploring the space of models more fully. Future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets.",
        "In this paper, Mitchell and Lapata proposed a framework for vector-based semantic composition. Central to their approach was vector composition which they operationalize in terms of additive and multiplicative functions. Under this framework, they introduced a wide range of composition models which they evaluated empirically on a sentence similarity task. Experimental results demonstrated that the multiplicative models were superior- at least, for the sentence similarity task attempted- to the additive alternatives when compared against human judgments. They conjectured that the additive models are not sensitive to the fine-grained meaning distinctions involved in their materials. Multiplicative models considered a subset, namely non-zero components whereas additive models captured composition by considering all vector components representing the meaning of the verb and its subject. The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. Further research would be needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures.",
        "Jeff Mitchell and Mirella Lapata in their paper 'Vector-based Models of Semantic Composition' present models of semantic\ncomposition that are empirically grounded and can represent similarity relations. They present a general framework for vector-based composition consider both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.They propose a framework to represent the meaning of the combination u+v as a function f operating on four components:p = f (u, v, R, K).R is the relation holding between u and v, and K additional knowledge. Their results show that the multiplicative models are superior to additive when compared to human judgement.The multiplicative model yields a better fit with the experimental data, \u03c1 = 0.17. The combined model is best overall with \u03c1 = 0.19. However,the difference between the two models is not statistically significant.In order to understand vector composition in deep more research in the area is required.\n"
    ],
    "P08-1102": [
        "Wenbin Jiang,Liang Huang,Qun Liu and Yajuan Lu in their paper 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging'proposes a cascaded linear model inspired by the log-linear model for joint Chinese word segmentation and POS tagging.the cascaded model has a two-layer architecture, with a character based perceptron as the core combined with other real-valued features such as language models.Other than the usual character-based features, additional features dependent on POS\u2019s or words are also employed to improve the performance.Experimental results show that, it achieves obvious improvement over the perceptron-only model, about from 0.973 to 0.978 on segmentation, and from 0.925 to 0.934 on Joint S&T, with error reductions of 18.5% and 12% respectively.\n",
        "In this paper, Jiang et all proposed a cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model was able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments showed that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, they obtained an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline. Under this model, many knowledge sources that might be intractable to be incorporated into the perceptron directly, could be utilized effectively in the outside-layer linear model. This was a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large.",
        "This paper aims at proposing a cascaded linear model for joint Chinese word segmentation and part-of- speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. However, as such features are generated dynamically during the decoding procedure, two limitations arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. To cope with this problem, we propose a cascaded linear model inspired by the log-linear model widely used in statistical machine translation to incorporate different kinds of knowledge sources. In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources can be used to improve performance."
    ],
    "J01-2004": [
        "This paper aims at describing the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modelling for speech recognition. With certain exceptions, computational linguists have in the past generally formed a separate research community from speech recognition researchers, despite obvious overlap of interest. While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task. Two features of our top-down parsing approach will emerge as key to its success. First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar. A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model. There are few implications that are to be taken further in this area. First, there is reason to believe that some of the conditioning information is not uniformly useful, and we would benefit from finer distinctions. Second, there are advantages to top-down parsing that have not been examined to date.",
        "Brian Roark in his paper'Probabilistic Top-Down Parsing and Language Modeling'proposes a new language model, based on probabilistic top-down parsing.Their top-down parser allows for the incremental calculation of generative conditional word probabilities and top-down guidance also improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.It uses a PCFG (Probabilistic (or stochastic) context-free grammars (PCFGs)) with a conditional probability model.The parser gave a remarkable precision rate of 94.1, for sentences of length less than or equal to 100.This implies that using multiple model in one parser to deal with different aspects of language can be fruitful.\n",
        "Roark, in this paper, described the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. They varied the base beam factor in trials on the Chelba and Jelinek corpora, keeping the level of conditioning information constant.    With a simple conditional probability model, and simple statistical search heuristics, they were able to find very accurate parses efficiently, and, assign word probabilities that yielded a perplexity improvement over previous results. Interpolation with a trigram model yielded an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by their parsing model was orthogonal to that captured by a trigram model."
    ],
    "P04-1036": [
        "This paper talks about Finding Predominant Word Senses in Untagged Text. We present work on the use of a thesaurus acquired from raw textual corpora and the Word Net similarity package to find predominant noun senses automatically. The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account. We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking. A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required. We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in Word Net. Whilst we have used Word Net as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbors and the senses.",
        "Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll in their paper 'Finding Predominant Word Senses in Untagged Text'present a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.ey chose documents from the SPORTS domain and a the FINANCE domain.they use an automatically acquired thesaurus and a WordNet Similarity measure.we use the SENSEVAL -2 all-words data (Palmer et al.,2001).This is a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II. The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL -2 English all-words task giving us a WSD precision of 64% on an all-nouns task.\n\n\n",
        "McCarthy et all presented work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses from two domain specific corpora automatically. The acquired predominant senses gave a of 64% on the nouns of the 2 English all-words task, a very promising result given that their method did not require any hand-tagged text. They had devised a method that used raw corpus data to automatically find a predominant sense for nouns in WordNet and used an automatically acquired thesaurus and a WordNet Similarity measure. They were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving them a WSD precision of 64% on an all-nouns task. In many cases, the sense ranking provided in SemCor differed to that obtained automatically because they used the BNC to produce their thesaurus. ",
        "This paper aims at presenting work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL- 2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account. They believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off and for systems that use it in lexical acquisition because of the limited size of hand-tagged resources. The first sense of star in SemCor is celestial body; however, if one were disambiguating popular news celebrity would be preferred. Later, they would perform a large scale evaluation on domain specific corpora. In particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora."
    ],
    "W99-0623": [
        "In this paper, Henderson and Brill combined three state-of-the-art statistical parsers to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. The two general approaches presented to studying parser combination were parser switching and parse hybridization and two combination techniques were described for each approach. Both parametric and non-parametric techniques were given for each experiment. All four of the techniques studied resulted in parsing systems that performed better than any previously reported. Both of the switching techniques and the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiment. Through parser combination, they reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published performance results for the Penn Treebank. ",
        "In this paper the author aims at three state-of-the-art statistical parsers which are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. The natural language processing community is in the strong position of having many available approaches to solve some of its most fundamental problems. Recently, combination techniques have been investigated for part of speech tagging with positive results. In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation, speech recognition and named entity recognition. They have presented two general approaches to studying parser combination: parser switching and parse hybridization. Combining multiple highly-accurate independent parsers yields promising results. The author plans to explore more powerful techniques for exploiting the diversity of parsing methods.",
        "This paper talks about Exploiting Diversity in Natural Language Processing: Combining Parsers. Two general approaches are presented and two combination techniques are described for each approach. Here both parametric and non-parametric models are explored. One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure. The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank. Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result. Combining multiple highly-accurate independent parsers yields promising results. "
    ],
    "W06-3114": [
        "Koehn and Monz carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs. While many systems had similar performance, the results offered interesting insights, especially, about the relative performance of statistical and rule-based systems. Due to many similarly performing systems, they are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favour of statistical systems seemed to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1&#8211;5 seemed to be very hard to perform. Human judges also pointed out difficulties with the evaluation of long sentences. They found replacing it with a ranked evaluation to be more suitable.",
        "The paper'Manual and Automatic Evaluation of Machine Translation between European Languages'by Philipp Koehn and Christof Monz talks about a shared task to evaluate machine translation performance.They assembled various forms of data and resources: a baseline MT system,language models, prepared training and test sets,resulting in actual machine translation output from several state-of-the-art systems and manual evaluations.Training and testing is based on the Europarl corpus.For translation they used automatic evaluation and manual evaluation.For automatic evaluation they used BLEU.They were not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.\n",
        "In this paper the author evaluates machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done automatically using the BLEU score and manually on fluency and adequacy. Due to many similarly performing systems, the author was not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favour of statistical systems seems to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1\u20135 seems to be very hard to perform. Replacing this with a ranked evaluation seems to be more suitable. Human judges also pointed out difficulties with the evaluation of long sentences."
    ]
}