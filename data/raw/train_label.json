{
    "D10-1083": [
        "In this paper, the authors are of the opinion that the sequence models-based approaches usually treat token-level tag assignment as the primary latent variable. However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity. In this work, they take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. Their work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. There are clustering approaches that assign a single POS tag to each word type. These clusters are computed using an SVD variant without relying on transitional structure. The departure from the traditional token-based tagging approach allow them to explicitly capture type-level distributional properties of valid POS tag assignments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive. Their empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints. This assumption, however, is not inherent to type-based tagging models."
    ],
    "W09-0621": [
        "This paper talks about Clustering and matching headlines for automatic paraphrase acquisition. For this purpose it is necessary to have a monolingual corpus of aligned paraphrased sentences. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pair wise similarity-based matching. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pair wise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision. "
    ],
    "P05-1004": [
        "The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction and class-based smoothing, to text classification and question answering. Some specialist topics are better covered in WORDNET than others. A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORDNET and the construction of new wordnet using the concept structure from English. Ciaramita and Johnson  implement a supersense tagger based on the multi-class preceptor classifier, which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. The authors demonstrate the use of a very efficient shallow NLP pipeline to process a massive corpus. Such a corpus is needed to acquire reliable contextual information for the often very rare nouns they are attempting to supersense tag.\n"
    ],
    "P07-1040": [
        "The paper \u201cImproved Word-Level System Combination for Machine Translation\u201d by Antti-Veikko I. Rosti and Spyros Matsoukas and Richard Schwartz describes an improved confusion network based method to combine outputs from multiple MT systems. In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and  re-scoring.Confusion network decoding in MT  picks one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network.An improved confusion network decoding method combining the word posteriors with arbitrary features was presented. This allows the addition of language model scores by expanding the lattices or re-scoring N-best lists.The new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER,BLEU and METEOR. The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese. It also seems like METEOR should not be used in tuning due to high insertion rate and low precision. "
    ],
    "P06-2124": [
        "In this paper, the authors proposed a probabilistic admixture model to capture latent topics underlying the context of document- pairs. They proposed a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. They used IBM Model I as the baseline. They investigated three instances of the BiTAM model, They were data-driven and did not need handcrafted knowledge engineering. The proposed models signi\ufb01cantly improved the alignment accuracy and lead to better translation qualities. Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations. "
    ],
    "H05-1115": [
        "The authors\u2019 aim is to consider the problem of question focused sentence retrieval from complex news\narticles describing multi-event stories published over time. Annotators generated a list of questions\ncentral to understanding each story in our corpus. Recent work has motivated the need for systems\nthat support \u201cInformation Synthesis\u201d tasks, in which a user seeks a global understanding of a topic or\nstory. The specific problem they consider differs from the classic task of PR for a Q&A system in\ninteresting ways, due to the time-sensitive nature of the stories in our corpus. They aim to develop a\nmethod for sentence retrieval that goes beyond finding sentences that are similar to a single query. To\nthis end, they propose to use a stochastic, graph-based method. To evaluate their sentence retrieval\nmechanism, they produced extract files, which contain a list of sentence deemed to be relevant to the\nquestion, for the system and from human judgment. During the task, they have shown that over a\nlarge set of unaltered questions written by their annotators, LexRank can, on average, outperform the\nbaseline system, particularly in terms of TRDR scores."
    ],
    "W06-3909": [
        "The author in this paper presents Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision. Most researches on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. Terms are commonly defined as surface representations of stable and key domain concepts. Defining regular expressions over POS-tagged corpora is the most commonly used technique to both define and extract terms. For our preliminary evaluation, the author consider the standard is-a and part-of relations as well as three novel relations i.e. succession, reaction and production. Preliminary results show that Espresso generates highly precise relations, but at the expense of lower recall. The authors are working on improving system recall with a web-based method to identify generic patterns and filter their instances. Early results appear very promising. The authors also plan to investigate the use of WordNet selectional constraints. The authors expect here that negative instances will play a key role in determining the selectional restriction on generic patterns. "
    ],
    "C10-1045": [
        "This paper offers a broad insight into of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. It is probably the first analysis of Arabic parsing of this kind. It is well-known that English constituency parsing models do not generalize to other languages and treebanks. Explanations for this phenomenon are relative informativeness of lexicalization, insensitivity to morphology and the effect of variable word order and these factors lead to syntactic disambiguation. The authors use linguistic and annotation insights to develop a manually annotated grammar and evaluate it and finally provide a realistic evaluation in which segmentation is performed in a pipeline jointly with parsing. The authors show that PATB is similar to other tree-banks but that annotation consistency remains low."
    ],
    "C02-1025": [
        "This paper presents a maximum entropy-based named entity recognizer (NER). NER is useful in many NLP applications such as information extraction, question answering, etc .Chieu and Ng have shown that the maximum entropy framework is able to use global information directly from various sources. They believe that global context is useful in most languages, as it is a natural tendency for authors to use abbreviations on entities already mentioned previously. They have made use of local and global features to deal with the instances of same token in a document. Their results show that their high performance NER use less training data than other systems.  The use of global features has shown excellent result in the performance on MUC-6 and MUC-7 test data. Using less training data than other systems, their NER can perform as well as other state-of-the-art NERs."
    ],
    "P00-1025": [
        "The author describes a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output. Regular-expression descriptions are compiled into finite-state automata or transducers as usual, and then the compiler is re-applied to its own output, producing a modified but still finite- state network. The technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation. If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (FST). The application of the compile-replace algorithm to the lower side of the network eliminates the markers, compiles the regular expression a* and maps the upper side of the path to the language resulting from the compilation. The compile-replace algorithm is essentially a variant of a simple recursive-descent copying routine. Traditional Two-Level implementations are already capable of describing some limited reduplication and infixation as in Tagalog. The merge algorithm is a pattern-filling operation that combines two regular languages, a template and filler, into a single one. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics. The compile-replace algorithm and the merge operator introduced in this paper are general techniques which are not limited to handling the specific morphotactic problems have been discussed. The author expects that they will have many other useful applications. "
    ],
    "P98-1046": [
        "In this paper the authors specifically address questions of polysemy of verbs and how regular extensions can be achieved. Current approaches to English classification, Levin classes and WordNet, have limitations in their applicability, thus they present a refined intersective Levin classes, which is a more fine-grained classification and have more coherece in syntactic and semantic frames. We have also examined related classes in Portuguese. They have also examined a mapping between the English verbs and their Portuguese translations, by using resources such as dictionaries and on-line corpora to investigate potential additional members of the verb classes. This implementation has given them maximum success."
    ],
    "W11-0815": [
        "The extensive use of Multiword Expressions (MWE) in natural language texts aims for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. The aim of the paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. The motivation of the work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction. The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. In terms of practical MWE identification systems, a well known approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora. This approach is implemented in a lexicographic tool called Xtract. More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates. There were several experiments carried out in MWE. The MWEs insertion improves the results of retrieval for relevant documents, because the indexing of specific terms makes it easier to retrieve specific documents related to these terms. Nevertheless, the use of these expressions made the results worse in some cases, because some topics require a semantic analysis to return relevant documents. Some of these documents are related to the query, but do not satisfy all criteria in the query topic. We conclude also that the quality of MWEs used directly influenced the results. "
    ],
    "J00-3003": [
        "In this paper the aim of the model is detecting and predicting dialogue acts based on lexical,\ncollocation, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.\nThe first level of analysis involves the identification of dialogue acts. DA represents the meaning of\nan utterance at the level of illocutionary force. The DAs tagset classifies utterances according to a\ncombination of pragmatic, semantic, and syntactic criteria. Another important role of DA information\ncould be feedback to lower-level processing. The goal of this article is twofold: On the one hand, we\naim to present a comprehensive framework for modelling and automatic classification of DAs,\nfounded on well-known statistical methods. Second, we present results obtained with this approach on\na large, widely available corpus of spontaneous conversational speech. The domain we chose to\nmodel is the Switchboard corpus of human-human conversational telephone speech distributed by the\nLinguistic Data Consortium. Automatic segmentation of spontaneous speech is an open research\nproblem in its own right. Finally, the author developed a principled way of incorporating DA\nmodelling into the probability model of a continuous speech recognizer, by constraining word\nhypotheses using the discourse context. However, the approach gives only a small reduction in word\nerror on our corpus, which can be attributed to a preponderance of a single dialogue act type."
    ],
    "J98-2005": [
        "This paper talks about estimated production probabilities of a context-free grammar that always yield proper distributions.they picked Context-free grammar because it is useful because of its relatively broad coverage\nand because of the availability of efficient parsing algorithms.there is a simple maximum-likelihood prescription for estimating the production probabilities\nfrom a corpus of trees that results in a PCFG.The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator.for them this is the best estimator to simplify the treatment by assuming that there are no null or unit productions.The iterative approach for unparsed corpus is an instance of the EM Algorithm.EM stands for Expectation-Maximization.they showed that in both cases when the corpus was unparsed and when the corpus was parsed,the estimated probability for both was tight(by tight they meant equality of total probability mass to one)."
    ],
    "W08-2222": [
        "In this paper, the authors describe Boxer, their open-domain software component for semantic analysis of text, based on Combinatory Categorical Grammar (CCG) and Discourse Representation Theory (DRT). Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorical Grammar (CCG) and Discourse Representation Theory (DRT). Based on Discourse Representation Theory Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. Boxer implements a syntax-semantics interface based on Combinatory Categorical Grammar, CCG. Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorical framework underlying it. Boxer\u2019s performance on the shared task for comparing semantic representations was promising. It was able to produce DRSs for all texts. The authors can\u2019t quantify the quality of Boxer\u2019s output, as they don\u2019t have gold standard representations at our disposal. "
    ],
    "C08-1098": [
        "In this paper, Schmid and Laws present a  RFTagger or Hidden-Markov-Model (HMM) part-of-speech tagger  using German and Czech corpora.HMM tagging decomposes the POS tags into a set of simple attributes, and uses decision tree to estimate the probability of each attribute. Decision tree assigns classes to objects which are represented as attribute vectors. Their tagger applies a beam-search strategy to increase the speed and uses dot to separate the attributes. It also applies pre-pruning citeria. Their tagger is fast and can be successfully applied to a wide range of languages and training corpora. Their tagger is highly accurate in comparison to TnTagger and SVMTool."
    ],
    "C04-1089": [
        "In this paper, the authors' aim is to present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighbourhood (i.e., the context) of w. They employ the language modelling approach for the retrieval problem. They attempted to improve named entity translation by combining phonetic and semantic information. The author evaluated the approach on six months of Chinese and English Gigaword corpora, with encouraging results."
    ],
    "J96-3004": [
        "In this paper the authors present a stochastic finite-state model for segmenting Chinese text into words.The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers. It also incorporates the Good-Turing methodin estimating the likelihoods of previously unseen constructions, including morphological derivatives and personal names. they evaluate various specific aspects of the segmentation, as well as the overall segmentation performance. The evaluation compares the performance of the system with that of several human judges and inter-human agreement on a single correct way to segment a text.they showed that the average agreement among the human judges is .76, and the average agreement between ST(system) and the humans is .75, or about 99% of the interhuman agreement.This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration. Other kinds of productive word classes, such as company names, abbreviations,and place names can easily be handled given appropriate models."
    ],
    "P05-1053": [
        "In this paper, the authors focus on the ACE RDC task and employ diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Their study illustrate that the base phrase chunking information contributes to most of the performance improvement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. They also demonstrate how semantic information such as WordNet and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enable their system to achieve best reported performance. This paper also explores the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. The effective incorporation of diverse features enables their system to outperform previously best- reported systems on the ACE corpus."
    ],
    "C90-2039": [
        "The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification method - the lazy incremental copy graph (LING) unification and the strategic incremental copy graph (SING) unification method. The LING unification method achieves structure sharing which avoids memory wastage and increases the portion of token identical substructures of FSs. The SING unification method introduces the feature unification strategy and lists the factors on which it\u2019s efficiency depends. The combined method increases the total efficiency of FS unification-based natural language processing systems."
    ],
    "N06-2049": [
        "In this paper, the authors proposed a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. Their word segmentation process is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subwordbased tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. They used the data from the Sighan Bakeoff 2005 to test their approaches.By these techniques, they achieved higher F-scores in City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR) corpora than the best results from Sighan Bakeoff 2005.This approach is effective for performing desired segmentation based on users\u2019 requirements to R-oov(range out-of-vocabulary) and R-iv(range inside-vocabulary)."
    ],
    "N09-1025": [
        "This paper talks about 11,001 New Features for Statistical Machine Translation. For this they use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and a syntax-based translation system. We add more than 250 features to improve a syntax-based MT system\u2014already the highest-scoring single system in the NIST 2008 Chinese-English common-data track\u2014by +1.1 Bleu. They also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 Bleu improvement. These features are discriminatively trained using MIRA, and led to significant improvements. The authors also provide a closer look at the results to see how the new features qualitatively improved translation quality. "
    ],
    "W04-0213": [
        "This paper discusses the Potsdam Commentary Corpus, a corpus of german assembeled by potsdam university.\nThe corpus was annoted with different linguitic information.the \u2018Potsdam Commentary Corpus\u2019 or PCC consists of\n170 commentaries from M\u00d8arkische Allgemeine Zeitung, a German regional daily.This corpus includes 173 texts on politics from the on-line newspaper\nM\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical\nstructure, connectors, correference and informative structure. Nevertheless, only a part of this corpus (10 texts), which the authors name \"core corpus\",\nis annotated with all this information. The texts were annotated with the RSTtool. This corpus has several advantages: it is annotated at different\nlevels.all the texts were annotated by two people.it is free for research purposes.The disadvantages are: the genre and domain of all the texts are the same, the\nmethodology of annotation was quite intuitive and the inter-annotator agreement is not given."
    ],
    "E03-1020": [
        "This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionary or, taxonomies. It is based on a graph model representing words and relationships between them. A word sense algorithm is outlined which bypasses the problem of parameter turning. The authors describe the experiment that examines the performance of their algorithm on a set of words with varying degree of ambiguity. They also present a sample of the results. The algorithm recognises and resolves ambiguity. It gives rise to an automatic, unsupervised word sense disambiguation algorithm.\n"
    ],
    "C00-2123": [
        "The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). From a DP-based solution to the traveling salesman problem, they present a novel technique to restrict the possible word reordering between source and target language in order to achieve an e\u00c6cient search algorithm. A beam search concept is applied as in speech recognition. There is no global pruning. An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account. In order to handle the necessary word reordering as an optimization problem within the dynamic programming approach, they describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming. The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task. The experimental tests are carried out on the Verbmobil task which is a limited-domain spoken-language task."
    ],
    "N01-1011": [
        "This paper presents a corpus-based approach to\nword sense disambiguation where a decision tree assigns\na sense to an ambiguous word based on the\nbigrams that occur nearby.for this purpose the\nsense inventory of word sense has already been determined.\nThis paper describes an approach where a deci-\nsion tree is learned from some number of sentences\nwhere each instance of an ambiguous word has been\nmanually annotated with a sense-tag that denotes\nthe most appropriate sense for that context and for Building a Feature Set of Bigrams;\ntwo alternatives, the power divergence family\nand the Dice Coefficient were explored.\nthis study utilizes the training and test\ndata from the 1998 SENSEVAL evaluation of word\nsense disambiguation systems.The results of this approach\nare compared with those from the 1998 SENSEVAL\nword sense disambiguation exercise and show that\nthe bigram based decision tree approach is more accurate\nthan the best SENSEVAL results for 19 of 36\nwords.\n"
    ],
    "W95-0104": [
        "This paper presents a hybrid method where two classes of methods have been used for resolving lexical ambiguity. The authors investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. Their method combines context-word and collocation methods where the former captures the lexical \"atmosphere\", while the latter captures local syntax. The authors have used five methods for spelling correction: baseline, context words, collocations, decision list and Bayesian classifier. Decision list and Bayesian classifiers are their hybrid methods. For testing this method, confusion sets have been used with ambiguous words like desert and dessert, there and their etc. In their evaluation, they found that their hybrid Bayesian classifer outperforms the component methods as well as decision lists."
    ],
    "N09-1001": [
        "Fangzhong Su and Katja Markert in their paper 'Subjectivity Recognition on Word Senses via Semi-supervised Mincuts' proposes the approach for supplementing WordNet entries with infor- mation on the subjectivity of its word senses.They propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.They  deal with classification at the word sense level,including subjectivity-ambiguous words.the main idea is binary classification with minimum cuts (Mincuts) in graphs which is based on the idea that similar items should be grouped in the same cut.The formulation of semi-supervised Mincut for sense subjectivity classification involves the steps:Selection of unlabeled data,Weighting of edges to the classification vertices and Assigning weights to WordNet relations.Their best result from Mincuts is 84.6%. "
    ],
    "H89-2014": [
        "The paper aims at describing refinements that are currently being investigated in a model for part-ofspeech\nassignment to words in unrestricted text. The model has the advantage that a pre-tagged\ntraining corpus is not required. The determination of part-of-speech categories for words is an\nimportant problem in language modelling, because both the syntactic and semantic roles of words\ndepend on their part-of-speech category. The statistical methods can be described in terms of Markov\nmodels. States in a model represent categories. In Hidden Markov model, the Baum-Welch algorithm\ncan be used to estimate the model parameters. This has the great advantage of eliminating the pretagged\ncorpus. It minimizes the resources required, facilitates experimentation with different word\ncategories, and is easily adapted for use with other languages. One aim of the work is to investigate\nthe quality and performance of models with minimal parameter descriptions. Many Lisp-specific word\ncategories were created for frequently occurring Lisp symbols in an attempt to reduce bias in the\nestimation. A stochastic method for assigning part-of-speech categories to unrestricted English text\nhas been described. It minimizes the resources required for high performance automatic tagging. "
    ],
    "P98-1081": [
        "This paper examines how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.This paper is concerned with the question whether the differences between models can indeed be exploited to yield a data driven model with superior performance.the approach is known as ensemble, stacked, or combined classifiers.the approach is applied mopho-syntactic wordclass tagging.for this purpose a traditional trig-ram model,the Transformation Based Learning system,Memory-Based Learning and the MXPOST system are used.the tagged LOB corpus has been used for data.The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%).the writers believe that there is still some room for improvement and this approach can be extended to other NLP tasks."
    ],
    "W03-0410": [
        "In this paper, the authors used a semi-supervised method for verb class discovery using Noisy Features. The feature set was previously shown to work well in a supervised learning setting, using known English verb classes. They face the problem of having a large number of irrelevant features for a particular clustering task. They used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all the unsupervised experiments. The evaluation was based on the accuracy of assigning members to the correct clusters. The authors discuss their clustering approach to identify that the task becomes difficult if the learning method has to distinguish multiple classes, rather than focus on the important properties of a single class."
    ],
    "C94-2154": [
        "In this paper, the authors try to show the kind of constraints expressible by appropriateness conditions cane be implemented in a practical system employing typed features structures and unification as the primary operation on feature structures. Being able to express the class of constraints by appropriateness conditions corresponding closely to the class of constraints that can be efficiently pre-compiled is taken as a justification for appropriateness formalisms."
    ],
    "E09-2008": [
        "This paper discusses Foma, a finite state compiler, programming language, and regular expression or finite-state library designed for multi-purpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological or phonological analyzers and spellchecking applications.  The aim of Foma is to provide specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. Several widecoverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma."
    ],
    "D09-1023": [
        "This paper describes Gimpel and Smith\u2019s novel decoder. It is based on efficient DP-based QG lattice parsing extended to handle \u00e2\u20ac\u0153non-local\u00e2\u20ac\u009d features using generic techniques that also support efficient parameter estimation. They used one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features and presented a machine translation framework that could incorporate arbitrary features of both input and output sentences. Their findings showed that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations were helpful for translation, and that substantial gains could be obtained by permitting certain types of non- isomorphism. Their decoding framework allowed them to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output. They used the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on non-isomorphism. "
    ],
    "I05-5011": [
        "This paper conducted research in the area of automatic paraphrase discovery.  The authors believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system. They proposed an unsupervised method to discover paraphrases from a large untagged corpus. They focused on phrases which two Named Entities, and proceed in two stages.  This topic has been getting more attention, driven by the needs of various NLP applications. Most IE researchers have been creating paraphrase knowledge by hand and specific tasks. The authors cluster NE instance pairs based on the words in the context using bag-of-words methods. In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold."
    ],
    "P98-2143": [
        "This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger in 1998.It makes use of only a part-of-speech tagger, plus simple noun phrase rules and operates on the basis of antecedent-tracking preferences or antecedent indicators.these indicators depend upon a number of salience factors like definiteness,immediate reference etc.this system does not require domain specific knowledge.it shows success rate of 89.7%.it can be applied to different languages other than english."
    ],
    "C98-1097": [
        "The paper 'Text Segmentation Using Reiteration and Collocation'  by Amanda C. Jobbins and Lindsay J. Evett presentes a method for segmenting a text into subtopics using Lexical cohesion.Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.For automatic detection of lexical cohesion  ties between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity. In first investigation word repetition alone achieved better results than using either collocation or relation weights individually. The combination of word repetition with another linguistic feature improved on its individual result, where less troughs were placed per text.In second investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments than the test subjects. Each text was only 500 words in length and was related to a specific subject area.These factors limited the degree of subject change that occurred. Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect.The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that  different lexical relations are detected by each linguistic feature considered. "
    ],
    "D10-1058": [
        "The paper 'A Fast Fertility Hidden Markov Model for Word Alignment Using MCMC ' by Shaojun Zhao and Daniel Gildea proposes a model that adds fertility feature to HMM(Hidden Markov Model).Their model is a combination of HMM and IBM model 4 for achieving less alignment error rate  than HMM and making it easier than IBM model 4.They use Gibbs sampling method than the neighborhood method used in IBM Model 4.They evaluate their model by computing the word alignment error rate (AER) as the word alignment evaluation criterion and machine translation quality.They have evaluated their fertility models on a Chinese-English corpus.The results from the experiment proves that the fertility hidden Markov model  runs faster and has lower AER than the HMM. Their model is thus much faster and easier to understand than IBM Model 4.While better word alignment results do not necessarily correspond to better translation quality,their translation results are comparable in translation quality to both the HMM and IBM Model 4. "
    ],
    "N04-1038": [
        "In this paper, Ben and Riloff present a coreference resolver called BABAR that focuses on the use of contextual-role knowledge for coreference resolution. The problem of coreference resolution has received considerable attention, including theoretical discourse models and supervised machine learning systems. BABAR\u2019s performance in both domains of terrorism and natural disaster, and the contextual-role knowledge in pronouns have shown successful results. However, using the top-level semantic classes of WordNet proved to be problematic as the class distinctions are too coarse. Bean and Riloff also used bootstrapping to extend their semantic compatibility model, proposed using caseframe network for anaphora resolution, information extraction patterns to identify contextual clues for determining compatibility between NPs. Finally, several coreference systems have successfully incorporated anaphoricity determination modules."
    ],
    "X96-1048": [
        "This paper is an overview of results of the Sixth Message Understanding Conference (MUC-6) in November.This paper surveys the results of the natural language processing system evaluation on tasks like named entity,coreference,template element and scenario template. Discussion of the results for each task is organized generally under the following topics: Results on task as whole, Results on some aspects of task, Performance on \"walkthrough article.\" The walkthrough article is an article selected from the test set. Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers.Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes.From the 100 test articles, a subset of 30 articles was selected for use as the test set for the Named Entity and Coreference tasks.although Named Entity, Coreference and Template Element are defined as domain-independent tasks,the articles that were used for MUC-6 testing were selected using domain-dependent criteria pertinent to the Scenario Template task.The results of the evaluation give clear evidence of the challenges that have been overcome and the ones that remain along dimensions of both breadth and depth in automated text analysis.a number of named entity systems performed very well for MUC6, well enough to compete with human performance.Performance on TE overall is as high as 80% on the F-measure, with performance on organization objects significantly lower than on person objects.Systems scored approximately 15-25 points lower (F-measure) on ST than on TE. As defined for MUC-6.The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations."
    ]
}