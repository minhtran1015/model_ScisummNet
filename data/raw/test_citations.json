{
    "P04-1036_sweta": [
        {
            "Number": 1,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account (McCarthy et al., 2004)."
        },
        {
            "Number": 2,
            "refer_ID": "P04-1036",
            "refer_sids": [
                82
            ],
            "refer_text": "We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We also calculate the WSD accuracy that would be obtained on SemCor when using our first sense in all contexts.",
            "GPT_cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41 Table 2: The SENSEVAL-2 first sense on the SENSEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available."
        },
        {
            "Number": 3,
            "refer_ID": "P04-1036",
            "refer_sids": [
                64
            ],
            "refer_text": "We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The method is described in (McCarthy et al, 2004), which we summarise here",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We briefly summarize the two measures here; for a more detailed summary see (Patwardhan et al., 2003).",
            "GPT_cite_text": "The method is described in (McCarthy et al., 2004), which we summarize here."
        },
        {
            "Number": 5,
            "refer_ID": "P04-1036",
            "refer_sids": [
                172
            ],
            "refer_text": "We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet; for example, those that use it for lexical acquisition or WSD.",
            "GPT_cite_text": "McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research and propose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges."
        },
        {
            "Number": 6,
            "refer_ID": "P04-1036",
            "refer_sids": [
                153
            ],
            "refer_text": "Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Most research in WSD concentrates on using contextual features, typically neighboring words, to help determine the correct sense of a target word.",
            "GPT_cite_text": "Previous research in inducing sense rankings from an untagged corpus (McCarthy et al., 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction."
        },
        {
            "Number": 7,
            "refer_ID": "P04-1036",
            "refer_sids": [
                1
            ],
            "refer_text": "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Word sense disambiguation: the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.",
            "GPT_cite_text": "McCarthy et al. (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus."
        },
        {
            "Number": 8,
            "refer_ID": "P04-1036",
            "refer_sids": [
                165
            ],
            "refer_text": "Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Lapata and Brew obtain their priors for verb classes directly from subcategorization evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbors) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.",
            "GPT_cite_text": "Research by McCarthy et al. (2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn."
        },
        {
            "Number": 9,
            "refer_ID": "P04-1036",
            "refer_sids": [
                171
            ],
            "refer_text": "In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In contrast, we use the neighbors lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.",
            "GPT_cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al., 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense."
        },
        {
            "Number": 11,
            "refer_ID": "P04-1036",
            "refer_sids": [
                89
            ],
            "refer_text": "Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.",
            "cite_ID": "P10-1155",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Since both measures gave comparable results, we restricted our remaining experiments to JCN because this gave good results for finding the predominant sense and is much more efficient than Lesk, given the precompilation of the IC files.",
            "GPT_cite_text": "McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the WordNet Similarity JCN measure (Jiang and Conrath, 1997)."
        },
        {
            "Number": 12,
            "refer_ID": "P04-1036",
            "refer_sids": [
                172
            ],
            "refer_text": "We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet; for example, those that use it for lexical acquisition or WSD.",
            "GPT_cite_text": "In doing so, we provide first results on the application of WordNet automatic sense ranking (ASR) to French parsing, using the method of McCarthy et al. (2004)."
        },
        {
            "Number": 13,
            "refer_ID": "P04-1036",
            "refer_sids": [
                189
            ],
            "refer_text": "Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Additionally, we need to determine whether senses that do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation-specific thesauruses.",
            "GPT_cite_text": "To define an appropriate categorical distribution over synsets for each lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s \u2208 Sx, following the approach of McCarthy et al. (2004)."
        },
        {
            "Number": 14,
            "refer_ID": "P04-1036",
            "refer_sids": [
                87
            ],
            "refer_text": "Again, the automatic ranking outperforms this by a large margin.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Again, the automatic ranking outperforms this by a large margin.",
            "GPT_cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al. (2004)"
        },
        {
            "Number": 16,
            "refer_ID": "P04-1036",
            "refer_sids": [
                1
            ],
            "refer_text": "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.",
            "cite_ID": "S12-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Word sense disambiguation: the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.",
            "GPT_cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al., 2004)."
        },
        {
            "Number": 17,
            "refer_ID": "P04-1036",
            "refer_sids": [
                89
            ],
            "refer_text": "Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.",
            "cite_ID": "W10-2803",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Since both measures gave comparable results, we restricted our remaining experiments to JCN because this gave good results for finding the predominant sense and is much more efficient than Lesk, given the precompilation of the IC files.",
            "GPT_cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining the predominant sense in a given domain (McCarthy et al., 2004), or to work directly with paraphrases (McCarthy and Navigli, 2009)."
        },
        {
            "Number": 18,
            "refer_ID": "P04-1036",
            "refer_sids": [
                137
            ],
            "refer_text": "Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.",
            "cite_ID": "W08-2107",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli\u00e0, 2000) which annotates WordNet synsets with domain labels.",
            "GPT_cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by WordNet, resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al. (2004))."
        },
        {
            "Number": 19,
            "refer_ID": "P04-1036",
            "refer_sids": [
                63
            ],
            "refer_text": "We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.",
            "cite_ID": "D07-1026",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We experimented using six of these to provide the results in equation 1 above and obtained results well over our baseline, but because of space limitations, we give results for the two that perform the best.",
            "GPT_cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al., 2004)."
        },
        {
            "Number": 20,
            "refer_ID": "P04-1036",
            "refer_sids": [
                159
            ],
            "refer_text": "Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.",
            "cite_ID": "W12-2429",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Magnini and Cavagli\u00e0 (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed, in section 5 we used these domain labels for evaluation.",
            "GPT_cite_text": "The first, most frequent sense (MFS) (McCarthy et al., 2004), is a widely used baseline for supervised WSD systems."
        }
    ],
    "P11-1060_swastika": [
        {
            "Number": 1,
            "refer_ID": "P11-1060",
            "refer_sids": [
                112.0
            ],
            "refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "cite_ID": "D11-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) present work on unsupervised learning."
        },
        {
            "Number": 2,
            "refer_ID": "P11-1060",
            "refer_sids": [
                112.0
            ],
            "refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "GPT_cite_text": "In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from question-answer pairs alone, which represents a significant advance."
        },
        {
            "Number": 4,
            "refer_ID": "P11-1060",
            "refer_sids": [
                25.0
            ],
            "refer_text": "We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.",
            "GPT_cite_text": "More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins."
        },
        {
            "Number": 5,
            "refer_ID": "P11-1060",
            "refer_sids": [
                21.0
            ],
            "refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "GPT_cite_text": "GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al., 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "P11-1060",
            "refer_sids": [
                25.0
            ],
            "refer_text": "We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.",
            "cite_ID": "W12-2802",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.",
            "GPT_cite_text": "Matuszek et al. [2010], Liang et al. [2011], and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world."
        },
        {
            "Number": 9,
            "refer_ID": "P11-1060",
            "refer_sids": [
                148.0
            ],
            "refer_text": "This bootstrapping behavior occurs naturally: The \u201ceasy\u201d examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.",
            "cite_ID": "D12-1069",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This bootstrapping behavior occurs naturally: The \u201ceasy\u201d examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree with scope variation.",
            "GPT_cite_text": "One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al. 2011) or even a binary correct/incorrect signal (Clarke et al. 2010)."
        },
        {
            "Number": 10,
            "refer_ID": "P11-1060",
            "refer_sids": [
                13.0
            ],
            "refer_text": "We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.",
            "cite_ID": "N12-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which are much cheaper to obtain than (x, z) pairs.",
            "GPT_cite_text": "For example, Liang et al. (2011) construct a latent parse similar in structure to a dependency grammar, but representing a logical form."
        },
        {
            "Number": 11,
            "refer_ID": "P11-1060",
            "refer_sids": [
                112.0
            ],
            "refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "cite_ID": "P12-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers."
        },
        {
            "Number": 12,
            "refer_ID": "P11-1060",
            "refer_sids": [
                21.0
            ],
            "refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "GPT_cite_text": "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model the semantics of questions by using simple dependency-like trees (Liang et al., 2011)."
        },
        {
            "Number": 13,
            "refer_ID": "P11-1060",
            "refer_sids": [
                47.0
            ],
            "refer_text": "This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This algorithm is linear in the number of nodes times the size of the denotations. Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.",
            "GPT_cite_text": "DCS trees have been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al., 2011) (Figure 1)."
        },
        {
            "Number": 14,
            "refer_ID": "P11-1060",
            "refer_sids": [
                112.0
            ],
            "refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "GPT_cite_text": "are explained in 2.5. http://nlp.stanford.edu/software/corenlp.shtml In (Liang et al., 2011) DCS trees are learned from QA pairs and database entries."
        },
        {
            "Number": 15,
            "refer_ID": "P11-1060",
            "refer_sids": [
                44.0
            ],
            "refer_text": "S sid=\"44\" ssid=\"20\">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v \u2208 w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) \u2208 S} with domain S1 = {x : (x, y) \u2208 S}.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v \u2208 w(p)), and S(x), where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) \u2208 S} with domain S1 = {x : (x, y) \u2208 S}.",
            "GPT_cite_text": "As in the sentence, \"Tropical storm Debby is blamed for death,\" which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al., 2011) of that variable."
        },
        {
            "Number": 16,
            "refer_ID": "P11-1060",
            "refer_sids": [
                106.0
            ],
            "refer_text": "Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.",
            "cite_ID": "D11-1140",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Learning: Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y | x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k\u2082\u2082, which sums over all DCS trees z that evaluate to the target answer y.",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers that are more easily available."
        },
        {
            "Number": 17,
            "refer_ID": "P11-1060",
            "refer_sids": [
                21.0
            ],
            "refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "cite_ID": "D11-1140",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "GPT_cite_text": "and Collins, 2005, 2007), -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al., 2010) systems and DCS (Liang et al., 2011)"
        },
        {
            "Number": 18,
            "refer_ID": "P11-1060",
            "refer_sids": [
                172.0
            ],
            "refer_text": "Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.",
            "cite_ID": "P13-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Free from the burden, it also allows us to easily add new lexical triggers for annotating logical forms. We hope to use our techniques in developing even more accurate and broader-coverage language understanding systems. Quantifiers and superlatives significantly complicate scoping in lambda calculus, and often type raising needs to be employed. Acknowledgments: We thank Luke Zettlemoyer.",
            "GPT_cite_text": "In general, every plural NP potentially introduces an implicit universal, ranging. For example, Liang et al. (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases explicitly devise quantifier scoping in the semantic model."
        },
        {
            "Number": 19,
            "refer_ID": "P11-1060",
            "refer_sids": [
                21.0
            ],
            "refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "cite_ID": "D11-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "GPT_cite_text": "DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011)"
        },
        {
            "Number": 20,
            "refer_ID": "P11-1060",
            "refer_sids": [
                171.0
            ],
            "refer_text": "This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.",
            "cite_ID": "P12-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This yields a system that is based on a new semantic representation, a factorized and flexible representation that is easier to search through and parametrize using features, native to lambda calculus.",
            "GPT_cite_text": "In fact, for any CFG G, see Liang et al. (2011) for work in representing lambda calculus expressions with trees."
        }
    ],
    "W99-0613_vardha": [
        {
            "Number": 1,
            "refer_ID": "W99-0613",
            "refer_sids": [
                9
            ],
            "refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "GPT_cite_text": "Co-training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web page classification (Blum and Mitchell, 1998), and named entity identification (Collins and Singer, 1999)."
        },
        {
            "Number": 2,
            "refer_ID": "W99-0613",
            "refer_sids": [
                35
            ],
            "refer_text": "AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.",
            "GPT_cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co-Booting)."
        },
        {
            "Number": 4,
            "refer_ID": "W99-0613",
            "refer_sids": [
                134
            ],
            "refer_text": "This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.",
            "cite_ID": "W03-1509",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This section describes an algorithm based on boosting techniques, which were previously developed for supervised machine learning problems.",
            "GPT_cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick et al. 1999], and so on."
        },
        {
            "Number": 6,
            "refer_ID": "W99-0613",
            "refer_sids": [
                236
            ],
            "refer_text": "We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We chose one of four labels for each example: location, person, organization, or noise, where the noise category was used for items that were outside the three categories.",
            "GPT_cite_text": "(Collins and Singer, 1999) also make use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify."
        },
        {
            "Number": 7,
            "refer_ID": "W99-0613",
            "refer_sids": [
                8
            ],
            "refer_text": "Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.",
            "cite_ID": "W06-2204",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Recent results (e.g., Yarowsky 1995; Brill 1995; Blum and Mitchell 1998) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.",
            "GPT_cite_text": "In (Collins and Singer, 1999), Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification."
        },
        {
            "Number": 8,
            "refer_ID": "W99-0613",
            "refer_sids": [
                42
            ],
            "refer_text": "(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &quot;vehicle&quot; or &quot;weapon&quot; categories).",
            "cite_ID": "W09-1116",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as \"vehicle\" or \"weapon\" categories).",
            "GPT_cite_text": "Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named entity recognition."
        },
        {
            "Number": 10,
            "refer_ID": "W99-0613",
            "refer_sids": [
                236
            ],
            "refer_text": "We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.",
            "cite_ID": "W03-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We chose one of four labels for each example: location, person, organization, or noise, where the noise category was used for items that were outside the three categories.",
            "GPT_cite_text": "Collins and Singer (1999), for example, report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)."
        },
        {
            "Number": 11,
            "refer_ID": "W99-0613",
            "refer_sids": [
                222
            ],
            "refer_text": "The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.",
            "cite_ID": "E09-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) is a common approach for unsupervised training; in this section, we describe its application to the named entity problem.",
            "GPT_cite_text": "While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al., 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999), and context-free grammar induction (numerous attempts, too many to mention)."
        },
        {
            "Number": 12,
            "refer_ID": "W99-0613",
            "refer_sids": [
                30
            ],
            "refer_text": "(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.",
            "cite_ID": "W10-3504",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of it can help classification, and suggest an objective function when training with unlabeled examples.",
            "GPT_cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labeled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)."
        },
        {
            "Number": 15,
            "refer_ID": "W99-0613",
            "refer_sids": [
                26
            ],
            "refer_text": "We present two algorithms.",
            "cite_ID": "W09-2208",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present two algorithms.",
            "GPT_cite_text": "Collins et al. (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by Blum and Mitchell (1998)."
        },
        {
            "Number": 16,
            "refer_ID": "W99-0613",
            "refer_sids": [
                7
            ],
            "refer_text": "Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Many statistical or machine learning approaches for natural language problems require a relatively large amount of supervision in the form of labeled training examples.",
            "GPT_cite_text": "This approach was shown to perform well on real-world natural language processing problems (Collins and Singer, 1999)."
        },
        {
            "Number": 17,
            "refer_ID": "W99-0613",
            "refer_sids": [
                32
            ],
            "refer_text": "The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(4 )prec (p, y)= count (p, y) count (p) (5) where prec (p, y) is the raw precision of pattern p in the set of documents labeled with category y. Criterion 2: Collins This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 1998); empirically it is shown to be quite successful in optimizing this criterion.",
            "GPT_cite_text": "(4) prec (p, y) = count (p, y) / count (p) (5) where prec (p, y) is the raw precision of pattern p in the set of documents labeled with category y. Criterion 2: Collins This criterion was used in a lightly supervised NE recognizer (Collins and Singer, 1999)."
        },
        {
            "Number": 19,
            "refer_ID": "W99-0613",
            "refer_sids": [
                47
            ],
            "refer_text": "971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "971,746 sentences of New York Times text were parsed using the parser of (Collins 96). Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).",
            "GPT_cite_text": "We use Collins and Singer (1999) for our exact specification of Yarowsky. It uses DL rule scores \\( f_j \\) + \\( f_j \\) + \\( f \\) + L (1) where \\( \\lambda \\) is a smoothing constant."
        },
        {
            "Number": 20,
            "refer_ID": "W99-0613",
            "refer_sids": [
                127
            ],
            "refer_text": "The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The DL-CoTrain algorithm can be motivated as a greedy method of satisfying the above two constraints.",
            "GPT_cite_text": "This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious."
        }
    ],
    "W06-2932_swastika": [
        {
            "Number": 2,
            "refer_ID": "W06-2932",
            "refer_sids": [
                86
            ],
            "refer_text": "Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Unlabeled accuracy for all verbs increases from 71% to 73%, and for all conjunctions from 71% to 74%.",
            "GPT_cite_text": "Introduce through post-processing, e.g., through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al., 2006)."
        },
        {
            "Number": 3,
            "refer_ID": "W06-2932",
            "refer_sids": [
                79
            ],
            "refer_text": "Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Table 5 shows the official results for submitted parser outputs.31 The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb \"ser,\" and 65% for coordinating conjunctions.",
            "GPT_cite_text": "Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al. (2006) and Nivre et al. (2006)."
        },
        {
            "Number": 4,
            "refer_ID": "W06-2932",
            "refer_sids": [
                79
            ],
            "refer_text": "Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb \"ser,\" and 65% for coordinating conjunctions.",
            "GPT_cite_text": "Even though McDonald et al. (2006) and Nivre et al. (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences."
        },
        {
            "Number": 5,
            "refer_ID": "W06-2932",
            "refer_sids": [
                79
            ],
            "refer_text": "Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.",
            "cite_ID": "W08-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The high est score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (anda different policy regarding the inclusion of punctuation) .The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb \"ser,\" and 65% for coordinating conjunctions.",
            "GPT_cite_text": "The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al. (2006) with a LAS of 87.34 based on the TIGER treebank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP), and labeled F-score (LF)."
        },
        {
            "Number": 6,
            "refer_ID": "W06-2932",
            "refer_sids": [
                22
            ],
            "refer_text": "An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.",
            "cite_ID": "W09-1210",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McDonald et al (2006) use an additional algorithm",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that non-projective dependency parsing becomes NP-hard when features are extended beyond a single edge.",
            "GPT_cite_text": "McDonald et al. (2006) use an additional algorithm."
        },
        {
            "Number": 7,
            "refer_ID": "W06-2932",
            "refer_sids": [
                54
            ],
            "refer_text": "Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.",
            "cite_ID": "W12-3407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese, and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish, and Turkish.",
            "GPT_cite_text": "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al., 2007b) and MST Parser (McDonald et al., 2006), two state-of-the-art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (McDonald and Nivre, 2007)."
        },
        {
            "Number": 8,
            "refer_ID": "W06-2932",
            "refer_sids": [
                12
            ],
            "refer_text": "In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.",
            "cite_ID": "I08-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)? s parser, (McDonald et al., 2006)? s parser, and so on",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.",
            "GPT_cite_text": "In fact, our approach can also be applied to other parsers, such as Yamada and Matsumoto's (2003) parser, McDonald et al.'s (2006) parser, and so on."
        },
        {
            "Number": 11,
            "refer_ID": "W06-2932",
            "refer_sids": [
                22
            ],
            "refer_text": "An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We have shown that, for languages with a7McDonald et al (2006) use post-processing for non projective dependencies and for labeling",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that non-projective dependency parsing becomes NP-hard when features are extended beyond a single edge.",
            "GPT_cite_text": "We have shown that, for languages, McDonald et al. (2006) use post-processing for non-projective dependencies and for labeling."
        },
        {
            "Number": 12,
            "refer_ID": "W06-2932",
            "refer_sids": [
                41
            ],
            "refer_text": "To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.",
            "cite_ID": "D07-1122",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To model this, we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem. We use a first-order Markov factorization of the score s(l(i,jM), l(i,jM\u22121), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jM) and (i, jM\u22121) in the tree y.",
            "GPT_cite_text": "As described in (McDonald et al., 2006), we treat the labeling of dependencies as a sequence labeling problem."
        },
        {
            "Number": 14,
            "refer_ID": "W06-2932",
            "refer_sids": [
                24
            ],
            "refer_text": "Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.",
            "cite_ID": "D07-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface-level features relative to these decisions.",
            "GPT_cite_text": "It should be noted that McDonald et al. (2006) use a richer feature set that is incomparable to our features."
        },
        {
            "Number": 18,
            "refer_ID": "W06-2932",
            "refer_sids": [
                79
            ],
            "refer_text": "Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.",
            "cite_ID": "D10-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Entries marked with? are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb \"ser,\" and 65% for coordinating conjunctions.",
            "GPT_cite_text": "Entries marked with * are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al. (2006), Martins et al. (2008), Martins et al. (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)."
        },
        {
            "Number": 19,
            "refer_ID": "W06-2932",
            "refer_sids": [
                12
            ],
            "refer_text": "In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.",
            "cite_ID": "P08-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.",
            "GPT_cite_text": "The specific graph-based model studied in this work is that presented by McDonald et al. (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near-exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation."
        },
        {
            "Number": 20,
            "refer_ID": "W06-2932",
            "refer_sids": [
                43
            ],
            "refer_text": "For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi\u2019s algorithm.",
            "cite_ID": "P08-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l)? Rk, where f is typically a bi nary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For score functions, we use simple dot products between high-dimensional feature representations and a weight vector. Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi\u2019s algorithm.",
            "GPT_cite_text": "More precisely, dependency arcs (or pairs of arcs) are first represented by a high-dimensional feature vector f(i, j, l) \u2208 R^k, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al., 2005a; McDonald et al., 2006)."
        }
    ],
    "A00-2018_sweta": [
        {
            "Number": 2,
            "refer_ID": "A00-2018",
            "refer_sids": [
                17
            ],
            "refer_text": "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.",
            "cite_ID": "N10-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advance by extracting a treebank grammar [3] from the training corpus.",
            "GPT_cite_text": "As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)."
        },
        {
            "Number": 3,
            "refer_ID": "A00-2018",
            "refer_sids": [
                5
            ],
            "refer_text": "We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.",
            "cite_ID": "W11-0610",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We present a new parser for parsing down to Penn Treebank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal Treebank.",
            "GPT_cite_text": "Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switchboard treebank."
        },
        {
            "Number": 4,
            "refer_ID": "A00-2018",
            "refer_sids": [
                17
            ],
            "refer_text": "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.",
            "cite_ID": "W06-3119",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advance by extracting a treebank grammar [3] from the training corpus.",
            "GPT_cite_text": "We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus."
        },
        {
            "Number": 5,
            "refer_ID": "A00-2018",
            "refer_sids": [
                120
            ],
            "refer_text": "Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.",
            "cite_ID": "N03-2024",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text; the output was taken as \"correct,\" and statistics were collected on the resulting parses.",
            "GPT_cite_text": "We were interested in the occurrence of features such as type and number of premodifiers, presence and type of postmodifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al., 1997)."
        },
        {
            "Number": 6,
            "refer_ID": "A00-2018",
            "refer_sids": [
                119
            ],
            "refer_text": "(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)",
            "cite_ID": "N06-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "(It is \"soft\" clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)",
            "GPT_cite_text": "After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article."
        },
        {
            "Number": 7,
            "refer_ID": "A00-2018",
            "refer_sids": [
                95
            ],
            "refer_text": "We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.",
            "cite_ID": "C04-1180",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given preterminal is realized using a previously unobserved word.",
            "GPT_cite_text": "The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al., 2003), sentence simplification (Carroll et al., 1999), and a linguist's search engine (Resnik and Elkiss, 2003)."
        },
        {
            "Number": 8,
            "refer_ID": "A00-2018",
            "refer_sids": [
                175
            ],
            "refer_text": "This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].",
            "cite_ID": "W05-0638",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This corresponds to an error reduction of 13% over the best previously published single-parser results on this test set, those of Collins [9].",
            "GPT_cite_text": "In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)."
        },
        {
            "Number": 9,
            "refer_ID": "A00-2018",
            "refer_sids": [
                92
            ],
            "refer_text": "For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.",
            "cite_ID": "P05-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics but is conditioned only on standard PCFG information.",
            "GPT_cite_text": "We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis."
        },
        {
            "Number": 10,
            "refer_ID": "A00-2018",
            "refer_sids": [
                1
            ],
            "refer_text": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.",
            "cite_ID": "P05-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We present a new parser for parsing down to Penn Treebank style parse trees that achieves 90.1% average precision/recall for sentences of 40 words or less, and for sentences of length 100 words or less when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal treebank.",
            "GPT_cite_text": "For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus."
        },
        {
            "Number": 11,
            "refer_ID": "A00-2018",
            "refer_sids": [
                126
            ],
            "refer_text": "This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This is indicated in Figure 2, where the model labeled \"Best\" has a precision of 89.8% and a recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.",
            "GPT_cite_text": "The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90% unlabelled and 84% labelled accuracy with respect to dependencies when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows."
        },
        {
            "Number": 12,
            "refer_ID": "A00-2018",
            "refer_sids": [
                12
            ],
            "refer_text": "The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &quot;tag&quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for \"tag\"), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).",
            "GPT_cite_text": "Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees."
        },
        {
            "Number": 13,
            "refer_ID": "A00-2018",
            "refer_sids": [
                174
            ],
            "refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn Treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "GPT_cite_text": "As an alternative to hard-coded heuristics, Blaheta and Charniak (2000) proposed recovering the Penn functional tags automatically."
        },
        {
            "Number": 17,
            "refer_ID": "A00-2018",
            "refer_sids": [
                63
            ],
            "refer_text": "As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &quot;before&quot;), and the label of the grandparent of c (la).",
            "cite_ID": "N06-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal, and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for \"before\"), and the label of the grandparent of c (la).",
            "GPT_cite_text": "The parser of Charniak (2000) is also a two-stage CTF model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents."
        },
        {
            "Number": 18,
            "refer_ID": "A00-2018",
            "refer_sids": [
                78
            ],
            "refer_text": "With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.",
            "cite_ID": "N06-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.",
            "GPT_cite_text": "Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))."
        },
        {
            "Number": 19,
            "refer_ID": "A00-2018",
            "refer_sids": [
                91
            ],
            "refer_text": "As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.",
            "cite_ID": "H05-1035",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate parses to be evaluated in the second pass by our probabilistic model.",
            "GPT_cite_text": "The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions."
        },
        {
            "Number": 20,
            "refer_ID": "A00-2018",
            "refer_sids": [
                180
            ],
            "refer_text": "From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.",
            "cite_ID": "P04-1042",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediate result improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.",
            "GPT_cite_text": "Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size."
        }
    ],
    "E03-1005_sweta": [
        {
            "Number": 2,
            "refer_ID": "E03-1005",
            "refer_sids": [
                20
            ],
            "refer_text": "Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).",
            "cite_ID": "N06-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Data-Oriented Parsing (DOP)? s methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest ranking parse, not derivation, that is desired",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Thus, the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones. Both have gained or are gaining wide usage and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).",
            "GPT_cite_text": "Data-Oriented Parsing (DOP) methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest-ranking parse, not derivation, that is desired."
        },
        {
            "Number": 3,
            "refer_ID": "E03-1005",
            "refer_sids": [
                74
            ],
            "refer_text": "Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.",
            "cite_ID": "D07-1058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goodman? s transform, in com bi nation with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probabilities.",
            "GPT_cite_text": "Goodman's transform, in combination with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model."
        },
        {
            "Number": 4,
            "refer_ID": "E03-1005",
            "refer_sids": [
                44
            ],
            "refer_text": "But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).",
            "cite_ID": "D07-1058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Zuidema (2006a) shows that also the estimator (Bod, 2003) uses is biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996).",
            "GPT_cite_text": "Zuidema (2006a) shows that the estimator (Bod, 2003) is also biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees."
        },
        {
            "Number": 5,
            "refer_ID": "E03-1005",
            "refer_sids": [
                143
            ],
            "refer_text": "While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.",
            "cite_ID": "P11-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the DOP approach, which is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.",
            "GPT_cite_text": "Second, we compare against a composed-rule system, which is analogous to the Data-Oriented Parsing (DOP) approach in parsing (Bod, 2003)."
        },
        {
            "Number": 6,
            "refer_ID": "E03-1005",
            "refer_sids": [
                145
            ],
            "refer_text": "This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.",
            "cite_ID": "P04-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our best performing model is more accurate than all these previous models except (Bod, 2003)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This paper showed that a PCFG reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal Treebank.",
            "GPT_cite_text": "Our best-performing model is more accurate than all these previous models except (Bod, 2003)."
        },
        {
            "Number": 7,
            "refer_ID": "E03-1005",
            "refer_sids": [
                134
            ],
            "refer_text": "This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.",
            "cite_ID": "P04-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current ac curacy level, and only 0.6% below the current best (Bod, 2003) .This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This may be explained by the fact that our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was achieved, while in the current experiment we used all subtrees as given by the PCFG reduction.",
            "GPT_cite_text": "We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003). This paper has also proposed a neural network training method which optimizes a discriminative criterion even when the parameters being estimated are those of a generative probability model."
        },
        {
            "Number": 10,
            "refer_ID": "E03-1005",
            "refer_sids": [
                22
            ],
            "refer_text": "DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).",
            "cite_ID": "E06-2025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similarly, (Bod, 2003) changes the way frequenciesfi are counted, with a similar effect",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "DOP1 combines subtrees from a treebank by means of node substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).",
            "GPT_cite_text": "Similarly, (Bod, 2003) changes the way frequencies are counted, with a similar effect."
        },
        {
            "Number": 12,
            "refer_ID": "E03-1005",
            "refer_sids": [
                133
            ],
            "refer_text": "It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).",
            "cite_ID": "W06-2905",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(henceforth, STSGs), which can represent single words, contiguous and noncontiguous MWEs, context-free rules or complete parse trees in a unified representation. My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences of 100 words).",
            "GPT_cite_text": "(henceforth, STSGs), which can represent single words, contiguous and non-contiguous MWEs, context-free rules, or complete parse trees in a unified representation. My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003)."
        },
        {
            "Number": 13,
            "refer_ID": "E03-1005",
            "refer_sids": [
                25
            ],
            "refer_text": "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).",
            "cite_ID": "W06-2905",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Shown are results were only elementary trees with scores higher than 0.3 and 0.1 respectively are used. However, more interesting is a qualitative analysis of the STSG induced, which shows that ,un like DOP1, push-n-pull arrives at a grammar that gives high weights (and scores) to those elementary3We approximated the most probable parse as follows (fol lowing (Bod, 2003))",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).",
            "GPT_cite_text": "Shown are results where only elementary trees with scores higher than 0.3 and 0.1 respectively are used. However, more interesting is a qualitative analysis of the STSG induced, which shows that, unlike DOP1, push-n-pull arrives at a grammar that gives high weights (and scores) to those elementary trees. We approximated the most probable parse as follows (following (Bod, 2003))."
        },
        {
            "Number": 14,
            "refer_ID": "E03-1005",
            "refer_sids": [
                38
            ],
            "refer_text": "Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This result is only slightly higher than the highest reported result for this test-set, Bod? s (.907) (Bod,2003)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod, 2003).",
            "GPT_cite_text": "This result is only slightly higher than the highest reported result for this test set, Bod's (.907) (Bod, 2003)."
        },
        {
            "Number": 16,
            "refer_ID": "E03-1005",
            "refer_sids": [
                100
            ],
            "refer_text": "In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.",
            "cite_ID": "P07-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.",
            "GPT_cite_text": "This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well."
        },
        {
            "Number": 17,
            "refer_ID": "E03-1005",
            "refer_sids": [
                32
            ],
            "refer_text": "However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.",
            "cite_ID": "P07-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees from which they are derived.",
            "GPT_cite_text": "But equally important is the fact that this new DOP model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)."
        },
        {
            "Number": 18,
            "refer_ID": "E03-1005",
            "refer_sids": [
                130
            ],
            "refer_text": "While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).",
            "cite_ID": "P07-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Of course, it is well-known that a supervised parser? s f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% f score if tested on the Brown corpus",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al.'s estimator performs worse and is comparable to Collins (1996).",
            "GPT_cite_text": "Of course, it is well-known that a supervised parser's F-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% F-score if tested on the Brown corpus."
        },
        {
            "Number": 19,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "cite_ID": "W04-0305",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 and 14: an LP of 90.8% and an LR of 90.7%.",
            "GPT_cite_text": "A moderately larger vocabulary version (4,215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003)."
        },
        {
            "Number": 20,
            "refer_ID": "E03-1005",
            "refer_sids": [
                27
            ],
            "refer_text": "Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.",
            "cite_ID": "P06-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The probability of a parse tree T is the sum of the 1 This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Goodman (1996, 1998) developed a polynomial-time PCFG reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.",
            "GPT_cite_text": "The probability of a parse tree T is the sum of the subtree probabilities, which are adjusted by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)."
        }
    ],
    "P08-1028_aakansha": [
        {
            "Number": 1,
            "refer_ID": "P08-1028",
            "refer_sids": [
                51
            ],
            "refer_text": "Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).",
            "cite_ID": "D08-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Our work proposes a framework for vector composition that allows the derivation of different types of models and includes two fundamental composition operations: multiplication and addition (and their combination).",
            "GPT_cite_text": "Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p + a as a function f operating on four components: c = f(p, a, R, K) (3) R is the relation holding between p and a, and K is additional knowledge."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1028",
            "refer_sids": [
                189,
                190
            ],
            "refer_text": "In this paper we presented a general framework for vector-based semantic composition.\nWe formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "P10-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "In this paper, we presented a general framework for vector-based semantic composition. We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "Mitchell and Lapata (2008), henceforth M&L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression."
        },
        {
            "Number": 7,
            "refer_ID": "P08-1028",
            "refer_sids": [
                185,
                186
            ],
            "refer_text": "The multiplicative model yields a better fit with the experimental data, \u03c1 = 0.17.\nThe combined model is best overall with \u03c1 = 0.19.\nHowever, the difference between the two models is not statistically significant.",
            "cite_ID": "P10-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The multiplicative model yields a better fit with the experimental data, \u03c1 = 0.17. The combined model is best overall with \u03c1 = 0.19. However, the difference between the two models is not statistically significant.",
            "GPT_cite_text": "Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting."
        },
        {
            "Number": 8,
            "refer_ID": "P08-1028",
            "refer_sids": [
                51
            ],
            "refer_text": "Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).",
            "cite_ID": "D11-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Our work proposes a framework for vector composition that allows the derivation of different types of models and includes two fundamental composition operations: multiplication and addition (and their combination).",
            "GPT_cite_text": "And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors."
        },
        {
            "Number": 9,
            "refer_ID": "P08-1028",
            "refer_sids": [
                189,
                190
            ],
            "refer_text": "In this paper we presented a general framework for vector-based semantic composition.\nWe formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "W11-0131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "In this paper, we presented a general framework for vector-based semantic composition. We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p = f(u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is the syntactic context, K is a semantic knowledge base, and p is the resulting composed vector (or tensor)."
        },
        {
            "Number": 10,
            "refer_ID": "P08-1028",
            "refer_sids": [
                48
            ],
            "refer_text": "The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.",
            "cite_ID": "W11-0131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.",
            "GPT_cite_text": "As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1028",
            "refer_sids": [
                53,
                57
            ],
            "refer_text": "We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word\u2019s vector typically represents its co-occurrence with neighboring words.Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.",
            "cite_ID": "P13-2083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrizations that have been suggested in the literature. We briefly note here that a word\u2019s vector typically represents its co-occurrence with neighboring words. Let p denote the composition of two vectors u and v, representing a pair of constituents that stand in some syntactic relation R. Let K stand for any additional knowledge or information that is needed to construct the semantics of their composition.",
            "GPT_cite_text": "Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1028",
            "refer_sids": [
                68,
                69,
                70
            ],
            "refer_text": "Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.\nSimply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.\nInstead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.",
            "cite_ID": "P13-2083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing. Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other. Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.",
            "GPT_cite_text": "As our final set of baselines, we extend two simple techniques proposed by Mitchell and Lapata (2008) that use element-wise addition and multiplication operators to perform composition."
        },
        {
            "Number": 13,
            "refer_ID": "P08-1028",
            "refer_sids": [
                189,
                190
            ],
            "refer_text": "In this paper we presented a general framework for vector-based semantic composition.\nWe formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "P10-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "In this paper, we presented a general framework for vector-based semantic composition. We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work, we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)."
        },
        {
            "Number": 14,
            "refer_ID": "P08-1028",
            "refer_sids": [
                189,
                190
            ],
            "refer_text": "In this paper we presented a general framework for vector-based semantic composition.\nWe formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "P10-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "In this paper, we presented a general framework for vector-based semantic composition. We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "Assuming that h is a linear function of the Cartesian product of u and v allows us to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi = ui * vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well-known tensor products (Smolensky 1990) and circular convolution (Plate 1995)."
        },
        {
            "Number": 15,
            "refer_ID": "P08-1028",
            "refer_sids": [
                24,
                25
            ],
            "refer_text": "In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.\nWe present a general framework for vector-based composition which allows us to consider different classes of models.",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "In this paper, we examine models of semantic composition that are empirically grounded and can represent similarity relations. We present a general framework for vector-based composition that allows us to consider different classes of models.",
            "GPT_cite_text": "Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and pointwise multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)."
        },
        {
            "Number": 16,
            "refer_ID": "P08-1028",
            "refer_sids": [
                64
            ],
            "refer_text": "Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the Cartesian product of u and v, then we generate a class of additive models: where A and B are matrices that determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.",
            "GPT_cite_text": "The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression."
        },
        {
            "Number": 17,
            "refer_ID": "P08-1028",
            "refer_sids": [
                176,
                177
            ],
            "refer_text": "The multiplicative and combined models yield means closer to the human ratings.\nThe difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "The multiplicative and combined models yield means closer to the human ratings. The difference between high and low similarity values estimated by these models is statistically significant (p < 0.01 using the Wilcoxon rank sum test).",
            "GPT_cite_text": "For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset."
        },
        {
            "Number": 19,
            "refer_ID": "P08-1028",
            "refer_sids": [
                191
            ],
            "refer_text": "Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.",
            "cite_ID": "W11-1310",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.",
            "GPT_cite_text": "Mitchell and Lapata (2008) observed that a simple multiplication function modeled compositionality better than addition."
        },
        {
            "Number": 20,
            "refer_ID": "P08-1028",
            "refer_sids": [
                24
            ],
            "refer_text": "In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.",
            "cite_ID": "W11-1310",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "In this paper, we examine models of semantic composition that are empirically grounded and can represent similarity relations.",
            "GPT_cite_text": "We use the compositionality functions, simple addition and simple multiplication, to build compositional vectors Vwr1 + wr2 and Vwr1 ? wr2. These are as described in (Mitchell and Lapata, 2008)."
        }
    ],
    "A97-1014_swastika": [
        {
            "Number": 1,
            "refer_ID": "A97-1014",
            "refer_sids": [
                168
            ],
            "refer_text": "We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.",
            "cite_ID": "E99-1016",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al, 1997)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We will closely coordinate the further development of our corpus with the annotation work in VerbMobil and with other German efforts in corpus annotation.",
            "GPT_cite_text": "This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al., 1997)."
        },
        {
            "Number": 2,
            "refer_ID": "A97-1014",
            "refer_sids": [
                168
            ],
            "refer_text": "We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.",
            "cite_ID": "E99-1016",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For our experiments, we use the NEGRA corpus (Skut et al, 1997)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We will closely coordinate the further development of our corpus with the annotation work in VerbMobil and with other German efforts in corpus annotation.",
            "GPT_cite_text": "For our experiments, we use the NEGRA corpus (Skut et al., 1997)."
        },
        {
            "Number": 3,
            "refer_ID": "A97-1014",
            "refer_sids": [
                151
            ],
            "refer_text": "For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).",
            "cite_ID": "E12-1047",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As data we use version 2 of the Negra (Skut et al1997) tree bank, with the common training ,devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 Frequenc y Parsing complexity head-driven optimal head-driven Figure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For evaluation, the already annotated sentences were divided into two disjoint sets: one for training (90% of the corpus) and the other for testing (10%).",
            "GPT_cite_text": "As data, we use version 2 of the Negra (Skut et al. 1997) treebank, with the common training, devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 Frequency Parsing complexity head-driven optimal head-driven Figure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25."
        },
        {
            "Number": 5,
            "refer_ID": "A97-1014",
            "refer_sids": [
                15
            ],
            "refer_text": "Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.",
            "cite_ID": "I05-6010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "According to Skut et al (1997) tree banks have to meet the following requirements: 1",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptiveness: Grammatical phenomena are to be described rather than explained.",
            "GPT_cite_text": "According to Skut et al. (1997), treebanks have to meet the following requirements: 1"
        },
        {
            "Number": 7,
            "refer_ID": "A97-1014",
            "refer_sids": [
                47
            ],
            "refer_text": "Argument structure can be represented in terms of unordered trees (with crossing branches).",
            "cite_ID": "C10-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In contrast, some other tree banks, such as the German NeGra and TIGER tree banks allow annotation with crossing branches (Skut et al, 1997) .Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Argument structure can be represented in terms of unordered trees (with crossing branches).",
            "GPT_cite_text": "In contrast, some other treebanks, such as the German NeGra and TIGER treebanks, allow annotation with crossing branches (Skut et al., 1997). Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node."
        },
        {
            "Number": 8,
            "refer_ID": "A97-1014",
            "refer_sids": [
                167
            ],
            "refer_text": "In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "cite_ID": "C10-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our data source is the German NeGra tree bank (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the second phase of the project Verbmobil, a treebank for 30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "GPT_cite_text": "Our data source is the German NeGra treebank (Skut et al., 1997)."
        },
        {
            "Number": 9,
            "refer_ID": "A97-1014",
            "refer_sids": [
                168
            ],
            "refer_text": "We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.",
            "cite_ID": "P05-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The parsing models we present are trained and tested on the NEGRA corpus (Skut et al, 1997), a hand parsed corpus of German newspaper text containing approximately 20,000 sentences",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We will closely coordinate the further development of our corpus with the annotation work in VerbMobil and with other German efforts in corpus annotation.",
            "GPT_cite_text": "The parsing models we present are trained and tested on the NEGRA corpus (Skut et al., 1997), a hand-parsed corpus of German newspaper text containing approximately 20,000 sentences."
        },
        {
            "Number": 10,
            "refer_ID": "A97-1014",
            "refer_sids": [
                4
            ],
            "refer_text": "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.",
            "cite_ID": "P03-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al, 1997), a syntactically annotated corpus for German",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The work reported in this paper aims at providing syntactically annotated corpora (treebanks) for stochastic grammar induction.",
            "GPT_cite_text": "The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German."
        },
        {
            "Number": 11,
            "refer_ID": "A97-1014",
            "refer_sids": [
                160
            ],
            "refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "cite_ID": "P03-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The annotation scheme (Skut et al, 1997) is modeled to a certain extent on that of the Penn Treebank (Marcuset al, 1993), with crucial differences",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "GPT_cite_text": "The annotation scheme (Skut et al., 1997) is modeled to a certain extent on that of the Penn Treebank (Marcus et al., 1993), with crucial differences."
        },
        {
            "Number": 13,
            "refer_ID": "A97-1014",
            "refer_sids": [
                127
            ],
            "refer_text": "As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.",
            "cite_ID": "W04-1505",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "German is con sider ably more in ectional which means that discarding functional information is more harmful, and which explains why the NEGRA an notation has been conceived to be quite at (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.",
            "GPT_cite_text": "German is considerably more inflectional, which means that discarding functional information is more harmful, and which explains why the NEGRA annotation has been conceived to be quite so (Skut et al, 1997)."
        },
        {
            "Number": 14,
            "refer_ID": "A97-1014",
            "refer_sids": [
                39
            ],
            "refer_text": "Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.",
            "cite_ID": "C04-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The factors used in the algorithms and the algorithms themselves are evaluated on a Germancorpus annotated with syntactic and co reference in formation (Negra) (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Consider the German sentence (1) daran wird ihn Anna erkennen, da er weint 'Anna will recognize him at his cry.' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.",
            "GPT_cite_text": "The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information (Negra) (Skut et al, 1997)."
        },
        {
            "Number": 16,
            "refer_ID": "A97-1014",
            "refer_sids": [
                72
            ],
            "refer_text": "In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.",
            "cite_ID": "P11-2067",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and final annotation.",
            "GPT_cite_text": "CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al., 1997)."
        },
        {
            "Number": 17,
            "refer_ID": "A97-1014",
            "refer_sids": [
                4
            ],
            "refer_text": "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.",
            "cite_ID": "W08-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negratreebank (Skut et al, 1997) reports that lexicaliza tion of PCFGs decrease the parsing accuracy when parsing Negra? s flat constituent structures",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The work reported in this paper aims at providing syntactically annotated corpora (treebanks) for stochastic grammar induction.",
            "GPT_cite_text": "Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al., 1997) report that lexicalization of PCFGs decreases the parsing accuracy when parsing Negra's flat constituent structures."
        },
        {
            "Number": 19,
            "refer_ID": "A97-1014",
            "refer_sids": [
                4
            ],
            "refer_text": "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.",
            "cite_ID": "D07-1066",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A comparison of unlexicalised PCFG parsing (Ku ?bler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the Tu? Ba D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The work reported in this paper aims at providing syntactically annotated corpora (treebanks) for stochastic grammar induction.",
            "GPT_cite_text": "A comparison of unlexicalised PCFG parsing (K\u00fcbler, 2005) trained and evaluated on the German NEGRA (Skut et al., 1997) and the TuBa-D/Z (Telljohann et al., 2004) treebanks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al., 1991)."
        }
    ],
    "E03-1005_aakansha": [
        {
            "Number": 2,
            "refer_ID": "E03-1005",
            "refer_sids": [
                105
            ],
            "refer_text": "The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.",
            "cite_ID": "N06-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Data-Oriented Parsing (DOP)? s methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest ranking parse, not derivation, that is desired",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP. Although Bod (2000b) reports that Simplicity-DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.",
            "GPT_cite_text": "Data-Oriented Parsing (DOP) methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest-ranking parse, not derivation, that is desired."
        },
        {
            "Number": 3,
            "refer_ID": "E03-1005",
            "refer_sids": [
                145
            ],
            "refer_text": "This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.",
            "cite_ID": "D07-1058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goodman? s transform, in com bi nation with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper showed that a PCFG reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal Treebank.",
            "GPT_cite_text": "Goodman's transform, in combination with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model."
        },
        {
            "Number": 4,
            "refer_ID": "E03-1005",
            "refer_sids": [
                80
            ],
            "refer_text": "DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).",
            "cite_ID": "D07-1058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Zuidema (2006a) shows that also the estimator (Bod, 2003) uses is biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al., 1999).",
            "GPT_cite_text": "Zuidema (2006a) shows that the estimator (Bod, 2003) is also biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees."
        },
        {
            "Number": 5,
            "refer_ID": "E03-1005",
            "refer_sids": [
                143
            ],
            "refer_text": "While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.",
            "cite_ID": "P11-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the DOP approach, which is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.",
            "GPT_cite_text": "Second, we compare against a composed-rule system, which is analogous to the Data-Oriented Parsing (DOP) approach in parsing (Bod, 2003)."
        },
        {
            "Number": 6,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140,
                141
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.",
            "cite_ID": "P04-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our best performing model is more accurate than all these previous models except (Bod, 2003)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod's PCFG-reduction reported in Table 1.",
            "GPT_cite_text": "Our best-performing model is more accurate than all these previous models except (Bod, 2003)."
        },
        {
            "Number": 7,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "cite_ID": "P04-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current ac curacy level, and only 0.6% below the current best (Bod, 2003) .This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 and 14: an LP of 90.8% and an LR of 90.7%.",
            "GPT_cite_text": "We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003). This paper has also proposed a neural network training method which optimizes a discriminative criterion even when the parameters being estimated are those of a generative probability model."
        },
        {
            "Number": 10,
            "refer_ID": "E03-1005",
            "refer_sids": [
                102,
                103
            ],
            "refer_text": "In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.\nThat is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.",
            "cite_ID": "E06-2025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similarly, (Bod, 2003) changes the way frequenciesfi are counted, with a similar effect",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees. That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.",
            "GPT_cite_text": "Similarly, (Bod, 2003) changes the way frequencies are counted, with a similar effect."
        },
        {
            "Number": 12,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "cite_ID": "W06-2905",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(henceforth, STSGs), which can represent single words, contiguous and noncontiguous MWEs, context-free rules or complete parse trees in a unified representation. My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 and 14: an LP of 90.8% and an LR of 90.7%.",
            "GPT_cite_text": "(henceforth, STSGs), which can represent single words, contiguous and non-contiguous MWEs, context-free rules, or complete parse trees in a unified representation. My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003)."
        },
        {
            "Number": 14,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This result is only slightly higher than the highest reported result for this test-set, Bod? s (.907) (Bod,2003)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 and 14: an LP of 90.8% and an LR of 90.7%.",
            "GPT_cite_text": "This result is only slightly higher than the highest reported result for this test set, Bod's (.907) (Bod, 2003)."
        },
        {
            "Number": 16,
            "refer_ID": "E03-1005",
            "refer_sids": [
                46
            ],
            "refer_text": "Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.",
            "cite_ID": "P07-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Most previous notions of the best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.",
            "GPT_cite_text": "This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well."
        },
        {
            "Number": 17,
            "refer_ID": "E03-1005",
            "refer_sids": [
                85,
                86
            ],
            "refer_text": "For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).\nBod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).",
            "cite_ID": "P07-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12). Bod reports state-of-the-art results with this method and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).",
            "GPT_cite_text": "But equally important is the fact that this new DOP model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)."
        },
        {
            "Number": 19,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "cite_ID": "W04-0305",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 and 14: an LP of 90.8% and an LR of 90.7%.",
            "GPT_cite_text": "A moderately larger vocabulary version (4,215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003)."
        },
        {
            "Number": 20,
            "refer_ID": "E03-1005",
            "refer_sids": [
                115
            ],
            "refer_text": "The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.",
            "cite_ID": "P06-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The probability of a parse tree T is the sum of the 1 This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.",
            "GPT_cite_text": "The probability of a parse tree T is the sum of the subtree probabilities, which are adjusted by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)."
        }
    ],
    "A97-1014_sweta": [
        {
            "Number": 1,
            "refer_ID": "A97-1014",
            "refer_sids": [
                168
            ],
            "refer_text": "We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.",
            "cite_ID": "E99-1016",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al, 1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We will closely coordinate the further development of our corpus with the annotation work in VerbMobil and with other German efforts in corpus annotation.",
            "GPT_cite_text": "This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al., 1997)."
        },
        {
            "Number": 2,
            "refer_ID": "A97-1014",
            "refer_sids": [
                165
            ],
            "refer_text": "In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.",
            "cite_ID": "E99-1016",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For our experiments, we use the NEGRA corpus (Skut et al, 1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In addition, the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linearizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.",
            "GPT_cite_text": "For our experiments, we use the NEGRA corpus (Skut et al., 1997)."
        },
        {
            "Number": 3,
            "refer_ID": "A97-1014",
            "refer_sids": [
                145
            ],
            "refer_text": "This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).",
            "cite_ID": "E12-1047",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As data we use version 2 of the Negra (Skut et al1997) tree bank, with the common training ,devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 Frequenc y Parsing complexity head-driven optimal head-driven Figure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).",
            "GPT_cite_text": "As data, we use version 2 of the Negra (Skut et al. 1997) treebank, with the common training, devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 Frequency Parsing complexity head-driven optimal head-driven Figure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25."
        },
        {
            "Number": 5,
            "refer_ID": "A97-1014",
            "refer_sids": [
                15
            ],
            "refer_text": "Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.",
            "cite_ID": "I05-6010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "According to Skut et al (1997) tree banks have to meet the following requirements: 1",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptiveness: Grammatical phenomena are to be described rather than explained.",
            "GPT_cite_text": "According to Skut et al. (1997), treebanks have to meet the following requirements: 1"
        },
        {
            "Number": 6,
            "refer_ID": "A97-1014",
            "refer_sids": [
                41
            ],
            "refer_text": "Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.",
            "cite_ID": "W04-1506",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "that could best deal with the free word order displayed by Basque syntax (Skut et al, 1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.",
            "GPT_cite_text": "That could best deal with the free word order displayed by Basque syntax (Skut et al., 1997)."
        },
        {
            "Number": 7,
            "refer_ID": "A97-1014",
            "refer_sids": [
                47
            ],
            "refer_text": "Argument structure can be represented in terms of unordered trees (with crossing branches).",
            "cite_ID": "C10-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In contrast, some other tree banks, such as the German NeGra and TIGER tree banks allow annotation with crossing branches (Skut et al, 1997) .Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Argument structure can be represented in terms of unordered trees (with crossing branches).",
            "GPT_cite_text": "In contrast, some other treebanks, such as the German NeGra and TIGER treebanks, allow annotation with crossing branches (Skut et al., 1997). Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node."
        },
        {
            "Number": 8,
            "refer_ID": "A97-1014",
            "refer_sids": [
                167
            ],
            "refer_text": "In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "cite_ID": "C10-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our data source is the German NeGra tree bank (Skut et al, 1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In the second phase of the project Verbmobil, a treebank for 30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "GPT_cite_text": "Our data source is the German NeGra treebank (Skut et al., 1997)."
        },
        {
            "Number": 9,
            "refer_ID": "A97-1014",
            "refer_sids": [
                167
            ],
            "refer_text": "In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "cite_ID": "P05-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The parsing models we present are trained and tested on the NEGRA corpus (Skut et al, 1997), a hand parsed corpus of German newspaper text containing approximately 20,000 sentences",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In the second phase of the project Verbmobil, a treebank for 30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "GPT_cite_text": "The parsing models we present are trained and tested on the NEGRA corpus (Skut et al., 1997), a hand-parsed corpus of German newspaper text containing approximately 20,000 sentences."
        },
        {
            "Number": 10,
            "refer_ID": "A97-1014",
            "refer_sids": [
                39
            ],
            "refer_text": "Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.",
            "cite_ID": "P03-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al, 1997), a syntactically annotated corpus for German",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Consider the German sentence (1) daran wird ihn Anna erkennen, da er weint 'Anna will recognize him at his cry.' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.",
            "GPT_cite_text": "The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German."
        },
        {
            "Number": 11,
            "refer_ID": "A97-1014",
            "refer_sids": [
                160
            ],
            "refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "cite_ID": "P03-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The annotation scheme (Skut et al, 1997) is modeled to a certain extent on that of the Penn Treebank (Marcuset al, 1993), with crucial differences",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "GPT_cite_text": "The annotation scheme (Skut et al., 1997) is modeled to a certain extent on that of the Penn Treebank (Marcus et al., 1993), with crucial differences."
        },
        {
            "Number": 13,
            "refer_ID": "A97-1014",
            "refer_sids": [
                166
            ],
            "refer_text": "Syntactically annotated corpora of German have been missing until now.",
            "cite_ID": "W04-1505",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "German is con sider ably more in ectional which means that discarding functional information is more harmful, and which explains why the NEGRA an notation has been conceived to be quite at (Skut et al, 1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Syntactically annotated corpora of German have been missing until now.",
            "GPT_cite_text": "German is considerably more inflectional, which means that discarding functional information is more harmful, and which explains why the NEGRA annotation has been conceived to be quite so (Skut et al, 1997)."
        },
        {
            "Number": 14,
            "refer_ID": "A97-1014",
            "refer_sids": [
                166
            ],
            "refer_text": "Syntactically annotated corpora of German have been missing until now.",
            "cite_ID": "C04-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The factors used in the algorithms and the algorithms themselves are evaluated on a Germancorpus annotated with syntactic and co reference in formation (Negra) (Skut et al, 1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Syntactically annotated corpora of German have been missing until now.",
            "GPT_cite_text": "The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information (Negra) (Skut et al, 1997)."
        },
        {
            "Number": 16,
            "refer_ID": "A97-1014",
            "refer_sids": [
                143
            ],
            "refer_text": "Sentences annotated in previous steps are used as training material for further processing.",
            "cite_ID": "P11-2067",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al, 1997)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Sentences annotated in previous steps are used as training materials for further processing.",
            "GPT_cite_text": "CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al., 1997)."
        },
        {
            "Number": 17,
            "refer_ID": "A97-1014",
            "refer_sids": [
                71
            ],
            "refer_text": "However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.",
            "cite_ID": "W08-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negratreebank (Skut et al, 1997) reports that lexicaliza tion of PCFGs decrease the parsing accuracy when parsing Negra? s flat constituent structures",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.",
            "GPT_cite_text": "Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al., 1997) report that lexicalization of PCFGs decreases the parsing accuracy when parsing Negra's flat constituent structures."
        },
        {
            "Number": 18,
            "refer_ID": "A97-1014",
            "refer_sids": [
                167
            ],
            "refer_text": "In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "cite_ID": "P06-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al 1997) and the Chinese CTB10 (Xue et al 2002) both containing 2200+ sentences? 10 words after removing punctuation",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In the second phase of the project Verbmobil, a treebank for 30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "GPT_cite_text": "We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al. 1997) and the Chinese CTB10 (Xue et al. 2002), both containing 2200+ sentences and 10 words after removing punctuation."
        },
        {
            "Number": 19,
            "refer_ID": "A97-1014",
            "refer_sids": [
                151
            ],
            "refer_text": "For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).",
            "cite_ID": "D07-1066",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A comparison of unlexicalised PCFG parsing (Ku ?bler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the Tu? Ba D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For evaluation, the already annotated sentences were divided into two disjoint sets: one for training (90% of the corpus) and the other for testing (10%).",
            "GPT_cite_text": "A comparison of unlexicalised PCFG parsing (K\u00fcbler, 2005) trained and evaluated on the German NEGRA (Skut et al., 1997) and the TuBa-D/Z (Telljohann et al., 2004) treebanks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al., 1991)."
        }
    ],
    "W11-2123_aakansha": [
        {
            "Number": 1,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "W11-2138",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "We used common tools for phrase-based translation: Moses (Koehn et al., 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modeling and GIZA++ (Och and Ney, 2000) for word alignments."
        },
        {
            "Number": 2,
            "refer_ID": "W11-2123",
            "refer_sids": [
                45
            ],
            "refer_text": "The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.",
            "cite_ID": "P14-2022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The PROBING data structure is a rather straightforward application of these hash tables to store n-gram language models.",
            "GPT_cite_text": "The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run."
        },
        {
            "Number": 3,
            "refer_ID": "W11-2123",
            "refer_sids": [
                136
            ],
            "refer_text": "We offer a state function s(wn1) = wn\ufffd where substring wn\ufffd is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.",
            "cite_ID": "W12-3145",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We offer a state function s(wn1) = wn1 where substring wn1 is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.",
            "GPT_cite_text": "Thus, given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) <s> tf, (ii) tf, and (iii) tf </s> and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state information in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states."
        },
        {
            "Number": 4,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "Our translation system uses cdec (Dyer et al., 2010), an implementation of the hierarchical phrase-based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference."
        },
        {
            "Number": 5,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "W12-3154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "The three data sets in use in this paper are summarized in Table 1. The translation systems consisted of phrase tables and lexicalized reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime."
        },
        {
            "Number": 6,
            "refer_ID": "W11-2123",
            "refer_sids": [
                8
            ],
            "refer_text": "Queries take the form p(wn|wn\u22121 1 ) where wn1 is an n-gram.",
            "cite_ID": "P12-2058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Queries take the form p(wn|wn\u22121 1) where wn1 is an n-gram.",
            "GPT_cite_text": "The features used are basic lexical features, word penalty, and a 3-gram language model (Heafield, 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W11-2139",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Inference was carried out using the language modeling library described by Heafield (2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "Inference was carried out using the language modeling library described by Heafield (2011)."
        },
        {
            "Number": 8,
            "refer_ID": "W11-2123",
            "refer_sids": [
                205
            ],
            "refer_text": "We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).",
            "cite_ID": "P13-2003",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).",
            "GPT_cite_text": "We used the MADA ATB segmentation for Arabic (Roth et al., 2008) and true casing for English, phrases of maximal length 7, Kneser-Ney smoothing, and lexicalized reordering (Koehn et al., 2005), and a 5-gram language model, trained on GigaWord v.5 using KenLM (Heafield, 2011)."
        },
        {
            "Number": 9,
            "refer_ID": "W11-2123",
            "refer_sids": [
                274
            ],
            "refer_text": "We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described two data structures for language modeling that achieve substantial reductions in time and memory costs.",
            "GPT_cite_text": "The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011); both language model implementations are now integrated with Joshua."
        },
        {
            "Number": 10,
            "refer_ID": "W11-2123",
            "refer_sids": [
                204
            ],
            "refer_text": "For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 in 256.",
            "GPT_cite_text": "Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets."
        },
        {
            "Number": 11,
            "refer_ID": "W11-2123",
            "refer_sids": [
                274
            ],
            "refer_text": "We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described two data structures for language modeling that achieve substantial reductions in time and memory costs.",
            "GPT_cite_text": "With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult-to-compile SRILM toolkit (Stolcke, 2002)."
        },
        {
            "Number": 12,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3160",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This was used to create a KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "This was used to create a KenLM (Heafield, 2011)."
        },
        {
            "Number": 13,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3706",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "In the Opinum system, we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application."
        },
        {
            "Number": 14,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W11-2147",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "Our baseline is a factored phrase-based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modeling, and minimum error rate training (Och, 2003) to tune model feature weights."
        },
        {
            "Number": 15,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "E12-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "For language modeling, we computed 5-gram models using IRSTLM (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)."
        },
        {
            "Number": 16,
            "refer_ID": "W11-2123",
            "refer_sids": [
                229
            ],
            "refer_text": "Then we ran binary search to determine the least amount of memory with which it would run.",
            "cite_ID": "P12-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Then we ran a binary search to determine the least amount of memory with which it would run.",
            "GPT_cite_text": "Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima \u010can, 2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well. 3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using KenLM (Heafield, 2011)."
        },
        {
            "Number": 17,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "D12-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3."
        },
        {
            "Number": 18,
            "refer_ID": "W11-2123",
            "refer_sids": [
                93
            ],
            "refer_text": "The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.",
            "cite_ID": "P12-2006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The cost of storing these averages, in bits, is that because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.",
            "GPT_cite_text": "Research efforts to increase search efficiency for phrase-based MT (Koehn et al., 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al., 2006) to additional early pruning techniques (Delaney et al., 2006), (Moore and Quirk, 2007), and more efficient language model (LM) querying (Heafield, 2011)."
        },
        {
            "Number": 19,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "P13-2073",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "For English language modeling, we use the English Gigaword Corpus with a 5-gram LM using the KenLM toolkit (Heafield, 2011)."
        },
        {
            "Number": 20,
            "refer_ID": "W11-2123",
            "refer_sids": [
                199
            ],
            "refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "cite_ID": "P13-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "GPT_cite_text": "For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing."
        }
    ],
    "P08-1102_swastika": [
        {
            "Number": 1,
            "refer_ID": "P08-1102",
            "refer_sids": [
                32
            ],
            "refer_text": "We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We trained a character-based perceptron for Chinese Joint S&T and found that the perceptron itself could achieve considerably high accuracy in segmentation and Joint S&T.",
            "GPT_cite_text": "Following Jiang et al. (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C1: n = C1 C2."
        },
        {
            "Number": 2,
            "refer_ID": "P08-1102",
            "refer_sids": [
                32
            ],
            "refer_text": "We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We trained a character-based perceptron for Chinese Joint S&T and found that the perceptron itself could achieve considerably high accuracy in segmentation and Joint S&T.",
            "GPT_cite_text": "As described in Ng and Low (2004) and Jiang et al. (2008), we use s to indicate a single character word, while b, m, and e indicate the begin, middle, and end of a word respectively."
        },
        {
            "Number": 3,
            "refer_ID": "P08-1102",
            "refer_sids": [
                38
            ],
            "refer_text": "Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "plates called lexical-target in the column below areintroduced by Jiang et al (2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named nonlexical target.",
            "GPT_cite_text": "Plates called lexical targets in the column below are introduced by Jiang et al. (2008)."
        },
        {
            "Number": 5,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "D12-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB 5.0) to test the performance of the cascaded model on segmentation and joint S&T.",
            "GPT_cite_text": "Jiang et al. (2008) propose a cascaded linear model for joint Chinese word segmentation and POS tagging."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1102",
            "refer_sids": [
                36
            ],
            "refer_text": "All feature templates and their instances are shown in Table 1.",
            "cite_ID": "C10-1135",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the feature templates the same as Jiang et al, (2008) to extract features form E model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "All feature templates and their instances are shown in Table 1.",
            "GPT_cite_text": "We use the feature templates the same as Jiang et al. (2008) to extract features from the model."
        },
        {
            "Number": 8,
            "refer_ID": "P08-1102",
            "refer_sids": [
                32
            ],
            "refer_text": "We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.",
            "cite_ID": "P12-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "approach, where basic processing units are characters which compose words (Jiangetal., 2008a)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We trained a character-based perceptron for Chinese Joint S&T and found that the perceptron itself could achieve considerably high accuracy in segmentation and Joint S&T.",
            "GPT_cite_text": "Approach, where basic processing units are characters that compose words (Jiang et al., 2008a)."
        },
        {
            "Number": 10,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "C10-2096",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and joint S&T.",
            "GPT_cite_text": "6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "C10-2096",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and joint S&T.",
            "GPT_cite_text": "6.1.2 Lattice-Forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1102",
            "refer_sids": [
                97
            ],
            "refer_text": "Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.",
            "GPT_cite_text": "However, when we repeat the work of (Jiang et al., 2008), which reports achieving state-of-the-art performance in the datasets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)."
        },
        {
            "Number": 13,
            "refer_ID": "P08-1102",
            "refer_sids": [
                91
            ],
            "refer_text": "The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The first was conducted to test the performance of the perceptron on segmentation on the corpus from the SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU), and the Microsoft Research Corpus (MSR).",
            "GPT_cite_text": "Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appear twice, which are generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al., 2008), with n=0, generates [C0C0])"
        },
        {
            "Number": 14,
            "refer_ID": "P08-1102",
            "refer_sids": [
                16
            ],
            "refer_text": "Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Shown in Figure 1, the cascaded model has a two-layer architecture, with a character-based perceptron as the core combined with other real-valued features such as language models.",
            "GPT_cite_text": "As all the features adopted in (Jiang et al., 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle."
        },
        {
            "Number": 17,
            "refer_ID": "P08-1102",
            "refer_sids": [
                34
            ],
            "refer_text": "The feature templates we adopted are selected from those of Ng and Low (2004).",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The feature templates we adopted are selected from those of Ng and Low (2004).",
            "GPT_cite_text": "Last, (Jiang et al., 2008) adds repeated features implicitly based on (Ng and Low, 2004)."
        },
        {
            "Number": 20,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "D12-1046",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and joint S&T.",
            "GPT_cite_text": "Previous joint models mainly focus on word segmentation and POS tagging tasks, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), and re-ranking (Jiang et al., 2008b)."
        }
    ],
    "W99-0613_sweta": [
        {
            "Number": 1,
            "refer_ID": "W99-0613",
            "refer_sids": [
                121
            ],
            "refer_text": "They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "They also describe an application of co-training to classifying web pages (the two feature sets are the words on the page, and other pages pointing to the page).",
            "GPT_cite_text": "Co-training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web page classification (Blum and Mitchell, 1998), and named entity identification (Collins and Singer, 1999)."
        },
        {
            "Number": 2,
            "refer_ID": "W99-0613",
            "refer_sids": [
                252
            ],
            "refer_text": "The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The method uses a \"soft\" measure of the agreement between two classifiers as an objective function; we described an algorithm that directly optimizes this function.",
            "GPT_cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co-Booting)."
        },
        {
            "Number": 4,
            "refer_ID": "W99-0613",
            "refer_sids": [
                91
            ],
            "refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "cite_ID": "W03-1509",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "GPT_cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick et al. 1999], and so on."
        },
        {
            "Number": 5,
            "refer_ID": "W99-0613",
            "refer_sids": [
                91
            ],
            "refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "GPT_cite_text": "DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus."
        },
        {
            "Number": 6,
            "refer_ID": "W99-0613",
            "refer_sids": [
                213
            ],
            "refer_text": "Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Thus, at each iteration, the algorithm is forced to pick features for the location, person, and organization in turn for the classifier being trained.",
            "GPT_cite_text": "(Collins and Singer, 1999) also make use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify."
        },
        {
            "Number": 7,
            "refer_ID": "W99-0613",
            "refer_sids": [
                250
            ],
            "refer_text": "Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.",
            "cite_ID": "W06-2204",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.",
            "GPT_cite_text": "In (Collins and Singer, 1999), Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification."
        },
        {
            "Number": 8,
            "refer_ID": "W99-0613",
            "refer_sids": [
                39
            ],
            "refer_text": "(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.",
            "cite_ID": "W09-1116",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "(Brin 98) describes a system for extracting (author, book title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.",
            "GPT_cite_text": "Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named entity recognition."
        },
        {
            "Number": 10,
            "refer_ID": "W99-0613",
            "refer_sids": [
                202
            ],
            "refer_text": "The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.",
            "cite_ID": "W03-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The CoBoost algorithm just described is for the case where there are two labels; for the named entity task, there are three labels, and in general, it will be useful to generalize the CoBoost algorithm to the multiclass case.",
            "GPT_cite_text": "Collins and Singer (1999), for example, report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)."
        },
        {
            "Number": 11,
            "refer_ID": "W99-0613",
            "refer_sids": [
                61
            ],
            "refer_text": "The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.",
            "cite_ID": "E09-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The following features were used: full-string=x The full string (e.g., for Maury Cooper, full-string=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains(Maury) and contains(Cooper). allcap1 This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.",
            "GPT_cite_text": "While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al., 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999), and context-free grammar induction (numerous attempts, too many to mention)."
        },
        {
            "Number": 12,
            "refer_ID": "W99-0613",
            "refer_sids": [
                176
            ],
            "refer_text": "(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.",
            "cite_ID": "W10-3504",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "(7) is at 0 when: 1) Vi : sign(g1(xi)) = sign(g2(xi)); 2) |g3(xi)| \u2192 \u221e; and 3) sign(g1(xi)) = yi for i = 1, ..., m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.",
            "GPT_cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labeled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)."
        },
        {
            "Number": 13,
            "refer_ID": "W99-0613",
            "refer_sids": [
                108
            ],
            "refer_text": "In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.",
            "cite_ID": "W07-1712",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In the co-training case, (Blum and Mitchell 98) argue that the task should be to induce functions I1 and f2 such that So I1 and I2 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.",
            "GPT_cite_text": "In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)."
        },
        {
            "Number": 15,
            "refer_ID": "W99-0613",
            "refer_sids": [
                27,
                28
            ],
            "refer_text": "The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).\n(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.",
            "cite_ID": "W09-2208",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The first method builds on results from (Yarowsky 1995) and (Blum and Mitchell 1998). (Yarowsky 1995) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features and gives impressive performance.",
            "GPT_cite_text": "Collins et al. (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by Blum and Mitchell (1998)."
        },
        {
            "Number": 16,
            "refer_ID": "W99-0613",
            "refer_sids": [
                7
            ],
            "refer_text": "Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Many statistical or machine learning approaches for natural language problems require a relatively large amount of supervision in the form of labeled training examples.",
            "GPT_cite_text": "This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)."
        },
        {
            "Number": 17,
            "refer_ID": "W99-0613",
            "refer_sids": [
                172
            ],
            "refer_text": "To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(4 )prec (p, y)= count (p, y) count (p) (5) where prec (p, y) is the raw precision of pattern p in the set of documents labeled with category y. Criterion 2: Collins This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "To see this, note that the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.",
            "GPT_cite_text": "(4) prec (p, y) = count (p, y) / count (p) (5) where prec (p, y) is the raw precision of pattern p in the set of documents labeled with category y. Criterion 2: Collins This criterion was used in a lightly supervised NE recognizer (Collins and Singer, 1999)."
        },
        {
            "Number": 18,
            "refer_ID": "W99-0613",
            "refer_sids": [
                85
            ],
            "refer_text": "(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "(If fewer than n rules have Precision greater than pin, we note that taking the top n most frequent rules already makes the method robust to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. Keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.",
            "GPT_cite_text": "(6) Similarly to Collins and Singer (1999), we used T = 0.95 for all experiments reported here."
        },
        {
            "Number": 20,
            "refer_ID": "W99-0613",
            "refer_sids": [
                214
            ],
            "refer_text": "This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating \u2014 this deserves more theoretical investigation.",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This modification brings the method closer to the DL-CoTrain algorithm described earlier and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating \u2014 this deserves more theoretical investigation.",
            "GPT_cite_text": "This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious."
        }
    ],
    "A00-2030_aakansha": [
        {
            "Number": 1,
            "refer_ID": "A00-2030",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Section 5 compares our approach tooth ers in the literature, in particular that of (Miller et al., 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) for information extraction.",
            "GPT_cite_text": "Section 5 compares our approach to others in the literature, in particular that of (Miller et al., 2000)."
        },
        {
            "Number": 2,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33,
                34
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.\nIn these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major di erences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "GPT_cite_text": "The basic approach we described is very similar to the one presented in (Miller et al., 2000); however, there are a few major differences: in our approach, the augmentation of the syntactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly. The approach in (Miller"
        },
        {
            "Number": 3,
            "refer_ID": "A00-2030",
            "refer_sids": [
                49,
                50
            ],
            "refer_text": "By necessity, we adopted the strategy of hand marking only the semantics.\nFigure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "By necessity, we adopted the strategy of hand marking only the semantics. Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.",
            "GPT_cite_text": "The semantic annotation required by our task is much simpler than that employed by Miller et al. (2000)."
        },
        {
            "Number": 4,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33,
                34
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.\nIn these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One possibly bene cial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "GPT_cite_text": "One possibly beneficial extension of our work suggested by (Miller et al., 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level."
        },
        {
            "Number": 5,
            "refer_ID": "A00-2030",
            "refer_sids": [
                10
            ],
            "refer_text": "Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similar to the approach in (Miller et al, 2000 )weinitialized the SLM statistics from the UPenn Tree bank parse trees (about 1Mwds of training data) at the rst training stage, see Section 3",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Instead, our parsing algorithm, trained on the UPenn Treebank, was run on the New York Times source to create unsupervised syntactic training that was constrained to be consistent with semantic annotation.",
            "GPT_cite_text": "Similar to the approach in (Miller et al., 2000), we initialized the SLM statistics from the UPenn Treebank parse trees (about 1 M words of training data) at the first training stage, see Section 3."
        },
        {
            "Number": 6,
            "refer_ID": "A00-2030",
            "refer_sids": [
                16
            ],
            "refer_text": "For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.",
            "cite_ID": "P14-1078",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the following example, the Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.",
            "GPT_cite_text": "Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns."
        },
        {
            "Number": 7,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33,
                34
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.\nIn these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "P05-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "GPT_cite_text": "One interesting system that does not belong to the above class is that of Miller et al. (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations."
        },
        {
            "Number": 8,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33,
                34
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.\nIn these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "P05-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "GPT_cite_text": "Miller et al. (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees."
        },
        {
            "Number": 9,
            "refer_ID": "A00-2030",
            "refer_sids": [
                23,
                24
            ],
            "refer_text": "An integrated model can limit the propagation of errors by making all decisions jointly.\nFor this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.",
            "cite_ID": "P05-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An integrated model can limit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, name finding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.",
            "GPT_cite_text": "Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al. (2000), which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction, and relation extraction in a single model."
        },
        {
            "Number": 10,
            "refer_ID": "A00-2030",
            "refer_sids": [
                23,
                24
            ],
            "refer_text": "An integrated model can limit the propagation of errors by making all decisions jointly.\nFor this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.",
            "cite_ID": "H05-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An integrated model can limit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, name finding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.",
            "GPT_cite_text": "(Miller et al., 2000) have combined entity recognition, parsing, and relation extraction into a jointly trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly labeled data is unavailable."
        },
        {
            "Number": 11,
            "refer_ID": "A00-2030",
            "refer_sids": [
                23,
                24,
                33,
                34
            ],
            "refer_text": "An integrated model can limit the propagation of errors by making all decisions jointly.\nFor this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.Our integrated model represents syntax and semantics jointly using augmented parse trees.\nIn these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "P04-1054",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An integrated model can limit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, name finding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other. Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "GPT_cite_text": "Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types."
        },
        {
            "Number": 12,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33,
                34
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.\nIn these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "P04-1054",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "WhereasMiller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "GPT_cite_text": "Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance."
        },
        {
            "Number": 13,
            "refer_ID": "A00-2030",
            "refer_sids": [
                60,
                61
            ],
            "refer_text": "In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).\nThe detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "cite_ID": "W05-0602",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The syntactic model in (Miller et al, 2000) is similar to Collins?, but doesnot use features like sub cat frames and distance measures",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997). The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, named entity recognition, syntactic parsing, and relation extraction in a single process.",
            "GPT_cite_text": "The syntactic model in (Miller et al., 2000) is similar to Collins, but does not use features like subcat frames and distance measures."
        },
        {
            "Number": 14,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.",
            "cite_ID": "N07-2041",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.",
            "GPT_cite_text": "Similar to the approach in (Miller et al., 2000) and (Kulick et al., 2004), our parser integrates both syntactic and semantic annotations into a single annotation, as shown in Figure 2."
        },
        {
            "Number": 15,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33,
                34
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.\nIn these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "W10-2924",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "GPT_cite_text": "Miller et al. (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels."
        },
        {
            "Number": 16,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33,
                34
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.\nIn these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "W06-0508",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "GPT_cite_text": "Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al., 2000), or clustering of semantically similar syntactic dependencies according to their selectional restrictions (Gamallo et al., 2002)."
        },
        {
            "Number": 17,
            "refer_ID": "A00-2030",
            "refer_sids": [
                11,
                12,
                16
            ],
            "refer_text": "We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).\nThe Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.",
            "cite_ID": "P07-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998). The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts). For the following example, the Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.",
            "GPT_cite_text": "This includes parsing and relation extraction (Miller et al., 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al., 2004)."
        },
        {
            "Number": 18,
            "refer_ID": "A00-2030",
            "refer_sids": [
                105
            ],
            "refer_text": "A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.",
            "cite_ID": "W05-0636",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks",
            "label": [
                "Result_citation"
            ],
            "GPT_refer_text": "A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.",
            "GPT_cite_text": "For example, Miller et al. (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks."
        },
        {
            "Number": 19,
            "refer_ID": "A00-2030",
            "refer_sids": [
                60,
                61
            ],
            "refer_text": "In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).\nThe detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997). The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "GPT_cite_text": "Miller et al. (2000) address the task of relation extraction from the statistical parsing viewpoint."
        },
        {
            "Number": 20,
            "refer_ID": "A00-2030",
            "refer_sids": [
                16
            ],
            "refer_text": "For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.",
            "cite_ID": "D11-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the following example, the Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.",
            "GPT_cite_text": "Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns."
        }
    ],
    "P87-1015_sweta": [
        {
            "Number": 1,
            "refer_ID": "P87-1015",
            "refer_sids": [
                205
            ],
            "refer_text": "Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.",
            "cite_ID": "P01-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach that Vijay-Shanker et al (1987) and Weir (1988) take, elaborated on by Becker et al (1992), is to identify a very general class of formalisms, which they call linear context free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Independence of paths at this level reflects context-freeness of rewriting and suggests why they can be recognized efficiently.",
            "GPT_cite_text": "The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear context-free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared."
        },
        {
            "Number": 2,
            "refer_ID": "P87-1015",
            "refer_sids": [
                229
            ],
            "refer_text": "In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).",
            "cite_ID": "E09-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Here we use the standard definition of LCFRS (Vijay-Shanker et al, 1987) and only fix our notation; for a more thorough discussion of this formal ism, we refer to the literature. Let G be an LCFRS",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In addition, the restricted version of CG (discussed in Section 6) generates tree sets with independent paths, and we hope that it can be included in a more general definition of LCFRS containing formalisms whose tree sets have path sets that are themselves LCFRL (as in the case of the restricted indexed grammars and the hierarchy defined by Weir).",
            "GPT_cite_text": "Here we use the standard definition of LCFRS (Vijay-Shanker et al., 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. Let G be an LCFRS."
        },
        {
            "Number": 3,
            "refer_ID": "P87-1015",
            "refer_sids": [
                146
            ],
            "refer_text": "Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.",
            "cite_ID": "W07-2214",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "There are many (structural) mildly context sensitive grammar formalisms ,e.g .mcfg ,lcfrs, mg, and they have been shown to be equivalent (Vijay-Shanker et al., 1987)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Since every CFL is known to be semilinear (Parikh, 1966), in order to show the semilinearity of some language, we need only show the existence of a letter-equivalent CFL. Our definition of LCFRS's insists that the composition operations are linear and non-erasing.",
            "GPT_cite_text": "There are many (structural) mildly context-sensitive grammar formalisms, e.g., MCFG, LCFRS, MG, and they have been shown to be equivalent (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 4,
            "refer_ID": "P87-1015",
            "refer_sids": [
                201
            ],
            "refer_text": "It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.",
            "cite_ID": "P09-2003",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al, 1987)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG, and IGs.",
            "GPT_cite_text": "They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 5,
            "refer_ID": "P87-1015",
            "refer_sids": [
                151
            ],
            "refer_text": "We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).",
            "cite_ID": "P09-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following this line, (Vijay-Shanker et al, 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We now turn our attention to the recognition of string languages generated by these formalisms (LCFRLs).",
            "GPT_cite_text": "Following this line, Vijay-Shanker et al. (1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years from the community."
        },
        {
            "Number": 6,
            "refer_ID": "P87-1015",
            "refer_sids": [
                222
            ],
            "refer_text": "However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.",
            "cite_ID": "P09-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al, 1987)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive than ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.",
            "GPT_cite_text": "We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 7,
            "refer_ID": "P87-1015",
            "refer_sids": [
                22,
                23
            ],
            "refer_text": "Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.\nHe also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.",
            "cite_ID": "P07-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We write REGD.k/ to refer to the class of regular dependency languages with a gap-degree bounded by k. Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al, 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwegian. He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.",
            "GPT_cite_text": "We write REGD.k/ to refer to the class of regular dependency languages with a gap-degree bounded by k. Linear Context-Free Rewriting Systems gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms."
        },
        {
            "Number": 8,
            "refer_ID": "P87-1015",
            "refer_sids": [
                156
            ],
            "refer_text": "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.",
            "cite_ID": "N09-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006) .In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al, 1987)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Giving a recognition algorithm for LCFRL involves describing the substrings of the input that are spanned by the structures derived by the LCFRS and how the composition operation combines these substrings.",
            "GPT_cite_text": "This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single-language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 9,
            "refer_ID": "P87-1015",
            "refer_sids": [
                221
            ],
            "refer_text": "Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).",
            "cite_ID": "N09-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al (1987)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).",
            "GPT_cite_text": "We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al. (1987)."
        },
        {
            "Number": 10,
            "refer_ID": "P87-1015",
            "refer_sids": [
                54
            ],
            "refer_text": "An IG can be viewed as a CFG in which each nonterminal is associated with a stack.",
            "cite_ID": "W10-1407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "LCFRS (Vijay-Shanker et al, 1987) are anatural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "An IG can be viewed as a CFG in which each non-terminal is associated with a stack.",
            "GPT_cite_text": "LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals."
        },
        {
            "Number": 11,
            "refer_ID": "P87-1015",
            "refer_sids": [
                128
            ],
            "refer_text": "As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.",
            "cite_ID": "W10-1407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A LCFRS (Vijay-Shanker et al, 1987) is a tuple G= (N, T, V, P, S )wherea) N is a finite set of non-terminals with a function dim: N? N that determines the fan-out of each A? N; b) T and V are disjoint finite sets of terminals and variables; c) S? N is the start symbol with dim (S)= 1; d) P is a finite set of rewriting rules A (? 1,..",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "As in the case of the derivation trees of CFGs, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAGs) used to denote derived structures.",
            "GPT_cite_text": "A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N \u2192 N that determines the fan-out of each A \u2208 N; b) T and V are disjoint finite sets of terminals and variables; c) S \u2208 N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules A (\u2208 1, ..."
        },
        {
            "Number": 12,
            "refer_ID": "P87-1015",
            "refer_sids": [
                217
            ],
            "refer_text": "In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al (1987))",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived from these formalisms and the substrings they span.",
            "GPT_cite_text": "In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes, 1997) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 13,
            "refer_ID": "P87-1015",
            "refer_sids": [
                16
            ],
            "refer_text": "From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "2 By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of deriva tional structures are all regular (Vijay-Shanker et al., 1987)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular.",
            "GPT_cite_text": "By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 14,
            "refer_ID": "P87-1015",
            "refer_sids": [
                16
            ],
            "refer_text": "From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al, 1987)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular.",
            "GPT_cite_text": "It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 15,
            "refer_ID": "P87-1015",
            "refer_sids": [
                214
            ],
            "refer_text": "LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).",
            "cite_ID": "N10-1035",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi,1985), including, among several others, the tree ad joining grammars (TAGs) of Joshi et al (1975) .Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings ,i.e., discontinuous phrases",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).",
            "GPT_cite_text": "On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting systems (LCFRS), introduced by Vijay-Shanker et al. (1987), are mildly context-sensitive formalisms that allow the derivation of tuples of strings, i.e., discontinuous phrases."
        },
        {
            "Number": 16,
            "refer_ID": "P87-1015",
            "refer_sids": [
                35
            ],
            "refer_text": "TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.",
            "cite_ID": "P12-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al (1991) .3 Thus, CFTG are mildly context-sensitive since their generated string languages are semi-linear and can be parsed in polynomial time (Go ?mez-Rodr? ?guez et al, 2010)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "TAGs can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwegian, for cross-serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.",
            "GPT_cite_text": "CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al. (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al. (1991). Thus, CFTG are mildly context-sensitive since their generated string languages are semi-linear and can be parsed in polynomial time (G\u00f3mez-Rodr\u00edguez et al., 2010)."
        }
    ],
    "W99-0623_sweta": [
        {
            "Number": 1,
            "refer_ID": "W99-0623",
            "refer_sids": [
                144
            ],
            "refer_text": "Combining multiple highly-accurate independent parsers yields promising results.",
            "cite_ID": "A00-2005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Combining multiple highly accurate independent parsers yields promising results.",
            "GPT_cite_text": "1 Introduction Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy."
        },
        {
            "Number": 2,
            "refer_ID": "W99-0623",
            "refer_sids": [
                125
            ],
            "refer_text": "The constituent voting and na\u00efve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.",
            "cite_ID": "A00-2005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The constituent voting and naive Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.",
            "GPT_cite_text": "The collection of hypotheses ti = fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)"
        },
        {
            "Number": 4,
            "refer_ID": "W99-0623",
            "refer_sids": [
                125
            ],
            "refer_text": "The constituent voting and na\u00efve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.",
            "cite_ID": "N10-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The constituent voting and naive Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.",
            "GPT_cite_text": "5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers."
        },
        {
            "Number": 5,
            "refer_ID": "W99-0623",
            "refer_sids": [
                48
            ],
            "refer_text": "\u2022 Similarly, when the na\u00efve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "\u2022 Similarly, when the na\u00efve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.",
            "GPT_cite_text": "A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)."
        },
        {
            "Number": 6,
            "refer_ID": "W99-0623",
            "refer_sids": [
                139
            ],
            "refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "GPT_cite_text": "This approach roughly corresponds to (Henderson and Brill, 1999)'s Na\u00efve Bayes parse hybridization."
        },
        {
            "Number": 7,
            "refer_ID": "W99-0623",
            "refer_sids": [
                134
            ],
            "refer_text": "As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) also reported that context did not help them to outperform simple voting",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.",
            "GPT_cite_text": "Henderson and Brill (1999) also reported that context did not help them outperform simple voting."
        },
        {
            "Number": 8,
            "refer_ID": "W99-0623",
            "refer_sids": [
                38
            ],
            "refer_text": "Under certain conditions the constituent voting and na\u00efve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Under certain conditions, the constituent voting and na\u00efve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.",
            "GPT_cite_text": "(Henderson and Brill, 1999) improved their best parser's F-measure from 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)."
        },
        {
            "Number": 10,
            "refer_ID": "W99-0623",
            "refer_sids": [
                120
            ],
            "refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "cite_ID": "P01-1005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision. Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "GPT_cite_text": "Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren et al., 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pedersen, 2000)."
        },
        {
            "Number": 11,
            "refer_ID": "W99-0623",
            "refer_sids": [
                139
            ],
            "refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "GPT_cite_text": "Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes: one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees."
        },
        {
            "Number": 12,
            "refer_ID": "W99-0623",
            "refer_sids": [
                13
            ],
            "refer_text": "These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).",
            "GPT_cite_text": "Henderson and Brill (1999) combine three parsers and obtain an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper."
        },
        {
            "Number": 13,
            "refer_ID": "W99-0623",
            "refer_sids": [
                108
            ],
            "refer_text": "From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "From this, we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.",
            "GPT_cite_text": "Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)."
        },
        {
            "Number": 14,
            "refer_ID": "W99-0623",
            "refer_sids": [
                139
            ],
            "refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "cite_ID": "N06-2033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "GPT_cite_text": "Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents of the initial trees."
        },
        {
            "Number": 15,
            "refer_ID": "W99-0623",
            "refer_sids": [
                98
            ],
            "refer_text": "Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated, it would invariably hurt our precision more than we would gain in recall.",
            "GPT_cite_text": "(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined."
        },
        {
            "Number": 16,
            "refer_ID": "W99-0623",
            "refer_sids": [
                27
            ],
            "refer_text": "Another technique for parse hybridization is to use a na\u00efve Bayes classifier to determine which constituents to include in the parse.",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Another technique for parse hybridization is to use a na\u00efve Bayes classifier to determine which constituents to include in the parse.",
            "GPT_cite_text": "(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents."
        },
        {
            "Number": 17,
            "refer_ID": "W99-0623",
            "refer_sids": [
                80,
                81,
                82
            ],
            "refer_text": "For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.\nF-measure is the harmonic mean of precision and recall, 2PR/(P + R).\nIt is closer to the smaller value of precision and recall when there is a large skew in their values.",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For our experiments, we also report the mean of precision and recall, which we denote by (P + R) / 2 and F-measure. F-measure is the harmonic mean of precision and recall, 2PR / (P + R). It is closer to the smaller value of precision and recall when there is a large skew in their values.",
            "GPT_cite_text": "Output (Figure 3). Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected F-score within the Minimum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs."
        },
        {
            "Number": 18,
            "refer_ID": "W99-0623",
            "refer_sids": [
                49
            ],
            "refer_text": "In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.",
            "cite_ID": "P09-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.",
            "GPT_cite_text": "System combination has benefited various NLP tasks in recent years, such as products of experts (e.g., Smith and Eisner, 2005) and ensemble-based parsing (e.g., Henderson and Brill, 1999)."
        },
        {
            "Number": 19,
            "refer_ID": "W99-0623",
            "refer_sids": [
                11
            ],
            "refer_text": "Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).",
            "cite_ID": "N03-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), and named entity recognition (Borthwick et al., 1998).",
            "GPT_cite_text": "In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994)."
        },
        {
            "Number": 20,
            "refer_ID": "W99-0623",
            "refer_sids": [
                98
            ],
            "refer_text": "Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.",
            "cite_ID": "C10-1151",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated, it would invariably hurt our precision more than we would gain in recall.",
            "GPT_cite_text": "Henderson and Brill (1999) perform parse selection by maximizing the expected precision of selected parses with respect to the set of parses to be combined."
        }
    ],
    "P11-1060_aakansha": [
        {
            "Number": 1,
            "refer_ID": "P11-1060",
            "refer_sids": [
                11
            ],
            "refer_text": "Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.",
            "cite_ID": "D11-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Figure 1 shows our probabilistic model with respect to a world w (database of facts), producing an answer y.",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) present work on unsupervised learning."
        },
        {
            "Number": 2,
            "refer_ID": "P11-1060",
            "refer_sids": [
                2
            ],
            "refer_text": "In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.",
            "GPT_cite_text": "In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from question-answer pairs alone, which represents a significant advance."
        },
        {
            "Number": 3,
            "refer_ID": "P11-1060",
            "refer_sids": [
                9
            ],
            "refer_text": "As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the end-to-end problem of mapping questions to answers.",
            "GPT_cite_text": "To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al. (2010) and Liang et al. (2011) used the annotated logical forms to compute answers for their experiments."
        },
        {
            "Number": 4,
            "refer_ID": "P11-1060",
            "refer_sids": [
                21
            ],
            "refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "GPT_cite_text": "More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins."
        },
        {
            "Number": 5,
            "refer_ID": "P11-1060",
            "refer_sids": [
                21
            ],
            "refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "GPT_cite_text": "GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al., 2011)."
        },
        {
            "Number": 8,
            "refer_ID": "P11-1060",
            "refer_sids": [
                112
            ],
            "refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "cite_ID": "D12-1069",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our learning algorithm alternates between (i) using the current parameters \u03b8 to generate the K-best set \u02dcZL,\u03b8(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.",
            "GPT_cite_text": "One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al. 2011) or even a binary correct/incorrect signal (Clarke et al. 2010)."
        },
        {
            "Number": 9,
            "refer_ID": "P11-1060",
            "refer_sids": [
                11
            ],
            "refer_text": "Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.",
            "cite_ID": "N12-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.",
            "GPT_cite_text": "For example, Liang et al. (2011) construct a latent parse similar in structure to a dependency grammar, but representing a logical form."
        },
        {
            "Number": 10,
            "refer_ID": "P11-1060",
            "refer_sids": [
                21
            ],
            "refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "cite_ID": "P12-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers."
        },
        {
            "Number": 11,
            "refer_ID": "P11-1060",
            "refer_sids": [
                21
            ],
            "refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).",
            "GPT_cite_text": "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model the semantics of questions by using simple dependency-like trees (Liang et al., 2011)."
        },
        {
            "Number": 12,
            "refer_ID": "P11-1060",
            "refer_sids": [
                10
            ],
            "refer_text": "However, we still model the logical form (now as a latent variable) to capture the complexities of language.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, we still model the logical form (now as a latent variable) to capture the complexities of language.",
            "GPT_cite_text": "DCS trees have been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al., 2011) (Figure 1)."
        },
        {
            "Number": 13,
            "refer_ID": "P11-1060",
            "refer_sids": [
                36
            ],
            "refer_text": "It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It is this transparency between syntax and semantics provided by DCS that leads to a simple and streamlined compositional semantics suitable for program induction.",
            "GPT_cite_text": "are explained in 2.5. http://nlp.stanford.edu/software/corenlp.shtml In (Liang et al., 2011) DCS trees are learned from QA pairs and database entries."
        },
        {
            "Number": 14,
            "refer_ID": "P11-1060",
            "refer_sids": [
                42
            ],
            "refer_text": "The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).",
            "GPT_cite_text": "As in the sentence, \"Tropical storm Debby is blamed for death,\" which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al., 2011) of that variable."
        },
        {
            "Number": 15,
            "refer_ID": "P11-1060",
            "refer_sids": [
                106
            ],
            "refer_text": "Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.",
            "cite_ID": "D11-1140",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Learning: Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y | x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k\u2082\u2082, which sums over all DCS trees z that evaluate to the target answer y.",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers that are more easily available."
        },
        {
            "Number": 16,
            "refer_ID": "P11-1060",
            "refer_sids": [
                45
            ],
            "refer_text": "The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.",
            "cite_ID": "D11-1140",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.",
            "GPT_cite_text": "and Collins, 2005, 2007), -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al., 2010) systems and DCS (Liang et al., 2011)"
        },
        {
            "Number": 17,
            "refer_ID": "P11-1060",
            "refer_sids": [
                88
            ],
            "refer_text": "Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.",
            "cite_ID": "P13-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.",
            "GPT_cite_text": "In general, every plural NP potentially introduces an implicit universal, ranging. For example, Liang et al. (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases explicitly devise quantifier scoping in the semantic model."
        },
        {
            "Number": 19,
            "refer_ID": "P11-1060",
            "refer_sids": [
                171
            ],
            "refer_text": "This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.",
            "cite_ID": "P12-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This yields a system that is based on a new semantic representation, a factorized and flexible representation that is easier to search through and parametrize using features, native to lambda calculus.",
            "GPT_cite_text": "In fact, for any CFG G, see Liang et al. (2011) for work in representing lambda calculus expressions with trees."
        }
    ],
    "P11-1060_sweta": [
        {
            "Number": 1,
            "refer_ID": "P11-1060",
            "refer_sids": [
                8
            ],
            "refer_text": "On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.",
            "cite_ID": "D11-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) present work on unsupervised learning."
        },
        {
            "Number": 2,
            "refer_ID": "P11-1060",
            "refer_sids": [
                132
            ],
            "refer_text": "Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Results: We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.",
            "GPT_cite_text": "In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from question-answer pairs alone, which represents a significant advance."
        },
        {
            "Number": 3,
            "refer_ID": "P11-1060",
            "refer_sids": [
                25
            ],
            "refer_text": "We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.",
            "GPT_cite_text": "To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al. (2010) and Liang et al. (2011) used the annotated logical forms to compute answers for their experiments."
        },
        {
            "Number": 4,
            "refer_ID": "P11-1060",
            "refer_sids": [
                45
            ],
            "refer_text": "The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.",
            "GPT_cite_text": "More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins."
        },
        {
            "Number": 5,
            "refer_ID": "P11-1060",
            "refer_sids": [
                51
            ],
            "refer_text": "It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent\u2019s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.",
            "cite_ID": "P13-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent\u2019s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.",
            "GPT_cite_text": "GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al., 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "P11-1060",
            "refer_sids": [
                166
            ],
            "refer_text": "Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language\u2014think grounded al., 2010). compositional semantics.",
            "cite_ID": "W12-2802",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Our employed (Zettlemoyer and Collins, 2007) words work pushes the grounded language agenda towards deeper representations of language\u2014think grounded compositional semantics (Kwiatkowski et al., 2010).",
            "GPT_cite_text": "Matuszek et al. [2010], Liang et al. [2011], and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world."
        },
        {
            "Number": 8,
            "refer_ID": "P11-1060",
            "refer_sids": [
                8
            ],
            "refer_text": "On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.",
            "cite_ID": "P13-2009",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.",
            "GPT_cite_text": "It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g., rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011)."
        },
        {
            "Number": 9,
            "refer_ID": "P11-1060",
            "refer_sids": [
                11
            ],
            "refer_text": "Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.",
            "cite_ID": "D12-1069",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Figure 1 shows our probabilistic model with respect to a world w (database of facts), producing an answer y.",
            "GPT_cite_text": "One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al. 2011) or even a binary correct/incorrect signal (Clarke et al. 2010)."
        },
        {
            "Number": 10,
            "refer_ID": "P11-1060",
            "refer_sids": [
                115
            ],
            "refer_text": "After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p\u03b8(T)(y  |x, z \u2208 \u02dcZL,\u03b8(T)).",
            "cite_ID": "N12-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmax_y p_\u03b8(T)(y | x, z \u2208 \u02dcZ_L, \u03b8(T)).",
            "GPT_cite_text": "For example, Liang et al. (2011) construct a latent parse similar in structure to a dependency grammar, but representing a logical form."
        },
        {
            "Number": 11,
            "refer_ID": "P11-1060",
            "refer_sids": [
                132
            ],
            "refer_text": "Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.",
            "cite_ID": "P12-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Results: We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers."
        },
        {
            "Number": 12,
            "refer_ID": "P11-1060",
            "refer_sids": [
                25
            ],
            "refer_text": "We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.",
            "GPT_cite_text": "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model the semantics of questions by using simple dependency-like trees (Liang et al., 2011)."
        },
        {
            "Number": 13,
            "refer_ID": "P11-1060",
            "refer_sids": [
                94
            ],
            "refer_text": "We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We now turn to the task of mapping natural language utterances to DCS trees. For the example in Figure 4(b), the de-",
            "GPT_cite_text": "DCS trees have been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al., 2011) (Figure 1)."
        },
        {
            "Number": 14,
            "refer_ID": "P11-1060",
            "refer_sids": [
                132
            ],
            "refer_text": "Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Results: We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.",
            "GPT_cite_text": "are explained in 2.5. http://nlp.stanford.edu/software/corenlp.shtml In (Liang et al., 2011) DCS trees are learned from QA pairs and database entries."
        },
        {
            "Number": 15,
            "refer_ID": "P11-1060",
            "refer_sids": [
                157
            ],
            "refer_text": "Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.",
            "cite_ID": "P14-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Eisenstein et al. (2009) induces conjunctive formulae and uses them as features in another learning problem due to data sparsity and having an insufficiently large K.",
            "GPT_cite_text": "As in the sentence, \"Tropical storm Debby is blamed for death,\" which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al., 2011) of that variable."
        },
        {
            "Number": 16,
            "refer_ID": "P11-1060",
            "refer_sids": [
                106
            ],
            "refer_text": "Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.",
            "cite_ID": "D11-1140",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Learning: Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y | x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k\u2082\u2082, which sums over all DCS trees z that evaluate to the target answer y.",
            "GPT_cite_text": "Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers that are more easily available."
        },
        {
            "Number": 17,
            "refer_ID": "P11-1060",
            "refer_sids": [
                167
            ],
            "refer_text": "In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.",
            "cite_ID": "D11-1140",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In DCS, we start with lexical triggers, which are more basic than CCG lexical entries.",
            "GPT_cite_text": "and Collins, 2005, 2007), -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al., 2010) systems and DCS (Liang et al., 2011)"
        },
        {
            "Number": 18,
            "refer_ID": "P11-1060",
            "refer_sids": [
                138
            ],
            "refer_text": "Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).",
            "cite_ID": "P13-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).",
            "GPT_cite_text": "In general, every plural NP potentially introduces an implicit universal, ranging. For example, Liang et al. (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases explicitly devise quantifier scoping in the semantic model."
        },
        {
            "Number": 19,
            "refer_ID": "P11-1060",
            "refer_sids": [
                40
            ],
            "refer_text": "The CSP has two types of constraints: (i) x \u2208 w(p) for each node x labeled with predicate p \u2208 P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j \u2208 R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.",
            "cite_ID": "D11-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The CSP has two types of constraints: (i) x \u2208 w(p) for each node x labeled with predicate p \u2208 P; and (ii) xj = yj0 (the j-th component of x must equal the j0-th component of y) for each edge (x, y) labeled with j0 \u2208 R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.",
            "GPT_cite_text": "DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011)"
        },
        {
            "Number": 20,
            "refer_ID": "P11-1060",
            "refer_sids": [
                171
            ],
            "refer_text": "This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.",
            "cite_ID": "P12-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This yields a system that is based on a new semantic representation, a factorized and flexible representation that is easier to search through and parametrize using features, native to lambda calculus.",
            "GPT_cite_text": "In fact, for any CFG G, see Liang et al. (2011) for work in representing lambda calculus expressions with trees."
        }
    ],
    "P08-1028_swastika": [
        {
            "Number": 1,
            "refer_ID": "P08-1028",
            "refer_sids": [
                21.0
            ],
            "refer_text": "Central in these models is the notion of compositionality \u2014 the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.",
            "cite_ID": "D08-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Central to these models is the notion of compositionality \u2014 the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.",
            "GPT_cite_text": "Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p + a as a function f operating on four components: c = f(p, a, R, K) (3) R is the relation holding between p and a, and K is additional knowledge."
        },
        {
            "Number": 2,
            "refer_ID": "P08-1028",
            "refer_sids": [
                27.0
            ],
            "refer_text": "Our results show that the multiplicative models are superior and correlate significantly with behavioral data.",
            "cite_ID": "D08-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In both experiments, we compare the SVS model against the state-of-the art model by Mitchell and Lapata 2008 (henceforth M& amp; L; cf",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Our results show that the multiplicative models are superior and correlate significantly with behavioral data.",
            "GPT_cite_text": "In both experiments, we compare the SVS model against the state-of-the-art model by Mitchell and Lapata 2008 (henceforth M&L; cf."
        },
        {
            "Number": 4,
            "refer_ID": "P08-1028",
            "refer_sids": [
                189.0
            ],
            "refer_text": "In this paper we presented a general framework for vector-based semantic composition.",
            "cite_ID": "P14-1060",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In this paper, we presented a general framework for vector-based semantic composition.",
            "GPT_cite_text": "While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bi-grams and trigrams, methods such as Mitchell and Lapata (2008), Socher et al. (2012), and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1028",
            "refer_sids": [
                53.0
            ],
            "refer_text": "We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word\u2019s vector typically represents its co-occurrence with neighboring words.",
            "cite_ID": "P10-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrizations that have been suggested in the literature. We briefly note here that a word\u2019s vector typically represents its co-occurrence with neighboring words.",
            "GPT_cite_text": "Mitchell and Lapata (2008), henceforth M&L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression."
        },
        {
            "Number": 8,
            "refer_ID": "P08-1028",
            "refer_sids": [
                76.0
            ],
            "refer_text": "The models considered so far assume that components do not \u2018interfere\u2019 with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.",
            "cite_ID": "D11-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The models considered so far assume that components do not \u2018interfere\u2019 with each other, i.e., that it is also possible to reintroduce the dependence on K into the model of vector composition.",
            "GPT_cite_text": "And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors."
        },
        {
            "Number": 9,
            "refer_ID": "P08-1028",
            "refer_sids": [
                57.0
            ],
            "refer_text": "Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.",
            "cite_ID": "W11-0131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Let p denote the composition of two vectors u and v, representing a pair of constituents that stand in some syntactic relation R. Let K stand for any additional knowledge or information that is needed to construct the semantics of their composition.",
            "GPT_cite_text": "Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p = f(u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is the syntactic context, K is a semantic knowledge base, and p is the resulting composed vector (or tensor)."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1028",
            "refer_sids": [
                57.0
            ],
            "refer_text": "Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.",
            "cite_ID": "P13-2083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Let p denote the composition of two vectors u and v, representing a pair of constituents that stand in some syntactic relation R. Let K stand for any additional knowledge or information that is needed to construct the semantics of their composition.",
            "GPT_cite_text": "Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1028",
            "refer_sids": [
                190.0
            ],
            "refer_text": "We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "P13-2083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We formulated a composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "As our final set of baselines, we extend two simple techniques proposed by Mitchell and Lapata (2008) that use element-wise addition and multiplication operators to perform composition."
        },
        {
            "Number": 13,
            "refer_ID": "P08-1028",
            "refer_sids": [
                60.0
            ],
            "refer_text": "To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.",
            "cite_ID": "P10-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.",
            "GPT_cite_text": "Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work, we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)."
        },
        {
            "Number": 14,
            "refer_ID": "P08-1028",
            "refer_sids": [
                190.0
            ],
            "refer_text": "We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "P10-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We formulated a composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "Assuming that h is a linear function of the Cartesian product of u and v allows us to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi = ui * vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well-known tensor products (Smolensky 1990) and circular convolution (Plate 1995)."
        },
        {
            "Number": 15,
            "refer_ID": "P08-1028",
            "refer_sids": [
                190.0
            ],
            "refer_text": "We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We formulated a composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and pointwise multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)."
        },
        {
            "Number": 16,
            "refer_ID": "P08-1028",
            "refer_sids": [
                73.0
            ],
            "refer_text": "Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Relaxing the assumption of symmetry in the case of the simple additive model produces a model that weighs the contribution of the two components differently: This allows additive models to become more syntax-aware, since semantically important constituents can participate more actively in the composition.",
            "GPT_cite_text": "The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression."
        },
        {
            "Number": 17,
            "refer_ID": "P08-1028",
            "refer_sids": [
                99.0
            ],
            "refer_text": "In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.",
            "GPT_cite_text": "For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset."
        },
        {
            "Number": 18,
            "refer_ID": "P08-1028",
            "refer_sids": [
                189.0
            ],
            "refer_text": "In this paper we presented a general framework for vector-based semantic composition.",
            "cite_ID": "W11-1310",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use other WSM settings following Mitchell and Lapata (2008)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In this paper, we presented a general framework for vector-based semantic composition.",
            "GPT_cite_text": "We use other WSM settings following Mitchell and Lapata (2008)."
        },
        {
            "Number": 19,
            "refer_ID": "P08-1028",
            "refer_sids": [
                191.0
            ],
            "refer_text": "Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.",
            "cite_ID": "W11-1310",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.",
            "GPT_cite_text": "Mitchell and Lapata (2008) observed that a simple multiplication function modeled compositionality better than addition."
        },
        {
            "Number": 20,
            "refer_ID": "P08-1028",
            "refer_sids": [
                190.0
            ],
            "refer_text": "We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "W11-1310",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We formulated a composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "We use the compositionality functions, simple addition and simple multiplication, to build compositional vectors Vwr1+wr2 and Vwr1*wr2. These are as described in (Mitchell and Lapata, 2008)."
        }
    ],
    "W06-2932_vardha": [
        {
            "Number": 2,
            "refer_ID": "W06-2932",
            "refer_sids": [
                19
            ],
            "refer_text": "The first stage of our system creates an unlabeled parse y for an input sentence x.",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first stage of our system creates an unlabeled parse tree for an input sentence x.",
            "GPT_cite_text": "Introduce through post-processing, e.g., through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al., 2006)."
        },
        {
            "Number": 3,
            "refer_ID": "W06-2932",
            "refer_sids": [
                36
            ],
            "refer_text": "However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Table 5 shows the official results for submitted parser outputs.31 The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, in a two-stage system, we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.",
            "GPT_cite_text": "Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al. (2006) and Nivre et al. (2006)."
        },
        {
            "Number": 4,
            "refer_ID": "W06-2932",
            "refer_sids": [
                57
            ],
            "refer_text": "Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.",
            "GPT_cite_text": "Even though McDonald et al. (2006) and Nivre et al. (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences."
        },
        {
            "Number": 5,
            "refer_ID": "W06-2932",
            "refer_sids": [
                76
            ],
            "refer_text": "For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.",
            "cite_ID": "W08-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The high est score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (anda different policy regarding the inclusion of punctuation) .The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For instance, sequential labeling improves the labeling of objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.",
            "GPT_cite_text": "The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al. (2006) with a LAS of 87.34 based on the TIGER treebank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP), and labeled F-score (LF)."
        },
        {
            "Number": 6,
            "refer_ID": "W06-2932",
            "refer_sids": [
                54
            ],
            "refer_text": "Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.",
            "cite_ID": "W09-1210",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McDonald et al (2006) use an additional algorithm",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese, and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish, and Turkish.",
            "GPT_cite_text": "McDonald et al. (2006) use an additional algorithm."
        },
        {
            "Number": 7,
            "refer_ID": "W06-2932",
            "refer_sids": [
                104
            ],
            "refer_text": "We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.",
            "cite_ID": "W12-3407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005; McDonald and Pereira, 2006) generalizes well to languages other than English.",
            "GPT_cite_text": "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al., 2007b) and MST Parser (McDonald et al., 2006), two state-of-the-art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (McDonald and Nivre, 2007)."
        },
        {
            "Number": 8,
            "refer_ID": "W06-2932",
            "refer_sids": [
                12
            ],
            "refer_text": "In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.",
            "cite_ID": "I08-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)? s parser, (McDonald et al., 2006)? s parser, and so on",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.",
            "GPT_cite_text": "In fact, our approach can also be applied to other parsers, such as Yamada and Matsumoto's (2003) parser, McDonald et al.'s (2006) parser, and so on."
        },
        {
            "Number": 9,
            "refer_ID": "W06-2932",
            "refer_sids": [
                57
            ],
            "refer_text": "Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "But whereas the spanning tree parser of McDonald et al (2006) and the pseudo-projective parser of Nivre et al (2006) achieve this performance only with special preorpost-processing,7 the approach presented here derives a labeled non-projective graph in a single incremental process and hence at least has the advantage of simplicity",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.",
            "GPT_cite_text": "But whereas the spanning tree parser of McDonald et al. (2006) and the pseudo-projective parser of Nivre et al. (2006) achieve this performance only with special pre- or post-processing, the approach presented here derives a labeled non-projective graph in a single incremental process and hence at least has the advantage of simplicity."
        },
        {
            "Number": 10,
            "refer_ID": "W06-2932",
            "refer_sids": [
                58
            ],
            "refer_text": "These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Moreover, it has better time complexity than the approximate second-order spanning tree parsing of McDonald et al (2006), which has exponential complexity in the worst case (although this does not appear to be a problem in practice)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005; McDonald and Pereira, 2006) is easily adapted across all these languages.",
            "GPT_cite_text": "Moreover, it has better time complexity than the approximate second-order spanning tree parsing of McDonald et al. (2006), which has exponential complexity in the worst case (although this does not appear to be a problem in practice)."
        },
        {
            "Number": 11,
            "refer_ID": "W06-2932",
            "refer_sids": [
                22
            ],
            "refer_text": "An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We have shown that, for languages with a7McDonald et al (2006) use post-processing for non projective dependencies and for labeling",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that non-projective dependency parsing becomes NP-hard when features are extended beyond a single edge.",
            "GPT_cite_text": "We have shown that, for languages, McDonald et al. (2006) use post-processing for non-projective dependencies and for labeling."
        },
        {
            "Number": 12,
            "refer_ID": "W06-2932",
            "refer_sids": [
                41
            ],
            "refer_text": "To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.",
            "cite_ID": "D07-1122",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To model this, we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem. We use a first-order Markov factorization of the score s(l(i,jM), l(i,jM\u22121), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jM) and (i, jM\u22121) in the tree y.",
            "GPT_cite_text": "As described in (McDonald et al., 2006), we treat the labeling of dependencies as a sequence labeling problem."
        },
        {
            "Number": 13,
            "refer_ID": "W06-2932",
            "refer_sids": [
                21
            ],
            "refer_text": "That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.",
            "cite_ID": "W11-0314",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "ULISSE was tested against the output of two really different data? driven parsers: the first? order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.",
            "GPT_cite_text": "ULISSE was tested against the output of two really different data-driven parsers: the first-order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as the learning algorithm."
        },
        {
            "Number": 14,
            "refer_ID": "W06-2932",
            "refer_sids": [
                64
            ],
            "refer_text": "N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.",
            "cite_ID": "D07-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphological features/No morphological features. Assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.",
            "GPT_cite_text": "It should be noted that McDonald et al. (2006) use a richer feature set that is incomparable to our features."
        },
        {
            "Number": 16,
            "refer_ID": "W06-2932",
            "refer_sids": [
                21
            ],
            "refer_text": "That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.",
            "cite_ID": "D10-1069",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al, 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.",
            "GPT_cite_text": "The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al., 2007) and the second-order minimum spanning tree algorithm-based MstParser (McDonald et al., 2006)."
        },
        {
            "Number": 19,
            "refer_ID": "W06-2932",
            "refer_sids": [
                57
            ],
            "refer_text": "Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.",
            "cite_ID": "P08-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.",
            "GPT_cite_text": "The specific graph-based model studied in this work is that presented by McDonald et al. (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near-exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation. 2 2.3 Transition-Based Models"
        },
        {
            "Number": 20,
            "refer_ID": "W06-2932",
            "refer_sids": [
                43
            ],
            "refer_text": "For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi\u2019s algorithm.",
            "cite_ID": "P08-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l)? Rk, where f is typically a bi nary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For score functions, we use simple dot products between high-dimensional feature representations and a weight vector. Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi\u2019s algorithm.",
            "GPT_cite_text": "More precisely, dependency arcs (or pairs of arcs) are first represented by a high-dimensional feature vector f(i, j, l) \u2208 R^k, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al., 2005a; McDonald et al., 2006)."
        }
    ],
    "E03-1005_swastika": [
        {
            "Number": 2,
            "refer_ID": "E03-1005",
            "refer_sids": [
                105.0
            ],
            "refer_text": "The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.",
            "cite_ID": "N06-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Data-Oriented Parsing (DOP)? s methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest ranking parse, not derivation, that is desired",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP. Although Bod (2000b) reports that Simplicity-DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.",
            "GPT_cite_text": "Data-Oriented Parsing (DOP) methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest-ranking parse, not derivation, that is desired."
        },
        {
            "Number": 3,
            "refer_ID": "E03-1005",
            "refer_sids": [
                41.0
            ],
            "refer_text": "This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.",
            "cite_ID": "D07-1058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goodman? s transform, in com bi nation with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper presents the first published results with Goodman's PCFG reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ.",
            "GPT_cite_text": "Goodman\u2019s transform, in combination with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model."
        },
        {
            "Number": 4,
            "refer_ID": "E03-1005",
            "refer_sids": [
                80.0
            ],
            "refer_text": "DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).",
            "cite_ID": "D07-1058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Zuidema (2006a) shows that also the estimator (Bod, 2003) uses is biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al., 1999).",
            "GPT_cite_text": "Zuidema (2006a) shows that the estimator (Bod, 2003) is also biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees."
        },
        {
            "Number": 5,
            "refer_ID": "E03-1005",
            "refer_sids": [
                143.0
            ],
            "refer_text": "While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.",
            "cite_ID": "P11-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the DOP approach, which is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.",
            "GPT_cite_text": "Second, we compare against a composed-rule system, which is analogous to the Data-Oriented Parsing (DOP) approach in parsing (Bod, 2003)."
        },
        {
            "Number": 6,
            "refer_ID": "E03-1005",
            "refer_sids": [
                146.0
            ],
            "refer_text": "This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).",
            "cite_ID": "P04-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our best performing model is more accurate than all these previous models except (Bod, 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper also reaffirmed that the coarse-grained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic dependencies (as e.g. in Collins 1999 and Charniak 2000).",
            "GPT_cite_text": "Our best-performing model is more accurate than all these previous models except (Bod, 2003)."
        },
        {
            "Number": 7,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140.0
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "cite_ID": "P04-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current ac curacy level, and only 0.6% below the current best (Bod, 2003) .This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 and 14: an LP of 90.8% and an LR of 90.7%.",
            "GPT_cite_text": "We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003). This paper has also proposed a neural network training method which optimizes a discriminative criterion even when the parameters being estimated are those of a generative probability model."
        },
        {
            "Number": 12,
            "refer_ID": "E03-1005",
            "refer_sids": [
                105.0
            ],
            "refer_text": "The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.",
            "cite_ID": "W06-2905",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(henceforth, STSGs), which can represent single words, contiguous and noncontiguous MWEs, context-free rules or complete parse trees in a unified representation. My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP. Although Bod (2000b) reports that Simplicity-DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.",
            "GPT_cite_text": "(henceforth, STSGs), which can represent single words, contiguous and non-contiguous MWEs, context-free rules, or complete parse trees in a unified representation. My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003)."
        },
        {
            "Number": 14,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140.0
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This result is only slightly higher than the highest reported result for this test-set, Bod? s (.907) (Bod,2003)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 and 14: an LP of 90.8% and an LR of 90.7%.",
            "GPT_cite_text": "This result is only slightly higher than the highest reported result for this test set, Bod's (.907) (Bod, 2003)."
        },
        {
            "Number": 16,
            "refer_ID": "E03-1005",
            "refer_sids": [
                100.0
            ],
            "refer_text": "In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.",
            "cite_ID": "P07-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.",
            "GPT_cite_text": "This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well."
        },
        {
            "Number": 17,
            "refer_ID": "E03-1005",
            "refer_sids": [
                30.0
            ],
            "refer_text": "Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.",
            "cite_ID": "P07-1051",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Bod (2000a) solved this problem by training the subtree probabilities using a maximum likelihood procedure based on Expectation-Maximization.",
            "GPT_cite_text": "But equally important is the fact that this new DOP model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)."
        },
        {
            "Number": 19,
            "refer_ID": "E03-1005",
            "refer_sids": [
                140.0
            ],
            "refer_text": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
            "cite_ID": "W04-0305",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The highest accuracy is obtained by SL-DOP at 12 and 14: an LP of 90.8% and an LR of 90.7%.",
            "GPT_cite_text": "A moderately larger vocabulary version (4,215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003)."
        },
        {
            "Number": 20,
            "refer_ID": "E03-1005",
            "refer_sids": [
                30.0
            ],
            "refer_text": "Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.",
            "cite_ID": "P06-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The probability of a parse tree T is the sum of the 1 This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Bod (2000a) solved this problem by training the subtree probabilities using a maximum likelihood procedure based on Expectation-Maximization.",
            "GPT_cite_text": "The probability of a parse tree T is the sum of the subtree probabilities, which are adjusted by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)."
        }
    ],
    "P08-1102_aakansha": [
        {
            "Number": 1,
            "refer_ID": "P08-1102",
            "refer_sids": [
                130
            ],
            "refer_text": "We proposed a cascaded linear model for Chinese Joint S&T.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We proposed a cascaded linear model for Chinese Joint Science and Technology.",
            "GPT_cite_text": "Following Jiang et al. (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C1: n = C1 C2."
        },
        {
            "Number": 3,
            "refer_ID": "P08-1102",
            "refer_sids": [
                42
            ],
            "refer_text": "As predications generated from such templates depend on the current character, we name these templates lexical-target.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "plates called lexical-target in the column below areintroduced by Jiang et al (2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As predictions generated from such templates depend on the current character, we name these templates lexical-target.",
            "GPT_cite_text": "Plates called lexical targets in the column below are introduced by Jiang et al. (2008)."
        },
        {
            "Number": 4,
            "refer_ID": "P08-1102",
            "refer_sids": [
                25
            ],
            "refer_text": "According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.",
            "cite_ID": "P12-1110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "According to Ng and Low (2004), the segmentation task can be transformed into a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation results by splitting the labeled result into subsequences of patterns s or bm*e, which denote single-character words and multi-character words, respectively.",
            "GPT_cite_text": "For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j."
        },
        {
            "Number": 5,
            "refer_ID": "P08-1102",
            "refer_sids": [
                130
            ],
            "refer_text": "We proposed a cascaded linear model for Chinese Joint S&T.",
            "cite_ID": "D12-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We proposed a cascaded linear model for Chinese Joint Science and Technology.",
            "GPT_cite_text": "Jiang et al. (2008) propose a cascaded linear model for joint Chinese word segmentation and POS tagging."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1102",
            "refer_sids": [
                34
            ],
            "refer_text": "The feature templates we adopted are selected from those of Ng and Low (2004).",
            "cite_ID": "C10-1135",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the feature templates the same as Jiang et al, (2008) to extract features form E model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The feature templates we adopted are selected from those of Ng and Low (2004).",
            "GPT_cite_text": "We use the feature templates the same as Jiang et al. (2008) to extract features from the E model."
        },
        {
            "Number": 8,
            "refer_ID": "P08-1102",
            "refer_sids": [
                12
            ],
            "refer_text": "Besides the usual character-based features, additional features dependent on POS\u2019s or words can also be employed to improve the performance.",
            "cite_ID": "P12-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "approach, where basic processing units are characters which compose words (Jiangetal., 2008a)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Besides the usual character-based features, additional features dependent on POS or words can also be employed to improve the performance.",
            "GPT_cite_text": "Approach, where basic processing units are characters that compose words (Jiang et al., 2008a)."
        },
        {
            "Number": 10,
            "refer_ID": "P08-1102",
            "refer_sids": [
                130
            ],
            "refer_text": "We proposed a cascaded linear model for Chinese Joint S&T.",
            "cite_ID": "C10-2096",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We proposed a cascaded linear model for Chinese Joint Science and Technology.",
            "GPT_cite_text": "6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1102",
            "refer_sids": [
                130
            ],
            "refer_text": "We proposed a cascaded linear model for Chinese Joint S&T.",
            "cite_ID": "C10-2096",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We proposed a cascaded linear model for Chinese Joint Science and Technology.",
            "GPT_cite_text": "6.1.2 Lattice-Forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1102",
            "refer_sids": [
                121
            ],
            "refer_text": "Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Among other features, the 4-gram POS LM plays the most important role; removing this feature causes an F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.",
            "GPT_cite_text": "However, when we repeat the work of (Jiang et al., 2008), which reports achieving state-of-the-art performance in the datasets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)."
        },
        {
            "Number": 13,
            "refer_ID": "P08-1102",
            "refer_sids": [
                37
            ],
            "refer_text": "C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "C represents a Chinese character, while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).",
            "GPT_cite_text": "Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appear twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al., 2008), with n=0, generates [C0C0])"
        },
        {
            "Number": 14,
            "refer_ID": "P08-1102",
            "refer_sids": [
                73
            ],
            "refer_text": "For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For instance, if the word w appears N times in the training corpus and is labeled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: the probability Pr(w|t) could be estimated through the same approach.",
            "GPT_cite_text": "As all the features adopted in (Jiang et al., 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle."
        },
        {
            "Number": 15,
            "refer_ID": "P08-1102",
            "refer_sids": [
                46
            ],
            "refer_text": "Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) \u2208 X \u00d7 Y to a feature vector 4)(x, y) \u2208 Rd, and a parameter vector \u03b1\ufffd \u2208 Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following Collins, we use a function GEN(x) generating all candidate results of an input x, a representation \u03c6 mapping each training example (x, y) \u2208 X \u00d7 Y to a feature vector \u03c6(x, y) \u2208 \u211dd, and a parameter vector \u03b1 \u2208 \u211dd corresponding to the feature vector. d means the dimension of the vector space; it equals the amount of features in the model.",
            "GPT_cite_text": "Inspired by (Jiang et al., 2008), we set the real d. Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1, C0, and C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model."
        },
        {
            "Number": 17,
            "refer_ID": "P08-1102",
            "refer_sids": [
                12
            ],
            "refer_text": "Besides the usual character-based features, additional features dependent on POS\u2019s or words can also be employed to improve the performance.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Besides the usual character-based features, additional features dependent on POS or words can also be employed to improve the performance.",
            "GPT_cite_text": "Last, (Jiang et al., 2008) adds repeated features implicitly based on (Ng and Low, 2004)."
        },
        {
            "Number": 20,
            "refer_ID": "P08-1102",
            "refer_sids": [
                130
            ],
            "refer_text": "We proposed a cascaded linear model for Chinese Joint S&T.",
            "cite_ID": "D12-1046",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We proposed a cascaded linear model for Chinese Joint Science and Technology.",
            "GPT_cite_text": "Previous joint models mainly focus on word segmentation and POS tagging tasks, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), and re-ranking (Jiang et al., 2008b)."
        }
    ],
    "P87-1015_vardha": [
        {
            "Number": 1,
            "refer_ID": "P87-1015",
            "refer_sids": [
                165
            ],
            "refer_text": "This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.",
            "cite_ID": "P01-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach that Vijay-Shanker et al (1987) and Weir (1988) take, elaborated on by Becker et al (1992), is to identify a very general class of formalisms, which they call linear context free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This class of formalisms has the properties that their derivation trees are local sets and manipulate objects using a finite number of composition operations that use a finite number of symbols.",
            "GPT_cite_text": "The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear context-free rewriting systems (CFRSs), and define for this class a large space of structural descriptions that serves as a common ground in which the strong generative capacities of these formalisms can be compared."
        },
        {
            "Number": 2,
            "refer_ID": "P87-1015",
            "refer_sids": [
                119
            ],
            "refer_text": "In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.",
            "cite_ID": "E09-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Here we use the standard definition of LCFRS (Vijay-Shanker et al, 1987) and only fix our notation; for a more thorough discussion of this formal ism, we refer to the literature. Let G be an LCFRS",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In defining LCFRS, we hope to generalize the definition of CFGs to formalisms manipulating any structure, e.g., strings, trees, or graphs.",
            "GPT_cite_text": "Here we use the standard definition of LCFRS (Vijay-Shanker et al., 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. Let G be an LCFRS."
        },
        {
            "Number": 3,
            "refer_ID": "P87-1015",
            "refer_sids": [
                3
            ],
            "refer_text": "Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.",
            "cite_ID": "W07-2214",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "There are many (structural) mildly context sensitive grammar formalisms ,e.g .mcfg ,lcfrs, mg, and they have been shown to be equivalent (Vijay-Shanker et al., 1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalisms.",
            "GPT_cite_text": "There are many (structural) mildly context-sensitive grammar formalisms, e.g., MCFG, LCFRS, MG, and they have been shown to be equivalent (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 4,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "P09-2003",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al, 1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 5,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "P09-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following this line, (Vijay-Shanker et al, 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "Following this line, Vijay-Shanker et al. (1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years from the community."
        },
        {
            "Number": 6,
            "refer_ID": "P87-1015",
            "refer_sids": [
                133
            ],
            "refer_text": "To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.",
            "cite_ID": "P09-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al, 1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To show that the derivation trees of any grammar in LCFRS are a local set, we can rewrite the annotated derivation trees such that every node is labeled by a pair to include the composition operations.",
            "GPT_cite_text": "We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 7,
            "refer_ID": "P87-1015",
            "refer_sids": [
                138
            ],
            "refer_text": "We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.",
            "cite_ID": "P07-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We write REGD.k/ to refer to the class of regular dependency languages with a gap-degree bounded by k. Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al, 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We can show that languages generated by LCFRSs are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.",
            "GPT_cite_text": "We write REGD.k/ to refer to the class of regular dependency languages with a gap-degree bounded by k. Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms."
        },
        {
            "Number": 8,
            "refer_ID": "P87-1015",
            "refer_sids": [
                156
            ],
            "refer_text": "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.",
            "cite_ID": "N09-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006) .In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al, 1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Giving a recognition algorithm for LCFRL involves describing the substrings of the input that are spanned by the structures derived by the LCFRS and how the composition operation combines these substrings.",
            "GPT_cite_text": "This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single-language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 9,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "N09-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al (1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al. (1987)."
        },
        {
            "Number": 10,
            "refer_ID": "P87-1015",
            "refer_sids": [
                119
            ],
            "refer_text": "In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.",
            "cite_ID": "W10-1407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "LCFRS (Vijay-Shanker et al, 1987) are anatural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In defining LCFRS, we hope to generalize the definition of CFGs to formalisms manipulating any structure, e.g., strings, trees, or graphs.",
            "GPT_cite_text": "LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals."
        },
        {
            "Number": 11,
            "refer_ID": "P87-1015",
            "refer_sids": [
                133
            ],
            "refer_text": "To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.",
            "cite_ID": "W10-1407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A LCFRS (Vijay-Shanker et al, 1987) is a tuple G= (N, T, V, P, S )wherea) N is a finite set of non-terminals with a function dim: N? N that determines the fan-out of each A? N; b) T and V are disjoint finite sets of terminals and variables; c) S? N is the start symbol with dim (S)= 1; d) P is a finite set of rewriting rules A (? 1,..",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To show that the derivation trees of any grammar in LCFRS are a local set, we can rewrite the annotated derivation trees such that every node is labeled by a pair to include the composition operations.",
            "GPT_cite_text": "A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N \u2192 N that determines the fan-out of each A \u2208 N; b) T and V are disjoint finite sets of terminals and variables; c) S \u2208 N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules A (\u2208 1, ..."
        },
        {
            "Number": 12,
            "refer_ID": "P87-1015",
            "refer_sids": [
                164
            ],
            "refer_text": "Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al (1987))",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Although embedding this version of LCFRS in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them by defining a class of related formalisms.",
            "GPT_cite_text": "In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes, 1997) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 13,
            "refer_ID": "P87-1015",
            "refer_sids": [
                204
            ],
            "refer_text": "The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "2 By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of deriva tional structures are all regular (Vijay-Shanker et al., 1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The similarities become apparent when they are studied at the level of derivation structures: derivation net sets of CFGs, HGs, TAGs, and MCTAGs are all local sets.",
            "GPT_cite_text": "By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 14,
            "refer_ID": "P87-1015",
            "refer_sids": [
                9
            ],
            "refer_text": "We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al, 1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We examine both the complexity of the paths of trees in the tree sets and the kinds of dependencies that the formalisms can impose between paths.",
            "GPT_cite_text": "It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 15,
            "refer_ID": "P87-1015",
            "refer_sids": [
                28
            ],
            "refer_text": "A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.",
            "cite_ID": "N10-1035",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi,1985), including, among several others, the tree ad joining grammars (TAGs) of Joshi et al (1975) .Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings ,i.e., discontinuous phrases",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.",
            "GPT_cite_text": "On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting systems (LCFRS), introduced by Vijay-Shanker et al. (1987), are mildly context-sensitive formalisms that allow the derivation of tuples of strings, i.e., discontinuous phrases."
        },
        {
            "Number": 16,
            "refer_ID": "P87-1015",
            "refer_sids": [
                138
            ],
            "refer_text": "We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.",
            "cite_ID": "P12-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al (1991) .3 Thus, CFTG are mildly context-sensitive since their generated string languages are semi-linear and can be parsed in polynomial time (Go ?mez-Rodr? ?guez et al, 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We can show that languages generated by LCFRSs are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.",
            "GPT_cite_text": "CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al. (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al. (1991). Thus, CFTG are mildly context-sensitive since their generated string languages are semi-linear and can be parsed in polynomial time (G\u00f3mez-Rodr\u00edguez et al., 2010)."
        }
    ],
    "P08-1102_sweta": [
        {
            "Number": 1,
            "refer_ID": "P08-1102",
            "refer_sids": [
                21
            ],
            "refer_text": "Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and joint S&T.",
            "GPT_cite_text": "Following Jiang et al. (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C1: n = C1 C2."
        },
        {
            "Number": 2,
            "refer_ID": "P08-1102",
            "refer_sids": [
                28
            ],
            "refer_text": "A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "A subsequence of boundary-POS labeling results indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal t. For example, a tag sequence b NN m NN e NN represents a three-character word with POS tag NN.",
            "GPT_cite_text": "As described in Ng and Low (2004) and Jiang et al. (2008), we use s to indicate a single character word, while b, m, and e indicate the begin, middle, and end of a word respectively."
        },
        {
            "Number": 3,
            "refer_ID": "P08-1102",
            "refer_sids": [
                43
            ],
            "refer_text": "Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "plates called lexical-target in the column below areintroduced by Jiang et al (2008)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Note that the templates of Ng and Low (2004) already contain some lexical-target ones.",
            "GPT_cite_text": "Plates called lexical targets in the column below are introduced by Jiang et al. (2008)."
        },
        {
            "Number": 4,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "P12-1110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and joint S&T.",
            "GPT_cite_text": "For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j."
        },
        {
            "Number": 5,
            "refer_ID": "P08-1102",
            "refer_sids": [
                9
            ],
            "refer_text": "To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).",
            "cite_ID": "D12-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "To segment and tag a character sequence, there are two strategies to choose from: performing POS tagging following segmentation, or joint segmentation and POS tagging (Joint S&T).",
            "GPT_cite_text": "Jiang et al. (2008) propose a cascaded linear model for joint Chinese word segmentation and POS tagging."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1102",
            "refer_sids": [
                33,
                34
            ],
            "refer_text": "In following subsections, we describe the feature templates and the perceptron training algorithm.\nThe feature templates we adopted are selected from those of Ng and Low (2004).",
            "cite_ID": "C10-1135",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the feature templates the same as Jiang et al, (2008) to extract features form E model",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "In the following subsections, we describe the feature templates and the perceptron training algorithm. The feature templates we adopted are selected from those of Ng and Low (2004).",
            "GPT_cite_text": "We use the feature templates the same as Jiang et al. (2008) to extract features from the E model."
        },
        {
            "Number": 8,
            "refer_ID": "P08-1102",
            "refer_sids": [
                12
            ],
            "refer_text": "Besides the usual character-based features, additional features dependent on POS\u2019s or words can also be employed to improve the performance.",
            "cite_ID": "P12-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "approach, where basic processing units are characters which compose words (Jiangetal., 2008a)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Besides the usual character-based features, additional features dependent on POS or words can also be employed to improve the performance.",
            "GPT_cite_text": "Approach, where basic processing units are characters that compose words (Jiang et al., 2008a)."
        },
        {
            "Number": 10,
            "refer_ID": "P08-1102",
            "refer_sids": [
                64
            ],
            "refer_text": "In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.",
            "cite_ID": "C10-2096",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "In our experiments, we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of state transition probabilities in HMM, and a word-POS co-occurrence model describing how probable a word sequence coexists with a POS sequence.",
            "GPT_cite_text": "6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1102",
            "refer_sids": [
                79
            ],
            "refer_text": "By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.",
            "cite_ID": "C10-2096",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labeled results of subsequence C1:i during decoding.",
            "GPT_cite_text": "6.1.2 Lattice-Forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1102",
            "refer_sids": [
                96
            ],
            "refer_text": "In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chose 2,000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84,294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both kinds of features.",
            "GPT_cite_text": "However, when we repeat the work of (Jiang et al., 2008), which reports achieving state-of-the-art performance in the datasets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)."
        },
        {
            "Number": 13,
            "refer_ID": "P08-1102",
            "refer_sids": [
                91
            ],
            "refer_text": "The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "The first was conducted to test the performance of the perceptron on segmentation of the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU), and the Microsoft Research Corpus (MSR).",
            "GPT_cite_text": "Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appear twice, which are generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al., 2008), with n=0, generates [C0C0])"
        },
        {
            "Number": 14,
            "refer_ID": "P08-1102",
            "refer_sids": [
                16
            ],
            "refer_text": "Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Shown in Figure 1, the cascaded model has a two-layer architecture, with a character-based perceptron as the core combined with other real-valued features such as language models.",
            "GPT_cite_text": "As all the features adopted in (Jiang et al., 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle."
        },
        {
            "Number": 15,
            "refer_ID": "P08-1102",
            "refer_sids": [
                35
            ],
            "refer_text": "To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "To compare with others conveniently, we excluded the ones forbidden by the closed test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation mark.",
            "GPT_cite_text": "Inspired by (Jiang et al., 2008), we set the real d. Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1, C0, and C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model."
        },
        {
            "Number": 17,
            "refer_ID": "P08-1102",
            "refer_sids": [
                91
            ],
            "refer_text": "We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.",
            "cite_ID": "D12-1046",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We propose a cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.",
            "GPT_cite_text": "Previous joint models mainly focus on word segmentation and POS tagging tasks, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), and re-ranking (Jiang et al., 2008b)."
        }
    ],
    "P08-1043_sweta": [
        {
            "Number": 1,
            "refer_ID": "P08-1043",
            "refer_sids": [
                156
            ],
            "refer_text": "To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).",
            "cite_ID": "C10-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "To evaluate the performance on the segmentation task, we report SEG, the standard harmonic mean for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).",
            "GPT_cite_text": "Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew."
        },
        {
            "Number": 2,
            "refer_ID": "P08-1043",
            "refer_sids": [
                4
            ],
            "refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "cite_ID": "P11-1141",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-token handling technique, our model outperforms previous pipelined, integrated, or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models."
        },
        {
            "Number": 3,
            "refer_ID": "P08-1043",
            "refer_sids": [
                70
            ],
            "refer_text": "Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.",
            "cite_ID": "P10-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of He brew, based on lattice parsing",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Each lattice arc corresponds to a segment and its corresponding POS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.",
            "GPT_cite_text": "Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging, and parsing of Hebrew, based on lattice parsing."
        },
        {
            "Number": 4,
            "refer_ID": "P08-1043",
            "refer_sids": [
                3
            ],
            "refer_text": "Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.",
            "cite_ID": "P11-1089",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) pro pose a generative joint model",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation, which bypasses the associated circularity.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) propose a generative joint model."
        },
        {
            "Number": 5,
            "refer_ID": "P08-1043",
            "refer_sids": [
                51
            ],
            "refer_text": "Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.",
            "cite_ID": "W10-1404",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general-purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1043",
            "refer_sids": [
                107
            ],
            "refer_text": "Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.",
            "cite_ID": "P11-2124",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrewtext",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple POS tags.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text."
        },
        {
            "Number": 7,
            "refer_ID": "P08-1043",
            "refer_sids": [
                14
            ],
            "refer_text": "The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).",
            "cite_ID": "P11-2124",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The input for the segmentation task is, however, highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses, as in (Bar-Haim et al., 2007; Adler and Elhadad, 2006).",
            "GPT_cite_text": "Following Goldberg and Tsarfaty (2008), we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice."
        },
        {
            "Number": 8,
            "refer_ID": "P08-1043",
            "refer_sids": [
                33
            ],
            "refer_text": "The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).",
            "cite_ID": "P12-2002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "2The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The current work treats both segmental and suprasegmental phenomena, yet we note that there may be more adequate ways to treat suprasegmental phenomena assuming word-based morphology as we explore in (Tsarfaty and Goldberg, 2008).",
            "GPT_cite_text": "The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008)."
        },
        {
            "Number": 9,
            "refer_ID": "P08-1043",
            "refer_sids": [
                53
            ],
            "refer_text": "Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.",
            "cite_ID": "D12-1046",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A study that is closely related toours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model that handles both tasks.",
            "GPT_cite_text": "A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew."
        },
        {
            "Number": 10,
            "refer_ID": "P08-1043",
            "refer_sids": [
                48
            ],
            "refer_text": "Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.",
            "cite_ID": "D12-1133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5,000-sentence treebank.",
            "GPT_cite_text": "Models that, in addition, incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1043",
            "refer_sids": [
                141
            ],
            "refer_text": "This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted as Nohsp, Parser, and Grammar. We used BitPar (Schmid, 2004), an efficient general-purpose parser, together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.",
            "GPT_cite_text": "4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1043",
            "refer_sids": [
                188
            ],
            "refer_text": "The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is the same grammar as described in (Goldberg and Tsarfaty, 2008)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone application results.",
            "GPT_cite_text": "It is the same grammar as described in (Goldberg and Tsarfaty, 2008)."
        },
        {
            "Number": 13,
            "refer_ID": "P08-1043",
            "refer_sids": [
                94
            ],
            "refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a priori equally likely; the only reason to prefer one segmentation over another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "GPT_cite_text": "Several studies followed this line (Cohen and Smith, 2007), the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task."
        },
        {
            "Number": 14,
            "refer_ID": "P08-1043",
            "refer_sids": [
                134
            ],
            "refer_text": "Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with that of the Hebrew Treebank. For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the treebank."
        },
        {
            "Number": 15,
            "refer_ID": "P08-1043",
            "refer_sids": [
                156
            ],
            "refer_text": "To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "To evaluate the performance on the segmentation task, we report SEG, the standard harmonic mean for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).",
            "GPT_cite_text": "The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to construct a lattice for each input token."
        },
        {
            "Number": 16,
            "refer_ID": "P08-1043",
            "refer_sids": [
                5
            ],
            "refer_text": "Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, \u2018tokens\u2019) that constitute the unanalyzed surface forms (utterances).",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parse val to use characters instead of space-delimited tokens as its basic units",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse trees) and the space-delimited tokens (henceforth, \u2018tokens\u2019) that constitute the unanalyzed surface forms (utterances).",
            "GPT_cite_text": "Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units."
        }
    ],
    "P05-1013_vardha": [
        {
            "Number": 1,
            "refer_ID": "P05-1013",
            "refer_sids": [
                20
            ],
            "refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.",
            "cite_ID": "W05-1505",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques.",
            "GPT_cite_text": "Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the nonterminals of constituents during parsing (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 2,
            "refer_ID": "P05-1013",
            "refer_sids": [
                9
            ],
            "refer_text": "This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).",
            "cite_ID": "P08-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "1http: //sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)? s dependencyparser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).",
            "GPT_cite_text": "1 http://sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)'s dependency parser, based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 3,
            "refer_ID": "P05-1013",
            "refer_sids": [
                104
            ],
            "refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.",
            "cite_ID": "W10-1401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than that of the best projective parsers.",
            "GPT_cite_text": "Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of Nivre and Nilsson (2005) improves accuracy for dependency parsing of Basque."
        },
        {
            "Number": 4,
            "refer_ID": "P05-1013",
            "refer_sids": [
                104
            ],
            "refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.",
            "cite_ID": "P12-3029",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For tree banks with non-projective trees weuse the pseudo-projective parsing technique to trans form the tree bank into projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than that of the best projective parsers.",
            "GPT_cite_text": "For tree banks with non-projective trees, we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 5,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109
            ],
            "refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "W10-1403",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 6,
            "refer_ID": "P05-1013",
            "refer_sids": [
                95
            ],
            "refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.",
            "cite_ID": "D08-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson,2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point in attachment score.",
            "GPT_cite_text": "To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which non-projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non-projective links at parse time."
        },
        {
            "Number": 7,
            "refer_ID": "P05-1013",
            "refer_sids": [
                20
            ],
            "refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.",
            "cite_ID": "D07-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": ",wn in O (n) time, producing a projective dependency graph satisfying conditions 1? 4 in section 2.1, possibly after adding arcs (0, i ,lr) for every node i 6= 0 that is a root in the output graph (where lr is a special label for root modifiers) .Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to pre process training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods ,e.g., memory-based learning or support vector machines",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques.",
            "GPT_cite_text": "In O(n) time, producing a projective dependency graph satisfying conditions 1-4 in section 2.1, possibly after adding arcs (0, i, lr) for every node i \u2260 0 that is a root in the output graph (where lr is a special label for root modifiers). Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods, e.g., memory-based learning or support vector machines."
        },
        {
            "Number": 8,
            "refer_ID": "P05-1013",
            "refer_sids": [
                36
            ],
            "refer_text": "As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.",
            "cite_ID": "D07-1119",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As observed by Kahane et al. (1998), any (non-projective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014* wj holds in the original graph.",
            "GPT_cite_text": "For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists of lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective."
        },
        {
            "Number": 9,
            "refer_ID": "P05-1013",
            "refer_sids": [
                62
            ],
            "refer_text": "In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Nova? k, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs, previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).",
            "GPT_cite_text": "Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non-projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Novak, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)."
        },
        {
            "Number": 10,
            "refer_ID": "P05-1013",
            "refer_sids": [
                104
            ],
            "refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.",
            "cite_ID": "W09-1207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "troduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than that of the best projective parsers.",
            "GPT_cite_text": "Introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German, and English."
        },
        {
            "Number": 11,
            "refer_ID": "P05-1013",
            "refer_sids": [
                23
            ],
            "refer_text": "By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.",
            "cite_ID": "E09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "non projective (Nivre and Nilsson, 2005), we char ac terise a sense in which the structures appearing in tree banks can be viewed as being only? slightly? ill-nested",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise to non-projective structures. The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence.",
            "GPT_cite_text": "Non-projective (Nivre and Nilsson, 2005), we characterize a sense in which the structures appearing in treebanks can be viewed as being only slightly ill-nested."
        },
        {
            "Number": 12,
            "refer_ID": "P05-1013",
            "refer_sids": [
                104
            ],
            "refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.",
            "cite_ID": "W09-1218",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than that of the best projective parsers.",
            "GPT_cite_text": "In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformations (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 13,
            "refer_ID": "P05-1013",
            "refer_sids": [
                95
            ],
            "refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point in the attachment score.",
            "GPT_cite_text": "Pseudo-projective parsing for recovering non-projective structures (Nivre and Nilsson, 2005)"
        },
        {
            "Number": 14,
            "refer_ID": "P05-1013",
            "refer_sids": [
                96
            ],
            "refer_text": "With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.",
            "GPT_cite_text": "Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (Section 3.4)."
        },
        {
            "Number": 15,
            "refer_ID": "P05-1013",
            "refer_sids": [
                95
            ],
            "refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point in attachment score.",
            "GPT_cite_text": "Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser."
        },
        {
            "Number": 16,
            "refer_ID": "P05-1013",
            "refer_sids": [
                40
            ],
            "refer_text": "Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Weprojectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r? h, where r is the original label and h is the label of the original head in the non-projective dependency graph",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (non-projective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).",
            "GPT_cite_text": "We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r/h, where r is the original label and h is the label of the original head in the non-projective dependency graph."
        },
        {
            "Number": 17,
            "refer_ID": "P05-1013",
            "refer_sids": [
                7
            ],
            "refer_text": "From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "From the point of view of computational implementation, this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and, in practice, also accuracy and robustness.",
            "GPT_cite_text": "For tree banks with non-projective trees, we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 18,
            "refer_ID": "P05-1013",
            "refer_sids": [
                14
            ],
            "refer_text": "While the proportion of sentences containing non-projective dependencies is often 15\u201325%, the total proportion of non-projective arcs is normally only 1\u20132%.",
            "cite_ID": "P11-2121",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "While the proportion of sentences containing non-projective dependencies is often 15\u201325%, the total proportion of non-projective arcs is normally only 1\u20132%.",
            "GPT_cite_text": "Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing in all cases."
        },
        {
            "Number": 19,
            "refer_ID": "P05-1013",
            "refer_sids": [
                49
            ],
            "refer_text": "The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d\u2191h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.",
            "cite_ID": "E06-1010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Itshould be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in them selves (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types. In the first encoding scheme, called Head, we use a new label d\u2191h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.",
            "GPT_cite_text": "It should be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in themselves (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 20,
            "refer_ID": "P05-1013",
            "refer_sids": [
                104
            ],
            "refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.",
            "cite_ID": "D07-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective trans formations as described in (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The overall parsing accuracy obtained with the pseudo-projective approach is still lower than that of the best projective parsers.",
            "GPT_cite_text": "The resulting algorithm is projective, and non-projectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005)."
        }
    ],
    "W11-2123_swastika": [
        {
            "Number": 1,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W11-2138",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "We used common tools for phrase-based translation: Moses (Koehn et al., 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modeling and GIZA++ (Och and Ney, 2000) for word alignments."
        },
        {
            "Number": 2,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "P14-2022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run."
        },
        {
            "Number": 3,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "W12-3145",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "Thus, given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) <s> tf, (ii) tf, and (iii) tf </s> and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state information in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states."
        },
        {
            "Number": 4,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "Our translation system uses cdec (Dyer et al., 2010), an implementation of the hierarchical phrase-based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference."
        },
        {
            "Number": 5,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "The three data sets in use in this paper are summarised in Table 1. The translation systems consisted of phrase tables and lexicalized reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime."
        },
        {
            "Number": 6,
            "refer_ID": "W11-2123",
            "refer_sids": [
                52
            ],
            "refer_text": "Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.",
            "cite_ID": "P12-2058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.",
            "GPT_cite_text": "The features used are basic lexical features, word penalty, and a 3-gram language model (Heafield, 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W11-2139",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Inference was carried out using the language modeling library described by Heafield (2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "Inference was carried out using the language modeling library described by Heafield (2011)."
        },
        {
            "Number": 8,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "P13-2003",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "We used the MADA ATB segmentation for Arabic (Roth et al., 2008) and true casing for English, phrases of maximal length 7, Kneser-Ney smoothing, and lexicalized reordering (Koehn et al., 2005), and a 5-gram language model, trained on GigaWord v.5 using KenLM (Heafield, 2011)."
        },
        {
            "Number": 9,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011); both language model implementations are now integrated with Joshua."
        },
        {
            "Number": 10,
            "refer_ID": "W11-2123",
            "refer_sids": [
                177
            ],
            "refer_text": "However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.",
            "GPT_cite_text": "Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets."
        },
        {
            "Number": 11,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult-to-compile SRILM toolkit (Stolcke, 2002)."
        },
        {
            "Number": 12,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3160",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This was used to create a KenLM (Heafield, 2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "This was used to create a KenLM (Heafield, 2011)."
        },
        {
            "Number": 13,
            "refer_ID": "W11-2123",
            "refer_sids": [
                200
            ],
            "refer_text": "The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.",
            "cite_ID": "W12-3706",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.",
            "GPT_cite_text": "In the Opinum system, we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application."
        },
        {
            "Number": 14,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "W11-2147",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "Our baseline is a factored phrase-based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modeling, and minimum error rate training (Och, 2003) to tune model feature weights."
        },
        {
            "Number": 15,
            "refer_ID": "W11-2123",
            "refer_sids": [
                200
            ],
            "refer_text": "The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.",
            "cite_ID": "E12-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.",
            "GPT_cite_text": "For language modeling, we computed 5-gram models using IRSTLM (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)."
        },
        {
            "Number": 16,
            "refer_ID": "W11-2123",
            "refer_sids": [
                200
            ],
            "refer_text": "The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.",
            "cite_ID": "P12-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.",
            "GPT_cite_text": "Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima \u010can, 2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well. 3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using KenLM (Heafield, 2011)."
        },
        {
            "Number": 17,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "D12-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3."
        },
        {
            "Number": 18,
            "refer_ID": "W11-2123",
            "refer_sids": [
                278
            ],
            "refer_text": "We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.",
            "cite_ID": "P12-2006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.",
            "GPT_cite_text": "Research efforts to increase search efficiency for phrase-based MT (Koehn et al., 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al., 2006) to additional early pruning techniques (Delaney et al., 2006), (Moore and Quirk, 2007), and more efficient language model (LM) querying (Heafield, 2011)."
        },
        {
            "Number": 19,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "P13-2073",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "For English language modeling, we use the English Gigaword Corpus with a 5-gram LM using the KenLM toolkit (Heafield, 2011)."
        },
        {
            "Number": 20,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "P13-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing."
        }
    ],
    "P05-1013_swastika": [
        {
            "Number": 1,
            "refer_ID": "P05-1013",
            "refer_sids": [
                49.0
            ],
            "refer_text": "The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d\u2191h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.",
            "cite_ID": "W05-1505",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types. In the first encoding scheme, called Head, we use a new label d\u2191h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.",
            "GPT_cite_text": "Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the nonterminals of constituents during parsing (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 2,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109.0
            ],
            "refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "P08-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "1http: //sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)? s dependencyparser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "1 http://sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)'s dependency parser, based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 3,
            "refer_ID": "P05-1013",
            "refer_sids": [
                95.0
            ],
            "refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.",
            "cite_ID": "W10-1401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point in attachment score.",
            "GPT_cite_text": "Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of Nivre and Nilsson (2005) improves accuracy for dependency parsing of Basque."
        },
        {
            "Number": 4,
            "refer_ID": "P05-1013",
            "refer_sids": [
                79.0
            ],
            "refer_text": "In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.",
            "cite_ID": "P12-3029",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For tree banks with non-projective trees weuse the pseudo-projective parsing technique to trans form the tree bank into projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in Section 2.",
            "GPT_cite_text": "For tree banks with non-projective trees, we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 5,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109.0
            ],
            "refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "W10-1403",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 6,
            "refer_ID": "P05-1013",
            "refer_sids": [
                38.0
            ],
            "refer_text": "Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.",
            "cite_ID": "D08-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson,2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Projectivizing a dependency graph by lifting non-projective arcs is a nondeterministic operation in the general case.",
            "GPT_cite_text": "To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which non-projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non-projective links at parse time."
        },
        {
            "Number": 7,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109.0
            ],
            "refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "D07-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": ",wn in O (n) time, producing a projective dependency graph satisfying conditions 1? 4 in section 2.1, possibly after adding arcs (0, i ,lr) for every node i 6= 0 that is a root in the output graph (where lr is a special label for root modifiers) .Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to pre process training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods ,e.g., memory-based learning or support vector machines",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "In O(n) time, producing a projective dependency graph satisfying conditions 1-4 in section 2.1, possibly after adding arcs (0, i, lr) for every node i \u2260 0 that is a root in the output graph (where lr is a special label for root modifiers). Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods, e.g., memory-based learning or support vector machines."
        },
        {
            "Number": 8,
            "refer_ID": "P05-1013",
            "refer_sids": [
                79.0
            ],
            "refer_text": "In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.",
            "cite_ID": "D07-1119",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in Section 2.",
            "GPT_cite_text": "For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists of lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective."
        },
        {
            "Number": 9,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109.0
            ],
            "refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Nova? k, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non-projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Novak, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)."
        },
        {
            "Number": 10,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109.0
            ],
            "refer_text": "S sid=\"109\" ssid=\"1\">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "W09-1207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "troduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "Introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German, and English."
        },
        {
            "Number": 11,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109.0
            ],
            "refer_text": "S sid=\"109\" ssid=\"1\">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "E09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "non projective (Nivre and Nilsson, 2005), we char ac terise a sense in which the structures appearing in tree banks can be viewed as being only? slightly? ill-nested",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "Non-projective (Nivre and Nilsson, 2005), we characterize a sense in which the structures appearing in treebanks can be viewed as being only slightly ill-nested."
        },
        {
            "Number": 13,
            "refer_ID": "P05-1013",
            "refer_sids": [
                86.0
            ],
            "refer_text": "As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both datasets.",
            "GPT_cite_text": "Pseudo-projective parsing for recovering non-projective structures (Nivre and Nilsson, 2005)"
        },
        {
            "Number": 14,
            "refer_ID": "P05-1013",
            "refer_sids": [
                95.0
            ],
            "refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point in attachment score.",
            "GPT_cite_text": "Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (Section 3.4)."
        },
        {
            "Number": 15,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109.0
            ],
            "refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser."
        },
        {
            "Number": 16,
            "refer_ID": "P05-1013",
            "refer_sids": [
                51.0
            ],
            "refer_text": "In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Weprojectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r? h, where r is the original label and h is the label of the original head in the non-projective dependency graph",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the second scheme, Head+Path, we additionally modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p, the new label is p\u2193.",
            "GPT_cite_text": "We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r/h, where r is the original label and h is the label of the original head in the non-projective dependency graph."
        },
        {
            "Number": 17,
            "refer_ID": "P05-1013",
            "refer_sids": [
                99.0
            ],
            "refer_text": "This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only require a single lift, where the encoding of path information is often redundant.",
            "GPT_cite_text": "For tree banks with non-projective trees, we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 18,
            "refer_ID": "P05-1013",
            "refer_sids": [
                7.0
            ],
            "refer_text": "From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.",
            "cite_ID": "P11-2121",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "From the point of view of computational implementation, this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and, in practice, also accuracy and robustness.",
            "GPT_cite_text": "Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing in all cases."
        },
        {
            "Number": 20,
            "refer_ID": "P05-1013",
            "refer_sids": [
                2.0
            ],
            "refer_text": "We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.",
            "cite_ID": "D07-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective trans formations as described in (Nivre and Nilsson, 2005)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We show how a data-driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.",
            "GPT_cite_text": "The resulting algorithm is projective, and non-projectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005)."
        }
    ],
    "W06-2932_sweta": [
        {
            "Number": 2,
            "refer_ID": "W06-2932",
            "refer_sids": [
                5
            ],
            "refer_text": "Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Often in language processing, we require a deep syntactic representation of a sentence in order to assist further processing.",
            "GPT_cite_text": "Introduce through post-processing, e.g., through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al., 2006)."
        },
        {
            "Number": 3,
            "refer_ID": "W06-2932",
            "refer_sids": [
                36
            ],
            "refer_text": "However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Table 5 shows the official results for submitted parser outputs.31 The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "However, in a two-stage system, we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.",
            "GPT_cite_text": "Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al. (2006) and Nivre et al. (2006)."
        },
        {
            "Number": 4,
            "refer_ID": "W06-2932",
            "refer_sids": [
                61
            ],
            "refer_text": "In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).",
            "cite_ID": "W06-2920",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In fact, for every language, our models perform significantly higher than the average performance of all the systems reported in Buchholz et al. (2006).",
            "GPT_cite_text": "Even though McDonald et al. (2006) and Nivre et al. (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences."
        },
        {
            "Number": 5,
            "refer_ID": "W06-2932",
            "refer_sids": [
                76
            ],
            "refer_text": "For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.",
            "cite_ID": "W08-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The high est score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (anda different policy regarding the inclusion of punctuation) .The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For instance, sequential labeling improves the labeling of objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.",
            "GPT_cite_text": "The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al. (2006) with a LAS of 87.34 based on the TIGER treebank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP), and labeled F-score (LF)."
        },
        {
            "Number": 6,
            "refer_ID": "W06-2932",
            "refer_sids": [
                45
            ],
            "refer_text": "Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).",
            "cite_ID": "W09-1210",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McDonald et al (2006) use an additional algorithm",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).",
            "GPT_cite_text": "McDonald et al. (2006) use an additional algorithm."
        },
        {
            "Number": 7,
            "refer_ID": "W06-2932",
            "refer_sids": [
                106
            ],
            "refer_text": "First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.",
            "cite_ID": "W12-3407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "First, we plan on examining the performance difference between two-stage dependency parsing (as presented here) and joint parsing plus labeling.",
            "GPT_cite_text": "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al., 2007b) and MST Parser (McDonald et al., 2006), two state-of-the-art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (McDonald and Nivre, 2007)."
        },
        {
            "Number": 8,
            "refer_ID": "W06-2932",
            "refer_sids": [
                12
            ],
            "refer_text": "In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.",
            "cite_ID": "I08-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)? s parser, (McDonald et al., 2006)? s parser, and so on",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this paper, we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.",
            "GPT_cite_text": "In fact, our approach can also be applied to other parsers, such as Yamada and Matsumoto's (2003) parser, McDonald et al.'s (2006) parser, and so on."
        },
        {
            "Number": 9,
            "refer_ID": "W06-2932",
            "refer_sids": [
                104
            ],
            "refer_text": "We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "But whereas the spanning tree parser of McDonald et al (2006) and the pseudo-projective parser of Nivre et al (2006) achieve this performance only with special preorpost-processing,7 the approach presented here derives a labeled non-projective graph in a single incremental process and hence at least has the advantage of simplicity",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005; McDonald and Pereira, 2006) generalizes well to languages other than English.",
            "GPT_cite_text": "But whereas the spanning tree parser of McDonald et al. (2006) and the pseudo-projective parser of Nivre et al. (2006) achieve this performance only with special pre- or post-processing, the approach presented here derives a labeled non-projective graph in a single incremental process and hence at least has the advantage of simplicity."
        },
        {
            "Number": 10,
            "refer_ID": "W06-2932",
            "refer_sids": [
                58
            ],
            "refer_text": "These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Moreover, it has better time complexity than the approximate second-order spanning tree parsing of McDonald et al (2006), which has exponential complexity in the worst case (although this does not appear to be a problem in practice)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005; McDonald and Pereira, 2006) is easily adapted across all these languages.",
            "GPT_cite_text": "Moreover, it has better time complexity than the approximate second-order spanning tree parsing of McDonald et al. (2006), which has exponential complexity in the worst case (although this does not appear to be a problem in practice)."
        },
        {
            "Number": 11,
            "refer_ID": "W06-2932",
            "refer_sids": [
                64
            ],
            "refer_text": "N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We have shown that, for languages with a7McDonald et al (2006) use post-processing for non projective dependencies and for labeling",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphological features/No morphological features. Assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.",
            "GPT_cite_text": "We have shown that, for languages, McDonald et al. (2006) use post-processing for non-projective dependencies and for labeling."
        },
        {
            "Number": 12,
            "refer_ID": "W06-2932",
            "refer_sids": [
                41
            ],
            "refer_text": "To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.",
            "cite_ID": "D07-1122",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "To model this, we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem. We use a first-order Markov factorization of the score s(l(i,jM), l(i,jM\u22121), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jM) and (i, jM\u22121) in the tree y.",
            "GPT_cite_text": "As described in (McDonald et al., 2006), we treat the labeling of dependencies as a sequence labeling problem."
        },
        {
            "Number": 13,
            "refer_ID": "W06-2932",
            "refer_sids": [
                21
            ],
            "refer_text": "That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.",
            "cite_ID": "W11-0314",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "ULISSE was tested against the output of two really different data? driven parsers: the first? order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.",
            "GPT_cite_text": "ULISSE was tested against the output of two really different data-driven parsers: the first-order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as the learning algorithm."
        },
        {
            "Number": 14,
            "refer_ID": "W06-2932",
            "refer_sids": [
                64
            ],
            "refer_text": "N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.",
            "cite_ID": "D07-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphological features/No morphological features. Assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.",
            "GPT_cite_text": "It should be noted that McDonald et al. (2006) use a richer feature set that is incomparable to our features."
        },
        {
            "Number": 16,
            "refer_ID": "W06-2932",
            "refer_sids": [
                104
            ],
            "refer_text": "We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.",
            "cite_ID": "D10-1069",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al, 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005; McDonald and Pereira, 2006) generalizes well to languages other than English.",
            "GPT_cite_text": "The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al., 2007) and the second-order minimum spanning tree algorithm-based MstParser (McDonald et al., 2006)."
        },
        {
            "Number": 18,
            "refer_ID": "W06-2932",
            "refer_sids": [
                33
            ],
            "refer_text": "Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.",
            "cite_ID": "D10-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Entries marked with? are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Ideally, one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.",
            "GPT_cite_text": "Entries marked with * are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al. (2006), Martins et al. (2008), Martins et al. (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)."
        },
        {
            "Number": 19,
            "refer_ID": "W06-2932",
            "refer_sids": [
                41
            ],
            "refer_text": "To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.",
            "cite_ID": "P08-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "To model this, we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem. We use a first-order Markov factorization of the score s(l(i,jM), l(i,jM\u22121), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jM) and (i, jM\u22121) in the tree y.",
            "GPT_cite_text": "The specific graph-based model studied in this work is that presented by McDonald et al. (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near-exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation. 2 2.3 Transition-Based Models"
        },
        {
            "Number": 20,
            "refer_ID": "W06-2932",
            "refer_sids": [
                43
            ],
            "refer_text": "For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi\u2019s algorithm.",
            "cite_ID": "P08-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l)? Rk, where f is typically a bi nary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For score functions, we use simple dot products between high-dimensional feature representations and a weight vector. Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi\u2019s algorithm.",
            "GPT_cite_text": "More precisely, dependency arcs (or pairs of arcs) are first represented by a high-dimensional feature vector f(i, j, l) \u2208 R^k, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al., 2005a; McDonald et al., 2006)."
        }
    ],
    "W99-0613_swastika": [
        {
            "Number": 1,
            "refer_ID": "W99-0613",
            "refer_sids": [
                9.0
            ],
            "refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "GPT_cite_text": "Co-training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web page classification (Blum and Mitchell, 1998), and named entity identification (Collins and Singer, 1999)."
        },
        {
            "Number": 2,
            "refer_ID": "W99-0613",
            "refer_sids": [
                159.0
            ],
            "refer_text": "To prevent this we &quot;smooth&quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To prevent this, we \"smooth\" the confidence by adding a small value, e, to both W+ and W-, giving at = Plugging the value of at from Equ.",
            "GPT_cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co-Booting)."
        },
        {
            "Number": 4,
            "refer_ID": "W99-0613",
            "refer_sids": [
                137.0
            ],
            "refer_text": "The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.",
            "cite_ID": "W03-1509",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.",
            "GPT_cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick et al. 1999], and so on."
        },
        {
            "Number": 5,
            "refer_ID": "W99-0613",
            "refer_sids": [
                91.0
            ],
            "refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "GPT_cite_text": "DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus."
        },
        {
            "Number": 6,
            "refer_ID": "W99-0613",
            "refer_sids": [
                213.0
            ],
            "refer_text": "Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Thus, at each iteration, the algorithm is forced to pick features for the location, person, and organization in turn for the classifier being trained.",
            "GPT_cite_text": "(Collins and Singer, 1999) also make use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify."
        },
        {
            "Number": 7,
            "refer_ID": "W99-0613",
            "refer_sids": [
                250.0
            ],
            "refer_text": "Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.",
            "cite_ID": "W06-2204",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.",
            "GPT_cite_text": "In (Collins and Singer, 1999), Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification."
        },
        {
            "Number": 10,
            "refer_ID": "W99-0613",
            "refer_sids": [
                213.0
            ],
            "refer_text": "Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.",
            "cite_ID": "W03-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Thus, at each iteration, the algorithm is forced to pick features for the location, person, and organization in turn for the classifier being trained.",
            "GPT_cite_text": "Collins and Singer (1999), for example, report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)."
        },
        {
            "Number": 11,
            "refer_ID": "W99-0613",
            "refer_sids": [
                9.0
            ],
            "refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "cite_ID": "E09-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "GPT_cite_text": "While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al., 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999), and context-free grammar induction (numerous attempts, too many to mention)."
        },
        {
            "Number": 12,
            "refer_ID": "W99-0613",
            "refer_sids": [
                36.0
            ],
            "refer_text": "Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.",
            "cite_ID": "W10-3504",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.",
            "GPT_cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labeled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)."
        },
        {
            "Number": 15,
            "refer_ID": "W99-0613",
            "refer_sids": [
                29.0
            ],
            "refer_text": "Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.",
            "cite_ID": "W09-2208",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data and set up the learning task as optimization of some appropriate objective function.",
            "GPT_cite_text": "Collins et al. (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by Blum and Mitchell (1998)."
        },
        {
            "Number": 16,
            "refer_ID": "W99-0613",
            "refer_sids": [
                7.0
            ],
            "refer_text": "Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Many statistical or machine learning approaches for natural language problems require a relatively large amount of supervision in the form of labeled training examples.",
            "GPT_cite_text": "This approach was shown to perform well on real-world natural language processing problems (Collins and Singer, 1999)."
        },
        {
            "Number": 18,
            "refer_ID": "W99-0613",
            "refer_sids": [
                85.0
            ],
            "refer_text": "(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "(If fewer than n rules have Precision greater than pin, we note that taking the top n most frequent rules already makes the method robust to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. Keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.",
            "GPT_cite_text": "(6) Similarly to Collins and Singer (1999), we used T = 0.95 for all experiments reported here."
        },
        {
            "Number": 19,
            "refer_ID": "W99-0613",
            "refer_sids": [
                95.0
            ],
            "refer_text": "(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)",
            "GPT_cite_text": "We use Collins and Singer (1999) for our exact specification of Yarowsky. It uses DL rule scores \\( f_j | f_j | + | f | + L \\) (1) where \\( \\lambda \\) is a smoothing constant."
        },
        {
            "Number": 20,
            "refer_ID": "W99-0613",
            "refer_sids": [
                213.0
            ],
            "refer_text": "Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Thus, at each iteration, the algorithm is forced to pick features for the location, person, and organization in turn for the classifier being trained.",
            "GPT_cite_text": "This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious."
        }
    ],
    "D09-1092_vardha": [
        {
            "Number": 1,
            "refer_ID": "D09-1092",
            "refer_sids": [
                17
            ],
            "refer_text": "We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.",
            "cite_ID": "P14-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple languages.",
            "GPT_cite_text": "This configuration is similar to PolyLDA (Mimno et al., 2009) or LinkLDA (Yano et al., 2009), such that utterances from different parties are treated as different languages or blog posts and comments pairs."
        },
        {
            "Number": 2,
            "refer_ID": "D09-1092",
            "refer_sids": [
                20
            ],
            "refer_text": "We also explore how the characteristics of different languages affect topic model performance.",
            "cite_ID": "P10-1044",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We also explore how the characteristics of different languages affect topic modeling performance.",
            "GPT_cite_text": "Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009)."
        },
        {
            "Number": 3,
            "refer_ID": "D09-1092",
            "refer_sids": [
                138
            ],
            "refer_text": "We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.",
            "cite_ID": "P11-2084",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexicon.",
            "GPT_cite_text": "(Mimno et al., 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then adding the Cartesian product of these sets for every topic to a set of candidate translations."
        },
        {
            "Number": 4,
            "refer_ID": "D09-1092",
            "refer_sids": [
                10
            ],
            "refer_text": "We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).",
            "cite_ID": "E12-1014",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European Parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).",
            "GPT_cite_text": "Our Wikipedia-based topic similarity feature, w(f, e), is similar in spirit to polylingual topic models (Mimno et al. 2009), but it is scalable to full bilingual lexicon induction."
        },
        {
            "Number": 5,
            "refer_ID": "D09-1092",
            "refer_sids": [
                55
            ],
            "refer_text": "The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.",
            "cite_ID": "D11-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "of English document and the second half of its aligned foreign language document (Mimno et al,2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The EuroParl corpus consists of parallel texts in eleven Western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish.",
            "GPT_cite_text": "Of the English document and the second half of its aligned foreign language document (Mimno et al., 2009)"
        },
        {
            "Number": 6,
            "refer_ID": "D09-1092",
            "refer_sids": [
                9
            ],
            "refer_text": "sid=\"9\" ssid=\"5\">In this paper, we present the polylingual topic model (PLTM).",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we present the polylingual topic model (PLTM).",
            "GPT_cite_text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al., 2009) for more details."
        },
        {
            "Number": 7,
            "refer_ID": "D09-1092",
            "refer_sids": [
                118
            ],
            "refer_text": "We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.",
            "GPT_cite_text": "Evaluation Corpus: The automatic evaluation of cross-lingual coreference systems requires annotated data. 10Mimno et al. (2009) showed that so long as the proportion of topically aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly."
        },
        {
            "Number": 8,
            "refer_ID": "D09-1092",
            "refer_sids": [
                35
            ],
            "refer_text": "The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.",
            "GPT_cite_text": "Similarly, Polylingual Topic Models (PLTM) (Mimno et al., 2009) generalized LDA to tuples of documents from multiple languages."
        },
        {
            "Number": 9,
            "refer_ID": "D09-1092",
            "refer_sids": [
                35
            ],
            "refer_text": "The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.",
            "GPT_cite_text": "Our baseline joint PLSA model (JPLSA) is closely related to the polylingual LDA model of (Mimno et al., 2009)."
        },
        {
            "Number": 10,
            "refer_ID": "D09-1092",
            "refer_sids": [
                55
            ],
            "refer_text": "The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The EuroParl corpus consists of parallel texts in eleven Western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish.",
            "GPT_cite_text": "We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al., 2009)."
        },
        {
            "Number": 11,
            "refer_ID": "D09-1092",
            "refer_sids": [
                102
            ],
            "refer_text": "In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In contrast, PLTM assigns a significant number of tokens to almost all 800 topics in very similar proportions in both languages.",
            "GPT_cite_text": "The difference between the JPLSA model and the polylingual topic model of (Mimno et al., 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al., 2009)."
        },
        {
            "Number": 12,
            "refer_ID": "D09-1092",
            "refer_sids": [
                38
            ],
            "refer_text": "This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.",
            "GPT_cite_text": "Another difference between our model and the polylingual LDA model of (Mimno et al., 2009) is that we use maximum a posteriori (MAP) instead of Bayesian inference."
        },
        {
            "Number": 13,
            "refer_ID": "D09-1092",
            "refer_sids": [
                156
            ],
            "refer_text": "We use both Jensen-Shannon divergence and cosine distance.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use both Jensen-Shannon divergence and cosine distance.",
            "GPT_cite_text": "For computing distance, we used the L1 norm of the difference, which worked a bit better than the Jensen-Shannon divergence between the topic vectors used in (Mimno et al., 2009)."
        },
        {
            "Number": 14,
            "refer_ID": "D09-1092",
            "refer_sids": [
                36
            ],
            "refer_text": "Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Documents are defined as speeches by a single speaker, as in (Mimno et al, 2009) .4 For the Wikipedia set, we use 43,380 training documents, 8,675 development documents, and 8,675 final test set documents. For both corpora, the terms are extracted by word breaking all documents, removing the top 50 most frequent terms and keeping the next 20,000 most frequent terms",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English, and German.",
            "GPT_cite_text": "Documents are defined as speeches by a single speaker, as in (Mimno et al., 2009). For the Wikipedia set, we use 43,380 training documents, 8,675 development documents, and 8,675 final test set documents. For both corpora, the terms are extracted by word-breaking all documents, removing the top 50 most frequent terms, and keeping the next 20,000 most frequent terms."
        },
        {
            "Number": 15,
            "refer_ID": "D09-1092",
            "refer_sids": [
                170
            ],
            "refer_text": "First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.",
            "GPT_cite_text": "In previously reported work, (Mimno et al., 2009) evaluated parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours."
        },
        {
            "Number": 16,
            "refer_ID": "D09-1092",
            "refer_sids": [
                29
            ],
            "refer_text": "A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.",
            "cite_ID": "W12-3117",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A recent extended abstract, developed concurrently by Ni et al. (2009), discusses a multilingual topic model similar to the one presented here.",
            "GPT_cite_text": "We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing, e.g., polylingual topic models (Mimno et al., 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)."
        },
        {
            "Number": 17,
            "refer_ID": "D09-1092",
            "refer_sids": [
                170
            ],
            "refer_text": "First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.",
            "GPT_cite_text": "ji = wjk? M j? m k = 1w j k, (1) where M j is the topic distribution of document j and w k is the number of occurrences of phrase pair X k in document j. Mimno et al. (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)."
        },
        {
            "Number": 18,
            "refer_ID": "D09-1092",
            "refer_sids": [
                168
            ],
            "refer_text": "However, the growth of the web, and in particular Wikipedia, has made comparable text corpora \u2013 documents that are topically similar but are not direct translations of one another \u2013 considerably more abundant than true parallel corpora.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, the growth of the web, and in particular Wikipedia, has made comparable text corpora \u2013 documents that are topically similar but are not direct translations of one another \u2013 considerably more abundant than true parallel corpora.",
            "GPT_cite_text": "Tuple-specific topic distributions are learned using LDA with distinct topic-word concentration parameters. Mimno et al. (2009) show that PLTM sufficiently aligns topics in parallel corpora."
        },
        {
            "Number": 19,
            "refer_ID": "D09-1092",
            "refer_sids": [
                29
            ],
            "refer_text": "A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A recent extended abstract, developed concurrently by Ni et al. (2009), discusses a multilingual topic model similar to the one presented here.",
            "GPT_cite_text": "A good candidate for multilingual topic analyses is polylingual topic models (Mimno et al., 2009), which learn topics for multiple languages, creating tuples of language-specific distributions over monolingual vocabularies for each topic."
        },
        {
            "Number": 20,
            "refer_ID": "D09-1092",
            "refer_sids": [
                110
            ],
            "refer_text": "Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.",
            "GPT_cite_text": "To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al. (2009): add a token-specific language variable and a process for identifying aligned topics. First, polylingual topic models require parallel or comparable corpora in which each document has an assigned language."
        }
    ],
    "J01-2004_swastika": [
        {
            "Number": 1,
            "refer_ID": "J01-2004",
            "refer_sids": [
                372.0
            ],
            "refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "cite_ID": "W05-0104",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "GPT_cite_text": "Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark; see Roark (2001))."
        },
        {
            "Number": 2,
            "refer_ID": "J01-2004",
            "refer_sids": [
                15.0
            ],
            "refer_text": "In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.",
            "cite_ID": "P08-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "In the past few years, however, some improvements have been made to these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.",
            "GPT_cite_text": "Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000)."
        },
        {
            "Number": 4,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31.0
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn Treebank."
        },
        {
            "Number": 5,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31.0
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn Treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search."
        },
        {
            "Number": 6,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31.0
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper, we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)."
        },
        {
            "Number": 7,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31.0
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights."
        },
        {
            "Number": 9,
            "refer_ID": "J01-2004",
            "refer_sids": [
                215.0
            ],
            "refer_text": "The first word in the string remaining to be parsed, w1, we will call the look-ahead word.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first word in the string remaining to be parsed, w1, we will call the lookahead word.",
            "GPT_cite_text": "Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word."
        },
        {
            "Number": 10,
            "refer_ID": "J01-2004",
            "refer_sids": [
                302.0
            ],
            "refer_text": "In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.",
            "GPT_cite_text": "A good example of this is the Roark parser (Roark, 2001), which works left-to-right through the sentence and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word and then re-pruning."
        },
        {
            "Number": 13,
            "refer_ID": "J01-2004",
            "refer_sids": [
                372.0
            ],
            "refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "cite_ID": "P04-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "GPT_cite_text": "We ran the first stage parser with 4-times over parsing for each string in the 7 best lists provided by Brian Roark (Roark, 2001). A local tree is an explicit expansion of an edge and its children."
        },
        {
            "Number": 14,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31.0
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "P05-1063",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models."
        },
        {
            "Number": 15,
            "refer_ID": "J01-2004",
            "refer_sids": [
                100.0
            ],
            "refer_text": "The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.",
            "cite_ID": "W10-2009",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The approach that we will subsequently present uses probabilistic grammar as its language model, but only includes probability mass from those parses that are found; that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.",
            "GPT_cite_text": "Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)."
        },
        {
            "Number": 19,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31.0
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here."
        },
        {
            "Number": 20,
            "refer_ID": "J01-2004",
            "refer_sids": [
                21.0
            ],
            "refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "GPT_cite_text": "At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures. Let H(D) be the entropy over a set of derivations D, calculated as follows: H(D) = \u03a3 D (D) P(D) log P(D) (10) If the set of derivations D = D(G, W[1, i]) is a set of partial derivations for string W[1, i], then H(D) is a measure of uncertainty over the partial derivations, i.e., the uncertainty regarding the correct analysis of what has already been processed."
        }
    ],
    "A97-1014_vardha": [
        {
            "Number": 1,
            "refer_ID": "A97-1014",
            "refer_sids": [
                72
            ],
            "refer_text": "In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.",
            "cite_ID": "E99-1016",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and final annotation.",
            "GPT_cite_text": "This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al., 1997)."
        },
        {
            "Number": 2,
            "refer_ID": "A97-1014",
            "refer_sids": [
                144
            ],
            "refer_text": "We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.",
            "cite_ID": "E99-1016",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For our experiments, we use the NEGRA corpus (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We distinguish five degrees of automation: So far, about 1,100 sentences of our corpus have been annotated.",
            "GPT_cite_text": "For our experiments, we use the NEGRA corpus (Skut et al., 1997)."
        },
        {
            "Number": 3,
            "refer_ID": "A97-1014",
            "refer_sids": [
                14
            ],
            "refer_text": "Corpora annotated with syntactic structures are commonly referred to as trctbank.5.",
            "cite_ID": "E12-1047",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As data we use version 2 of the Negra (Skut et al1997) tree bank, with the common training ,devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 Frequenc y Parsing complexity head-driven optimal head-driven Figure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Corpora annotated with syntactic structures are commonly referred to as Treebank.",
            "GPT_cite_text": "As data, we use version 2 of the Negra (Skut et al. 1997) treebank, with the common training, devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 Frequency Parsing complexity head-driven optimal head-driven Figure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25."
        },
        {
            "Number": 5,
            "refer_ID": "A97-1014",
            "refer_sids": [
                15
            ],
            "refer_text": "Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.",
            "cite_ID": "I05-6010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "According to Skut et al (1997) tree banks have to meet the following requirements: 1",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptiveness: Grammatical phenomena are to be described rather than explained.",
            "GPT_cite_text": "According to Skut et al. (1997), treebanks have to meet the following requirements: 1"
        },
        {
            "Number": 6,
            "refer_ID": "A97-1014",
            "refer_sids": [
                36
            ],
            "refer_text": "As for free word order languages, the following features may cause problems: sition between the two poles.",
            "cite_ID": "W04-1506",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "that could best deal with the free word order displayed by Basque syntax (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As for free word order languages, the following features may cause problems: position between the two poles.",
            "GPT_cite_text": "That could best deal with the free word order displayed by Basque syntax (Skut et al., 1997)."
        },
        {
            "Number": 7,
            "refer_ID": "A97-1014",
            "refer_sids": [
                24
            ],
            "refer_text": "The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.",
            "cite_ID": "C10-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In contrast, some other tree banks, such as the German NeGra and TIGER tree banks allow annotation with crossing branches (Skut et al, 1997) .Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of nonlocal dependencies.",
            "GPT_cite_text": "In contrast, some other treebanks, such as the German NeGra and TIGER treebanks, allow annotation with crossing branches (Skut et al., 1997). Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node."
        },
        {
            "Number": 8,
            "refer_ID": "A97-1014",
            "refer_sids": [
                160
            ],
            "refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "cite_ID": "C10-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our data source is the German NeGra tree bank (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "GPT_cite_text": "Our data source is the German NeGra treebank (Skut et al., 1997)."
        },
        {
            "Number": 9,
            "refer_ID": "A97-1014",
            "refer_sids": [
                151
            ],
            "refer_text": "For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).",
            "cite_ID": "P05-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The parsing models we present are trained and tested on the NEGRA corpus (Skut et al, 1997), a hand parsed corpus of German newspaper text containing approximately 20,000 sentences",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For evaluation, the already annotated sentences were divided into two disjoint sets: one for training (90% of the corpus) and the other for testing (10%).",
            "GPT_cite_text": "The parsing models we present are trained and tested on the NEGRA corpus (Skut et al., 1997), a hand-parsed corpus of German newspaper text containing approximately 20,000 sentences."
        },
        {
            "Number": 10,
            "refer_ID": "A97-1014",
            "refer_sids": [
                4
            ],
            "refer_text": "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.",
            "cite_ID": "P03-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al, 1997), a syntactically annotated corpus for German",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The work reported in this paper aims at providing syntactically annotated corpora (treebanks) for stochastic grammar induction.",
            "GPT_cite_text": "The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German."
        },
        {
            "Number": 11,
            "refer_ID": "A97-1014",
            "refer_sids": [
                160
            ],
            "refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "cite_ID": "P03-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The annotation scheme (Skut et al, 1997) is modeled to a certain extent on that of the Penn Treebank (Marcuset al, 1993), with crucial differences",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "GPT_cite_text": "The annotation scheme (Skut et al., 1997) is modeled to a certain extent on that of the Penn Treebank (Marcus et al., 1993), with crucial differences."
        },
        {
            "Number": 13,
            "refer_ID": "A97-1014",
            "refer_sids": [
                167
            ],
            "refer_text": "In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "cite_ID": "W04-1505",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "German is con sider ably more in ectional which means that discarding functional information is more harmful, and which explains why the NEGRA an notation has been conceived to be quite at (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the second phase of the project Verbmobil, a treebank for 30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.",
            "GPT_cite_text": "German is considerably more inflectional, which means that discarding functional information is more harmful, and which explains why the NEGRA annotation has been conceived to be quite so (Skut et al, 1997)."
        },
        {
            "Number": 14,
            "refer_ID": "A97-1014",
            "refer_sids": [
                39
            ],
            "refer_text": "Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.",
            "cite_ID": "C04-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The factors used in the algorithms and the algorithms themselves are evaluated on a Germancorpus annotated with syntactic and co reference in formation (Negra) (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Consider the German sentence (1) daran wird ihn Anna erkennen, da er weint 'Anna will recognize him at his cry.' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.",
            "GPT_cite_text": "The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information (Negra) (Skut et al, 1997)."
        },
        {
            "Number": 16,
            "refer_ID": "A97-1014",
            "refer_sids": [
                151
            ],
            "refer_text": "For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).",
            "cite_ID": "P11-2067",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For evaluation, the already annotated sentences were divided into two disjoint sets: one for training (90% of the corpus) and the other for testing (10%).",
            "GPT_cite_text": "CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al., 1997)."
        },
        {
            "Number": 17,
            "refer_ID": "A97-1014",
            "refer_sids": [
                71
            ],
            "refer_text": "However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.",
            "cite_ID": "W08-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negratreebank (Skut et al, 1997) reports that lexicaliza tion of PCFGs decrease the parsing accuracy when parsing Negra? s flat constituent structures",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.",
            "GPT_cite_text": "Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negri treebank (Skut et al., 1997) report that lexicalization of PCFGs decreases the parsing accuracy when parsing Negra's flat constituent structures."
        },
        {
            "Number": 18,
            "refer_ID": "A97-1014",
            "refer_sids": [
                144
            ],
            "refer_text": "We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.",
            "cite_ID": "P06-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al 1997) and the Chinese CTB10 (Xue et al 2002) both containing 2200+ sentences? 10 words after removing punctuation",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We distinguish five degrees of automation: So far, about 1,100 sentences of our corpus have been annotated.",
            "GPT_cite_text": "We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al. 1997) and the Chinese CTB10 (Xue et al. 2002), both containing 2200+ sentences, 10 words after removing punctuation."
        },
        {
            "Number": 19,
            "refer_ID": "A97-1014",
            "refer_sids": [
                160
            ],
            "refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "cite_ID": "D07-1066",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A comparison of unlexicalised PCFG parsing (Ku ?bler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the Tu? Ba D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.",
            "GPT_cite_text": "A comparison of unlexicalized PCFG parsing (K\u00fcbler, 2005) trained and evaluated on the German NEGRA (Skut et al., 1997) and the TuBa-D/Z (Telljohann et al., 2004) treebanks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al., 1991)."
        }
    ],
    "W11-2123_vardha": [
        {
            "Number": 1,
            "refer_ID": "W11-2123",
            "refer_sids": [
                12
            ],
            "refer_text": "Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.",
            "cite_ID": "W11-2138",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Throughout this paper, we compare several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.",
            "GPT_cite_text": "We used common tools for phrase-based translation: Moses (Koehn et al., 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modeling and GIZA++ (Och and Ney, 2000) for word alignments."
        },
        {
            "Number": 2,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "P14-2022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run."
        },
        {
            "Number": 3,
            "refer_ID": "W11-2123",
            "refer_sids": [
                131
            ],
            "refer_text": "Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N \u2212 1 preceding words.",
            "cite_ID": "W12-3145",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N \u2212 1 preceding words.",
            "GPT_cite_text": "Thus, given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) <s> tf, (ii) tf, and (iii) tf </s> and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state information in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states."
        },
        {
            "Number": 4,
            "refer_ID": "W11-2123",
            "refer_sids": [
                21
            ],
            "refer_text": "Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.",
            "cite_ID": "W12-3131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.",
            "GPT_cite_text": "Our translation system uses cdec (Dyer et al., 2010), an implementation of the hierarchical phrase-based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference."
        },
        {
            "Number": 5,
            "refer_ID": "W11-2123",
            "refer_sids": [
                21
            ],
            "refer_text": "Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.",
            "cite_ID": "W12-3154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.",
            "GPT_cite_text": "The three data sets in use in this paper are summarized in Table 1. The translation systems consisted of phrase tables and lexicalized reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime."
        },
        {
            "Number": 6,
            "refer_ID": "W11-2123",
            "refer_sids": [
                108
            ],
            "refer_text": "Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.",
            "cite_ID": "P12-2058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.",
            "GPT_cite_text": "The features used are basic lexical features, word penalty, and a 3-gram language model (Heafield, 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "W11-2123",
            "refer_sids": [
                129
            ],
            "refer_text": "In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.",
            "cite_ID": "W11-2139",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Inference was carried out using the language modeling library described by Heafield (2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In addition to the optimizations specific to each data structure described in Section 2, we implement several general optimizations for language modeling.",
            "GPT_cite_text": "Inference was carried out using the language modeling library described by Heafield (2011)."
        },
        {
            "Number": 8,
            "refer_ID": "W11-2123",
            "refer_sids": [
                199
            ],
            "refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "cite_ID": "P13-2003",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "GPT_cite_text": "We used the MADA ATB segmentation for Arabic (Roth et al., 2008) and true casing for English, phrases of maximal length 7, Kneser-Ney smoothing, and lexicalized reordering (Koehn et al., 2005), and a 5-gram language model, trained on GigaWord v.5 using KenLM (Heafield, 2011)."
        },
        {
            "Number": 9,
            "refer_ID": "W11-2123",
            "refer_sids": [
                52
            ],
            "refer_text": "Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.",
            "GPT_cite_text": "The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011); both language model implementations are now integrated with Joshua."
        },
        {
            "Number": 10,
            "refer_ID": "W11-2123",
            "refer_sids": [
                263
            ],
            "refer_text": "Quantization can be improved by jointly encoding probability and backoff.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Quantization can be improved by jointly encoding probabilities and backoff.",
            "GPT_cite_text": "Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets."
        },
        {
            "Number": 11,
            "refer_ID": "W11-2123",
            "refer_sids": [
                52
            ],
            "refer_text": "Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.",
            "GPT_cite_text": "With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult-to-compile SRILM toolkit (Stolcke, 2002)."
        },
        {
            "Number": 12,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3160",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This was used to create a KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and cost.",
            "GPT_cite_text": "This was used to create a KenLM (Heafield, 2011)."
        },
        {
            "Number": 13,
            "refer_ID": "W11-2123",
            "refer_sids": [
                145
            ],
            "refer_text": "If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.",
            "cite_ID": "W12-3706",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.",
            "GPT_cite_text": "In the Opinum system, we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application."
        },
        {
            "Number": 14,
            "refer_ID": "W11-2123",
            "refer_sids": [
                182
            ],
            "refer_text": "This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.",
            "cite_ID": "W11-2147",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.",
            "GPT_cite_text": "Our baseline is a factored phrase-based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modeling and minimum error rate training (Och, 2003) to tune model feature weights."
        },
        {
            "Number": 15,
            "refer_ID": "W11-2123",
            "refer_sids": [
                274
            ],
            "refer_text": "We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.",
            "cite_ID": "E12-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described two data structures for language modeling that achieve substantial reductions in time and memory costs.",
            "GPT_cite_text": "For language modeling, we computed 5-gram models using IRSTLM (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)."
        },
        {
            "Number": 16,
            "refer_ID": "W11-2123",
            "refer_sids": [
                12
            ],
            "refer_text": "Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.",
            "cite_ID": "P12-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Throughout this paper, we compare several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.",
            "GPT_cite_text": "Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima \u010can, 2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well. 3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using KenLM (Heafield, 2011)."
        },
        {
            "Number": 17,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "D12-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper presents methods to query n-gram language models, minimizing time and space costs.",
            "GPT_cite_text": "n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3."
        },
        {
            "Number": 18,
            "refer_ID": "W11-2123",
            "refer_sids": [
                140
            ],
            "refer_text": "We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.",
            "cite_ID": "P12-2006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.",
            "GPT_cite_text": "Research efforts to increase search efficiency for phrase-based MT (Koehn et al., 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al., 2006) to additional early pruning techniques (Delaney et al., 2006), (Moore and Quirk, 2007), and more efficient language model (LM) querying (Heafield, 2011)."
        },
        {
            "Number": 19,
            "refer_ID": "W11-2123",
            "refer_sids": [
                199
            ],
            "refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "cite_ID": "P13-2073",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "GPT_cite_text": "For English language modeling, we use the English GigaWord Corpus with a 5-gram LM using the KenLM toolkit (Heafield, 2011)."
        },
        {
            "Number": 20,
            "refer_ID": "W11-2123",
            "refer_sids": [
                199
            ],
            "refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "cite_ID": "P13-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "GPT_cite_text": "For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing."
        }
    ],
    "W06-3114_aakansha": [
        {
            "Number": 1,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "W06-3120",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "GPT_cite_text": "The official results were slightly better because a lowercase evaluation was used; see (Koehn and Monz, 2006)."
        },
        {
            "Number": 2,
            "refer_ID": "W06-3114",
            "refer_sids": [
                8
            ],
            "refer_text": "The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.",
            "cite_ID": "D07-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The evaluation framework for the shared task is similar to the one used in last year's shared task.",
            "GPT_cite_text": "We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted of translating Spanish, German, and French texts from and to English."
        },
        {
            "Number": 3,
            "refer_ID": "W06-3114",
            "refer_sids": [
                9
            ],
            "refer_text": "Training and testing is based on the Europarl corpus.",
            "cite_ID": "C08-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Training and testing are based on the Europarl corpus.",
            "GPT_cite_text": "For our training and test data, we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference."
        },
        {
            "Number": 4,
            "refer_ID": "W06-3114",
            "refer_sids": [
                36
            ],
            "refer_text": "The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.",
            "cite_ID": "W07-0718",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The BLEU metric, like all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.",
            "GPT_cite_text": "The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)."
        },
        {
            "Number": 5,
            "refer_ID": "W06-3114",
            "refer_sids": [
                9
            ],
            "refer_text": "Training and testing is based on the Europarl corpus.",
            "cite_ID": "P07-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Training and testing are based on the Europarl corpus.",
            "GPT_cite_text": "For the bi-text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for French-English (Fr), Spanish-English (Es), and German-English (De) (Koehn and Monz, 2006)."
        },
        {
            "Number": 6,
            "refer_ID": "W06-3114",
            "refer_sids": [
                36
            ],
            "refer_text": "The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The BLEU metric, like all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.",
            "GPT_cite_text": "Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006) revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator."
        },
        {
            "Number": 7,
            "refer_ID": "W06-3114",
            "refer_sids": [
                140
            ],
            "refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "GPT_cite_text": "For instance, Callison-Burch et al. (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001)."
        },
        {
            "Number": 8,
            "refer_ID": "W06-3114",
            "refer_sids": [
                140
            ],
            "refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "GPT_cite_text": "We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006) (see Section 3)."
        },
        {
            "Number": 9,
            "refer_ID": "W06-3114",
            "refer_sids": [
                140
            ],
            "refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We confirm the findings by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "GPT_cite_text": "We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006)"
        },
        {
            "Number": 10,
            "refer_ID": "W06-3114",
            "refer_sids": [
                102
            ],
            "refer_text": "Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the same method described in (Koehn and Monz, 2006) to perform the significance test",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Confidence Interval: To estimate confidence intervals for the average mean scores of the systems, we use standard significance testing.",
            "GPT_cite_text": "We use the same method described in (Koehn and Monz, 2006) to perform the significance test."
        },
        {
            "Number": 11,
            "refer_ID": "W06-3114",
            "refer_sids": [
                84
            ],
            "refer_text": "The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The human judges were presented with the following definitions of adequacy and fluency, but no additional instructions:",
            "GPT_cite_text": "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)."
        },
        {
            "Number": 12,
            "refer_ID": "W06-3114",
            "refer_sids": [
                11
            ],
            "refer_text": "To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.",
            "cite_ID": "W08-0406",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To lower the barrier of entry to the competition, we provided a complete baseline MT system, along with data resources.",
            "GPT_cite_text": "The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)."
        },
        {
            "Number": 13,
            "refer_ID": "W06-3114",
            "refer_sids": [
                140
            ],
            "refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "cite_ID": "W11-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "GPT_cite_text": "Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality."
        },
        {
            "Number": 14,
            "refer_ID": "W06-3114",
            "refer_sids": [
                126
            ],
            "refer_text": "The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences of out-of-domain test data.",
            "GPT_cite_text": "The English-German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)."
        },
        {
            "Number": 15,
            "refer_ID": "W06-3114",
            "refer_sids": [
                15
            ],
            "refer_text": "Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Out-of-domain test data is from the Project Syndicate website, a compendium of political commentary.",
            "GPT_cite_text": "We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)."
        },
        {
            "Number": 16,
            "refer_ID": "W06-3114",
            "refer_sids": [
                8
            ],
            "refer_text": "The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.",
            "cite_ID": "P07-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The evaluation framework for the shared task is similar to the one used in last year's shared task.",
            "GPT_cite_text": "A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)."
        },
        {
            "Number": 18,
            "refer_ID": "W06-3114",
            "refer_sids": [
                90
            ],
            "refer_text": "Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.",
            "cite_ID": "E12-3010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another way to view the judgments is that they are less quality judgments of machine translation systems per se, but rankings of machine translation systems.",
            "GPT_cite_text": "For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)."
        },
        {
            "Number": 19,
            "refer_ID": "W06-3114",
            "refer_sids": [
                5,
                6
            ],
            "refer_text": "\u2022 We evaluated translation from English, in addition to into English.\nEnglish was again paired with German, French, and Spanish.",
            "cite_ID": "W09-0402",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "\u2022 We evaluated translation from English, in addition to into English. English was again paired with German, French, and Spanish.",
            "GPT_cite_text": "The correlations on the document level were computed on the English, French, Spanish, and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al., 2007), and third shared translation task (Callison-Burch et al., 2008)."
        }
    ],
    "D09-1092_swastika": [
        {
            "Number": 1,
            "refer_ID": "D09-1092",
            "refer_sids": [
                193
            ],
            "refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "cite_ID": "P14-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA and demonstrated that it is possible to discover aligned topics.",
            "GPT_cite_text": "This configuration is similar to PolyLDA (Mimno et al., 2009) or LinkLDA (Yano et al., 2009), such that utterances from different parties are treated as different languages or blog posts and comments pairs."
        },
        {
            "Number": 2,
            "refer_ID": "D09-1092",
            "refer_sids": [
                114
            ],
            "refer_text": "Ideally, the \u201cglue\u201d documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.",
            "cite_ID": "P10-1044",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Ideally, the \u201cglue\u201d documents in G will be sufficient to align the topics across languages and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.",
            "GPT_cite_text": "Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009)."
        },
        {
            "Number": 3,
            "refer_ID": "D09-1092",
            "refer_sids": [
                138
            ],
            "refer_text": "We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.",
            "cite_ID": "P11-2084",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexicon.",
            "GPT_cite_text": "(Mimno et al., 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then adding the Cartesian product of these sets for every topic to a set of candidate translations."
        },
        {
            "Number": 4,
            "refer_ID": "D09-1092",
            "refer_sids": [
                18
            ],
            "refer_text": "In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.",
            "cite_ID": "E12-1014",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we use two multilingual corpora to answer various critical questions related to multilingual topic models.",
            "GPT_cite_text": "Our Wikipedia-based topic similarity feature, w(f, e), is similar in spirit to polylingual topic models (Mimno et al. 2009), but it is scalable to full bilingual lexicon induction."
        },
        {
            "Number": 5,
            "refer_ID": "D09-1092",
            "refer_sids": [
                55
            ],
            "refer_text": "The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.",
            "cite_ID": "D11-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "of English document and the second half of its aligned foreign language document (Mimno et al,2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The EuroParl corpus consists of parallel texts in eleven Western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish.",
            "GPT_cite_text": "Of the English document and the second half of its aligned foreign language document (Mimno et al., 2009)"
        },
        {
            "Number": 6,
            "refer_ID": "D09-1092",
            "refer_sids": [
                192
            ],
            "refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "GPT_cite_text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al., 2009) for more details."
        },
        {
            "Number": 7,
            "refer_ID": "D09-1092",
            "refer_sids": [
                119
            ],
            "refer_text": "The lower the divergence, the more similar the distributions are to each other.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The lower the divergence, the more similar the distributions are to each other.",
            "GPT_cite_text": "Evaluation Corpus: The automatic evaluation of cross-lingual coreference systems requires annotated data. 10Mimno et al. (2009) showed that so long as the proportion of topically aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly."
        },
        {
            "Number": 8,
            "refer_ID": "D09-1092",
            "refer_sids": [
                148
            ],
            "refer_text": "To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.",
            "GPT_cite_text": "Similarly, Polylingual Topic Models (PLTM) (Mimno et al., 2009) generalized LDA to tuples of documents from multiple languages."
        },
        {
            "Number": 9,
            "refer_ID": "D09-1092",
            "refer_sids": [
                193
            ],
            "refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA and demonstrated that it is possible to discover aligned topics.",
            "GPT_cite_text": "Our baseline joint PLSA model (JPLSA) is closely related to the polylingual LDA model of (Mimno et al., 2009)."
        },
        {
            "Number": 10,
            "refer_ID": "D09-1092",
            "refer_sids": [
                184
            ],
            "refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "GPT_cite_text": "We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al., 2009)."
        },
        {
            "Number": 11,
            "refer_ID": "D09-1092",
            "refer_sids": [
                193
            ],
            "refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA and demonstrated that it is possible to discover aligned topics.",
            "GPT_cite_text": "The difference between the JPLSA model and the polylingual topic model of (Mimno et al., 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al., 2009)."
        },
        {
            "Number": 12,
            "refer_ID": "D09-1092",
            "refer_sids": [
                163
            ],
            "refer_text": "Performance continues to improve with longer documents, most likely due to better topic inference.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Performance continues to improve with longer documents, most likely due to better topic inference.",
            "GPT_cite_text": "Another difference between our model and the polylingual LDA model of (Mimno et al., 2009) is that we use maximum a posteriori (MAP) instead of Bayesian inference."
        },
        {
            "Number": 13,
            "refer_ID": "D09-1092",
            "refer_sids": [
                184
            ],
            "refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "GPT_cite_text": "For computing distance, we used the L1-norm of the difference, which worked a bit better than the Jensen-Shannon divergence between the topic vectors used in (Mimno et al., 2009)."
        },
        {
            "Number": 15,
            "refer_ID": "D09-1092",
            "refer_sids": [
                148
            ],
            "refer_text": "To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.",
            "GPT_cite_text": "In previously reported work, (Mimno et al., 2009) evaluated parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours."
        },
        {
            "Number": 16,
            "refer_ID": "D09-1092",
            "refer_sids": [
                184
            ],
            "refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "cite_ID": "W12-3117",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "GPT_cite_text": "We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing, e.g., polylingual topic models (Mimno et al., 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)."
        },
        {
            "Number": 17,
            "refer_ID": "D09-1092",
            "refer_sids": [
                193
            ],
            "refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA and demonstrated that it is possible to discover aligned topics.",
            "GPT_cite_text": "ji = wjk? M j? m k = 1w j k, (1) where M j is the topic distribution of document j and w k is the number of occurrences of phrase pair X k in document j. Mimno et al. (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)."
        },
        {
            "Number": 18,
            "refer_ID": "D09-1092",
            "refer_sids": [
                192
            ],
            "refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "GPT_cite_text": "Tuple-specific topic distributions are learned using LDA with distinct topic-word concentration parameters. Mimno et al. (2009) show that PLTM sufficiently aligns topics in parallel corpora."
        },
        {
            "Number": 19,
            "refer_ID": "D09-1092",
            "refer_sids": [
                105
            ],
            "refer_text": "An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.",
            "GPT_cite_text": "A good candidate for multilingual topic analyses is polylingual topic models (Mimno et al., 2009), which learn topics for multiple languages, creating tuples of language-specific distributions over monolingual vocabularies for each topic."
        },
        {
            "Number": 20,
            "refer_ID": "D09-1092",
            "refer_sids": [
                10
            ],
            "refer_text": "We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European Parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).",
            "GPT_cite_text": "To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al. (2009): add a token-specific language variable and a process for identifying aligned topics. First, polylingual topic models require parallel or comparable corpora in which each document has an assigned language."
        }
    ],
    "P11-1061_aakansha": [
        {
            "Number": 1,
            "refer_ID": "P11-1061",
            "refer_sids": [
                40
            ],
            "refer_text": "We extend Subramanya et al.\u2019s intuitions to our bilingual setup.",
            "cite_ID": "P11-1144",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We extend Subramanya et al.'s intuitions to our bilingual setup.",
            "GPT_cite_text": "Subramanya et al.'s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers."
        },
        {
            "Number": 3,
            "refer_ID": "P11-1061",
            "refer_sids": [
                10
            ],
            "refer_text": "To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "cite_ID": "P14-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Fortunately, some recently proposed POS taggers, such as the POStagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To bridge this gap, we consider a practically motivated scenario in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages. We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "GPT_cite_text": "Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach."
        },
        {
            "Number": 4,
            "refer_ID": "P11-1061",
            "refer_sids": [
                15
            ],
            "refer_text": "First, we use a novel graph-based framework for projecting syntactic information across language boundaries.",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, we use a novel graph-based framework for projecting syntactic information across language boundaries.",
            "GPT_cite_text": "Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011)."
        },
        {
            "Number": 5,
            "refer_ID": "P11-1061",
            "refer_sids": [
                25
            ],
            "refer_text": "Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.",
            "GPT_cite_text": "Following Das and Petrov (2011) and Subramanya et al. (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics."
        },
        {
            "Number": 6,
            "refer_ID": "P11-1061",
            "refer_sids": [
                29
            ],
            "refer_text": "To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (\u00a73.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (\u00a73.3). Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.",
            "GPT_cite_text": "Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type are unavailable (e.g., Das and Petrov, 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "P11-1061",
            "refer_sids": [
                18
            ],
            "refer_text": "To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).",
            "cite_ID": "N12-1052",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).",
            "GPT_cite_text": "Specifically, by replacing fine-grained language-specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features."
        },
        {
            "Number": 8,
            "refer_ID": "P11-1061",
            "refer_sids": [
                18
            ],
            "refer_text": "To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).",
            "cite_ID": "N12-1052",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).",
            "GPT_cite_text": "We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al. (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters."
        },
        {
            "Number": 9,
            "refer_ID": "P11-1061",
            "refer_sids": [
                158
            ],
            "refer_text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "cite_ID": "N12-1090",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "GPT_cite_text": "MT-based projection has been applied to various NLP tasks, such as part-of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus."
        },
        {
            "Number": 10,
            "refer_ID": "P11-1061",
            "refer_sids": [
                21
            ],
            "refer_text": "These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.",
            "cite_ID": "W11-2205",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.",
            "GPT_cite_text": "For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)."
        },
        {
            "Number": 11,
            "refer_ID": "P11-1061",
            "refer_sids": [
                158
            ],
            "refer_text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "cite_ID": "P13-1155",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "GPT_cite_text": "(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfer to induce POS tags between two languages."
        },
        {
            "Number": 12,
            "refer_ID": "P11-1061",
            "refer_sids": [
                21
            ],
            "refer_text": "These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.",
            "cite_ID": "D12-1127",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.",
            "GPT_cite_text": "Recent work by Das and Petrov (2011) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text."
        },
        {
            "Number": 13,
            "refer_ID": "P11-1061",
            "refer_sids": [
                10
            ],
            "refer_text": "To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "cite_ID": "D12-1127",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To bridge this gap, we consider a practically motivated scenario in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages. We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "GPT_cite_text": "These approaches build a dictionary by transferring labeled data from a resource-rich language (English) to a resource-poor language (Das and Petrov, 2011)."
        },
        {
            "Number": 14,
            "refer_ID": "P11-1061",
            "refer_sids": [
                24
            ],
            "refer_text": "The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.",
            "cite_ID": "P12-3012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.",
            "GPT_cite_text": "In recent years, research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever-growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactic-semantic (Peirsman and Pado, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011)."
        },
        {
            "Number": 15,
            "refer_ID": "P11-1061",
            "refer_sids": [
                17
            ],
            "refer_text": "Second, we treat the projected labels as features in an unsupervised model (\u00a75), rather than using them directly for supervised training.",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Second, we treat the projected labels as features in an unsupervised model (\u00a75), rather than using them directly for supervised training.",
            "GPT_cite_text": "Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of-speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English and achieves accuracies around 85% on the languages that we consider."
        },
        {
            "Number": 16,
            "refer_ID": "P11-1061",
            "refer_sids": [
                111
            ],
            "refer_text": "We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use the universal POS tagset of Petrov et al. (2011) in our experiments. This set consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks), and X (a catch-all for other categories such as abbreviations or foreign words).",
            "GPT_cite_text": "In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second, we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language."
        },
        {
            "Number": 17,
            "refer_ID": "P11-1061",
            "refer_sids": [
                10
            ],
            "refer_text": "To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To bridge this gap, we consider a practically motivated scenario in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages. We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "GPT_cite_text": "This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly resourced language to a lesser resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)."
        },
        {
            "Number": 18,
            "refer_ID": "P11-1061",
            "refer_sids": [
                29
            ],
            "refer_text": "To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (\u00a73.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (\u00a73.3). Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.",
            "GPT_cite_text": "Das and Petrov (2011) achieved the current state-of-the-art in unsupervised tagging by exploiting high-confidence alignments to copy tags from the source language to the target language."
        },
        {
            "Number": 19,
            "refer_ID": "P11-1061",
            "refer_sids": [
                161
            ],
            "refer_text": "Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections and bridge the gap between purely supervised and unsupervised POS tagging models.",
            "GPT_cite_text": "We have proposed a method for unsupervised POS tagging that performs on par with the current state-of-the-art (Das and Petrov, 2011), but is substantially less sophisticated (specifically not requiring convex optimization or a feature-based HMM)."
        }
    ],
    "A00-2030_vardha": [
        {
            "Number": 1,
            "refer_ID": "A00-2030",
            "refer_sids": [
                11
            ],
            "refer_text": "We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Section 5 compares our approach tooth ers in the literature, in particular that of (Miller et al., 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported it in (Marsh, 1998).",
            "GPT_cite_text": "Section 5 compares our approach to others in the literature, in particular that of (Miller et al., 2000)."
        },
        {
            "Number": 2,
            "refer_ID": "A00-2030",
            "refer_sids": [
                52
            ],
            "refer_text": "In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major di erences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machine-generated syntactic parse trees.",
            "GPT_cite_text": "The basic approach we described is very similar to the one presented in (Miller et al., 2000); however, there are a few major differences: in our approach, the augmentation of the syntactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly. The approach in (Miller"
        },
        {
            "Number": 3,
            "refer_ID": "A00-2030",
            "refer_sids": [
                50
            ],
            "refer_text": "Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.",
            "GPT_cite_text": "The semantic annotation required by our task is much simpler than that employed by Miller et al. (2000)."
        },
        {
            "Number": 4,
            "refer_ID": "A00-2030",
            "refer_sids": [
                34
            ],
            "refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One possibly bene cial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information; that is, entities and relations.",
            "GPT_cite_text": "One possibly beneficial extension of our work suggested by (Miller et al., 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level."
        },
        {
            "Number": 5,
            "refer_ID": "A00-2030",
            "refer_sids": [
                106
            ],
            "refer_text": "We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similar to the approach in (Miller et al, 2000 )weinitialized the SLM statistics from the UPenn Tree bank parse trees (about 1Mwds of training data) at the rst training stage, see Section 3",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We were able to use the Penn Treebank to estimate the syntactic parameters; no additional syntactic training was required.",
            "GPT_cite_text": "Similar to the approach in (Miller et al., 2000), we initialized the SLM statistics from the UPenn Treebank parse trees (about 1 M words of training data) at the first training stage, see Section 3."
        },
        {
            "Number": 6,
            "refer_ID": "A00-2030",
            "refer_sids": [
                6
            ],
            "refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.",
            "cite_ID": "P14-1078",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) for information extraction.",
            "GPT_cite_text": "Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns."
        },
        {
            "Number": 7,
            "refer_ID": "A00-2030",
            "refer_sids": [
                6
            ],
            "refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.",
            "cite_ID": "P05-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) for information extraction.",
            "GPT_cite_text": "One interesting system that does not belong to the above class is that of Miller et al. (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations."
        },
        {
            "Number": 8,
            "refer_ID": "A00-2030",
            "refer_sids": [
                34
            ],
            "refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "P05-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information; that is, entities and relations.",
            "GPT_cite_text": "Miller et al. (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees."
        },
        {
            "Number": 9,
            "refer_ID": "A00-2030",
            "refer_sids": [
                12
            ],
            "refer_text": "The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).",
            "cite_ID": "P05-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).",
            "GPT_cite_text": "Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al. (2000), which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction, and relation extraction in a single model."
        },
        {
            "Number": 10,
            "refer_ID": "A00-2030",
            "refer_sids": [
                3
            ],
            "refer_text": "Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.",
            "cite_ID": "H05-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparkhi, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania Treebank as a gold standard.",
            "GPT_cite_text": "(Miller et al., 2000) have combined entity recognition, parsing, and relation extraction into a jointly trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly labeled data is unavailable."
        },
        {
            "Number": 11,
            "refer_ID": "A00-2030",
            "refer_sids": [
                34
            ],
            "refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "P04-1054",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information; that is, entities and relations.",
            "GPT_cite_text": "Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types."
        },
        {
            "Number": 12,
            "refer_ID": "A00-2030",
            "refer_sids": [
                32
            ],
            "refer_text": "Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties \u2014 especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs \u2014 would also benefit semantic analysis.",
            "cite_ID": "P04-1054",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "WhereasMiller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties \u2014 especially their ability to learn from large amounts of data and their robustness when presented with unexpected inputs \u2014 would also benefit semantic analysis.",
            "GPT_cite_text": "Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance."
        },
        {
            "Number": 13,
            "refer_ID": "A00-2030",
            "refer_sids": [
                10
            ],
            "refer_text": "Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.",
            "cite_ID": "W05-0602",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The syntactic model in (Miller et al, 2000) is similar to Collins?, but doesnot use features like sub cat frames and distance measures",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Instead, our parsing algorithm, trained on the UPenn Treebank, was run on the New York Times source to create unsupervised syntactic training that was constrained to be consistent with semantic annotation.",
            "GPT_cite_text": "The syntactic model in (Miller et al., 2000) is similar to Collins, but does not use features like subcat frames and distance measures."
        },
        {
            "Number": 14,
            "refer_ID": "A00-2030",
            "refer_sids": [
                94
            ],
            "refer_text": "Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.",
            "cite_ID": "N07-2041",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structures of the sentence.",
            "GPT_cite_text": "Similar to the approach in (Miller et al., 2000) and (Kulick et al., 2004), our parser integrates both syntactic and semantic annotations into a single annotation, as shown in Figure 2."
        },
        {
            "Number": 15,
            "refer_ID": "A00-2030",
            "refer_sids": [
                6
            ],
            "refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.",
            "cite_ID": "W10-2924",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) for information extraction.",
            "GPT_cite_text": "Miller et al. (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels."
        },
        {
            "Number": 16,
            "refer_ID": "A00-2030",
            "refer_sids": [
                104
            ],
            "refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.",
            "cite_ID": "W06-0508",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) can be used effectively for information extraction.",
            "GPT_cite_text": "Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al., 2000), or clustering of semantically similar syntactic dependencies according to their selectional restrictions (Gamallo et al., 2002)."
        },
        {
            "Number": 17,
            "refer_ID": "A00-2030",
            "refer_sids": [
                26
            ],
            "refer_text": "We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.",
            "cite_ID": "P07-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.).",
            "GPT_cite_text": "This includes parsing and relation extraction (Miller et al., 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al., 2004)."
        },
        {
            "Number": 18,
            "refer_ID": "A00-2030",
            "refer_sids": [
                6
            ],
            "refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.",
            "cite_ID": "W05-0636",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) for information extraction.",
            "GPT_cite_text": "For example, Miller et al. (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks."
        },
        {
            "Number": 19,
            "refer_ID": "A00-2030",
            "refer_sids": [
                19
            ],
            "refer_text": "Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.",
            "GPT_cite_text": "Miller et al. (2000) address the task of relation extraction from the statistical parsing viewpoint."
        }
    ],
    "P08-1043_aakansha": [
        {
            "Number": 1,
            "refer_ID": "P08-1043",
            "refer_sids": [
                94
            ],
            "refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "cite_ID": "C10-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a priori equally likely; the only reason to prefer one segmentation over another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "GPT_cite_text": "Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew."
        },
        {
            "Number": 2,
            "refer_ID": "P08-1043",
            "refer_sids": [
                4
            ],
            "refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "cite_ID": "P11-1141",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-token handling technique, our model outperforms previous pipelined, integrated, or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models."
        },
        {
            "Number": 3,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "P10-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of He brew, based on lattice parsing",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging, and parsing of Hebrew, based on lattice parsing."
        },
        {
            "Number": 4,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "P11-1089",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) pro pose a generative joint model",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) propose a generative joint model."
        },
        {
            "Number": 5,
            "refer_ID": "P08-1043",
            "refer_sids": [
                4
            ],
            "refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "cite_ID": "W10-1404",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-token handling technique, our model outperforms previous pipelined, integrated, or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "P11-2124",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrewtext",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text."
        },
        {
            "Number": 7,
            "refer_ID": "P08-1043",
            "refer_sids": [
                69,
                70
            ],
            "refer_text": "We represent all morphological analyses of a given utterance using a lattice structure.\nEach lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.",
            "cite_ID": "P11-2124",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "We represent all morphological analyses of a given utterance using a lattice structure. Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.",
            "GPT_cite_text": "Following Goldberg and Tsarfaty (2008), we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice."
        },
        {
            "Number": 8,
            "refer_ID": "P08-1043",
            "refer_sids": [
                85
            ],
            "refer_text": "The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.",
            "cite_ID": "P12-2002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "2The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008)",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "The input: The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.",
            "GPT_cite_text": "The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008)."
        },
        {
            "Number": 9,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "D12-1046",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A study that is closely related toours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew."
        },
        {
            "Number": 10,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "D12-1133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "Models that, in addition, incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1043",
            "refer_sids": [
                21
            ],
            "refer_text": "Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is the same grammar as described in (Goldberg and Tsarfaty, 2008)",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG, and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling, our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on par with current respective standalone models.",
            "GPT_cite_text": "It is the same grammar as described in (Goldberg and Tsarfaty, 2008)."
        },
        {
            "Number": 14,
            "refer_ID": "P08-1043",
            "refer_sids": [
                94
            ],
            "refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a priori equally likely; the only reason to prefer one segmentation over another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "GPT_cite_text": "Several studies followed this line (Cohen and Smith, 2007), the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task."
        },
        {
            "Number": 15,
            "refer_ID": "P08-1043",
            "refer_sids": [
                133,
                134
            ],
            "refer_text": "Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.\nSuch resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training \tsimilar to (Cohen and Smith, 2007).",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Morphological Analyzer: Ideally, we would use an off-the-shelf morphological analyzer for mapping each input token to its possible analyses. Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme that is incompatible with the one used by the Hebrew Treebanks. For this reason, we use a data-driven morphological analyzer derived from the training similar to (Cohen and Smith, 2007).",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the treebank."
        },
        {
            "Number": 16,
            "refer_ID": "P08-1043",
            "refer_sids": [
                85
            ],
            "refer_text": "The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "The input: The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.",
            "GPT_cite_text": "The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to construct a lattice for each input token."
        },
        {
            "Number": 17,
            "refer_ID": "P08-1043",
            "refer_sids": [
                155
            ],
            "refer_text": "Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parse val to use characters instead of space-delimited tokens as its basic units",
            "label": [
                "Method_citation"
            ],
            "GPT_refer_text": "Evaluation: We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.",
            "GPT_cite_text": "Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units."
        }
    ],
    "D09-1092_sweta": [
        {
            "Number": 1,
            "refer_ID": "D09-1092",
            "refer_sids": [
                32
            ],
            "refer_text": "Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.",
            "cite_ID": "P14-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Outside of the field of topic modeling, Kawabata et al. (Kawabata et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.",
            "GPT_cite_text": "This configuration is similar to PolyLDA (Mimno et al., 2009) or LinkLDA (Yano et al., 2009), such that utterances from different parties are treated as different languages or blog posts and comments pairs."
        },
        {
            "Number": 2,
            "refer_ID": "D09-1092",
            "refer_sids": [
                39
            ],
            "refer_text": "Additionally, PLTM assumes that each \u201ctopic\u201d consists of a set of discrete distributions over words\u2014one for each language l = 1, ... , L. In other words, rather than using a single set of topics \u03a6 = {\u03c61, ... , \u03c6T}, as in LDA, there are L sets of language-specific topics, \u03a61, ... , \u03a6L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter \u03b2l.",
            "cite_ID": "P10-1044",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Additionally, PLTM assumes that each \u201ctopic\u201d consists of a set of discrete distributions over words\u2014one for each language l = 1, ..., L. In other words, rather than using a single set of topics \u03a6 = {\u03c61, ..., \u03c6T}, as in LDA, there are L sets of language-specific topics, \u03a61, ..., \u03a6L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter \u03b2l.",
            "GPT_cite_text": "Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009)."
        },
        {
            "Number": 3,
            "refer_ID": "D09-1092",
            "refer_sids": [
                138
            ],
            "refer_text": "We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.",
            "cite_ID": "P11-2084",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexicon.",
            "GPT_cite_text": "(Mimno et al., 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then adding the Cartesian product of these sets for every topic to a set of candidate translations."
        },
        {
            "Number": 4,
            "refer_ID": "D09-1092",
            "refer_sids": [
                196
            ],
            "refer_text": "When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
            "cite_ID": "E12-1014",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
            "GPT_cite_text": "Our Wikipedia-based topic similarity feature, w(f, e), is similar in spirit to polylingual topic models (Mimno et al. 2009), but it is scalable to full bilingual lexicon induction."
        },
        {
            "Number": 5,
            "refer_ID": "D09-1092",
            "refer_sids": [
                111
            ],
            "refer_text": "In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts \u2013 i.e., we put each of these documents in a single-document tuple.",
            "cite_ID": "D11-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "of English document and the second half of its aligned foreign language document (Mimno et al,2009)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In order to simulate this scenario, we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts \u2013 i.e., we put each of these documents in a single-document tuple.",
            "GPT_cite_text": "Of the English document and the second half of its aligned foreign language document (Mimno et al., 2009)"
        },
        {
            "Number": 6,
            "refer_ID": "D09-1092",
            "refer_sids": [
                128
            ],
            "refer_text": "Although the PLTM is clearly not a substitute for a machine translation system\u2014it has no way to represent syntax or even multi-word phrases\u2014it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Although the PLTM is clearly not a substitute for a machine translation system\u2014it has no way to represent syntax or even multi-word phrases\u2014it is clear from the examples in Figure 2 that the sets of high-probability words in different languages for a given topic are likely to include translations.",
            "GPT_cite_text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al., 2009) for more details."
        },
        {
            "Number": 7,
            "refer_ID": "D09-1092",
            "refer_sids": [
                122
            ],
            "refer_text": "Divergence drops significantly when the proportion of \u201cglue\u201d tuples increases from 0.01 to 0.25.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Divergence drops significantly when the proportion of \u201cglue\u201d tuples increases from 0.01 to 0.25.",
            "GPT_cite_text": "Evaluation Corpus: The automatic evaluation of cross-lingual coreference systems requires annotated data. Mimno et al. (2009) showed that so long as the proportion of topically aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly."
        },
        {
            "Number": 8,
            "refer_ID": "D09-1092",
            "refer_sids": [
                35
            ],
            "refer_text": "The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.",
            "GPT_cite_text": "Similarly, Polylingual Topic Models (PLTM) (Mimno et al., 2009) generalized LDA to tuples of documents from multiple languages."
        },
        {
            "Number": 9,
            "refer_ID": "D09-1092",
            "refer_sids": [
                110
            ],
            "refer_text": "Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.",
            "GPT_cite_text": "Our baseline joint PLSA model (JPLSA) is closely related to the polylingual LDA model of (Mimno et al., 2009)."
        },
        {
            "Number": 10,
            "refer_ID": "D09-1092",
            "refer_sids": [
                30
            ],
            "refer_text": "However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "However, they evaluate their model on only two languages (English and Chinese) and do not use the model to detect differences between languages.",
            "GPT_cite_text": "We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al., 2009)."
        },
        {
            "Number": 11,
            "refer_ID": "D09-1092",
            "refer_sids": [
                146
            ],
            "refer_text": "In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near-translations in an unstructured corpus.",
            "GPT_cite_text": "The difference between the JPLSA model and the polylingual topic model of (Mimno et al., 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al., 2009)."
        },
        {
            "Number": 12,
            "refer_ID": "D09-1092",
            "refer_sids": [
                77
            ],
            "refer_text": "Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Maximum topic probability in document: Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the tokens in each of the languages. We use the R density function.",
            "GPT_cite_text": "Another difference between our model and the polylingual LDA model of (Mimno et al., 2009) is that we use maximum a posteriori (MAP) instead of Bayesian inference."
        },
        {
            "Number": 13,
            "refer_ID": "D09-1092",
            "refer_sids": [
                156
            ],
            "refer_text": "We use both Jensen-Shannon divergence and cosine distance.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We use both Jensen-Shannon divergence and cosine distance.",
            "GPT_cite_text": "For computing distance, we used the L1 norm of the difference, which worked a bit better than the Jensen-Shannon divergence between the topic vectors used in (Mimno et al., 2009)."
        },
        {
            "Number": 14,
            "refer_ID": "D09-1092",
            "refer_sids": [
                31
            ],
            "refer_text": "They also provide little analysis of the differences between polylingual and single-language topic models.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Documents are defined as speeches by a single speaker, as in (Mimno et al, 2009) .4 For the Wikipedia set, we use 43,380 training documents, 8,675 development documents, and 8,675 final test set documents. For both corpora, the terms are extracted by word breaking all documents, removing the top 50 most frequent terms and keeping the next 20,000 most frequent terms",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "They also provide little analysis of the differences between polylingual and single-language topic models.",
            "GPT_cite_text": "Documents are defined as speeches by a single speaker, as in (Mimno et al., 2009). For the Wikipedia set, we use 43,380 training documents, 8,675 development documents, and 8,675 final test set documents. For both corpora, the terms are extracted by word-breaking all documents, removing the top 50 most frequent terms, and keeping the next 20,000 most frequent terms."
        },
        {
            "Number": 15,
            "refer_ID": "D09-1092",
            "refer_sids": [
                19
            ],
            "refer_text": "We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.",
            "GPT_cite_text": "In previously reported work, (Mimno et al., 2009) evaluated parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours."
        },
        {
            "Number": 16,
            "refer_ID": "D09-1092",
            "refer_sids": [
                131
            ],
            "refer_text": "We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).",
            "cite_ID": "W12-3117",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).",
            "GPT_cite_text": "We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing, e.g., polylingual topic models (Mimno et al., 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)."
        },
        {
            "Number": 17,
            "refer_ID": "D09-1092",
            "refer_sids": [
                196
            ],
            "refer_text": "When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
            "GPT_cite_text": "ji = wjk? M j? m k = 1w j k, (1) where M j is the topic distribution of document j and w k is the number of occurrences of phrase pair X k in document j. Mimno et al. (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)."
        },
        {
            "Number": 18,
            "refer_ID": "D09-1092",
            "refer_sids": [
                192
            ],
            "refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "GPT_cite_text": "Tuple-specific topic distributions are learned using LDA with distinct topic-word concentration parameters. Mimno et al. (2009) show that PLTM sufficiently aligns topics in parallel corpora."
        },
        {
            "Number": 19,
            "refer_ID": "D09-1092",
            "refer_sids": [
                195
            ],
            "refer_text": "Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Additionally, PLTM can support the creation of bilingual lexicons for low-resource language pairs, providing candidate translations for more computationally intensive alignment processes without the sentence-aligned translations typically used in such tasks.",
            "GPT_cite_text": "A good candidate for multilingual topic analyses is polylingual topic models (Mimno et al., 2009), which learn topics for multiple languages, creating tuples of language-specific distributions over monolingual vocabularies for each topic."
        },
        {
            "Number": 20,
            "refer_ID": "D09-1092",
            "refer_sids": [
                6
            ],
            "refer_text": "Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).",
            "GPT_cite_text": "To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al. (2009): add a token-specific language variable and a process for identifying aligned topics. First, polylingual topic models require parallel or comparable corpora in which each document has an assigned language."
        }
    ],
    "D10-1044_swastika": [
        {
            "Number": 1,
            "refer_ID": "D10-1044",
            "refer_sids": [
                9.0
            ],
            "refer_text": "In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "cite_ID": "P11-2074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another popular task in SMT is domain adaptation (Foster et al, 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "GPT_cite_text": "Another popular task in SMT is domain adaptation (Foster et al., 2010)."
        },
        {
            "Number": 3,
            "refer_ID": "D10-1044",
            "refer_sids": [
                9.0
            ],
            "refer_text": "In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "cite_ID": "D12-1129",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In this paper, we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "GPT_cite_text": "Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now."
        },
        {
            "Number": 4,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62.0
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "P14-2093",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "GPT_cite_text": "Yasuda et al. (2008) and Foster et al. (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models."
        },
        {
            "Number": 5,
            "refer_ID": "D10-1044",
            "refer_sids": [
                71.0
            ],
            "refer_text": "Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c\u03bb(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Finally, we incorporate the instance-weighting model into a general linear combination and learn weights and mixing parameters simultaneously, where c\u03bb(s, t) is a modified count for the pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.",
            "GPT_cite_text": "However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies. Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al. (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMT translation model: the phrase translation probabilities p(t|s) and p(s|t), and the lexical weights lex(t|s) and lex(s|t)."
        },
        {
            "Number": 6,
            "refer_ID": "D10-1044",
            "refer_sids": [
                96.0
            ],
            "refer_text": "We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We used it to score all phrase pairs in the OUT table in order to provide a feature for the instance-weighting model.",
            "GPT_cite_text": "Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs."
        },
        {
            "Number": 7,
            "refer_ID": "D10-1044",
            "refer_sids": [
                71.0
            ],
            "refer_text": "Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c\u03bb(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Finally, we incorporate the instance-weighting model into a general linear combination and learn weights and mixing parameters simultaneously, where c\u03bb(s, t) is a modified count for the pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.",
            "GPT_cite_text": "These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al. (2010) combine the two, applying linear interpolation to combine the instance-weighted out-of-domain model with an in-domain model."
        },
        {
            "Number": 8,
            "refer_ID": "D10-1044",
            "refer_sids": [
                144.0
            ],
            "refer_text": "In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "In this paper, we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "GPT_cite_text": "Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE:EN and 1:5 for HT:EN). Previous research has been performed with ratios of 1:100 (Foster et al. 2010) or 1:400 (Axelrod et al. 2011)."
        },
        {
            "Number": 9,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62.0
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "GPT_cite_text": "We expand on work by (Foster et al. 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models. We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research."
        },
        {
            "Number": 10,
            "refer_ID": "D10-1044",
            "refer_sids": [
                45.0
            ],
            "refer_text": "An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).",
            "GPT_cite_text": "In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model (Foster et al., 2010) for conditional phrase-pair probabilities over IN and OUT."
        },
        {
            "Number": 12,
            "refer_ID": "D10-1044",
            "refer_sids": [
                75.0
            ],
            "refer_text": "However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, it is robust, efficient, and easy to implement. To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "GPT_cite_text": "For efficiency and stability, we use the EM algorithm to find, rather than L-BFGS as in (Foster et al., 2010)."
        },
        {
            "Number": 14,
            "refer_ID": "D10-1044",
            "refer_sids": [
                22.0
            ],
            "refer_text": "Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Within this framework, we use features intended to capture the degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.",
            "GPT_cite_text": "Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality."
        },
        {
            "Number": 15,
            "refer_ID": "D10-1044",
            "refer_sids": [
                96.0
            ],
            "refer_text": "We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.",
            "cite_ID": "P13-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As in (Foster et al, 2010), this approach works at the level of phrase pairs",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We used it to score all phrase pairs in the OUT table in order to provide a feature for the instance-weighting model.",
            "GPT_cite_text": "As in (Foster et al., 2010), this approach works at the level of phrase pairs."
        },
        {
            "Number": 16,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62.0
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "GPT_cite_text": "The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al. (2008) and Foster et al. (2010)."
        },
        {
            "Number": 17,
            "refer_ID": "D10-1044",
            "refer_sids": [
                42.0
            ],
            "refer_text": "The natural baseline approach is to concatenate data from IN and OUT.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The natural baseline approach is to concatenate data from IN and OUT.",
            "GPT_cite_text": "Foster et al. (2010) do not mention what percentage of the corpus they select for their IR baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance."
        },
        {
            "Number": 18,
            "refer_ID": "D10-1044",
            "refer_sids": [
                96.0
            ],
            "refer_text": "We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) further perform this on extracted phrase pairs, not just sentences",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We used it to score all phrase pairs in the OUT table in order to provide a feature for the instance-weighting model.",
            "GPT_cite_text": "Foster et al. (2010) further performed this on extracted phrase pairs, not just sentences."
        }
    ],
    "P04-1036_aakansha": [
        {
            "Number": 1,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account (McCarthy et al., 2004)."
        },
        {
            "Number": 2,
            "refer_ID": "P04-1036",
            "refer_sids": [
                15
            ],
            "refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first or predominant sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "GPT_cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41 Table 2: The SENSEVAL-2 first sense on the SENSEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available."
        },
        {
            "Number": 3,
            "refer_ID": "P04-1036",
            "refer_sids": [
                41
            ],
            "refer_text": "In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The method is described in (McCarthy et al, 2004), which we summarise here",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbors from automatically acquired thesauri.",
            "GPT_cite_text": "The method is described in (McCarthy et al., 2004), which we summarize here."
        },
        {
            "Number": 5,
            "refer_ID": "P04-1036",
            "refer_sids": [
                4
            ],
            "refer_text": "We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.",
            "GPT_cite_text": "McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research and propose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges."
        },
        {
            "Number": 6,
            "refer_ID": "P04-1036",
            "refer_sids": [
                15
            ],
            "refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first or predominant sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "GPT_cite_text": "Previous research in inducing sense rankings from an untagged corpus (McCarthy et al., 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007), will provide the starting point for research in this direction."
        },
        {
            "Number": 7,
            "refer_ID": "P04-1036",
            "refer_sids": [
                180,
                181
            ],
            "refer_text": "The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.\nThis is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task, giving us a WSD precision of 64% on an all-nouns task. This is just 5% lower than results using the first sense in the manually labeled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.",
            "GPT_cite_text": "McCarthy et al. (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus."
        },
        {
            "Number": 8,
            "refer_ID": "P04-1036",
            "refer_sids": [
                126
            ],
            "refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "GPT_cite_text": "Research by McCarthy et al. (2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn."
        },
        {
            "Number": 9,
            "refer_ID": "P04-1036",
            "refer_sids": [
                48
            ],
            "refer_text": "To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To find the first sense of a word, we take each sense in turn and obtain a score reflecting the prevalence, which is used for ranking.",
            "GPT_cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al., 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense."
        },
        {
            "Number": 11,
            "refer_ID": "P04-1036",
            "refer_sids": [
                169
            ],
            "refer_text": "This method obtains precision of 61% and recall 51%.",
            "cite_ID": "P10-1155",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "This method obtains a precision of 61% and a recall of 51%.",
            "GPT_cite_text": "McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the WordNet Similarity JCN measure (Jiang and Conrath, 1997)."
        },
        {
            "Number": 12,
            "refer_ID": "P04-1036",
            "refer_sids": [
                171
            ],
            "refer_text": "In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In contrast, we use the neighbors lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.",
            "GPT_cite_text": "In doing so, we provide first results on the application of WordNet automatic sense ranking (ASR) to French parsing, using the method of McCarthy et al. (2004)."
        },
        {
            "Number": 13,
            "refer_ID": "P04-1036",
            "refer_sids": [
                171
            ],
            "refer_text": "In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In contrast, we use the neighbors lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.",
            "GPT_cite_text": "To define an appropriate categorical distribution over synsets for each lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s \u2208 Sx, following the approach of McCarthy et al. (2004)."
        },
        {
            "Number": 14,
            "refer_ID": "P04-1036",
            "refer_sids": [
                115
            ],
            "refer_text": "Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labeling.",
            "GPT_cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al. (2004)"
        },
        {
            "Number": 16,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "S12-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al., 2004)."
        },
        {
            "Number": 17,
            "refer_ID": "P04-1036",
            "refer_sids": [
                126
            ],
            "refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "cite_ID": "W10-2803",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "GPT_cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining the predominant sense in a given domain (McCarthy et al., 2004), or to work directly with paraphrases (McCarthy and Navigli, 2009)."
        },
        {
            "Number": 18,
            "refer_ID": "P04-1036",
            "refer_sids": [
                180,
                181
            ],
            "refer_text": "The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.\nThis is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.",
            "cite_ID": "W08-2107",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task, giving us a WSD precision of 64% on an all-nouns task. This is just 5% lower than results using the first sense in the manually labeled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.",
            "GPT_cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by WordNet, resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al. (2004))."
        },
        {
            "Number": 19,
            "refer_ID": "P04-1036",
            "refer_sids": [
                180
            ],
            "refer_text": "The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.",
            "cite_ID": "D07-1026",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task, giving us a WSD precision of 64% on an all-nouns task.",
            "GPT_cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al., 2004)."
        },
        {
            "Number": 20,
            "refer_ID": "P04-1036",
            "refer_sids": [
                41
            ],
            "refer_text": "In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.",
            "cite_ID": "W12-2429",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbors from automatically acquired thesauruses.",
            "GPT_cite_text": "The first, most frequent sense (MFS) (McCarthy et al., 2004), is a widely used baseline for supervised WSD systems."
        }
    ],
    "P05-1013_aakansha": [
        {
            "Number": 1,
            "refer_ID": "P05-1013",
            "refer_sids": [
                20
            ],
            "refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.",
            "cite_ID": "W05-1505",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques.",
            "GPT_cite_text": "Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 2,
            "refer_ID": "P05-1013",
            "refer_sids": [
                24
            ],
            "refer_text": "We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).",
            "cite_ID": "P08-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "1http: //sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)? s dependencyparser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "We call this pseudoprojective dependency parsing since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).",
            "GPT_cite_text": "1 http://sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)'s dependency parser, based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 3,
            "refer_ID": "P05-1013",
            "refer_sids": [
                106
            ],
            "refer_text": "However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).",
            "cite_ID": "W10-1401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).",
            "GPT_cite_text": "Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of Nivre and Nilsson (2005) improves accuracy for dependency parsing of Basque."
        },
        {
            "Number": 4,
            "refer_ID": "P05-1013",
            "refer_sids": [
                86
            ],
            "refer_text": "As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.",
            "cite_ID": "P12-3029",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For tree banks with non-projective trees weuse the pseudo-projective parsing technique to trans form the tree bank into projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both datasets.",
            "GPT_cite_text": "For tree banks with non-projective trees, we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 5,
            "refer_ID": "P05-1013",
            "refer_sids": [
                20
            ],
            "refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.",
            "cite_ID": "W10-1403",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques.",
            "GPT_cite_text": "It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 6,
            "refer_ID": "P05-1013",
            "refer_sids": [
                36
            ],
            "refer_text": "As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.",
            "cite_ID": "D08-1008",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson,2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "As observed by Kahane et al. (1998), any (non-projective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014* wj holds in the original graph.",
            "GPT_cite_text": "To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which non-projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non-projective links at parse time."
        },
        {
            "Number": 7,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109
            ],
            "refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "D07-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": ",wn in O (n) time, producing a projective dependency graph satisfying conditions 1? 4 in section 2.1, possibly after adding arcs (0, i ,lr) for every node i 6= 0 that is a root in the output graph (where lr is a special label for root modifiers) .Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to pre process training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods ,e.g., memory-based learning or support vector machines",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "```, run in O(n) time, producing a projective dependency graph satisfying conditions 1-4 in section 2.1, possibly after adding arcs (0, i, lr) for every node i \u2260 0 that is a root in the output graph (where lr is a special label for root modifiers). Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods, e.g., memory-based learning or support vector machines.```"
        },
        {
            "Number": 8,
            "refer_ID": "P05-1013",
            "refer_sids": [
                36
            ],
            "refer_text": "As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.",
            "cite_ID": "D07-1119",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "As observed by Kahane et al. (1998), any (non-projective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014* wj holds in the original graph.",
            "GPT_cite_text": "For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists of lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective."
        },
        {
            "Number": 9,
            "refer_ID": "P05-1013",
            "refer_sids": [
                23
            ],
            "refer_text": "By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.",
            "cite_ID": "N07-1050",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Nova? k, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise to non-projective structures. The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence.",
            "GPT_cite_text": "Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non-projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Novak, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)."
        },
        {
            "Number": 10,
            "refer_ID": "P05-1013",
            "refer_sids": [
                24
            ],
            "refer_text": "We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).",
            "cite_ID": "W09-1207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "troduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).",
            "GPT_cite_text": "Introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German, and English."
        },
        {
            "Number": 11,
            "refer_ID": "P05-1013",
            "refer_sids": [
                80,
                81
            ],
            "refer_text": "As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.\nHowever, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.",
            "cite_ID": "E09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "non projective (Nivre and Nilsson, 2005), we char ac terise a sense in which the structures appearing in tree banks can be viewed as being only? slightly? ill-nested",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "As shown in Table 3, the proportion of sentences containing some non-projective dependencies ranges from about 15% in DDT to almost 25% in PDT. However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.",
            "GPT_cite_text": "Non-projective (Nivre and Nilsson, 2005), we characterize a sense in which the structures appearing in treebanks can be viewed as being only slightly ill-nested."
        },
        {
            "Number": 12,
            "refer_ID": "P05-1013",
            "refer_sids": [
                24
            ],
            "refer_text": "We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).",
            "cite_ID": "W09-1218",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).",
            "GPT_cite_text": "In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformations (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 13,
            "refer_ID": "P05-1013",
            "refer_sids": [
                20
            ],
            "refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques.",
            "GPT_cite_text": "Pseudo-projective parsing for recovering non-projective structures (Nivre and Nilsson, 2005)"
        },
        {
            "Number": 14,
            "refer_ID": "P05-1013",
            "refer_sids": [
                24
            ],
            "refer_text": "We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).",
            "GPT_cite_text": "Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (Section 3.4)."
        },
        {
            "Number": 15,
            "refer_ID": "P05-1013",
            "refer_sids": [
                20
            ],
            "refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques.",
            "GPT_cite_text": "Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser."
        },
        {
            "Number": 16,
            "refer_ID": "P05-1013",
            "refer_sids": [
                49
            ],
            "refer_text": "The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d\u2191h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.",
            "cite_ID": "C08-1081",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Weprojectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r? h, where r is the original label and h is the label of the original head in the non-projective dependency graph",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types. In the first encoding scheme, called Head, we use a new label d\u2191h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.",
            "GPT_cite_text": "We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r/h, where r is the original label and h is the label of the original head in the non-projective dependency graph."
        },
        {
            "Number": 17,
            "refer_ID": "P05-1013",
            "refer_sids": [
                109
            ],
            "refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.",
            "GPT_cite_text": "For tree banks with non-projective trees, we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)."
        },
        {
            "Number": 18,
            "refer_ID": "P05-1013",
            "refer_sids": [
                14
            ],
            "refer_text": "While the proportion of sentences containing non-projective dependencies is often 15\u201325%, the total proportion of non-projective arcs is normally only 1\u20132%.",
            "cite_ID": "P11-2121",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "While the proportion of sentences containing non-projective dependencies is often 15\u201325%, the total proportion of non-projective arcs is normally only 1\u20132%.",
            "GPT_cite_text": "Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing in all cases."
        },
        {
            "Number": 20,
            "refer_ID": "P05-1013",
            "refer_sids": [
                20
            ],
            "refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.",
            "cite_ID": "D07-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective trans formations as described in (Nivre and Nilsson, 2005)",
            "label": [
                "method_Citation"
            ],
            "GPT_refer_text": "In this paper, we show how non-projective dependency parsing can be achieved by combining a data-driven projective parser with special graph transformation techniques.",
            "GPT_cite_text": "The resulting algorithm is projective, and non-projectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005)."
        }
    ],
    "W99-0623_vardha": [
        {
            "Number": 1,
            "refer_ID": "W99-0623",
            "refer_sids": [
                85
            ],
            "refer_text": "We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.",
            "cite_ID": "A00-2005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.",
            "GPT_cite_text": "1 Introduction Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy."
        },
        {
            "Number": 2,
            "refer_ID": "W99-0623",
            "refer_sids": [
                117
            ],
            "refer_text": "Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.",
            "cite_ID": "A00-2005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.",
            "GPT_cite_text": "The collection of hypotheses ti = fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)"
        },
        {
            "Number": 4,
            "refer_ID": "W99-0623",
            "refer_sids": [
                72
            ],
            "refer_text": "The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.",
            "cite_ID": "N10-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.",
            "GPT_cite_text": "5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers."
        },
        {
            "Number": 5,
            "refer_ID": "W99-0623",
            "refer_sids": [
                120
            ],
            "refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision. Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "GPT_cite_text": "A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)."
        },
        {
            "Number": 6,
            "refer_ID": "W99-0623",
            "refer_sids": [
                139
            ],
            "refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "GPT_cite_text": "This approach roughly corresponds to (Henderson and Brill, 1999)'s Na\u00efve Bayes parse hybridization."
        },
        {
            "Number": 7,
            "refer_ID": "W99-0623",
            "refer_sids": [
                84
            ],
            "refer_text": "The first shows how constituent features and context do not help in deciding which parser to trust.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) also reported that context did not help them to outperform simple voting",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first shows how constituent features and context do not help in deciding which parser to trust.",
            "GPT_cite_text": "Henderson and Brill (1999) also reported that context did not help them outperform simple voting."
        },
        {
            "Number": 8,
            "refer_ID": "W99-0623",
            "refer_sids": [
                76
            ],
            "refer_text": "The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.",
            "GPT_cite_text": "(Henderson and Brill, 1999) improved their best parser's F-measure from 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)."
        },
        {
            "Number": 10,
            "refer_ID": "W99-0623",
            "refer_sids": [
                38
            ],
            "refer_text": "Under certain conditions the constituent voting and na\u00efve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.",
            "cite_ID": "P01-1005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Under certain conditions, the constituent voting and na\u00efve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.",
            "GPT_cite_text": "Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren et al., 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pedersen, 2000)."
        },
        {
            "Number": 11,
            "refer_ID": "W99-0623",
            "refer_sids": [
                25
            ],
            "refer_text": "In our particular case the majority requires the agreement of only two parsers because we have only three.",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In our particular case, the majority requires the agreement of only two parties because we have only three.",
            "GPT_cite_text": "Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes: one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees."
        },
        {
            "Number": 12,
            "refer_ID": "W99-0623",
            "refer_sids": [
                72
            ],
            "refer_text": "The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.",
            "GPT_cite_text": "Henderson and Brill (1999) combine three parsers and obtain an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper."
        },
        {
            "Number": 13,
            "refer_ID": "W99-0623",
            "refer_sids": [
                87
            ],
            "refer_text": "It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It is possible that one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.",
            "GPT_cite_text": "Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)."
        },
        {
            "Number": 14,
            "refer_ID": "W99-0623",
            "refer_sids": [
                51
            ],
            "refer_text": "One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.",
            "cite_ID": "N06-2033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.",
            "GPT_cite_text": "Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents of the initial trees."
        },
        {
            "Number": 15,
            "refer_ID": "W99-0623",
            "refer_sids": [
                79
            ],
            "refer_text": "Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Precision is the portion of hypothesized constituents that are correct, and recall is the portion of the Treebank constituents that are hypothesized.",
            "GPT_cite_text": "(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined."
        },
        {
            "Number": 16,
            "refer_ID": "W99-0623",
            "refer_sids": [
                27
            ],
            "refer_text": "Another technique for parse hybridization is to use a na\u00efve Bayes classifier to determine which constituents to include in the parse.",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another technique for parse hybridization is to use a na\u00efve Bayes classifier to determine which constituents to include in the parse.",
            "GPT_cite_text": "(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents."
        },
        {
            "Number": 17,
            "refer_ID": "W99-0623",
            "refer_sids": [
                77
            ],
            "refer_text": "Each parse is converted into a set of constituents represented as a tuples: (label, start, end).",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Each parse is converted into a set of constituents represented as tuples: (label, start, end).",
            "GPT_cite_text": "Output (Figure 3). Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected F-score within the Minimum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs."
        },
        {
            "Number": 19,
            "refer_ID": "W99-0623",
            "refer_sids": [
                11
            ],
            "refer_text": "Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).",
            "cite_ID": "N03-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), and named entity recognition (Borthwick et al., 1998).",
            "GPT_cite_text": "In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994)."
        },
        {
            "Number": 20,
            "refer_ID": "W99-0623",
            "refer_sids": [
                116
            ],
            "refer_text": "The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.",
            "cite_ID": "C10-1151",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.",
            "GPT_cite_text": "Henderson and Brill (1999) perform parse selection by maximizing the expected precision of selected parses with respect to the set of parses to be combined."
        }
    ],
    "W99-0623_swastika": [
        {
            "Number": 1,
            "refer_ID": "W99-0623",
            "refer_sids": [
                85
            ],
            "refer_text": "We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.",
            "cite_ID": "A00-2005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.",
            "GPT_cite_text": "1 Introduction Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy."
        },
        {
            "Number": 2,
            "refer_ID": "W99-0623",
            "refer_sids": [
                120
            ],
            "refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "cite_ID": "A00-2005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision. Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "GPT_cite_text": "The collection of hypotheses ti = fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)"
        },
        {
            "Number": 4,
            "refer_ID": "W99-0623",
            "refer_sids": [
                25
            ],
            "refer_text": "In our particular case the majority requires the agreement of only two parsers because we have only three.",
            "cite_ID": "N10-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In our particular case, the majority requires the agreement of only two parties because we have only three.",
            "GPT_cite_text": "5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers."
        },
        {
            "Number": 5,
            "refer_ID": "W99-0623",
            "refer_sids": [
                120
            ],
            "refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision. Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "GPT_cite_text": "A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)."
        },
        {
            "Number": 6,
            "refer_ID": "W99-0623",
            "refer_sids": [
                38
            ],
            "refer_text": "Under certain conditions the constituent voting and na\u00efve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Under certain conditions, the constituent voting and na\u00efve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.",
            "GPT_cite_text": "This approach roughly corresponds to (Henderson and Brill, 1999)'s Na\u00efve Bayes parse hybridization."
        },
        {
            "Number": 7,
            "refer_ID": "W99-0623",
            "refer_sids": [
                91
            ],
            "refer_text": "Features and context were initially introduced into the models, but they refused to offer any gains in performance.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) also reported that context did not help them to outperform simple voting",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Features and context were initially introduced into the models, but they failed to offer any gains in performance.",
            "GPT_cite_text": "Henderson and Brill (1999) also reported that context did not help them outperform simple voting."
        },
        {
            "Number": 8,
            "refer_ID": "W99-0623",
            "refer_sids": [
                120
            ],
            "refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision. Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "GPT_cite_text": "(Henderson and Brill, 1999) improved their best parser's F-measure from 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)."
        },
        {
            "Number": 10,
            "refer_ID": "W99-0623",
            "refer_sids": [
                120
            ],
            "refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "cite_ID": "P01-1005",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision. Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.",
            "GPT_cite_text": "Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren et al., 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pedersen, 2000)."
        },
        {
            "Number": 11,
            "refer_ID": "W99-0623",
            "refer_sids": [
                139
            ],
            "refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "GPT_cite_text": "Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes: one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees."
        },
        {
            "Number": 12,
            "refer_ID": "W99-0623",
            "refer_sids": [
                25
            ],
            "refer_text": "In our particular case the majority requires the agreement of only two parsers because we have only three.",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In our particular case, the majority requires the agreement of only two parties because we have only three.",
            "GPT_cite_text": "Henderson and Brill (1999) combine three parsers and obtain an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper."
        },
        {
            "Number": 13,
            "refer_ID": "W99-0623",
            "refer_sids": [
                103
            ],
            "refer_text": "The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.",
            "cite_ID": "D09-1161",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The counts represent portions of the approximately 44,000 constituents hypothesized by the parsers in the development set.",
            "GPT_cite_text": "Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)."
        },
        {
            "Number": 14,
            "refer_ID": "W99-0623",
            "refer_sids": [
                139
            ],
            "refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "cite_ID": "N06-2033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented two general approaches to studying parser combination: parser switching and parse hybridization.",
            "GPT_cite_text": "Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents of the initial trees."
        },
        {
            "Number": 15,
            "refer_ID": "W99-0623",
            "refer_sids": [
                70
            ],
            "refer_text": "In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In this case, we are interested in finding the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.",
            "GPT_cite_text": "(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined."
        },
        {
            "Number": 16,
            "refer_ID": "W99-0623",
            "refer_sids": [
                140
            ],
            "refer_text": "For each experiment we gave an nonparametric and a parametric technique for combining parsers.",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each experiment, we gave a nonparametric and a parametric technique for combining parsers.",
            "GPT_cite_text": "(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents."
        },
        {
            "Number": 17,
            "refer_ID": "W99-0623",
            "refer_sids": [
                70
            ],
            "refer_text": "In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.",
            "cite_ID": "N09-2064",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In this case, we are interested in finding the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.",
            "GPT_cite_text": "Output (Figure 3). Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected F-score within the Minimum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs."
        },
        {
            "Number": 18,
            "refer_ID": "W99-0623",
            "refer_sids": [
                85
            ],
            "refer_text": "We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.",
            "cite_ID": "P09-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.",
            "GPT_cite_text": "System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., Smith and Eisner, 2005) and ensemble-based parsing (e.g., Henderson and Brill, 1999)."
        },
        {
            "Number": 20,
            "refer_ID": "W99-0623",
            "refer_sids": [
                70
            ],
            "refer_text": "In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.",
            "cite_ID": "C10-1151",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In this case, we are interested in finding the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.",
            "GPT_cite_text": "Henderson and Brill (1999) perform parse selection by maximizing the expected precision of selected parses with respect to the set of parses to be combined."
        }
    ],
    "A00-2018_akanksha": [
        {
            "Number": 2,
            "refer_ID": "A00-2018",
            "refer_sids": [
                90,
                91
            ],
            "refer_text": "We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.\nAs the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.",
            "cite_ID": "N10-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We created a parser based upon the maximum entropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.",
            "GPT_cite_text": "As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)."
        },
        {
            "Number": 3,
            "refer_ID": "A00-2018",
            "refer_sids": [
                5
            ],
            "refer_text": "We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.",
            "cite_ID": "W11-0610",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present a new parser for parsing down to Penn Treebank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal Treebank.",
            "GPT_cite_text": "Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switchboard treebank."
        },
        {
            "Number": 4,
            "refer_ID": "A00-2018",
            "refer_sids": [
                90
            ],
            "refer_text": "We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.",
            "cite_ID": "W06-3119",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We created a parser based upon the maximum entropy-inspired model of the last section, smoothed using standard deleted interpolation.",
            "GPT_cite_text": "We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus."
        },
        {
            "Number": 5,
            "refer_ID": "A00-2018",
            "refer_sids": [
                48,
                49,
                51
            ],
            "refer_text": "Maximum-entropy models have two benefits for a parser builder.\nFirst, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable \u2014 just change the set of features used.\nSecond, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.",
            "cite_ID": "N03-2024",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Maximum-entropy models have two benefits for a parser builder. First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various \"features\" suggests that the probability model should be easily changeable \u2014 just change the set of features used. Second, and this is a point we have not yet mentioned, the features used in these models need not have any particular independence of one another.",
            "GPT_cite_text": "We were interested in the occurrence of features such as type and number of premodifiers, presence and type of postmodifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al., 1997)."
        },
        {
            "Number": 6,
            "refer_ID": "A00-2018",
            "refer_sids": [
                90,
                91,
                92,
                93,
                94
            ],
            "refer_text": "We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.\nAs the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.\nFor runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.\nThis allows the second pass to see expansions not present in the training corpus.\nWe use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.",
            "cite_ID": "N06-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We created a parser based upon the maximum entropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information. This allows the second pass to see expansions not present in the training corpus. We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.",
            "GPT_cite_text": "After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article."
        },
        {
            "Number": 7,
            "refer_ID": "A00-2018",
            "refer_sids": [
                0
            ],
            "refer_text": "\u201cNA\u201d",
            "cite_ID": "C04-1180",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)",
            "label": [
                "Result_Citation"
            ],
            "GPT_cite_text": "The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al., 2003), sentence simplification (Carroll et al., 1999), and a linguist's search engine (Resnik and Elkiss, 2003)."
        },
        {
            "Number": 8,
            "refer_ID": "A00-2018",
            "refer_sids": [
                90
            ],
            "refer_text": "We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.",
            "cite_ID": "W05-0638",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We created a parser based upon the maximum entropy-inspired model of the last section, smoothed using standard deleted interpolation.",
            "GPT_cite_text": "In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)."
        },
        {
            "Number": 9,
            "refer_ID": "A00-2018",
            "refer_sids": [
                90,
                91,
                92,
                93,
                94
            ],
            "refer_text": "We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.\nAs the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.\nFor runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.\nThis allows the second pass to see expansions not present in the training corpus.\nWe use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.",
            "cite_ID": "P05-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We created a parser based upon the maximum entropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information. This allows the second pass to see expansions not present in the training corpus. We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.",
            "GPT_cite_text": "We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis."
        },
        {
            "Number": 10,
            "refer_ID": "A00-2018",
            "refer_sids": [
                38,
                39,
                40
            ],
            "refer_text": "To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.\nIn our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.\nIn the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.",
            "cite_ID": "P05-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To compute a probability in a log-linear model, one first defines a set of \"features,\" functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input. In our work, we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. In the parser, we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.",
            "GPT_cite_text": "For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus."
        },
        {
            "Number": 11,
            "refer_ID": "A00-2018",
            "refer_sids": [
                174
            ],
            "refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn Treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "GPT_cite_text": "The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90% unlabelled and 84% labelled accuracy with respect to dependencies when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows."
        },
        {
            "Number": 13,
            "refer_ID": "A00-2018",
            "refer_sids": [
                85
            ],
            "refer_text": "As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As the partition function calculation is typically the major online computational problem for maximum entropy models, this simplifies the model significantly.",
            "GPT_cite_text": "As an alternative to hard-coded heuristics, Blaheta and Charniak (2000) proposed recovering the Penn functional tags automatically."
        },
        {
            "Number": 17,
            "refer_ID": "A00-2018",
            "refer_sids": [
                63,
                143,
                146
            ],
            "refer_text": "As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &quot;before&quot;), and the label of the grandparent of c (la).The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.",
            "cite_ID": "N06-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal, and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for \"before\"), and the label of the grandparent of c (la). The first is simply that if we first guess the pre-terminal, when we go to guess the head, the first thing we can condition upon is the pre-terminal, i.e., we compute p(h | t). The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.",
            "GPT_cite_text": "The parser of Charniak (2000) is also a two-stage CFG model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents."
        },
        {
            "Number": 18,
            "refer_ID": "A00-2018",
            "refer_sids": [
                78,
                79
            ],
            "refer_text": "With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence. We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.",
            "cite_ID": "N06-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence. We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.",
            "GPT_cite_text": "Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))."
        },
        {
            "Number": 19,
            "refer_ID": "A00-2018",
            "refer_sids": [
                90
            ],
            "refer_text": "We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.",
            "cite_ID": "H05-1035",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We created a parser based upon the maximum entropy-inspired model of the last section, smoothed using standard deleted interpolation.",
            "GPT_cite_text": "The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions."
        },
        {
            "Number": 20,
            "refer_ID": "A00-2018",
            "refer_sids": [
                174
            ],
            "refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "cite_ID": "P04-1042",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn Treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "GPT_cite_text": "Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size."
        }
    ],
    "W99-0613_aakansha": [
        {
            "Number": 1,
            "refer_ID": "W99-0613",
            "refer_sids": [
                9
            ],
            "refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "GPT_cite_text": "Co-training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web page classification (Blum and Mitchell, 1998), and named entity identification (Collins and Singer, 1999)."
        },
        {
            "Number": 2,
            "refer_ID": "W99-0613",
            "refer_sids": [
                36
            ],
            "refer_text": "Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.",
            "GPT_cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. Collins and Singer (1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co-Booting)."
        },
        {
            "Number": 3,
            "refer_ID": "W99-0613",
            "refer_sids": [
                137
            ],
            "refer_text": "The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.",
            "cite_ID": "W03-1509",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.",
            "GPT_cite_text": "Recent methods for English NER focus on machine learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick et al. 1999], and so on."
        },
        {
            "Number": 4,
            "refer_ID": "W99-0613",
            "refer_sids": [
                79
            ],
            "refer_text": "2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list; the term CoTrain is taken from Blum and Mitchell 98).",
            "GPT_cite_text": "DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus."
        },
        {
            "Number": 5,
            "refer_ID": "W99-0613",
            "refer_sids": [
                10
            ],
            "refer_text": "The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories: Person, Organization, or Location.",
            "GPT_cite_text": "(Collins and Singer, 1999) also make use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify."
        },
        {
            "Number": 6,
            "refer_ID": "W99-0613",
            "refer_sids": [
                18
            ],
            "refer_text": "But we will show that the use of unlabeled data can drastically reduce the need for supervision.",
            "cite_ID": "W06-2204",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "But we will show that the use of unlabeled data can drastically reduce the need for supervision.",
            "GPT_cite_text": "In (Collins and Singer, 1999), Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification."
        },
        {
            "Number": 8,
            "refer_ID": "W99-0613",
            "refer_sids": [
                236,
                237
            ],
            "refer_text": "We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.\nThe numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.",
            "cite_ID": "W03-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We chose one of four labels for each example: location, person, organization, or noise, where the noise category was used for items that were outside the three categories. The numbers falling into the location, person, and organization categories were 186, 289, and 402, respectively.",
            "GPT_cite_text": "Collins and Singer (1999), for example, report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)."
        },
        {
            "Number": 9,
            "refer_ID": "W99-0613",
            "refer_sids": [
                9,
                10
            ],
            "refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.\nThe task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.",
            "cite_ID": "E09-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification. The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories: Person, Organization, or Location.",
            "GPT_cite_text": "While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al., 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999), and context-free grammar induction (numerous attempts, too many to mention)."
        },
        {
            "Number": 11,
            "refer_ID": "W99-0613",
            "refer_sids": [
                137,
                39
            ],
            "refer_text": "The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.This section describes AdaBoost, which is the basis for the CoBoost algorithm.",
            "cite_ID": "W07-1712",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel. This section describes AdaBoost, which is the basis for the CoBoost algorithm.",
            "GPT_cite_text": "In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)."
        },
        {
            "Number": 12,
            "refer_ID": "W99-0613",
            "refer_sids": [
                26,
                27
            ],
            "refer_text": "We present two algorithms.\nThe first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).",
            "cite_ID": "W09-2208",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present two algorithms. The first method builds on results from (Yarowsky 1995) and (Blum and Mitchell 1998).",
            "GPT_cite_text": "Collins et al. (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by Blum and Mitchell (1998)."
        },
        {
            "Number": 13,
            "refer_ID": "W99-0613",
            "refer_sids": [
                18
            ],
            "refer_text": "But we will show that the use of unlabeled data can drastically reduce the need for supervision.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "But we will show that the use of unlabeled data can drastically reduce the need for supervision.",
            "GPT_cite_text": "This approach was shown to perform well on real-world natural language processing problems (Collins and Singer, 1999)."
        },
        {
            "Number": 15,
            "refer_ID": "W99-0613",
            "refer_sids": [
                85
            ],
            "refer_text": "(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "(If fewer than n rules have Precision greater than pin, we note that taking the top n most frequent rules already makes the method robust to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. Keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.",
            "GPT_cite_text": "(6) Similarly to Collins and Singer (1999), we used T = 0.95 for all experiments reported here."
        },
        {
            "Number": 16,
            "refer_ID": "W99-0613",
            "refer_sids": [
                8,
                9
            ],
            "refer_text": "Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.\nThis paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Recent results (e.g., Yarowsky 95; Brill 95; Blum and Mitchell 98) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision. This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "GPT_cite_text": "We use Collins and Singer (1999) for our exact specification of Yarowsky. It uses DL rule scores \\( f_j | f_j | + | f | + L \\) (1) where \\( \\lambda \\) is a smoothing constant."
        }
    ],
    "J01-2004_sweta": [
        {
            "Number": 1,
            "refer_ID": "J01-2004",
            "refer_sids": [
                372
            ],
            "refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "cite_ID": "W05-0104",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "GPT_cite_text": "Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark; see Roark (2001))."
        },
        {
            "Number": 2,
            "refer_ID": "J01-2004",
            "refer_sids": [
                40,
                41,
                42
            ],
            "refer_text": "The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.\nThere will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.\nThree parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.",
            "cite_ID": "P08-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition. There will also be a brief review of previous work using syntactic information for language modeling before we introduce our model in Section 4. Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.",
            "GPT_cite_text": "Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000)."
        },
        {
            "Number": 4,
            "refer_ID": "J01-2004",
            "refer_sids": [
                25
            ],
            "refer_text": "A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "A parser that is not left to right, but which has rooted derivations, e.g., a head-first parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.",
            "GPT_cite_text": "The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn Treebank."
        },
        {
            "Number": 5,
            "refer_ID": "J01-2004",
            "refer_sids": [
                364
            ],
            "refer_text": "We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.",
            "GPT_cite_text": "We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn Treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search."
        },
        {
            "Number": 6,
            "refer_ID": "J01-2004",
            "refer_sids": [
                302
            ],
            "refer_text": "In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.",
            "GPT_cite_text": "One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper, we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)."
        },
        {
            "Number": 7,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights."
        },
        {
            "Number": 9,
            "refer_ID": "J01-2004",
            "refer_sids": [
                231
            ],
            "refer_text": "Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Since we do not know the POS for the word, we must sum the LAP for all POS. For a PCFG G, a stack S = A\u2080 A\u2099$ (which we will write A\u2099) and a look-ahead terminal item w\u1d62, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A: P(A, w, a) and P(A, c).",
            "GPT_cite_text": "Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word."
        },
        {
            "Number": 10,
            "refer_ID": "J01-2004",
            "refer_sids": [
                297
            ],
            "refer_text": "The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "The differences between a k-best and a beam search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.",
            "GPT_cite_text": "A good example of this is the Roark parser (Roark, 2001), which works left-to-right through the sentence and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word and then re-pruning."
        },
        {
            "Number": 11,
            "refer_ID": "J01-2004",
            "refer_sids": [
                133
            ],
            "refer_text": "Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.",
            "GPT_cite_text": "At the end, one has a beam-width's number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search."
        },
        {
            "Number": 12,
            "refer_ID": "J01-2004",
            "refer_sids": [
                291
            ],
            "refer_text": "Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1 for sentences of length < 100.",
            "GPT_cite_text": "To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses."
        },
        {
            "Number": 13,
            "refer_ID": "J01-2004",
            "refer_sids": [
                355
            ],
            "refer_text": "In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.",
            "cite_ID": "P04-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "In order to get a sense of whether these perplexity reduction results can translate to improvements in a speech recognition task, we performed a very small preliminary experiment on n-best lists.",
            "GPT_cite_text": "We ran the first stage parser with 4-times over parsing for each string in the 7 best lists provided by Brian Roark (Roark, 2001). A local tree is an explicit expansion of an edge and its children."
        },
        {
            "Number": 14,
            "refer_ID": "J01-2004",
            "refer_sids": [
                59
            ],
            "refer_text": "A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.",
            "cite_ID": "P05-1063",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "A PCFG is a CFG with a probability assigned to each rule; specifically, each right-hand side has a probability given the left-hand side of the rule.",
            "GPT_cite_text": "Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models."
        },
        {
            "Number": 15,
            "refer_ID": "J01-2004",
            "refer_sids": [
                100
            ],
            "refer_text": "The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.",
            "cite_ID": "W10-2009",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "The approach that we will subsequently present uses probabilistic grammar as its language model, but only includes probability mass from those parses that are found; that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.",
            "GPT_cite_text": "Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)."
        },
        {
            "Number": 18,
            "refer_ID": "J01-2004",
            "refer_sids": [
                108
            ],
            "refer_text": "Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &quot;surface&quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to \"surface\" c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.",
            "GPT_cite_text": "We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time."
        },
        {
            "Number": 19,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here."
        },
        {
            "Number": 20,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures. Let H(D) be the entropy over a set of derivations D, calculated as follows: H(D) = \u03a3 D (D) P(D) log P(D) (10) If the set of derivations D = D(G, W[1, i]) is a set of partial derivations for string W[1, i], then H(D) is a measure of uncertainty over the partial derivations, i.e., the uncertainty regarding the correct analysis of what has already been processed."
        }
    ],
    "J01-2004_aakansha": [
        {
            "Number": 1,
            "refer_ID": "J01-2004",
            "refer_sids": [
                372
            ],
            "refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "cite_ID": "W05-0104",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "GPT_cite_text": "Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark; see Roark (2001))."
        },
        {
            "Number": 2,
            "refer_ID": "J01-2004",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper will examine language modeling for speech recognition from a natural language processing point of view.",
            "cite_ID": "P08-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will examine language modeling for speech recognition from a natural language processing point of view.",
            "GPT_cite_text": "Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000)."
        },
        {
            "Number": 4,
            "refer_ID": "J01-2004",
            "refer_sids": [
                21
            ],
            "refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "GPT_cite_text": "The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn Treebank."
        },
        {
            "Number": 5,
            "refer_ID": "J01-2004",
            "refer_sids": [
                21
            ],
            "refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "GPT_cite_text": "We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn Treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search."
        },
        {
            "Number": 6,
            "refer_ID": "J01-2004",
            "refer_sids": [
                302
            ],
            "refer_text": "In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.",
            "GPT_cite_text": "One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper, we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)."
        },
        {
            "Number": 7,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 1970).",
            "GPT_cite_text": "The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights."
        },
        {
            "Number": 9,
            "refer_ID": "J01-2004",
            "refer_sids": [
                79,
                80
            ],
            "refer_text": "This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.\nIt also brings words further downstream into the look-ahead at the point of specification.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP) allows lexical items to become part of the left context and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree. It also brings words further downstream into the look-ahead at the point of specification.",
            "GPT_cite_text": "Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word."
        },
        {
            "Number": 10,
            "refer_ID": "J01-2004",
            "refer_sids": [
                21
            ],
            "refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "GPT_cite_text": "A good example of this is the Roark parser (Roark, 2001), which works left-to-right through the sentence and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word and then re-pruning."
        },
        {
            "Number": 11,
            "refer_ID": "J01-2004",
            "refer_sids": [
                138
            ],
            "refer_text": "Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our approach is found to yield very accurate parses efficiently and, in addition, to lend itself straightforwardly to estimating word probabilities online, that is, in a single pass from left to right.",
            "GPT_cite_text": "At the end, one has a beam-width's number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search."
        },
        {
            "Number": 12,
            "refer_ID": "J01-2004",
            "refer_sids": [
                291
            ],
            "refer_text": "Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1 for sentences of length < 100.",
            "GPT_cite_text": "To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses."
        },
        {
            "Number": 13,
            "refer_ID": "J01-2004",
            "refer_sids": [
                372
            ],
            "refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "cite_ID": "P04-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "GPT_cite_text": "We ran the first stage parser with 4-times over parsing for each string in the 7 best lists provided by Brian Roark (Roark, 2001). A local tree is an explicit expansion of an edge and its children."
        },
        {
            "Number": 14,
            "refer_ID": "J01-2004",
            "refer_sids": [
                209,
                210
            ],
            "refer_text": "This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).\nIt uses a PCFG with a conditional probability model of the sort defined in the previous section.",
            "cite_ID": "P05-1063",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986). It uses a PCFG with a conditional probability model of the sort defined in the previous section.",
            "GPT_cite_text": "Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models."
        },
        {
            "Number": 15,
            "refer_ID": "J01-2004",
            "refer_sids": [
                372
            ],
            "refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "cite_ID": "W10-2009",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "GPT_cite_text": "Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)."
        },
        {
            "Number": 18,
            "refer_ID": "J01-2004",
            "refer_sids": [
                20,
                21,
                32
            ],
            "refer_text": "Two features of our top-down parsing approach will emerge as key to its success.\nFirst, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Two features of our top-down parsing approach will emerge as key to its success. First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar. A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.",
            "GPT_cite_text": "We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time."
        },
        {
            "Number": 19,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31
            ],
            "refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.",
            "GPT_cite_text": "In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here."
        },
        {
            "Number": 20,
            "refer_ID": "J01-2004",
            "refer_sids": [
                33
            ],
            "refer_text": "Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.",
            "GPT_cite_text": "At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures. Let H(D) be the entropy over a set of derivations D, calculated as follows: H(D) = \u03a3 D (D) P(D) log P(D) (10) If the set of derivations D = D(G, W[1, i]) is a set of partial derivations for string W[1, i], then H(D) is a measure of uncertainty over the partial derivations, i.e., the uncertainty regarding the correct analysis of what has already been processed."
        }
    ],
    "P87-1015_swastika": [
        {
            "Number": 1,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "P01-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach that Vijay-Shanker et al (1987) and Weir (1988) take, elaborated on by Becker et al (1992), is to identify a very general class of formalisms, which they call linear context free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear context-free rewriting systems (CFRSs), and define for this class a large space of structural descriptions that serves as a common ground in which the strong generative capacities of these formalisms can be compared."
        },
        {
            "Number": 2,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "E09-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Here we use the standard definition of LCFRS (Vijay-Shanker et al, 1987) and only fix our notation; for a more thorough discussion of this formal ism, we refer to the literature. Let G be an LCFRS",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "Here we use the standard definition of LCFRS (Vijay-Shanker et al., 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. Let G be an LCFRS."
        },
        {
            "Number": 3,
            "refer_ID": "P87-1015",
            "refer_sids": [
                149
            ],
            "refer_text": "We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A \u2014* A1 Anup where tk (up) = cp.",
            "cite_ID": "W07-2214",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "There are many (structural) mildly context sensitive grammar formalisms ,e.g .mcfg ,lcfrs, mg, and they have been shown to be equivalent (Vijay-Shanker et al., 1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We can obtain a letter equivalent CFL defined by a CFG in which for each rule as above, we have the production A \u2014> A1 Anup where tk (up) = cp.",
            "GPT_cite_text": "There are many (structural) mildly context-sensitive grammar formalisms, e.g., MCFG, LCFRS, MG, and they have been shown to be equivalent (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 5,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "P09-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following this line, (Vijay-Shanker et al, 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "Following this line, Vijay-Shanker et al. (1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years from the community."
        },
        {
            "Number": 6,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "P09-1111",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al, 1987)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 7,
            "refer_ID": "P87-1015",
            "refer_sids": [
                2
            ],
            "refer_text": "In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.",
            "cite_ID": "P07-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We write REGD.k/ to refer to the class of regular dependency languages with a gap-degree bounded by k. Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al, 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism and examine the nature of their derivation process as reflected by properties of their trees. We find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars. On the basis of this observation, we describe a class of formalisms which we call Linear Context-Free Rewriting Systems and show they are recognizable in polynomial time and generate only semilinear languages.",
            "GPT_cite_text": "We write REGD.k/ to refer to the class of regular dependency languages with a gap-degree bounded by k. Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms."
        },
        {
            "Number": 8,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "N09-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006) .In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al, 1987)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single-language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 9,
            "refer_ID": "P87-1015",
            "refer_sids": [
                222
            ],
            "refer_text": "However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.",
            "cite_ID": "N09-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al (1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive than ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.",
            "GPT_cite_text": "We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al. (1987)."
        },
        {
            "Number": 10,
            "refer_ID": "P87-1015",
            "refer_sids": [
                119
            ],
            "refer_text": "In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.",
            "cite_ID": "W10-1407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "LCFRS (Vijay-Shanker et al, 1987) are anatural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In defining LCFRS, we hope to generalize the definition of CFGs to formalisms manipulating any structure, e.g., strings, trees, or graphs.",
            "GPT_cite_text": "LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals."
        },
        {
            "Number": 11,
            "refer_ID": "P87-1015",
            "refer_sids": [],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "W10-1407",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A LCFRS (Vijay-Shanker et al, 1987) is a tuple G= (N, T, V, P, S )wherea) N is a finite set of non-terminals with a function dim: N? N that determines the fan-out of each A? N; b) T and V are disjoint finite sets of terminals and variables; c) S? N is the start symbol with dim (S)= 1; d) P is a finite set of rewriting rules A (? 1,..",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N \u2192 N that determines the fan-out of each A \u2208 N; b) T and V are disjoint finite sets of terminals and variables; c) S \u2208 N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules A (\u2208 1, ..."
        },
        {
            "Number": 12,
            "refer_ID": "P87-1015",
            "refer_sids": [
                207
            ],
            "refer_text": "We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al (1987))",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We outlined the definition of a family of constrained grammatical formalisms called Linear Context-Free Rewriting Systems.",
            "GPT_cite_text": "In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes, 1997) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 13,
            "refer_ID": "P87-1015",
            "refer_sids": [
                207
            ],
            "refer_text": "We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "2 By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of deriva tional structures are all regular (Vijay-Shanker et al., 1987)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We outlined the definition of a family of constrained grammatical formalisms called Linear Context-Free Rewriting Systems.",
            "GPT_cite_text": "By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 14,
            "refer_ID": "P87-1015",
            "refer_sids": [
                19
            ],
            "refer_text": "It can be easily shown from Thatcher's result that the path set of every local set is a regular set.",
            "cite_ID": "E09-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al, 1987)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It can be easily shown from Thatcher's result that the path set of every local set is a regular set.",
            "GPT_cite_text": "It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al., 1987)."
        },
        {
            "Number": 15,
            "refer_ID": "P87-1015",
            "refer_sids": [
                119
            ],
            "refer_text": "In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.",
            "cite_ID": "N10-1035",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi,1985), including, among several others, the tree ad joining grammars (TAGs) of Joshi et al (1975) .Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings ,i.e., discontinuous phrases",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In defining LCFRS, we hope to generalize the definition of CFGs to formalisms manipulating any structure, e.g., strings, trees, or graphs.",
            "GPT_cite_text": "On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting systems (LCFRS), introduced by Vijay-Shanker et al. (1987), are mildly context-sensitive formalisms that allow the derivation of tuples of strings, i.e., discontinuous phrases."
        },
        {
            "Number": 16,
            "refer_ID": "P87-1015",
            "refer_sids": [
                118
            ],
            "refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.",
            "cite_ID": "P12-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al (1991) .3 Thus, CFTG are mildly context-sensitive since their generated string languages are semi-linear and can be parsed in polynomial time (Go ?mez-Rodr? ?guez et al, 2010)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS) may be defined and sketch how semilinearity and polynomial recognition of these systems follow.",
            "GPT_cite_text": "CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al. (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al. (1991). Thus, CFTG are mildly context-sensitive since their generated string languages are semi-linear and can be parsed in polynomial time (G\u00f3mez-Rodr\u00edguez et al., 2010)."
        }
    ],
    "W06-3114_swastika": [
        {
            "Number": 1,
            "refer_ID": "W06-3114",
            "refer_sids": [
                47.0
            ],
            "refer_text": "Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.",
            "cite_ID": "W06-3120",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Because of this, we retokenized and lowercased the submitted output with our own tokenizer, which was also used to prepare the training and test data.",
            "GPT_cite_text": "The official results were slightly better because a lowercase evaluation was used; see (Koehn and Monz, 2006)."
        },
        {
            "Number": 2,
            "refer_ID": "W06-3114",
            "refer_sids": [
                8.0
            ],
            "refer_text": "The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.",
            "cite_ID": "D07-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The evaluation framework for the shared task is similar to the one used in last year's shared task.",
            "GPT_cite_text": "We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted of translating Spanish, German, and French texts from and to English."
        },
        {
            "Number": 3,
            "refer_ID": "W06-3114",
            "refer_sids": [
                18.0
            ],
            "refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.",
            "cite_ID": "C08-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website, which are published in all four languages of the shared task.",
            "GPT_cite_text": "For our training and test data, we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference."
        },
        {
            "Number": 5,
            "refer_ID": "W06-3114",
            "refer_sids": [
                18.0
            ],
            "refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.",
            "cite_ID": "P07-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website, which are published in all four languages of the shared task.",
            "GPT_cite_text": "For the bi-text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for French-English (Fr), Spanish-English (Es), and German-English (De) (Koehn and Monz, 2006)."
        },
        {
            "Number": 6,
            "refer_ID": "W06-3114",
            "refer_sids": [
                144.0
            ],
            "refer_text": "Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.",
            "GPT_cite_text": "Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006) revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator."
        },
        {
            "Number": 7,
            "refer_ID": "W06-3114",
            "refer_sids": [
                145.0
            ],
            "refer_text": "This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "This cannot be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.",
            "GPT_cite_text": "For instance, Callison-Burch et al. (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001)."
        },
        {
            "Number": 8,
            "refer_ID": "W06-3114",
            "refer_sids": [
                103.0
            ],
            "refer_text": "Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \u00b7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Given a set of n sentences, we can compute the sample mean x\u0304 and sample variance s\u00b2 of the individual sentence judgments x\u1d62: The extent of the confidence interval [x\u2212d, x+d] can be computed by d = 1.96 \u00b7 (s/\u221an) (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.",
            "GPT_cite_text": "We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006) (see Section 3)."
        },
        {
            "Number": 11,
            "refer_ID": "W06-3114",
            "refer_sids": [
                50.0
            ],
            "refer_text": "Following this method, we repeatedly \u2014 say, 1000 times \u2014 sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the same method described in (Koehn and Monz, 2006) to perform the significance test",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following this method, we repeatedly \u2014 say, 1000 times \u2014 sample sets of sentences from the output of each system, measure their BLEU scores, and use these 1000 BLEU scores as a basis for estimating a confidence interval.",
            "GPT_cite_text": "We use the same method described in (Koehn and Monz, 2006) to perform the significance test."
        },
        {
            "Number": 12,
            "refer_ID": "W06-3114",
            "refer_sids": [
                68.0
            ],
            "refer_text": "We asked participants to each judge 200\u2013300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We asked participants to each judge 200\u2013300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.",
            "GPT_cite_text": "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)."
        },
        {
            "Number": 13,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170.0
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "W08-0406",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "GPT_cite_text": "The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)."
        },
        {
            "Number": 15,
            "refer_ID": "W06-3114",
            "refer_sids": [
                18.0
            ],
            "refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website, which are published in all four languages of the shared task.",
            "GPT_cite_text": "The English-German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)."
        },
        {
            "Number": 16,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170.0
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "GPT_cite_text": "We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)."
        },
        {
            "Number": 17,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170.0
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "P07-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "GPT_cite_text": "A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)."
        },
        {
            "Number": 19,
            "refer_ID": "W06-3114",
            "refer_sids": [
                92.0
            ],
            "refer_text": "The way judgements are collected, human judges tend to use the scores to rank systems against each other.",
            "cite_ID": "E12-3010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The way judgments are collected, human judges tend to use the scores to rank systems against each other.",
            "GPT_cite_text": "For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)."
        },
        {
            "Number": 20,
            "refer_ID": "W06-3114",
            "refer_sids": [
                145.0
            ],
            "refer_text": "This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.",
            "cite_ID": "W09-0402",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This cannot be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.",
            "GPT_cite_text": "The correlations on the document level were computed on the English, French, Spanish, and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al., 2007), and third shared translation task (Callison-Burch et al., 2008)."
        }
    ],
    "P08-1043_swastika": [
        {
            "Number": 1,
            "refer_ID": "P08-1043",
            "refer_sids": [
                94.0
            ],
            "refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "cite_ID": "C10-1045",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a priori equally likely; the only reason to prefer one segmentation over another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "GPT_cite_text": "Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew."
        },
        {
            "Number": 2,
            "refer_ID": "P08-1043",
            "refer_sids": [
                4.0
            ],
            "refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "cite_ID": "P11-1141",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-token handling technique, our model outperforms previous pipelined, integrated, or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models."
        },
        {
            "Number": 3,
            "refer_ID": "P08-1043",
            "refer_sids": [
                94.0
            ],
            "refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "cite_ID": "P10-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of He brew, based on lattice parsing",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a priori equally likely; the only reason to prefer one segmentation over another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "GPT_cite_text": "Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging, and parsing of Hebrew, based on lattice parsing."
        },
        {
            "Number": 4,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19.0
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "P11-1089",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) pro pose a generative joint model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) propose a generative joint model."
        },
        {
            "Number": 5,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19.0
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "W10-1404",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1043",
            "refer_sids": [
                4.0
            ],
            "refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "cite_ID": "P11-2124",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrewtext",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-token handling technique, our model outperforms previous pipelined, integrated, or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text."
        },
        {
            "Number": 7,
            "refer_ID": "P08-1043",
            "refer_sids": [
                105.0
            ],
            "refer_text": "3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.",
            "cite_ID": "P11-2124",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes corresponds to a word, and every possible PoS assignment for this word is a connecting arc.",
            "GPT_cite_text": "Following Goldberg and Tsarfaty (2008), we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice."
        },
        {
            "Number": 9,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19.0
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "D12-1046",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A study that is closely related toours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew."
        },
        {
            "Number": 10,
            "refer_ID": "P08-1043",
            "refer_sids": [
                19.0
            ],
            "refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "cite_ID": "D12-1133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.",
            "GPT_cite_text": "Models that, in addition, incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1043",
            "refer_sids": [
                163.0
            ],
            "refer_text": "The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The accuracy results for segmentation, tagging, and parsing using our different models and our standard data split are summarized in Table 1.",
            "GPT_cite_text": "4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1043",
            "refer_sids": [
                100.0
            ],
            "refer_text": "This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is the same grammar as described in (Goldberg and Tsarfaty, 2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of nonterminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).",
            "GPT_cite_text": "It is the same grammar as described in (Goldberg and Tsarfaty, 2008)."
        },
        {
            "Number": 14,
            "refer_ID": "P08-1043",
            "refer_sids": [
                94.0
            ],
            "refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a priori equally likely; the only reason to prefer one segmentation over another is due to the overall syntactic context which is modeled via the PCFG derivations.",
            "GPT_cite_text": "Several studies followed this line (Cohen and Smith, 2007), the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task."
        },
        {
            "Number": 15,
            "refer_ID": "P08-1043",
            "refer_sids": [
                188.0
            ],
            "refer_text": "The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone application results.",
            "GPT_cite_text": "Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the treebank."
        },
        {
            "Number": 16,
            "refer_ID": "P08-1043",
            "refer_sids": [
                86.0
            ],
            "refer_text": "A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W \u2208 W) to their corresponding lattices (M(W) = L \u2208 L).",
            "GPT_cite_text": "The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to construct a lattice for each input token."
        },
        {
            "Number": 17,
            "refer_ID": "P08-1043",
            "refer_sids": [
                97.0
            ],
            "refer_text": "Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.",
            "cite_ID": "E09-1038",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parse val to use characters instead of space-delimited tokens as its basic units",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Thus our proposed model is a proper model assigning probability mass to all (\u03c0, L) pairs, where \u03c0 is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alphabet gives rise to.",
            "GPT_cite_text": "Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units."
        }
    ],
    "W06-3114_sweta": [
        {
            "Number": 1,
            "refer_ID": "W06-3114",
            "refer_sids": [
                108
            ],
            "refer_text": "The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.",
            "cite_ID": "W06-3120",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The results of the manual and automatic evaluation of the participating system translations are detailed in the figures at the end of this paper.",
            "GPT_cite_text": "The official results were slightly better because a lowercase evaluation was used; see (Koehn and Monz, 2006)."
        },
        {
            "Number": 2,
            "refer_ID": "W06-3114",
            "refer_sids": [
                34
            ],
            "refer_text": "For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.",
            "cite_ID": "D07-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For more on the participating systems, please refer to the respective system descriptions in the proceedings of the workshop.",
            "GPT_cite_text": "We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted of translating Spanish, German, and French texts from and to English."
        },
        {
            "Number": 3,
            "refer_ID": "W06-3114",
            "refer_sids": [
                18
            ],
            "refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.",
            "cite_ID": "C08-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website, which are published in all four languages of the shared task.",
            "GPT_cite_text": "For our training and test data, we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference."
        },
        {
            "Number": 4,
            "refer_ID": "W06-3114",
            "refer_sids": [
                151
            ],
            "refer_text": "The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.",
            "cite_ID": "W07-0718",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphologically rich languages, as demonstrated by the results for English-German and English-French.",
            "GPT_cite_text": "The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)."
        },
        {
            "Number": 5,
            "refer_ID": "W06-3114",
            "refer_sids": [
                16
            ],
            "refer_text": "The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.",
            "cite_ID": "P07-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.",
            "GPT_cite_text": "For the bi-text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for French-English (Fr), Spanish-English (Es), and German-English (De) (Koehn and Monz, 2006)."
        },
        {
            "Number": 6,
            "refer_ID": "W06-3114",
            "refer_sids": [
                172
            ],
            "refer_text": "Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Due to many similarly performing systems, we are not able to draw strong conclusions on the question of the correlation between manual and automatic evaluation metrics.",
            "GPT_cite_text": "Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006) revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator."
        },
        {
            "Number": 7,
            "refer_ID": "W06-3114",
            "refer_sids": [
                36
            ],
            "refer_text": "The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The BLEU metric, like all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.",
            "GPT_cite_text": "For instance, Callison-Burch et al. (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001)."
        },
        {
            "Number": 8,
            "refer_ID": "W06-3114",
            "refer_sids": [
                103
            ],
            "refer_text": "Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \u00b7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Given a set of n sentences, we can compute the sample mean x\u0304 and sample variance s\u00b2 of the individual sentence judgments x\u1d62: The extent of the confidence interval [x\u2212d, x+d] can be computed by d = 1.96 \u00b7 (s/\u221an) (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.",
            "GPT_cite_text": "We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006) (see Section 3)."
        },
        {
            "Number": 9,
            "refer_ID": "W06-3114",
            "refer_sids": [
                167
            ],
            "refer_text": "One annotator suggested that this was the case for as much as 10% of our test sentences.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "One annotator suggested that this was the case for as much as 10% of our test sentences.",
            "GPT_cite_text": "We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006)"
        },
        {
            "Number": 11,
            "refer_ID": "W06-3114",
            "refer_sids": [
                102
            ],
            "refer_text": "Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the same method described in (Koehn and Monz, 2006) to perform the significance test",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Confidence Interval: To estimate confidence intervals for the average mean scores of the systems, we use standard significance testing.",
            "GPT_cite_text": "We use the same method described in (Koehn and Monz, 2006) to perform the significance test."
        },
        {
            "Number": 12,
            "refer_ID": "W06-3114",
            "refer_sids": [
                123
            ],
            "refer_text": "For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.",
            "GPT_cite_text": "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)."
        },
        {
            "Number": 13,
            "refer_ID": "W06-3114",
            "refer_sids": [
                34
            ],
            "refer_text": "For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.",
            "cite_ID": "W08-0406",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For more on the participating systems, please refer to the respective system descriptions in the proceedings of the workshop.",
            "GPT_cite_text": "The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with a phrase length of 3 and a trigram language model (Stolcke, 2002)."
        },
        {
            "Number": 14,
            "refer_ID": "W06-3114",
            "refer_sids": [
                62
            ],
            "refer_text": "While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.",
            "cite_ID": "W11-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only an imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.",
            "GPT_cite_text": "Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality."
        },
        {
            "Number": 15,
            "refer_ID": "W06-3114",
            "refer_sids": [
                126
            ],
            "refer_text": "The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences of out-of-domain test data.",
            "GPT_cite_text": "The English-German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)."
        },
        {
            "Number": 16,
            "refer_ID": "W06-3114",
            "refer_sids": [
                173
            ],
            "refer_text": "The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.",
            "GPT_cite_text": "We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)."
        },
        {
            "Number": 17,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "P07-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "GPT_cite_text": "A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)."
        },
        {
            "Number": 19,
            "refer_ID": "W06-3114",
            "refer_sids": [
                84
            ],
            "refer_text": "The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:",
            "cite_ID": "E12-3010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The human judges were presented with the following definitions of adequacy and fluency, but no additional instructions:",
            "GPT_cite_text": "For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)."
        },
        {
            "Number": 20,
            "refer_ID": "W06-3114",
            "refer_sids": [
                8
            ],
            "refer_text": "The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.",
            "cite_ID": "W09-0402",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The evaluation framework for the shared task is similar to the one used in last year's shared task.",
            "GPT_cite_text": "The correlations on the document level were computed on the English, French, Spanish, and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al., 2007), and third shared translation task (Callison-Burch et al., 2008)."
        }
    ],
    "P11-1061_swastika": [
        {
            "Number": 1,
            "refer_ID": "P11-1061",
            "refer_sids": [
                144
            ],
            "refer_text": "For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.",
            "cite_ID": "P11-1144",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and it goes up to 88.7% with a treebank dictionary.",
            "GPT_cite_text": "Subramanya et al.'s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers."
        },
        {
            "Number": 3,
            "refer_ID": "P11-1061",
            "refer_sids": [
                44
            ],
            "refer_text": "Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.",
            "cite_ID": "P14-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Fortunately, some recently proposed POS taggers, such as the POStagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.",
            "GPT_cite_text": "Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach."
        },
        {
            "Number": 4,
            "refer_ID": "P11-1061",
            "refer_sids": [
                16
            ],
            "refer_text": "To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\u00a73), and then use graph label propagation to project syntactic information from English to the foreign language (\u00a74).",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\u00a73), and then use graph label propagation to project syntactic information from English to the foreign language (\u00a74).",
            "GPT_cite_text": "Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011)."
        },
        {
            "Number": 5,
            "refer_ID": "P11-1061",
            "refer_sids": [
                44
            ],
            "refer_text": "Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.",
            "GPT_cite_text": "Following Das and Petrov (2011) and Subramanya et al. (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics."
        },
        {
            "Number": 6,
            "refer_ID": "P11-1061",
            "refer_sids": [
                110
            ],
            "refer_text": "We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.",
            "GPT_cite_text": "Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type are unavailable (e.g., Das and Petrov, 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "P11-1061",
            "refer_sids": [
                115
            ],
            "refer_text": "The taggers were trained on datasets labeled with the universal tags.",
            "cite_ID": "N12-1052",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The taggers were trained on datasets labeled with the universal tags.",
            "GPT_cite_text": "Specifically, by replacing fine-grained language-specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features."
        },
        {
            "Number": 8,
            "refer_ID": "P11-1061",
            "refer_sids": [
                115
            ],
            "refer_text": "The taggers were trained on datasets labeled with the universal tags.",
            "cite_ID": "N12-1052",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The taggers were trained on datasets labeled with the universal tags.",
            "GPT_cite_text": "We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al. (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters."
        },
        {
            "Number": 9,
            "refer_ID": "P11-1061",
            "refer_sids": [
                158
            ],
            "refer_text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "cite_ID": "N12-1090",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "GPT_cite_text": "MT-based projection has been applied to various NLP tasks, such as part-of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus."
        },
        {
            "Number": 10,
            "refer_ID": "P11-1061",
            "refer_sids": [
                23
            ],
            "refer_text": "Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).",
            "cite_ID": "W11-2205",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%) and considerably bridges the gap to fully supervised POS tagging performance (96.6%).",
            "GPT_cite_text": "For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)."
        },
        {
            "Number": 11,
            "refer_ID": "P11-1061",
            "refer_sids": [
                24
            ],
            "refer_text": "The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.",
            "cite_ID": "P13-1155",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.",
            "GPT_cite_text": "(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfer to induce POS tags between two languages."
        },
        {
            "Number": 12,
            "refer_ID": "P11-1061",
            "refer_sids": [
                10
            ],
            "refer_text": "To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "cite_ID": "D12-1127",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To bridge this gap, we consider a practically motivated scenario in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages. We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "GPT_cite_text": "Recent work by Das and Petrov (2011) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text."
        },
        {
            "Number": 13,
            "refer_ID": "P11-1061",
            "refer_sids": [],
            "refer_text": "To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "cite_ID": "D12-1127",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To bridge this gap, we consider a practically motivated scenario in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages. We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "GPT_cite_text": "These approaches build a dictionary by transferring labeled data from a resource-rich language (English) to a resource-poor language (Das and Petrov, 2011)."
        },
        {
            "Number": 14,
            "refer_ID": "P11-1061",
            "refer_sids": [
                16
            ],
            "refer_text": "To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\u00a73), and then use graph label propagation to project syntactic information from English to the foreign language (\u00a74).",
            "cite_ID": "P12-3012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\u00a73), and then use graph label propagation to project syntactic information from English to the foreign language (\u00a74).",
            "GPT_cite_text": "In recent years, research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever-growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactic-semantic (Peirsman and Pado, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011)."
        },
        {
            "Number": 15,
            "refer_ID": "P11-1061",
            "refer_sids": [
                23
            ],
            "refer_text": "Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).",
            "GPT_cite_text": "Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of-speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English and achieves accuracies around 85% on the languages that we consider."
        },
        {
            "Number": 16,
            "refer_ID": "P11-1061",
            "refer_sids": [
                158
            ],
            "refer_text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.",
            "GPT_cite_text": "In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second, we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language."
        },
        {
            "Number": 17,
            "refer_ID": "P11-1061",
            "refer_sids": [
                10
            ],
            "refer_text": "To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To bridge this gap, we consider a practically motivated scenario in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages. We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "GPT_cite_text": "This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly resourced language to a lesser resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)."
        },
        {
            "Number": 18,
            "refer_ID": "P11-1061",
            "refer_sids": [
                56
            ],
            "refer_text": "To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.",
            "GPT_cite_text": "Das and Petrov (2011) achieved the current state-of-the-art in unsupervised tagging by exploiting high-confidence alignments to copy tags from the source language to the target language."
        },
        {
            "Number": 20,
            "refer_ID": "P11-1061",
            "refer_sids": [
                161
            ],
            "refer_text": "Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections and bridge the gap between purely supervised and unsupervised POS tagging models.",
            "GPT_cite_text": "We have proposed a method for unsupervised POS tagging that performs on par with the current state-of-the-art (Das and Petrov, 2011), but is substantially less sophisticated (specifically not requiring convex optimization or a feature-based HMM)."
        }
    ],
    "A00-2018_vardha": [
        {
            "Number": 2,
            "refer_ID": "A00-2018",
            "refer_sids": [
                5
            ],
            "refer_text": "We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.",
            "cite_ID": "N10-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present a new parser for parsing down to Penn Treebank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal Treebank.",
            "GPT_cite_text": "As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)."
        },
        {
            "Number": 3,
            "refer_ID": "A00-2018",
            "refer_sids": [
                5
            ],
            "refer_text": "We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.",
            "cite_ID": "W11-0610",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present a new parser for parsing down to Penn Treebank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal Treebank.",
            "GPT_cite_text": "Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switchboard treebank."
        },
        {
            "Number": 4,
            "refer_ID": "A00-2018",
            "refer_sids": [
                91
            ],
            "refer_text": "As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.",
            "cite_ID": "W06-3119",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate parses to be evaluated in the second pass by our probabilistic model.",
            "GPT_cite_text": "We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus."
        },
        {
            "Number": 5,
            "refer_ID": "A00-2018",
            "refer_sids": [
                39
            ],
            "refer_text": "In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.",
            "cite_ID": "N03-2024",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In our work, we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.",
            "GPT_cite_text": "We were interested in the occurrence of features such as type and number of premodifiers, presence and type of postmodifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al., 1997)."
        },
        {
            "Number": 7,
            "refer_ID": "A00-2018",
            "refer_sids": [
                162
            ],
            "refer_text": "Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.",
            "cite_ID": "C04-1180",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worthwhile.",
            "GPT_cite_text": "The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al., 2003), sentence simplification (Carroll et al., 1999), and a linguist's search engine (Resnik and Elkiss, 2003)."
        },
        {
            "Number": 8,
            "refer_ID": "A00-2018",
            "refer_sids": [
                87
            ],
            "refer_text": "In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].",
            "cite_ID": "W05-0638",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In a pure maximum entropy model, this is done by feature selection, as in Ratnaparkhi's maximum entropy parser [17].",
            "GPT_cite_text": "In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)."
        },
        {
            "Number": 9,
            "refer_ID": "A00-2018",
            "refer_sids": [
                91
            ],
            "refer_text": "As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.",
            "cite_ID": "P05-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate parses to be evaluated in the second pass by our probabilistic model.",
            "GPT_cite_text": "We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis."
        },
        {
            "Number": 10,
            "refer_ID": "A00-2018",
            "refer_sids": [
                176
            ],
            "refer_text": "That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.",
            "cite_ID": "P05-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "That the previous three best parsers on this test [5, 9, 17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.",
            "GPT_cite_text": "For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus."
        },
        {
            "Number": 11,
            "refer_ID": "A00-2018",
            "refer_sids": [
                174
            ],
            "refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn Treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "GPT_cite_text": "The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90% unlabelled and 84% labelled accuracy with respect to dependencies when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows."
        },
        {
            "Number": 12,
            "refer_ID": "A00-2018",
            "refer_sids": [
                101
            ],
            "refer_text": "In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In keeping with the standard methodology [5, 9, 10, 15, 17], we used the Penn Wall Street Journal Treebank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).",
            "GPT_cite_text": "Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees."
        },
        {
            "Number": 13,
            "refer_ID": "A00-2018",
            "refer_sids": [
                155
            ],
            "refer_text": "For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For example, in the Penn Treebank, a VP with both main and auxiliary verbs has the structure shown in Figure 3.",
            "GPT_cite_text": "As an alternative to hard-coded heuristics, Blaheta and Charniak (2000) proposed recovering the Penn functional tags automatically."
        },
        {
            "Number": 14,
            "refer_ID": "A00-2018",
            "refer_sids": [
                17
            ],
            "refer_text": "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Thus, it is informative to compare our results with those reported in (Blaheta and Charniak, 2000) for this same task",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advance by extracting a treebank grammar [3] from the training corpus.",
            "GPT_cite_text": "Thus, it is informative to compare our results with those reported in (Blaheta and Charniak, 2000) for this same task."
        },
        {
            "Number": 15,
            "refer_ID": "A00-2018",
            "refer_sids": [
                162
            ],
            "refer_text": "Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Method Accuracy P R f Blaheta 98.6 87.2 87.4 87.3 This paper 94.7 90.2 86.9 88.5 The difference in the accuracy is due to two reasons. First, because of the different definition of a correctly identified constituent in the parser? s output, we apply our method to a greater portion of all labels produced by the parser (95% vs. 89% reported in (Blaheta and Charniak, 2000))",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worthwhile.",
            "GPT_cite_text": "Method Accuracy P R F Blaheta 98.6 87.2 87.4 87.3 This paper 94.7 90.2 86.9 88.5 The difference in the accuracy is due to two reasons. First, because of the different definition of a correctly identified constituent in the parser's output, we apply our method to a greater portion of all labels produced by the parser (95% vs. 89% reported in (Blaheta and Charniak, 2000))."
        },
        {
            "Number": 17,
            "refer_ID": "A00-2018",
            "refer_sids": [
                40
            ],
            "refer_text": "In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.",
            "cite_ID": "N06-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the parser, we further assume that features are chosen from certain feature schemata and that every feature is a Boolean conjunction of sub-features.",
            "GPT_cite_text": "The parser of Charniak (2000) is also a two-stage CFG model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents."
        },
        {
            "Number": 18,
            "refer_ID": "A00-2018",
            "refer_sids": [
                79
            ],
            "refer_text": "We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.",
            "cite_ID": "N06-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.",
            "GPT_cite_text": "Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))."
        },
        {
            "Number": 19,
            "refer_ID": "A00-2018",
            "refer_sids": [
                40
            ],
            "refer_text": "In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.",
            "cite_ID": "H05-1035",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In the parser, we further assume that features are chosen from certain feature schemata and that every feature is a Boolean conjunction of sub-features.",
            "GPT_cite_text": "The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions."
        },
        {
            "Number": 20,
            "refer_ID": "A00-2018",
            "refer_sids": [
                175
            ],
            "refer_text": "This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].",
            "cite_ID": "P04-1042",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This corresponds to an error reduction of 13% over the best previously published single-parser results on this test set, those of Collins [9].",
            "GPT_cite_text": "Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size."
        }
    ],
    "P04-1036_vardha": [
        {
            "Number": 1,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account (McCarthy et al., 2004)."
        },
        {
            "Number": 2,
            "refer_ID": "P04-1036",
            "refer_sids": [
                15
            ],
            "refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first or predominant sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "GPT_cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41 Table 2: The SENSEVAL-2 first sense on the SENSEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available."
        },
        {
            "Number": 3,
            "refer_ID": "P04-1036",
            "refer_sids": [
                45
            ],
            "refer_text": "In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The method is described in (McCarthy et al, 2004), which we summarise here",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In order to find the predominant sense of a target word, we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).",
            "GPT_cite_text": "The method is described in (McCarthy et al., 2004), which we summarize here."
        },
        {
            "Number": 5,
            "refer_ID": "P04-1036",
            "refer_sids": [
                66
            ],
            "refer_text": "It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It uses the glosses of semantically related (according to WordNet) senses too. Jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.",
            "GPT_cite_text": "McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research and propose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges."
        },
        {
            "Number": 6,
            "refer_ID": "P04-1036",
            "refer_sids": [
                126
            ],
            "refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "GPT_cite_text": "Previous research in inducing sense rankings from an untagged corpus (McCarthy et al., 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction."
        },
        {
            "Number": 7,
            "refer_ID": "P04-1036",
            "refer_sids": [
                101
            ],
            "refer_text": "Thus, if we used the sense ranking as a heuristic for an \u201call nouns\u201d task we would expect to get precision in the region of 60%.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Thus, if we used the sense ranking as a heuristic for an \u201call nouns\u201d task, we would expect to get precision in the region of 60%.",
            "GPT_cite_text": "McCarthy et al. (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus."
        },
        {
            "Number": 8,
            "refer_ID": "P04-1036",
            "refer_sids": [
                126
            ],
            "refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "GPT_cite_text": "Research by McCarthy et al. (2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn."
        },
        {
            "Number": 9,
            "refer_ID": "P04-1036",
            "refer_sids": [
                115
            ],
            "refer_text": "Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labeling.",
            "GPT_cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al., 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense."
        },
        {
            "Number": 11,
            "refer_ID": "P04-1036",
            "refer_sids": [
                89
            ],
            "refer_text": "Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.",
            "cite_ID": "P10-1155",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Since both measures gave comparable results, we restricted our remaining experiments to JCN because this gave good results for finding the predominant sense and is much more efficient than Lesk, given the precompilation of the IC files.",
            "GPT_cite_text": "McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the WordNet Similarity JCN measure (Jiang and Conrath, 1997)."
        },
        {
            "Number": 12,
            "refer_ID": "P04-1036",
            "refer_sids": [
                137
            ],
            "refer_text": "Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli\u00e0, 2000) which annotates WordNet synsets with domain labels.",
            "GPT_cite_text": "In doing so, we provide first results on the application of WordNet automatic sense ranking (ASR) to French parsing, using the method of McCarthy et al. (2004)."
        },
        {
            "Number": 13,
            "refer_ID": "P04-1036",
            "refer_sids": [
                75
            ],
            "refer_text": "75 ssid=\"4\">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We generated a thesaurus entry for all polysemous nouns that occurred in SemCor with a frequency of 2, and in the BNC with a frequency of 10 in the grammatical relations listed in section 2.1 above.",
            "GPT_cite_text": "To define an appropriate categorical distribution over synsets for each lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s \u2208 Sx, following the approach of McCarthy et al. (2004)."
        },
        {
            "Number": 14,
            "refer_ID": "P04-1036",
            "refer_sids": [
                155
            ],
            "refer_text": "A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.",
            "GPT_cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al. (2004)"
        },
        {
            "Number": 16,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "S12-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al., 2004)."
        },
        {
            "Number": 17,
            "refer_ID": "P04-1036",
            "refer_sids": [
                155
            ],
            "refer_text": "A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.",
            "cite_ID": "W10-2803",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.",
            "GPT_cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining the predominant sense in a given domain (McCarthy et al., 2004), or to work directly with paraphrases (McCarthy and Navigli, 2009)."
        },
        {
            "Number": 18,
            "refer_ID": "P04-1036",
            "refer_sids": [
                68
            ],
            "refer_text": "We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.",
            "cite_ID": "W08-2107",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We are, of course, able to apply the method to other versions of WordNet. The synset is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.",
            "GPT_cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by WordNet, resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al. (2004))."
        },
        {
            "Number": 19,
            "refer_ID": "P04-1036",
            "refer_sids": [
                13
            ],
            "refer_text": "The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.",
            "cite_ID": "D07-1026",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.",
            "GPT_cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al., 2004)."
        },
        {
            "Number": 20,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "W12-2429",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "The first, most frequent sense (MFS) (McCarthy et al., 2004) is a widely used baseline for supervised WSD systems."
        }
    ],
    "P08-1028_sweta": [
        {
            "Number": 1,
            "refer_ID": "P08-1028",
            "refer_sids": [
                65
            ],
            "refer_text": "So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.",
            "cite_ID": "D08-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.",
            "GPT_cite_text": "Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p + a as a function f operating on four components: c = f(p, a, R, K) (3) R is the relation holding between p and a, and K is additional knowledge."
        },
        {
            "Number": 2,
            "refer_ID": "P08-1028",
            "refer_sids": [
                75
            ],
            "refer_text": "An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.",
            "cite_ID": "D08-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In both experiments, we compare the SVS model against the state-of-the art model by Mitchell and Lapata 2008 (henceforth M& amp; L; cf",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination. Admittedly, the model in (8) is impoverished and rather simplistic; however, it can serve as a simple baseline against which to compare more sophisticated models.",
            "GPT_cite_text": "In both experiments, we compare the SVS model against the state-of-the-art model by Mitchell and Lapata 2008 (henceforth M&L; cf."
        },
        {
            "Number": 4,
            "refer_ID": "P08-1028",
            "refer_sids": [
                42
            ],
            "refer_text": "This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.",
            "cite_ID": "P14-1060",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This poses problems for modeling linguistic data, which is typically represented by vectors with a non-random structure.",
            "GPT_cite_text": "While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008), Socher et al. (2012), and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations."
        },
        {
            "Number": 6,
            "refer_ID": "P08-1028",
            "refer_sids": [
                21
            ],
            "refer_text": "Central in these models is the notion of compositionality \u2014 the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.",
            "cite_ID": "P10-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Central to these models is the notion of compositionality \u2014 the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.",
            "GPT_cite_text": "Mitchell and Lapata (2008), henceforth M&L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression."
        },
        {
            "Number": 7,
            "refer_ID": "P08-1028",
            "refer_sids": [
                195
            ],
            "refer_text": "The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.",
            "cite_ID": "P10-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modeled more accurately.",
            "GPT_cite_text": "Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting."
        },
        {
            "Number": 8,
            "refer_ID": "P08-1028",
            "refer_sids": [
                25
            ],
            "refer_text": "We present a general framework for vector-based composition which allows us to consider different classes of models.",
            "cite_ID": "D11-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We present a general framework for vector-based composition that allows us to consider different classes of models.",
            "GPT_cite_text": "And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors."
        },
        {
            "Number": 9,
            "refer_ID": "P08-1028",
            "refer_sids": [
                25
            ],
            "refer_text": "We present a general framework for vector-based composition which allows us to consider different classes of models.",
            "cite_ID": "W11-0131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We present a general framework for vector-based composition that allows us to consider different classes of models.",
            "GPT_cite_text": "Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p = f(u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)."
        },
        {
            "Number": 10,
            "refer_ID": "P08-1028",
            "refer_sids": [
                57
            ],
            "refer_text": "Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.",
            "cite_ID": "W11-0131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Let p denote the composition of two vectors u and v, representing a pair of constituents that stand in some syntactic relation R. Let K stand for any additional knowledge or information that is needed to construct the semantics of their composition.",
            "GPT_cite_text": "As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now."
        },
        {
            "Number": 11,
            "refer_ID": "P08-1028",
            "refer_sids": [
                51
            ],
            "refer_text": "Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).",
            "cite_ID": "P13-2083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Our work proposes a framework for vector composition that allows the derivation of different types of models and includes two fundamental composition operations: multiplication and addition (and their combination).",
            "GPT_cite_text": "Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition."
        },
        {
            "Number": 12,
            "refer_ID": "P08-1028",
            "refer_sids": [
                190
            ],
            "refer_text": "We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "P13-2083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We formulated a composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "As our final set of baselines, we extend two simple techniques proposed by Mitchell and Lapata (2008) that use element-wise addition and multiplication operators to perform composition."
        },
        {
            "Number": 13,
            "refer_ID": "P08-1028",
            "refer_sids": [
                29
            ],
            "refer_text": "While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.",
            "cite_ID": "P10-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "While neural networks can readily represent single distinct objects, in the case of multiple objects, there are fundamental difficulties in keeping track of which features are bound to which objects.",
            "GPT_cite_text": "Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work, we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)."
        },
        {
            "Number": 14,
            "refer_ID": "P08-1028",
            "refer_sids": [
                38
            ],
            "refer_text": "The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.",
            "cite_ID": "P10-1021",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The projection is defined in terms of circular convolution, a mathematical function that compresses the tensor product of two vectors.",
            "GPT_cite_text": "Assuming that h is a linear function of the Cartesian product of u and v allows us to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi = ui * vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well-known tensor products (Smolensky 1990) and circular convolution (Plate 1995)."
        },
        {
            "Number": 15,
            "refer_ID": "P08-1028",
            "refer_sids": [
                51
            ],
            "refer_text": "Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Our work proposes a framework for vector composition that allows the derivation of different types of models and includes two fundamental composition operations: multiplication and addition (and their combination).",
            "GPT_cite_text": "Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and pointwise multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)."
        },
        {
            "Number": 16,
            "refer_ID": "P08-1028",
            "refer_sids": [
                64
            ],
            "refer_text": "Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the Cartesian product of u and v, then we generate a class of additive models: where A and B are matrices that determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.",
            "GPT_cite_text": "The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression."
        },
        {
            "Number": 17,
            "refer_ID": "P08-1028",
            "refer_sids": [
                163
            ],
            "refer_text": "We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).",
            "cite_ID": "W11-0115",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).",
            "GPT_cite_text": "For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset."
        },
        {
            "Number": 19,
            "refer_ID": "P08-1028",
            "refer_sids": [
                185
            ],
            "refer_text": "The multiplicative model yields a better fit with the experimental data, \u03c1 = 0.17.",
            "cite_ID": "W11-1310",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The multiplicative model yields a better fit with the experimental data, \u03c1 = 0.17.",
            "GPT_cite_text": "Mitchell and Lapata (2008) observed that a simple multiplication function modeled compositionality better than addition."
        },
        {
            "Number": 20,
            "refer_ID": "P08-1028",
            "refer_sids": [
                190
            ],
            "refer_text": "We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "cite_ID": "W11-1310",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We formulated a composition as a function of two vectors and introduced several models based on addition and multiplication.",
            "GPT_cite_text": "We use the compositionality functions, simple addition and simple multiplication, to build compositional vectors Vwr1+wr2 and Vwr1*wr2. These are as described in (Mitchell and Lapata, 2008)."
        }
    ],
    "D10-1044_aakansha": [
        {
            "Number": 1,
            "refer_ID": "D10-1044",
            "refer_sids": [
                144
            ],
            "refer_text": "In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "cite_ID": "P11-2074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another popular task in SMT is domain adaptation (Foster et al, 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "GPT_cite_text": "Another popular task in SMT is domain adaptation (Foster et al., 2010)."
        },
        {
            "Number": 2,
            "refer_ID": "D10-1044",
            "refer_sids": [
                95,
                96
            ],
            "refer_text": "Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.\nWe used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Phrase tables were extracted from the IN and OUT training corpora (not the dev set as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the second definition had higher accuracy on a development set. We used it to score all phrase pairs in the OUT table in order to provide a feature for the instance-weighting model.",
            "GPT_cite_text": "In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from the training corpus (Matsoukas et al., 2009) or the phrase pairs of the phrase table (Foster et al., 2010)."
        },
        {
            "Number": 3,
            "refer_ID": "D10-1044",
            "refer_sids": [
                9
            ],
            "refer_text": "In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "cite_ID": "D12-1129",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "GPT_cite_text": "Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now."
        },
        {
            "Number": 4,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "P14-2093",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "GPT_cite_text": "Yasuda et al. (2008) and Foster et al. (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models."
        },
        {
            "Number": 5,
            "refer_ID": "D10-1044",
            "refer_sids": [
                28
            ],
            "refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "GPT_cite_text": "However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs and merely varies. Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al. (2010), we propose perplexity optimization for weighted counts (equation 3) and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMT translation model: the phrase translation probabilities p(t|s) and p(s|t), and the lexical weights lex(t|s) and lex(s|t)."
        },
        {
            "Number": 6,
            "refer_ID": "D10-1044",
            "refer_sids": [
                23
            ],
            "refer_text": "Our second contribution is to apply instance weighting at the level of phrase pairs.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our second contribution is to apply instance weighting at the level of phrase pairs.",
            "GPT_cite_text": "Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs."
        },
        {
            "Number": 7,
            "refer_ID": "D10-1044",
            "refer_sids": [
                144
            ],
            "refer_text": "In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "GPT_cite_text": "These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al. (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model."
        },
        {
            "Number": 9,
            "refer_ID": "D10-1044",
            "refer_sids": [
                9
            ],
            "refer_text": "In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "GPT_cite_text": "We expand on work by (Foster et al. 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models. We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research."
        },
        {
            "Number": 10,
            "refer_ID": "D10-1044",
            "refer_sids": [
                28
            ],
            "refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "GPT_cite_text": "In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al., 2010) for conditional phrase-pair probabilities over IN and OUT."
        },
        {
            "Number": 12,
            "refer_ID": "D10-1044",
            "refer_sids": [
                75
            ],
            "refer_text": "However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, it is robust, efficient, and easy to implement. To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "GPT_cite_text": "For efficiency and stability, we use the EM algorithm to find, rather than L-BFGS as in (Foster et al., 2010)."
        },
        {
            "Number": 13,
            "refer_ID": "D10-1044",
            "refer_sids": [
                31
            ],
            "refer_text": "For comparison to information-retrieval inspired baselines, eg (L\u00a8u et al., 2007), we select sentences from OUT using language model perplexities from IN.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010), however, uses a different approach to select related sentences from OUT",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For comparison to information-retrieval inspired baselines, e.g. (L\u00fc et al., 2007), we select sentences from OUT using language model perplexities from IN.",
            "GPT_cite_text": "Foster et al. (2010), however, use a different approach to select related sentences from OUT."
        },
        {
            "Number": 14,
            "refer_ID": "D10-1044",
            "refer_sids": [
                22
            ],
            "refer_text": "Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Within this framework, we use features intended to capture the degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.",
            "GPT_cite_text": "Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality."
        },
        {
            "Number": 15,
            "refer_ID": "D10-1044",
            "refer_sids": [
                23
            ],
            "refer_text": "Our second contribution is to apply instance weighting at the level of phrase pairs.",
            "cite_ID": "P13-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As in (Foster et al, 2010), this approach works at the level of phrase pairs",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our second contribution is to apply instance weighting at the level of phrase pairs.",
            "GPT_cite_text": "As in (Foster et al., 2010), this approach works at the level of phrase pairs."
        },
        {
            "Number": 16,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "GPT_cite_text": "The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al. (2008) and Foster et al. (2010)."
        },
        {
            "Number": 17,
            "refer_ID": "D10-1044",
            "refer_sids": [
                119,
                120
            ],
            "refer_text": "The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.\nThis significantly underperforms log-linear combination.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance. This significantly underperforms log-linear combinations.",
            "GPT_cite_text": "Foster et al. (2010) do not mention what percentage of the corpus they select for their IR baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance."
        },
        {
            "Number": 18,
            "refer_ID": "D10-1044",
            "refer_sids": [
                23,
                24
            ],
            "refer_text": "Our second contribution is to apply instance weighting at the level of phrase pairs.\nSentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) further perform this on extracted phrase pairs, not just sentences",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our second contribution is to apply instance weighting at the level of phrase pairs. Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.",
            "GPT_cite_text": "Foster et al. (2010) further performed this on extracted phrase pairs, not just sentences."
        },
        {
            "Number": 19,
            "refer_ID": "D10-1044",
            "refer_sids": [
                40
            ],
            "refer_text": "We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.",
            "cite_ID": "P14-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(w|h) of a target word w following an n-gram h; and the translation models (TM) p(s|t) and p(t|s), which give the probability of source phrase s translating to target phrase t, and vice versa.",
            "GPT_cite_text": "To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown to significantly improve SMT, such as phrase pair similarity (Zhao et al., 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al., 2010), which also show further improvement for new phrase feature learning in our experiments."
        }
    ],
    "P11-1061_sweta": [
        {
            "Number": 1,
            "refer_ID": "P11-1061",
            "refer_sids": [
                9
            ],
            "refer_text": "Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.",
            "cite_ID": "P11-1144",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.",
            "GPT_cite_text": "Subramanya et al.'s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers."
        },
        {
            "Number": 2,
            "refer_ID": "P11-1061",
            "refer_sids": [
                47
            ],
            "refer_text": "Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).",
            "cite_ID": "P11-1144",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To this end, we use a variant of the quadratic cost criterion of Bengio et al (2006), also used by Subramanya et al (2010) and Das and Petrov (2011) .7 Let V denote the set of all vertices in the graph, Vl? V be the set of known targets and F denote theset of all frames",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).",
            "GPT_cite_text": "To this end, we use a variant of the quadratic cost criterion of Bengio et al. (2006), also used by Subramanya et al. (2010) and Das and Petrov (2011). Let V denote the set of all vertices in the graph, V\u2097 \u2286 V be the set of known targets, and F denote the set of all frames."
        },
        {
            "Number": 3,
            "refer_ID": "P11-1061",
            "refer_sids": [
                10
            ],
            "refer_text": "To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "cite_ID": "P14-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "To bridge this gap, we consider a practically motivated scenario in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages. We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.",
            "GPT_cite_text": "Das and Petrov (2011) achieved the current state-of-the-art in unsupervised tagging by exploiting high-confidence alignments to copy tags from the source language to the target language."
        },
        {
            "Number": 4,
            "refer_ID": "P11-1061",
            "refer_sids": [
                70
            ],
            "refer_text": "Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.",
            "GPT_cite_text": "Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011)."
        },
        {
            "Number": 5,
            "refer_ID": "P11-1061",
            "refer_sids": [
                52
            ],
            "refer_text": "This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.",
            "GPT_cite_text": "Following Das and Petrov (2011) and Subramanya et al. (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics."
        },
        {
            "Number": 6,
            "refer_ID": "P11-1061",
            "refer_sids": [
                83
            ],
            "refer_text": "We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \u00a76.4.",
            "cite_ID": "N12-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We then extract a set of possible tags \\( t_x(y) \\) by eliminating labels whose probability is below a threshold value \\( \\tau \\): We describe how we choose \\( \\tau \\) in \u00a76.4.",
            "GPT_cite_text": "Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type are unavailable (e.g., Das and Petrov, 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "P11-1061",
            "refer_sids": [
                113
            ],
            "refer_text": "For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.",
            "cite_ID": "N12-1052",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language-specific POS tags in the foreign treebank to the universal POS tags.",
            "GPT_cite_text": "Specifically, by replacing fine-grained language-specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features."
        },
        {
            "Number": 8,
            "refer_ID": "P11-1061",
            "refer_sids": [
                3
            ],
            "refer_text": "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).",
            "cite_ID": "N12-1052",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010).",
            "GPT_cite_text": "We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al. (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters."
        },
        {
            "Number": 9,
            "refer_ID": "P11-1061",
            "refer_sids": [
                18
            ],
            "refer_text": "To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).",
            "cite_ID": "N12-1090",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).",
            "GPT_cite_text": "MT-based projection has been applied to various NLP tasks, such as part-of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus."
        },
        {
            "Number": 10,
            "refer_ID": "P11-1061",
            "refer_sids": [
                13
            ],
            "refer_text": "(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.",
            "cite_ID": "W11-2205",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all are available.",
            "GPT_cite_text": "For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)."
        },
        {
            "Number": 11,
            "refer_ID": "P11-1061",
            "refer_sids": [
                3
            ],
            "refer_text": "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).",
            "cite_ID": "P13-1155",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010).",
            "GPT_cite_text": "(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfer to induce POS tags between two languages."
        },
        {
            "Number": 12,
            "refer_ID": "P11-1061",
            "refer_sids": [
                120
            ],
            "refer_text": "We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.",
            "cite_ID": "D12-1127",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.",
            "GPT_cite_text": "Recent work by Das and Petrov (2011) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text."
        },
        {
            "Number": 13,
            "refer_ID": "P11-1061",
            "refer_sids": [
                2
            ],
            "refer_text": "Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.",
            "cite_ID": "D12-1127",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Our method does not assume any knowledge about the target language (in particular, no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.",
            "GPT_cite_text": "These approaches build a dictionary by transferring labeled data from a resource-rich language (English) to a resource-poor language (Das and Petrov, 2011)."
        },
        {
            "Number": 14,
            "refer_ID": "P11-1061",
            "refer_sids": [
                19
            ],
            "refer_text": "Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.",
            "cite_ID": "P12-3012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Syntactic universals are a well-studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.",
            "GPT_cite_text": "In recent years, research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever-growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactic-semantic (Peirsman and Pado, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011)."
        },
        {
            "Number": 15,
            "refer_ID": "P11-1061",
            "refer_sids": [
                153
            ],
            "refer_text": "Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.",
            "GPT_cite_text": "Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of-speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English and achieves accuracies around 85% on the languages that we consider."
        },
        {
            "Number": 16,
            "refer_ID": "P11-1061",
            "refer_sids": [
                18
            ],
            "refer_text": "To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).",
            "cite_ID": "D11-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).",
            "GPT_cite_text": "In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second, we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language."
        },
        {
            "Number": 17,
            "refer_ID": "P11-1061",
            "refer_sids": [
                161
            ],
            "refer_text": "Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections and bridge the gap between purely supervised and unsupervised POS tagging models.",
            "GPT_cite_text": "This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly resourced language to a lesser resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)."
        },
        {
            "Number": 18,
            "refer_ID": "P11-1061",
            "refer_sids": [
                23
            ],
            "refer_text": "Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).",
            "GPT_cite_text": "Das and Petrov (2011) achieved the current state-of-the-art in unsupervised tagging by exploiting high-confidence alignments to copy tags from the source language to the target language."
        },
        {
            "Number": 20,
            "refer_ID": "P11-1061",
            "refer_sids": [
                23
            ],
            "refer_text": "Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).",
            "cite_ID": "P13-2112",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).",
            "GPT_cite_text": "We have proposed a method for unsupervised POS tagging that performs on par with the current state-of-the-art (Das and Petrov, 2011), but is substantially less sophisticated (specifically not requiring convex optimization or a feature-based HMM)."
        }
    ],
    "A00-2030_sweta": [
        {
            "Number": 1,
            "refer_ID": "A00-2030",
            "refer_sids": [
                18
            ],
            "refer_text": "Almost all approaches to information extraction \u2014 even at the sentence level \u2014 are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Section 5 compares our approach tooth ers in the literature, in particular that of (Miller et al., 2000)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Almost all approaches to information extraction \u2014 even at the sentence level \u2014 are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.",
            "GPT_cite_text": "Section 5 compares our approach to others in the literature, in particular that of (Miller et al., 2000)."
        },
        {
            "Number": 2,
            "refer_ID": "A00-2030",
            "refer_sids": [
                100
            ],
            "refer_text": "Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major di erences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a while our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.",
            "GPT_cite_text": "The basic approach we described is very similar to the one presented in (Miller et al., 2000); however, there are a few major differences: in our approach, the augmentation of the syntactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly. The approach in (Miller"
        },
        {
            "Number": 3,
            "refer_ID": "A00-2030",
            "refer_sids": [
                52
            ],
            "refer_text": "In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machine-generated syntactic parse trees.",
            "GPT_cite_text": "The semantic annotation required by our task is much simpler than that employed by Miller et al. (2000)."
        },
        {
            "Number": 4,
            "refer_ID": "A00-2030",
            "refer_sids": [
                34
            ],
            "refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One possibly bene cial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information; that is, entities and relations.",
            "GPT_cite_text": "One possibly beneficial extension of our work suggested by (Miller et al., 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level."
        },
        {
            "Number": 5,
            "refer_ID": "A00-2030",
            "refer_sids": [
                1
            ],
            "refer_text": "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.",
            "cite_ID": "W01-0510",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similar to the approach in (Miller et al, 2000 )weinitialized the SLM statistics from the UPenn Tree bank parse trees (about 1Mwds of training data) at the rst training stage, see Section 3",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the Penn Treebank as a gold standard.",
            "GPT_cite_text": "Similar to the approach in (Miller et al., 2000), we initialized the SLM statistics from the UPenn Treebank parse trees (about 1 M words of training data) at the first training stage, see Section 3."
        },
        {
            "Number": 6,
            "refer_ID": "A00-2030",
            "refer_sids": [
                61
            ],
            "refer_text": "The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "cite_ID": "P14-1078",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "GPT_cite_text": "Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns."
        },
        {
            "Number": 7,
            "refer_ID": "A00-2030",
            "refer_sids": [
                104
            ],
            "refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.",
            "cite_ID": "P05-1061",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) can be used effectively for information extraction.",
            "GPT_cite_text": "One interesting system that does not belong to the above class is that of Miller et al. (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations."
        },
        {
            "Number": 8,
            "refer_ID": "A00-2030",
            "refer_sids": [
                32
            ],
            "refer_text": "Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties \u2014 especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs \u2014 would also benefit semantic analysis.",
            "cite_ID": "P05-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties \u2014 especially their ability to learn from large amounts of data and their robustness when presented with unexpected inputs \u2014 would also benefit semantic analysis.",
            "GPT_cite_text": "Miller et al. (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees."
        },
        {
            "Number": 9,
            "refer_ID": "A00-2030",
            "refer_sids": [
                2
            ],
            "refer_text": "In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
            "cite_ID": "P05-1053",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
            "GPT_cite_text": "Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al. (2000), which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction, and relation extraction in a single model."
        },
        {
            "Number": 10,
            "refer_ID": "A00-2030",
            "refer_sids": [
                61
            ],
            "refer_text": "The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "cite_ID": "H05-1094",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "GPT_cite_text": "(Miller et al., 2000) have combined entity recognition, parsing, and relation extraction into a jointly trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly labeled data is unavailable."
        },
        {
            "Number": 11,
            "refer_ID": "A00-2030",
            "refer_sids": [
                33
            ],
            "refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.",
            "cite_ID": "P04-1054",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Our integrated model represents syntax and semantics jointly using augmented parse trees.",
            "GPT_cite_text": "Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types."
        },
        {
            "Number": 12,
            "refer_ID": "A00-2030",
            "refer_sids": [
                34
            ],
            "refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.",
            "cite_ID": "P04-1054",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "WhereasMiller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In these trees, the standard TREEBANK structures are augmented to convey semantic information; that is, entities and relations.",
            "GPT_cite_text": "Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance."
        },
        {
            "Number": 13,
            "refer_ID": "A00-2030",
            "refer_sids": [
                3
            ],
            "refer_text": "Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.",
            "cite_ID": "W05-0602",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The syntactic model in (Miller et al, 2000) is similar to Collins?, but doesnot use features like sub cat frames and distance measures",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparkhi, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania Treebank as a gold standard.",
            "GPT_cite_text": "The syntactic model in (Miller et al., 2000) is similar to Collins, but does not use features like subcat frames and distance measures."
        },
        {
            "Number": 14,
            "refer_ID": "A00-2030",
            "refer_sids": [
                52
            ],
            "refer_text": "In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.",
            "cite_ID": "N07-2041",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machine-generated syntactic parse trees.",
            "GPT_cite_text": "Similar to the approach in (Miller et al., 2000) and (Kulick et al., 2004), our parser integrates both syntactic and semantic annotations into a single annotation, as shown in Figure 2."
        },
        {
            "Number": 15,
            "refer_ID": "A00-2030",
            "refer_sids": [
                104
            ],
            "refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.",
            "cite_ID": "W10-2924",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) can be used effectively for information extraction.",
            "GPT_cite_text": "Miller et al. (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels."
        },
        {
            "Number": 16,
            "refer_ID": "A00-2030",
            "refer_sids": [
                104
            ],
            "refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.",
            "cite_ID": "W06-0508",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) can be used effectively for information extraction.",
            "GPT_cite_text": "Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al., 2000), or clustering of semantically similar syntactic dependencies according to their selectional restrictions (Gamallo et al., 2002)."
        },
        {
            "Number": 17,
            "refer_ID": "A00-2030",
            "refer_sids": [
                2
            ],
            "refer_text": "In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
            "cite_ID": "P07-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
            "GPT_cite_text": "This includes parsing and relation extraction (Miller et al., 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al., 2004)."
        },
        {
            "Number": 18,
            "refer_ID": "A00-2030",
            "refer_sids": [
                104
            ],
            "refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction",
            "cite_ID": "W05-0636",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) can be used effectively for information extraction.",
            "GPT_cite_text": "For example, Miller et al. (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks."
        },
        {
            "Number": 19,
            "refer_ID": "A00-2030",
            "refer_sids": [
                2
            ],
            "refer_text": "In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
            "GPT_cite_text": "Miller et al. (2000) address the task of relation extraction from the statistical parsing viewpoint."
        },
        {
            "Number": 20,
            "refer_ID": "A00-2030",
            "refer_sids": [
                61
            ],
            "refer_text": "The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "cite_ID": "D11-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.",
            "GPT_cite_text": "Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns."
        }
    ],
    "D10-1044_sweta": [
        {
            "Number": 1,
            "refer_ID": "D10-1044",
            "refer_sids": [
                4
            ],
            "refer_text": "Domain adaptation is a common concern when optimizing empirical NLP applications.",
            "cite_ID": "P11-2074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another popular task in SMT is domain adaptation (Foster et al, 2010)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Domain adaptation is a common concern when optimizing empirical NLP applications.",
            "GPT_cite_text": "Another popular task in SMT is domain adaptation (Foster et al., 2010)."
        },
        {
            "Number": 2,
            "refer_ID": "D10-1044",
            "refer_sids": [
                132
            ],
            "refer_text": "We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We have already mentioned the closely related work by Matsoukas et al. (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.",
            "GPT_cite_text": "In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from the training corpus (Matsoukas et al., 2009) or the phrase pairs of the phrase table (Foster et al., 2010)."
        },
        {
            "Number": 3,
            "refer_ID": "D10-1044",
            "refer_sids": [
                7
            ],
            "refer_text": "For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.",
            "cite_ID": "D12-1129",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.).",
            "GPT_cite_text": "Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now."
        },
        {
            "Number": 4,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "P14-2093",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "GPT_cite_text": "Yasuda et al. (2008) and Foster et al. (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models."
        },
        {
            "Number": 5,
            "refer_ID": "D10-1044",
            "refer_sids": [
                50
            ],
            "refer_text": "Linear weights are difficult to incorporate into the standard MERT procedure because they are \u201chidden\u201d within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Linear weights are difficult to incorporate into the standard MERT procedure because they are \u201chidden\u201d within a top-level probability that represents the linear combination. Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus log-likelihood, which is roughly speaking the training criterion used by the LM and TM themselves.",
            "GPT_cite_text": "However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs and merely varies. Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al. (2010), we propose perplexity optimization for weighted counts (equation 3) and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMT translation model: the phrase translation probabilities p(t|s) and p(s|t), and the lexical weights lex(t|s) and lex(s|t)."
        },
        {
            "Number": 6,
            "refer_ID": "D10-1044",
            "refer_sids": [
                152
            ],
            "refer_text": "We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We will also directly compare with a baseline similar to the Matsoukas et al. approach in order to measure the benefit from weighting phrase pairs (or n-grams) rather than full sentences.",
            "GPT_cite_text": "Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs."
        },
        {
            "Number": 7,
            "refer_ID": "D10-1044",
            "refer_sids": [
                144
            ],
            "refer_text": "In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this paper, we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "GPT_cite_text": "These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al. (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model."
        },
        {
            "Number": 8,
            "refer_ID": "D10-1044",
            "refer_sids": [
                9
            ],
            "refer_text": "In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this paper, we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "GPT_cite_text": "Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE/EN and 1:5 for HT/EN). Previous research has been performed with ratios of 1:100 (Foster et al. 2010) or 1:400 (Axelrod et al. 2011)."
        },
        {
            "Number": 9,
            "refer_ID": "D10-1044",
            "refer_sids": [
                75
            ],
            "refer_text": "However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "However, it is robust, efficient, and easy to implement. To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "GPT_cite_text": "We expand on work by (Foster et al. 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models. We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research."
        },
        {
            "Number": 10,
            "refer_ID": "D10-1044",
            "refer_sids": [
                28
            ],
            "refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "GPT_cite_text": "In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al., 2010) for conditional phrase-pair probabilities over IN and OUT."
        },
        {
            "Number": 11,
            "refer_ID": "D10-1044",
            "refer_sids": [
                97
            ],
            "refer_text": "We carried out translation experiments in two different settings.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We carried out translation experiments in two different settings.",
            "GPT_cite_text": "Our technique for setting m is similar to that outlined in Foster et al. (2010)"
        },
        {
            "Number": 12,
            "refer_ID": "D10-1044",
            "refer_sids": [
                75
            ],
            "refer_text": "However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "However, it is robust, efficient, and easy to implement. To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "GPT_cite_text": "For efficiency and stability, we use the EM algorithm to find, rather than L-BFGS as in (Foster et al., 2010)."
        },
        {
            "Number": 13,
            "refer_ID": "D10-1044",
            "refer_sids": [
                143
            ],
            "refer_text": "Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010), however, uses a different approach to select related sentences from OUT",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Other work includes transferring latent topic distributions from source to target language for LM adaptation (Tam et al., 2007) and adapting features at the sentence level to different categories of sentences (Finch and Sumita, 2008).",
            "GPT_cite_text": "Foster et al. (2010), however, use a different approach to select related sentences from OUT."
        },
        {
            "Number": 14,
            "refer_ID": "D10-1044",
            "refer_sids": [
                153
            ],
            "refer_text": "Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Finally, we intend to explore more sophisticated instance-weighting features for capturing the degree of generality of phrase pairs.",
            "GPT_cite_text": "Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality."
        },
        {
            "Number": 15,
            "refer_ID": "D10-1044",
            "refer_sids": [
                144
            ],
            "refer_text": "In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "cite_ID": "P13-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As in (Foster et al, 2010), this approach works at the level of phrase pairs",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In this paper, we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "GPT_cite_text": "As in (Foster et al., 2010), this approach works at the level of phrase pairs."
        },
        {
            "Number": 16,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "GPT_cite_text": "The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al. (2008) and Foster et al. (2010)."
        },
        {
            "Number": 17,
            "refer_ID": "D10-1044",
            "refer_sids": [
                141
            ],
            "refer_text": "Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L\u00a8u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L\u00a8u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L\u00fc et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L\u00fc et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self-training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).",
            "GPT_cite_text": "Foster et al. (2010) do not mention what percentage of the corpus they select for their IR baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance."
        },
        {
            "Number": 18,
            "refer_ID": "D10-1044",
            "refer_sids": [
                28
            ],
            "refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) further perform this on extracted phrase pairs, not just sentences",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "GPT_cite_text": "Foster et al. (2010) further performed this on extracted phrase pairs, not just sentences."
        },
        {
            "Number": 19,
            "refer_ID": "D10-1044",
            "refer_sids": [
                37
            ],
            "refer_text": "Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.",
            "cite_ID": "P14-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.",
            "GPT_cite_text": "To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown to significantly improve SMT, such as phrase pair similarity (Zhao et al., 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al., 2010), which also show further improvement for new phrase feature learning in our experiments."
        }
    ],
    "P04-1036_swastika": [
        {
            "Number": 1,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account (McCarthy et al., 2004)."
        },
        {
            "Number": 2,
            "refer_ID": "P04-1036",
            "refer_sids": [
                15
            ],
            "refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first or predominant sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "GPT_cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41 Table 2: The SENSEVAL-2 first sense on the SENSEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available."
        },
        {
            "Number": 3,
            "refer_ID": "P04-1036",
            "refer_sids": [
                68
            ],
            "refer_text": "We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The method is described in (McCarthy et al, 2004), which we summarise here",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We are, of course, able to apply the method to other versions of WordNet. The synset is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.",
            "GPT_cite_text": "The method is described in (McCarthy et al., 2004), which we summarize here."
        },
        {
            "Number": 5,
            "refer_ID": "P04-1036",
            "refer_sids": [
                83
            ],
            "refer_text": "The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The results in Table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2,595 polysemous nouns in SemCor with the JCN and Lesk WordNet similarity measures.",
            "GPT_cite_text": "McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research and propose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges."
        },
        {
            "Number": 6,
            "refer_ID": "P04-1036",
            "refer_sids": [
                15
            ],
            "refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first or predominant sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.",
            "GPT_cite_text": "Previous research in inducing sense rankings from an untagged corpus (McCarthy et al., 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction."
        },
        {
            "Number": 7,
            "refer_ID": "P04-1036",
            "refer_sids": [
                101
            ],
            "refer_text": "Thus, if we used the sense ranking as a heuristic for an \u201call nouns\u201d task we would expect to get precision in the region of 60%.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Thus, if we used the sense ranking as a heuristic for an \u201call nouns\u201d task, we would expect to get precision in the region of 60%.",
            "GPT_cite_text": "McCarthy et al. (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus."
        },
        {
            "Number": 8,
            "refer_ID": "P04-1036",
            "refer_sids": [
                126
            ],
            "refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "GPT_cite_text": "Research by McCarthy et al. (2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn."
        },
        {
            "Number": 9,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al., 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense."
        },
        {
            "Number": 11,
            "refer_ID": "P04-1036",
            "refer_sids": [
                83
            ],
            "refer_text": "The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.",
            "cite_ID": "P10-1155",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The results in Table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2,595 polysemous nouns in SemCor with the JCN and Lesk WordNet similarity measures.",
            "GPT_cite_text": "McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the WordNet Similarity JCN measure (Jiang and Conrath, 1997)."
        },
        {
            "Number": 12,
            "refer_ID": "P04-1036",
            "refer_sids": [
                126
            ],
            "refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "GPT_cite_text": "In doing so, we provide first results on the application of WordNet automatic sense ranking (ASR) to French parsing, using the method of McCarthy et al. (2004)."
        },
        {
            "Number": 13,
            "refer_ID": "P04-1036",
            "refer_sids": [
                46
            ],
            "refer_text": "This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "This provides the nearest neighbors to each target word, along with the distributional similarity score between the target word and its neighbor.",
            "GPT_cite_text": "To define an appropriate categorical distribution over synsets for each lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s \u2208 Sx, following the approach of McCarthy et al. (2004)."
        },
        {
            "Number": 14,
            "refer_ID": "P04-1036",
            "refer_sids": [
                126
            ],
            "refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.",
            "GPT_cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al. (2004)"
        },
        {
            "Number": 16,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "S12-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al., 2004)."
        },
        {
            "Number": 17,
            "refer_ID": "P04-1036",
            "refer_sids": [
                105
            ],
            "refer_text": "We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.",
            "cite_ID": "W10-2803",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We use an all-words task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.",
            "GPT_cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining the predominant sense in a given domain (McCarthy et al., 2004), or to work directly with paraphrases (McCarthy and Navigli, 2009)."
        },
        {
            "Number": 18,
            "refer_ID": "P04-1036",
            "refer_sids": [
                13
            ],
            "refer_text": "The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.",
            "cite_ID": "W08-2107",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.",
            "GPT_cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by WordNet, resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al. (2004))."
        },
        {
            "Number": 19,
            "refer_ID": "P04-1036",
            "refer_sids": [
                13
            ],
            "refer_text": "The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.",
            "cite_ID": "D07-1026",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.",
            "GPT_cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al., 2004)."
        },
        {
            "Number": 20,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "W12-2429",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account.",
            "GPT_cite_text": "The first, most frequent sense (MFS) (McCarthy et al., 2004), is a widely used baseline for supervised WSD systems."
        }
    ]
}