{
    "C94-2154": [
        {
            "Number": 1,
            "refer_ID": "C94-2154",
            "refer_sids": [
                67
            ],
            "refer_text": "By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness",
            "cite_ID": "E95-1024",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "By contrast, the Troll system described in this paper has an effective algorithm for deciding well-formedness.",
            "GPT_cite_text": "This test grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994)."
        },
        {
            "Number": 2,
            "refer_ID": "C94-2154",
            "refer_sids": [
                67,
                68,
                69,
                70,
                83,
                84
            ],
            "refer_text": "By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively.Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'.Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability.It would, of course, not be very efficient to work with such large disjunctions of feature structures.",
            "cite_ID": "C94-2204",
            "cite_maker_sids": [
                119
            ],
            "cite_sids": [
                119,
                120,
                121,
                122,
                123,
                124
            ],
            "cite_text": "Gerdemann and G6tz's Troll system (see [G6Tz 1993], [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures.In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure.Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction.Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions.The '1\u00a5oll unifier is closed on these representations.Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "By contrast, the Troll system described in this paper has an effective algorithm for deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure. Call a well-typed feature structure in which all nodes are labeled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write FS, RFS, and DRFS for the collections of feature structures, resolved feature structures, and disjunctive resolved feature structures respectively. Say that F' E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R: -> DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) \u2260 \u2205. APPROPRIATENESS CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F' E DRFS then F \u222a F' E DRFS. Given this property, it would in principle be possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures.",
            "GPT_cite_text": "Gerdemann and G\u00f6tz's Troll system (see [G\u00f6tz 1993], [GERDEMANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output function. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output functions. The Troll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time."
        },
        {
            "Number": 3,
            "refer_ID": "C94-2154",
            "refer_sids": [
                14
            ],
            "refer_text": "1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing",
            "cite_ID": "W97-1506",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "Unlike previous systems such as ALl, Troll does not employ any type inferencing.",
            "GPT_cite_text": "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inference over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories."
        },
        {
            "Number": 4,
            "refer_ID": "C94-2154",
            "refer_sids": [
                70,
                72,
                73
            ],
            "refer_text": "Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl\" the feature structure is satisfiable.The Troll system, which is based on this idea, effectively inqflements type resolution.",
            "cite_ID": "C96-1076",
            "cite_maker_sids": [
                61
            ],
            "cite_sids": [
                61,
                62,
                63,
                64
            ],
            "cite_text": "449 PIION ~ lleben, liebt / [dl VFORM~ bse, fill} I, el SUBJ [] f rVFORM bse] ] ) [lieben -] [AaG~ 2t!]] { rv,,'OaMbsol } s,,,,s,, d2 Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994).Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types.They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types.Many of the groups of disjunctions in their feature structures can be made more efficient via modularization.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Let type resolution be the total function R: -> DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F \u2208 FS is satisfiable iff R(F) \u2260 \u2205. Erdmann and King [8] have also shown that a feature structure meets all encoded FCRs if the feature structure is satisfiable. The Troll system, which is based on this idea, effectively implements type resolution.",
            "GPT_cite_text": "Another example of where modularization might prove useful is in the treatment of typed feature structures presented in (Gerdemann and King, 1994). Their approach produces a set of feature structures from a satisfiability algorithm such that all of the feature structures have the same shape but the nodes may be labeled by different types. They then collapse this set down to a single feature structure where nodes are labeled with dependent disjunctions of types. Many of the groups of disjunctions in their feature structures can be made more efficient via modularization."
        }
    ],
    "W03-0410": [
        {
            "Number": 1,
            "refer_ID": "W03-0410",
            "refer_sids": [
                11
            ],
            "refer_text": "We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.",
            "cite_ID": "D07-1018",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "Stevenson and Joanis, 2003 for English semantic verb classes",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1997), to the lexical semantic classification of verbs.",
            "GPT_cite_text": "Stevenson and Joanis (2003) for English semantic verb classes."
        },
        {
            "Number": 2,
            "refer_ID": "W03-0410",
            "refer_sids": [
                15
            ],
            "refer_text": "Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers.",
            "cite_ID": "D09-1138",
            "cite_maker_sids": [
                25
            ],
            "cite_sids": [
                25
            ],
            "cite_text": "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labeled data are not available for training classifiers.",
            "GPT_cite_text": "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008)."
        },
        {
            "Number": 3,
            "refer_ID": "W03-0410",
            "refer_sids": [
                17
            ],
            "refer_text": "We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).",
            "cite_ID": "D09-1138",
            "cite_maker_sids": [
                70
            ],
            "cite_sids": [
                70
            ],
            "cite_text": "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).",
            "GPT_cite_text": "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008)."
        },
        {
            "Number": 4,
            "refer_ID": "W03-0410",
            "refer_sids": [
                131
            ],
            "refer_text": "We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.",
            "cite_ID": "D11-1095",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We used the hierarchical clustering command in MATLAB, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.",
            "GPT_cite_text": "We adopt as our baseline method a well-known hierarchical method \u2013 agglomerative clustering (AGG) \u2013 which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)."
        },
        {
            "Number": 5,
            "refer_ID": "W03-0410",
            "refer_sids": [
                34
            ],
            "refer_text": "Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research",
            "cite_ID": "D11-1095",
            "cite_maker_sids": [
                38
            ],
            "cite_sids": [
                38
            ],
            "cite_text": "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research.",
            "GPT_cite_text": "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard that includes 13 classes appearing in Levin\u2019s original taxonomy (Stevenson and Joanis, 2003)."
        },
        {
            "Number": 6,
            "refer_ID": "W03-0410",
            "refer_sids": [
                123
            ],
            "refer_text": "All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).",
            "cite_ID": "D11-1095",
            "cite_maker_sids": [
                40
            ],
            "cite_sids": [
                40
            ],
            "cite_text": "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).",
            "GPT_cite_text": "Following Stevenson and Joanis (2003), we selected 20 verbs from each class that occur at least 100 times in our corpus."
        },
        {
            "Number": 7,
            "refer_ID": "W03-0410",
            "refer_sids": [
                34,
                117
            ],
            "refer_text": "Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics researchWe started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).",
            "cite_ID": "D11-1095",
            "cite_maker_sids": [
                54
            ],
            "cite_sids": [
                54
            ],
            "cite_text": "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research. We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).",
            "GPT_cite_text": "Previous works on Levin-style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)."
        },
        {
            "Number": 8,
            "refer_ID": "W03-0410",
            "refer_sids": [
                137
            ],
            "refer_text": "In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff.",
            "cite_ID": "D11-1095",
            "cite_maker_sids": [
                82
            ],
            "cite_sids": [
                82,
                83
            ],
            "cite_text": "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).In addition, a significant amount of information is lost in pairwise clustering.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "In the experiments here, however, we report only results for, since we found no principled way of automatically determining a good cutoff.",
            "GPT_cite_text": "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering."
        },
        {
            "Number": 9,
            "refer_ID": "W03-0410",
            "refer_sids": [
                27
            ],
            "refer_text": "In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).",
            "cite_ID": "D11-1095",
            "cite_maker_sids": [
                167
            ],
            "cite_sids": [
                167
            ],
            "cite_text": "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s result on T1 (using similar features).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi-supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).",
            "GPT_cite_text": "Table 1: Comparison against Stevenson and Joanis (2003)\u2019s results on T1 (using similar features)."
        },
        {
            "Number": 10,
            "refer_ID": "W03-0410",
            "refer_sids": [
                28
            ],
            "refer_text": "Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).",
            "cite_ID": "D11-1095",
            "cite_maker_sids": [
                168
            ],
            "cite_sids": [
                168
            ],
            "cite_text": "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).",
            "GPT_cite_text": "Table 1 shows our results and the results of Stevenson and Joannis (2003) on T1 when employing AGG using Ward as the linkage criterion."
        },
        {
            "Number": 11,
            "refer_ID": "W03-0410",
            "refer_sids": [
                114
            ],
            "refer_text": "7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).",
            "cite_ID": "D11-1095",
            "cite_maker_sids": [
                169
            ],
            "cite_sids": [
                169
            ],
            "cite_text": "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).",
            "GPT_cite_text": "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%."
        },
        {
            "Number": 12,
            "refer_ID": "W03-0410",
            "refer_sids": [
                144
            ],
            "refer_text": "Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.",
            "cite_ID": "J06-2001",
            "cite_maker_sids": [
                397
            ],
            "cite_sids": [
                397
            ],
            "cite_text": "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Then accuracy has the standard definition: 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.",
            "GPT_cite_text": "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to the correct cluster with respect to the gold standard class of the majority of cluster members."
        },
        {
            "Number": 13,
            "refer_ID": "W03-0410",
            "refer_sids": [
                117,
                229,
                230
            ],
            "refer_text": "We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder.Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks.",
            "cite_ID": "J06-2001",
            "cite_maker_sids": [
                586
            ],
            "cite_sids": [
                586,
                587,
                588,
                589
            ],
            "cite_text": "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.Low- frequency and ambiguous verbs were excluded from the classes.They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). This feature selection method is highly successful, outperforming the full feature set (Full) on most tasks, and performing the same or very close on the remainder. Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks.",
            "GPT_cite_text": "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low-frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space."
        },
        {
            "Number": 15,
            "refer_ID": "W03-0410",
            "refer_sids": [
                144,
                157
            ],
            "refer_text": "Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.",
            "cite_ID": "P03-1009",
            "cite_maker_sids": [
                124
            ],
            "cite_sids": [
                124
            ],
            "cite_text": "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Then accuracy has the standard definition: 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.",
            "GPT_cite_text": "Our second measure is derived from purity, a global measure that evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003)."
        },
        {
            "Number": 16,
            "refer_ID": "W03-0410",
            "refer_sids": [
                195
            ],
            "refer_text": "All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.",
            "cite_ID": "P03-1009",
            "cite_maker_sids": [
                143
            ],
            "cite_sids": [
                143
            ],
            "cite_text": "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R \u2264 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.",
            "label": [
                "Results_Citation"
            ],
            "GPT_cite_text": "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mPUR \u2264 29%), but their task involves classifying 841 verbs into 14 classes based on differences in the predicate-argument structure."
        },
        {
            "Number": 17,
            "refer_ID": "W03-0410",
            "refer_sids": [
                11
            ],
            "refer_text": "We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.",
            "cite_ID": "P04-2007",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical-semantic classification of verbs.",
            "GPT_cite_text": "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Schulte im Walde, 2003)."
        },
        {
            "Number": 20,
            "refer_ID": "W03-0410",
            "refer_sids": [
                23
            ],
            "refer_text": "In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to \u201cthe curse of dimensionality\u201d?",
            "cite_ID": "P07-3016",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                39
            ],
            "cite_text": "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to \u201cthe curse of dimensionality\u201d?",
            "GPT_cite_text": "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks."
        },
        {
            "Number": 21,
            "refer_ID": "W03-0410",
            "refer_sids": [
                261
            ],
            "refer_text": "We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.",
            "cite_ID": "W06-2910",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                7,
                8
            ],
            "cite_text": "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).",
            "label": [
                "Method_Citation",
                "Aim_Citation"
            ],
            "GPT_refer_text": "We have explored manual, unsupervised, and semi-supervised methods for feature selection in a clustering approach for verb class discovery.",
            "GPT_cite_text": "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Ferrer, 2004)."
        },
        {
            "Number": 22,
            "refer_ID": "W03-0410",
            "refer_sids": [
                35,
                254
            ],
            "refer_text": "Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax.Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.",
            "cite_ID": "W06-2910",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Levin\u2019s classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.",
            "GPT_cite_text": "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient."
        },
        {
            "Number": 23,
            "refer_ID": "W03-0410",
            "refer_sids": [
                144
            ],
            "refer_text": "Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.",
            "cite_ID": "W06-2910",
            "cite_maker_sids": [
                118
            ],
            "cite_sids": [
                118,
                119
            ],
            "cite_text": "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Then accuracy has the standard definition: 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.",
            "GPT_cite_text": "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003)."
        },
        {
            "Number": 24,
            "refer_ID": "W03-0410",
            "refer_sids": [
                17
            ],
            "refer_text": "We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).",
            "cite_ID": "E09-1072",
            "cite_maker_sids": [
                80
            ],
            "cite_sids": [
                80
            ],
            "cite_text": "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).",
            "GPT_cite_text": "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus."
        }
    ],
    "N09-1001": [
        {
            "Number": 1,
            "refer_ID": "N09-1001",
            "refer_sids": [
                126,
                127,
                133,
                207
            ],
            "refer_text": "We conduct the experiments on two different gold standard datasets. One is the MicroWNOp corpus, ntu.edu.tw/cjlin/libsvm/.It includes 298 words with 703 objective and 358 subjective WordNet senses.Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2).",
            "cite_ID": "N09_csl2013",
            "cite_maker_sids": [
                147
            ],
            "cite_sids": [
                146,
                147
            ],
            "cite_text": "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We conduct the experiments on two different gold standard datasets. One is the MicroWNOp corpus, ntu.edu.tw/cjlin/libsvm/. It includes 298 words with 703 objective and 358 subjective WordNet senses. Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2).",
            "GPT_cite_text": "The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words."
        },
        {
            "Number": 2,
            "refer_ID": "N09-1001",
            "refer_sids": [
                58,
                59,
                60
            ],
            "refer_text": "Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework. Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components. More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data to the right cuts (categories).",
            "cite_ID": "N09_qwn",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.",
            "label": [
                "Method_Citation |"
            ],
            "GPT_refer_text": "Also, WordNet connections between different parts of the WordNet hierarchy can be sparse, leading to relatively isolated senses in a graph in a supervised framework. Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components. More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, it might help pull test data to the right cuts (categories).",
            "GPT_cite_text": "Although it can be used in its current form for data-driven sentiment analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; Danescu-Niculescu-Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet."
        },
        {
            "Number": 3,
            "refer_ID": "N09-1001",
            "refer_sids": [
                20
            ],
            "refer_text": "Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.",
            "cite_ID": "N09_qwn",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "Although it can be used in its current form for data-driven Sentiment Analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; DanescuNiculescu- Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Qc 2009 Association for Computational Linguistics. We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses. Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.",
            "GPT_cite_text": "Although it can be used in its current form for data-driven sentiment analysis (Pang et al., 2002; Pang and Lee, 2004; Kim and Hovy, 2004; Popescu and Etzioni, 2005; Su and Markert, 2009; Danescu-Niculescu-Mizil et al., 2009), or for lexical sentiment analysis tasks (Strapparava and Mihalcea, 2007; Su and Markert, 2009), it could also be used as a training set for supervised classifiers that would subsequently be applied for the improvement of Q-WordNet."
        },
        {
            "Number": 4,
            "refer_ID": "N09-1001",
            "refer_sids": [
                4
            ],
            "refer_text": "We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.",
            "cite_ID": "N09prod",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35
            ],
            "cite_text": "Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relational structure.",
            "GPT_cite_text": "Pang and Su both optimized their multi-class classification results using the Mincut model."
        },
        {
            "Number": 5,
            "refer_ID": "N09-1001",
            "refer_sids": [
                4
            ],
            "refer_text": "We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.",
            "cite_ID": "N09prod",
            "cite_maker_sids": [
                121
            ],
            "cite_sids": [
                121
            ],
            "cite_text": "Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.",
            "label": [
                "Aim_citation"
            ],
            "GPT_refer_text": "We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relational structure.",
            "GPT_cite_text": "Inspired by the work of Pang[4] and Su[5], we also use the Minimum Cut (Mincut) model to optimize the two-stage SVM result."
        },
        {
            "Number": 6,
            "refer_ID": "N09-1001",
            "refer_sids": [
                1
            ],
            "refer_text": "We supplement WordNet entries with information on the subjectivity of its word senses.",
            "cite_ID": "N15-1071",
            "cite_maker_sids": [
                147
            ],
            "cite_sids": [
                147
            ],
            "cite_text": "The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We supplement WordNet entries with information on the subjectivity of its word senses.",
            "GPT_cite_text": "The MWE productions seem to overlap with well-known linguistic phenomena; consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category)."
        },
        {
            "Number": 7,
            "refer_ID": "N09-1001",
            "refer_sids": [
                104,
                105,
                106
            ],
            "refer_text": "Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie..",
            "cite_ID": "P101167",
            "cite_maker_sids": [
                146
            ],
            "cite_sids": [
                146
            ],
            "cite_text": "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity-preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations. We employ LIBSVM, available at http://www.csie.",
            "GPT_cite_text": "Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information."
        },
        {
            "Number": 8,
            "refer_ID": "N09-1001",
            "refer_sids": [
                72,
                73,
                74,
                75
            ],
            "refer_text": "We define two vertices s (source) and t (sink),. which correspond to the subjective and objective category, respectively. Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices. Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge.",
            "cite_ID": "P1018",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "In more recent work it has been argued that the classification of subjectivity vs. objectivity needs to be done independently from the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009).",
            "label": [
                "Method_Citation |"
            ],
            "GPT_refer_text": "We define two vertices s (source) and t (sink), which correspond to the subjective and objective categories, respectively. Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices. Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge.",
            "GPT_cite_text": "In more recent work, it has been argued that the classification of subjectivity vs. objectivity needs to be done independently of the polarity identification (Gyamfi et al., 2009; Su and Markert, 2009)."
        },
        {
            "Number": 9,
            "refer_ID": "N09-1001",
            "refer_sids": [
                4,
                207
            ],
            "refer_text": "We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2).",
            "cite_ID": "P1018",
            "cite_maker_sids": [
                180
            ],
            "cite_sids": [
                180
            ],
            "cite_text": "For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech.",
            "label": [
                "Method_Citation",
                " Results_Citation"
            ],
            "GPT_refer_text": "We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2).",
            "GPT_cite_text": "For example, Su and Markert (2009) make use of both WordNet definitions and WordNet relations and achieve an accuracy of 84.6% on all parts of speech."
        },
        {
            "Number": 10,
            "refer_ID": "N09-1001",
            "refer_sids": [
                65
            ],
            "refer_text": "We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.",
            "cite_ID": "PEAAI_n09",
            "cite_maker_sids": [
                70
            ],
            "cite_sids": [
                70
            ],
            "cite_text": "Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We propose semi-supervised min-cuts for subjectivity recognition on senses for several reasons.",
            "GPT_cite_text": "Semi-supervised techniques in text mining were applied by Fangzhong and Markert (2009)."
        },
        {
            "Number": 11,
            "refer_ID": "N09-1001",
            "refer_sids": [
                20
            ],
            "refer_text": "Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.",
            "cite_ID": "Pproc2014_n09",
            "cite_maker_sids": [
                33
            ],
            "cite_sids": [
                33
            ],
            "cite_text": "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Qc 2009 Association for Computational Linguistics. We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses. Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.",
            "GPT_cite_text": "The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill. While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/- effects are not the same; a single event may have different sentiment and +/- effect polarities, for example."
        },
        {
            "Number": 12,
            "refer_ID": "N09-1001",
            "refer_sids": [
                20
            ],
            "refer_text": "Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.",
            "cite_ID": "Pproc2014_n09",
            "cite_maker_sids": [
                70
            ],
            "cite_sids": [
                70
            ],
            "cite_text": "Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Qc 2009 Association for Computational Linguistics. We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses. Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.",
            "GPT_cite_text": "Su and Markert (2009) adopt a semi-supervised min-cut method to recognize the subjectivity of word senses."
        },
        {
            "Number": 13,
            "refer_ID": "N09-1001",
            "refer_sids": [
                1
            ],
            "refer_text": "We supplement WordNet entries with information on the subjectivity of its word senses.",
            "cite_ID": "W11-0311",
            "cite_maker_sids": [
                231
            ],
            "cite_sids": [
                231
            ],
            "cite_text": "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We supplement WordNet entries with information on the subjectivity of its word senses.",
            "GPT_cite_text": "One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009)."
        }
    ],
    "C90-2039": [
        {
            "Number": 1,
            "refer_ID": "C90-2039",
            "refer_sids": [
                21,
                22,
                23
            ],
            "refer_text": "itowever, the problem with his method is that a unitication result graph consists only of newly created structures.This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.Copying sharable parts is called redundant copying.",
            "cite_ID": "P99-1061",
            "cite_maker_sids": [
                48
            ],
            "cite_sids": [
                48
            ],
            "cite_text": "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "However, the problem with his method is that a unification result graph consists only of newly created structures. This is unnecessary because there are often input subgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying.",
            "GPT_cite_text": "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (1990) calls redundant copying."
        },
        {
            "Number": 2,
            "refer_ID": "C90-2039",
            "refer_sids": [
                205,
                206
            ],
            "refer_text": "Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.This reduces repeated calculation of substructures.",
            "cite_ID": "C90-3046",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102,
                103
            ],
            "cite_text": "This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].Thus treatments such as strategic unification [6] have been developed.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Furthermore, structure sharing increases the portion of token-identical substructures of FSs, which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculations of substructures.",
            "GPT_cite_text": "This is inefficient with many copy operations due to unifications of unnecessary features that do not contribute to successful unification [6]. Thus, treatments such as strategic unification [6] have been developed."
        },
        {
            "Number": 3,
            "refer_ID": "C90-2039",
            "refer_sids": [
                3
            ],
            "refer_text": "The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.",
            "cite_ID": "P91-1031",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49
            ],
            "cite_text": "This observation is the basis for a reordering method proposed by Kogure [1990].",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The other, called the strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify substructures tending to fail in unification; this method is based on stochastic data on the likelihood of failure and reduces unnecessary computation.",
            "GPT_cite_text": "This observation is the basis for a reordering method proposed by Kogure (1990)."
        },
        {
            "Number": 4,
            "refer_ID": "C90-2039",
            "refer_sids": [
                186,
                187,
                188
            ],
            "refer_text": "in this method, theretbre, the failure tendency information is acquired by a learning process.That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.",
            "cite_ID": "P91-1031",
            "cite_maker_sids": [
                62
            ],
            "cite_sids": [
                62
            ],
            "cite_text": "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this method, therefore, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. In the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.",
            "GPT_cite_text": "Thus, for any automatic counting scheme, some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990])."
        },
        {
            "Number": 5,
            "refer_ID": "C90-2039",
            "refer_sids": [
                39,
                40,
                78
            ],
            "refer_text": "This paper proposes an FS unification method that allows structure sharing with constant m'der node access time.This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet.",
            "cite_ID": "E93-1008",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49
            ],
            "cite_text": "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper proposes an FS unification method that allows structure sharing with constant m-der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. Then, the unification of t1 and t2 is defined as their greatest lower bound or the meet.",
            "GPT_cite_text": "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure."
        },
        {
            "Number": 6,
            "refer_ID": "C90-2039",
            "refer_sids": [
                23,
                205
            ],
            "refer_text": "Copying sharable parts is called redundant copying.Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.",
            "cite_ID": "C92-2068",
            "cite_maker_sids": [
                21
            ],
            "cite_sids": [
                21
            ],
            "cite_text": "\u2022 Data-Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node the notion of structure-sharing at the data structure level. [Kogure, 1990] calls copying of such structures Redundant Copying.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Copying sharable parts is called redundant copying. Furthermore, structure sharing increases the portion of token-identical substructures of FSs, which makes it efficient to keep unification results of substructures of FSs and reuse them.",
            "GPT_cite_text": "\u2022 Data Structure Sharing: Two or more distinct graphs share the same subgraph by converging on the same node, the notion of structure sharing at the data structure level. [Kogure, 1990] calls the copying of such structures Redundant Copying."
        },
        {
            "Number": 7,
            "refer_ID": "C90-2039",
            "refer_sids": [
                205
            ],
            "refer_text": "Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.",
            "cite_ID": "P91-1041",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Furthermore, structure sharing increases the portion of token-identical substructures of FSs, which makes it efficient to keep unification results of substructures of FSs and reuse them.",
            "GPT_cite_text": "In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification (Kogure, 1990)."
        },
        {
            "Number": 8,
            "refer_ID": "C90-2039",
            "refer_sids": [
                11,
                14
            ],
            "refer_text": "For example, a spoken Present.Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.",
            "cite_ID": "P91-1041",
            "cite_maker_sids": [
                230
            ],
            "cite_sids": [
                230
            ],
            "cite_text": "That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "For example, a spoken present Japanese analysis system based on llPSG [Kogure 891] uses 90% - 98% of the elapsed time in FS unification.",
            "GPT_cite_text": "That is, unless some new scheme for reducing excessive copying is introduced such as structure-sharing of an unchanged shared-forest ([Kogure, 1990])."
        },
        {
            "Number": 9,
            "refer_ID": "C90-2039",
            "refer_sids": [
                203,
                22,
                23,
                24
            ],
            "refer_text": "The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method.This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.Copying sharable parts is called redundant copying.A better method would nfinimize the copying of sharable varts.",
            "cite_ID": "C94-2143",
            "cite_maker_sids": [
                56
            ],
            "cite_sids": [
                56
            ],
            "cite_text": "A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. This is unnecessary because there are often input subgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would minimize the copying of sharable parts.",
            "GPT_cite_text": "A more efficient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990)."
        },
        {
            "Number": 10,
            "refer_ID": "C90-2039",
            "refer_sids": [
                22,
                23,
                24
            ],
            "refer_text": "This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.Copying sharable parts is called redundant copying.A better method would nfinimize the copying of sharable varts.",
            "cite_ID": "C94-2143",
            "cite_maker_sids": [
                69
            ],
            "cite_sids": [
                69
            ],
            "cite_text": "Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This is unnecessary because there are often input subgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would minimize the copying of sharable parts.",
            "GPT_cite_text": "Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug."
        },
        {
            "Number": 11,
            "refer_ID": "C90-2039",
            "refer_sids": [
                23,
                24
            ],
            "refer_text": "Copying sharable parts is called redundant copying.A better method would nfinimize the copying of sharable varts.",
            "cite_ID": "P91-1042",
            "cite_maker_sids": [
                37
            ],
            "cite_sids": [
                37
            ],
            "cite_text": "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Copying sharable parts is called redundant copying. A better method would minimize the copying of sharable parts.",
            "GPT_cite_text": "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 1990]."
        },
        {
            "Number": 12,
            "refer_ID": "C90-2039",
            "refer_sids": [
                23,
                141,
                142
            ],
            "refer_text": "Copying sharable parts is called redundant copying.5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.",
            "cite_ID": "P91-1042",
            "cite_maker_sids": [
                116
            ],
            "cite_sids": [
                116,
                117,
                118,
                124
            ],
            "cite_text": "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily.Copying of nodes will be delayed until a destructive change is about to take place.Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Copying sharable parts is called redundant copying. It disables structure sharing; however, this whole copying is not necessary if a lazy evaluation method is used. With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node that needs to be copied (e.g., node X G3/<a c> in Fig.",
            "GPT_cite_text": "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Kogure uses a revised copy node procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied."
        },
        {
            "Number": 13,
            "refer_ID": "C90-2039",
            "refer_sids": [
                24
            ],
            "refer_text": "A better method would nfinimize the copying of sharable varts.",
            "cite_ID": "W97-1503",
            "cite_maker_sids": [
                136
            ],
            "cite_sids": [
                136
            ],
            "cite_text": "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A better method would minimize the copying of sharable parts.",
            "GPT_cite_text": "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987; Kogure, 1990)."
        }
    ],
    "I05-5011": [
        {
            "Number": 1,
            "refer_ID": "I05-5011",
            "refer_sids": [
                204
            ],
            "refer_text": "We proposed an unsupervised method to discover paraphrases from a large untagged corpus.",
            "cite_ID": "C08-1107",
            "cite_maker_sids": [
                19
            ],
            "cite_sids": [
                18,
                19
            ],
            "cite_text": "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We proposed an unsupervised method to discover paraphrases from a large untagged corpus.",
            "GPT_cite_text": "This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases), e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005)."
        },
        {
            "Number": 2,
            "refer_ID": "I05-5011",
            "refer_sids": [
                13,
                15,
                22
            ],
            "refer_text": "For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question.We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.",
            "cite_ID": "C10-2017",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.",
            "label": [
                "Implication_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "For example, in Information Retrieval (IR), we have to match a user\u2019s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user\u2019s question even if the formulation of the answer in the document is different from the question. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.",
            "GPT_cite_text": "In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator."
        },
        {
            "Number": 3,
            "refer_ID": "I05-5011",
            "refer_sids": [
                22
            ],
            "refer_text": "In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.",
            "cite_ID": "D12-1094",
            "cite_maker_sids": [
                70
            ],
            "cite_sids": [
                70
            ],
            "cite_text": "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.",
            "GPT_cite_text": "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extend the idea of distributional similarity to phrases."
        },
        {
            "Number": 4,
            "refer_ID": "I05-5011",
            "refer_sids": [
                22,
                40
            ],
            "refer_text": "In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].",
            "cite_ID": "E09-1025",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].",
            "GPT_cite_text": "A number of automatically acquired inference rule/paraphrase collections is available, such as (Szpektor et al., 2004) and (Sekine, 2005)."
        },
        {
            "Number": 5,
            "refer_ID": "I05-5011",
            "refer_sids": [
                40,
                41
            ],
            "refer_text": "The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object).",
            "cite_ID": "I08-1070",
            "cite_maker_sids": [
                53
            ],
            "cite_sids": [
                53
            ],
            "cite_text": "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUC\u2019s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object).",
            "GPT_cite_text": "To avoid the drawbacks, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006)."
        },
        {
            "Number": 6,
            "refer_ID": "I05-5011",
            "refer_sids": [
                110,
                167
            ],
            "refer_text": "Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].",
            "cite_ID": "J10-3003",
            "cite_maker_sids": [
                429
            ],
            "cite_sids": [
                429
            ],
            "cite_text": "As a later re\ufb01nement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speci\ufb01c concepts represented by keywords.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Here, a set is represented by the keyword, and the number in parentheses indicates the number of shared NE pair instances. Another approach to finding paraphrases is to find phrases that take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].",
            "GPT_cite_text": "As a later refinement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for specific concepts represented by keywords."
        },
        {
            "Number": 7,
            "refer_ID": "I05-5011",
            "refer_sids": [
                29,
                30
            ],
            "refer_text": "For each pair we also record the context, i.e. the phrase between the two NEs (Step1).Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair.",
            "cite_ID": "P06-2094",
            "cite_maker_sids": [
                65
            ],
            "cite_sids": [
                65
            ],
            "cite_text": "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each pair, we also record the context, i.e., the phrase between the two NEs (Step 1). Next, for each pair of NE categories, we collect all the contexts and find the keywords that are topical for that NE category pair.",
            "GPT_cite_text": "We also proposed a method to find paraphrases in the context of two named entity instances in a large unannotated corpus (Sekine 05)."
        },
        {
            "Number": 8,
            "refer_ID": "I05-5011",
            "refer_sids": [
                38
            ],
            "refer_text": "Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.",
            "cite_ID": "P07-1058",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49
            ],
            "cite_text": "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don\u00e2\u20ac\u2122t.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Extract NE instance pairs with contexts. First, we extract NE pair instances with their context from the corpus.",
            "GPT_cite_text": "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don't."
        },
        {
            "Number": 9,
            "refer_ID": "I05-5011",
            "refer_sids": [
                38
            ],
            "refer_text": "Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.",
            "cite_ID": "P07-1058",
            "cite_maker_sids": [
                57
            ],
            "cite_sids": [
                57
            ],
            "cite_text": "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Extract NE instance pairs with contexts. First, we extract NE pair instances with their context from the corpus.",
            "GPT_cite_text": "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005)."
        },
        {
            "Number": 10,
            "refer_ID": "I05-5011",
            "refer_sids": [
                18,
                20
            ],
            "refer_text": "Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.",
            "cite_ID": "P07-1058",
            "cite_maker_sids": [
                67
            ],
            "cite_sids": [
                67
            ],
            "cite_text": "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. In order to create an IE system for a new domain, one has to spend a long time creating the knowledge.",
            "GPT_cite_text": "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005)."
        },
        {
            "Number": 11,
            "refer_ID": "I05-5011",
            "refer_sids": [
                22,
                23
            ],
            "refer_text": "In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.",
            "cite_ID": "P09-2063",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases that have two Named Entities (NEs), as those types of phrases are very important for IE applications.",
            "GPT_cite_text": "Similarly, Sekine (2005) improved information retrieval based on pattern recognition by introducing paraphrase generation."
        },
        {
            "Number": 12,
            "refer_ID": "I05-5011",
            "refer_sids": [
                197
            ],
            "refer_text": "Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.",
            "cite_ID": "P11-1109",
            "cite_maker_sids": [
                218
            ],
            "cite_sids": [
                218
            ],
            "cite_text": "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Rather, we believe several methods have to be developed using different heuristics to discover a wider variety of paraphrases.",
            "GPT_cite_text": "We agree with Sekine (2005), who claims that several different methods are required to discover a wider variety of paraphrases."
        },
        {
            "Number": 14,
            "refer_ID": "I05-5011",
            "refer_sids": [
                164
            ],
            "refer_text": "There have been other kinds of efforts to discover paraphrase automatically from corpora.",
            "cite_ID": "P12-2031",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "There have been other kinds of efforts to discover paraphrases automatically from corpora.",
            "GPT_cite_text": "The significance of inference rules has led to substantial effort in developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010) and generate knowledge resources for inference systems."
        },
        {
            "Number": 15,
            "refer_ID": "I05-5011",
            "refer_sids": [
                18
            ],
            "refer_text": "Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.",
            "cite_ID": "P12-2031",
            "cite_maker_sids": [
                20
            ],
            "cite_sids": [
                20
            ],
            "cite_text": "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Up to now, most IE researchers have been creating paraphrased knowledge (or IE patterns) by hand and for specific tasks.",
            "GPT_cite_text": "Some attempts were made to let annotators judge rule correctness directly, that is, by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005)."
        },
        {
            "Number": 16,
            "refer_ID": "I05-5011",
            "refer_sids": [
                62,
                65
            ],
            "refer_text": "Cluster phrases based on Links We now have a set of phrases which share a keyword.At this step, we will try to link those sets, and put them into a single cluster.",
            "cite_ID": "P13-1131",
            "cite_maker_sids": [
                134
            ],
            "cite_sids": [
                134
            ],
            "cite_text": "Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Cluster phrases based on links. We now have a set of phrases that share a keyword. At this step, we will try to link those sets and put them into a single cluster.",
            "GPT_cite_text": "Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co-occurrence statistics per predicate."
        },
        {
            "Number": 17,
            "refer_ID": "I05-5011",
            "refer_sids": [
                32,
                33
            ],
            "refer_text": "Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC- domain (Step 2).For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3).",
            "cite_ID": "W12-4006",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                14,
                15
            ],
            "cite_text": "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Hereafter, each pair of NE categories will be called a domain; e.g. the \u201cCompany \u2013 Company\u201d domain, which we will call CC-domain (Step 2). For each domain, phrases that contain the same keyword are gathered to build a set of phrases (Step 3).",
            "GPT_cite_text": "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g., (Sekine, 2005), (Callison-Burch, 2008)), but most do not."
        },
        {
            "Number": 18,
            "refer_ID": "I05-5011",
            "refer_sids": [
                22,
                23
            ],
            "refer_text": "In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.",
            "cite_ID": "W12-4006",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41
            ],
            "cite_text": "As for paraphrase, Sekine\u00e2\u20ac\u2122s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases that have two Named Entities (NEs), as those types of phrases are very important for IE applications.",
            "GPT_cite_text": "As for paraphrase, Sekine's Paraphrase Database (Sekine, 2005) is collected using an unsupervised method and focuses on phrases connecting two Named Entities."
        }
    ],
    "C00-2123": [
        {
            "Number": 1,
            "refer_ID": "C00-2123",
            "refer_sids": [
                179
            ],
            "refer_text": "For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.",
            "cite_ID": "C02-1050",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                39,
                40,
                41
            ],
            "cite_text": "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For our demonstration system, we typically use the pruning threshold t0 = 5.0 to speed up the search by a factor of 5 while allowing for a small degradation in translation accuracy.",
            "GPT_cite_text": "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000), Garcia-Varea and Casacuberta (2001), and Germann et al."
        },
        {
            "Number": 2,
            "refer_ID": "C00-2123",
            "refer_sids": [
                1,
                94
            ],
            "refer_text": "In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.",
            "cite_ID": "C02-1050",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.",
            "GPT_cite_text": "There exists a stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997), and dynamic programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word by word and render the translation in left to right, with pruning technologies assuming almost linearly aligned translation source and target texts."
        },
        {
            "Number": 3,
            "refer_ID": "C00-2123",
            "refer_sids": [
                46,
                47
            ],
            "refer_text": "The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.",
            "cite_ID": "C02-1050",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The algorithm works due to the fact that not all permutations of cities have to be considered explicitly. For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j); only the score for the best path reaching j has to be stored.",
            "GPT_cite_text": "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al."
        },
        {
            "Number": 4,
            "refer_ID": "C00-2123",
            "refer_sids": [
                94,
                139
            ],
            "refer_text": "When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.This approach leads to a search procedure with complexity O(E3 J4).",
            "cite_ID": "C02-1050",
            "cite_maker_sids": [
                80
            ],
            "cite_sids": [
                80
            ],
            "cite_text": "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. This approach leads to a search procedure with complexity O(E^3 J^4).",
            "GPT_cite_text": "The computational complexity for the left-to-right and right-to-left is the same, O(|E|^3 * m^2), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences."
        },
        {
            "Number": 5,
            "refer_ID": "C00-2123",
            "refer_sids": [
                39
            ],
            "refer_text": "In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).",
            "cite_ID": "C04-1091",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) that is based on dynamic programming (Held, Karp, 1962).",
            "GPT_cite_text": "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave an O(l^3m^4) \u2248 O(m^7) algorithm for French-English translation (Tillman and Ney, 2000)."
        },
        {
            "Number": 6,
            "refer_ID": "C00-2123",
            "refer_sids": [
                6
            ],
            "refer_text": "We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability",
            "cite_ID": "E06-1004",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                21,
                22
            ],
            "cite_text": "\u00e2\u20ac\u00a2 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We are given a source string fJ1 = f1:::fj:::fJ of length J, which is to be translated into a target string eI1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability.",
            "GPT_cite_text": "\u2022 Conditional Probability Given the model parameters and a sentence pair (f, e), compute P(f | e). (1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004)."
        },
        {
            "Number": 7,
            "refer_ID": "C00-2123",
            "refer_sids": [
                169
            ],
            "refer_text": "For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.",
            "cite_ID": "H01-1062",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u00e2\u20ac\u00a2 single-word based approach [20];",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each source word f, the list of its possible translations e is sorted according to p(f|e) puni(e), where puni(e) is the unigram probability of the English word e. It is sufficient to consider only the best 50 words.",
            "GPT_cite_text": "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u2022 single-word based approach [20];"
        },
        {
            "Number": 8,
            "refer_ID": "C00-2123",
            "refer_sids": [
                165
            ],
            "refer_text": "We apply a beam search concept as in speech recognition.",
            "cite_ID": "J03-1005",
            "cite_maker_sids": [
                117
            ],
            "cite_sids": [
                115,
                117
            ],
            "cite_text": "This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We apply a beam search concept as in speech recognition.",
            "GPT_cite_text": "This article will present a DP-based beam search decoder for the IBM Model 4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000)."
        },
        {
            "Number": 9,
            "refer_ID": "C00-2123",
            "refer_sids": [
                20
            ],
            "refer_text": "For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.",
            "cite_ID": "J04-2003",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35
            ],
            "cite_text": "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the translation model Pr(fJ 1 | eI 1), we go on the assumption that each source word is aligned to exactly one target word.",
            "GPT_cite_text": "Many existing systems for statistical machine translation (Garc\u00eda-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position."
        },
        {
            "Number": 11,
            "refer_ID": "C00-2123",
            "refer_sids": [
                179
            ],
            "refer_text": "For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.",
            "cite_ID": "J04-4002",
            "cite_maker_sids": [
                282
            ],
            "cite_sids": [
                282
            ],
            "cite_text": "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For our demonstration system, we typically use the pruning threshold t0 = 5.0 to speed up the search by a factor of 5 while allowing for a small degradation in translation accuracy.",
            "GPT_cite_text": "We call this selection of highly probable words observation pruning (Tillmann and Ney, 2000)."
        },
        {
            "Number": 12,
            "refer_ID": "C00-2123",
            "refer_sids": [
                136,
                165
            ],
            "refer_text": "A dynamic programming recursion similar to the one in Eq. 2 is evaluated.We apply a beam search concept as in speech recognition.",
            "cite_ID": "N03-1010",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A dynamic programming recursion similar to the one in Eq. 2 is evaluated. We apply a beam search concept as in speech recognition.",
            "GPT_cite_text": "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm) and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000)."
        },
        {
            "Number": 13,
            "refer_ID": "C00-2123",
            "refer_sids": [
                169
            ],
            "refer_text": "For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.",
            "cite_ID": "P01-1027",
            "cite_maker_sids": [
                127
            ],
            "cite_sids": [
                127
            ],
            "cite_text": "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each source word f, the list of its possible translations e is sorted according to p(f|e) puni(e), where puni(e) is the unigram probability of the English word e. It is sufficient to consider only the best 50 words.",
            "GPT_cite_text": "We use the top-10 list of hypotheses provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypotheses using the ME models and sorting them according to the new maximum entropy score."
        },
        {
            "Number": 14,
            "refer_ID": "C00-2123",
            "refer_sids": [
                165
            ],
            "refer_text": "We apply a beam search concept as in speech recognition.",
            "cite_ID": "P03-1039",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "The decoding algorithm employed for this chunk + weight \u00c3\u2014 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We apply a beam search concept as in speech recognition.",
            "GPT_cite_text": "The decoding algorithm employed for this chunk + weight \u00d7 j freq(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm(E) are translation model and language model probability, respectively presented in (Tillmann and Ney, 2000), freq(EA j , J j ) is the frequency for which generates outputs in left-to-right order by consuming input in an arbitrary order."
        },
        {
            "Number": 15,
            "refer_ID": "C00-2123",
            "refer_sids": [
                58
            ],
            "refer_text": "The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either \u00c6 = 0 or \u00c6 = 1 new target words.",
            "cite_ID": "P03-1039",
            "cite_maker_sids": [
                120
            ],
            "cite_sids": [
                120
            ],
            "cite_text": "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).",
            "label": [
                "Implication_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "The type of alignment we have considered so far requires the same length for source and target sentences, i.e., I = J. Evidently, this is an unrealistic assumption; therefore, we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either \u00c6 = 0 or \u00c6 = 1 new target words.",
            "GPT_cite_text": "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000)."
        },
        {
            "Number": 17,
            "refer_ID": "C00-2123",
            "refer_sids": [
                159
            ],
            "refer_text": "This measure has the advantage of being completely automatic.",
            "cite_ID": "W01-1404",
            "cite_maker_sids": [
                5
            ],
            "cite_sids": [
                5
            ],
            "cite_text": "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "This measure has the advantage of being completely automatic.",
            "GPT_cite_text": "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar et al., 1999); others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown et al., 1990) and (Tillmann and Ney, 2000)."
        },
        {
            "Number": 18,
            "refer_ID": "C00-2123",
            "refer_sids": [
                169
            ],
            "refer_text": "For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.",
            "cite_ID": "W01-1407",
            "cite_maker_sids": [
                110
            ],
            "cite_sids": [
                110
            ],
            "cite_text": "We used a translation system called \u00e2\u20ac\u0153single- word based approach\u00e2\u20ac described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each source word f, the list of its possible translations e is sorted according to p(f|e) puni(e), where puni(e) is the unigram probability of the English word e. It is sufficient to consider only the best 50 words.",
            "GPT_cite_text": "We used a translation system called \u201csingle-word based approach\u201d described in (Tillmann and Ney, 2000) and compared it to other approaches in (Ney et al., 2000)."
        },
        {
            "Number": 19,
            "refer_ID": "C00-2123",
            "refer_sids": [
                165
            ],
            "refer_text": "We apply a beam search concept as in speech recognition.",
            "cite_ID": "W01-1408",
            "cite_maker_sids": [
                47
            ],
            "cite_sids": [
                47
            ],
            "cite_text": "Search algorithms We evaluate the following two search algorithms: \u00e2\u20ac\u00a2 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We apply a beam search concept as in speech recognition.",
            "GPT_cite_text": "Search algorithms: We evaluate the following two search algorithms: \u2022 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000). In this algorithm, the search space is explored in a breadth-first manner."
        },
        {
            "Number": 20,
            "refer_ID": "C00-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).",
            "cite_ID": "W02-1020",
            "cite_maker_sids": [
                62
            ],
            "cite_sids": [
                61,
                62
            ],
            "cite_text": "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).",
            "GPT_cite_text": "It is faster because the search problem for noisy-channel models is NP-complete (Knight, 1999), and even the fastest dynamic programming heuristics used in statistical MT (Niessen et al., 1998; Tillmann and Ney, 2000) are polynomial in J \u2014 for instance O(mJ^4V^3) in (Tillmann and Ney, 2000)."
        }
    ],
    "E09-2008": [
        {
            "Number": 1,
            "refer_ID": "E09-2008",
            "refer_sids": [
                5
            ],
            "refer_text": "Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.",
            "cite_ID": "N13-1140",
            "cite_maker_sids": [
                161
            ],
            "cite_sids": [
                161
            ],
            "cite_text": "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata-theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spell-checking applications.",
            "GPT_cite_text": "Recently, open-source tools have been released; in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser."
        },
        {
            "Number": 2,
            "refer_ID": "E09-2008",
            "refer_sids": [
                6
            ],
            "refer_text": "The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T\u00e2\u20ac\u2122s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).",
            "cite_ID": "W11-2605",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                73
            ],
            "cite_text": "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T's fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite-state toolkit (Beesley and Karttunen, 2003), and the SFST toolkit (Schmid, 2005).",
            "GPT_cite_text": "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002), which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a)."
        },
        {
            "Number": 3,
            "refer_ID": "E09-2008",
            "refer_sids": [
                9
            ],
            "refer_text": "Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.",
            "cite_ID": "W12-1003",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Foma is licensed under the GNU General Public License; in keeping with the traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.",
            "GPT_cite_text": "The syllable counter is implemented using the Foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application."
        },
        {
            "Number": 4,
            "refer_ID": "E09-2008",
            "refer_sids": [
                48
            ],
            "refer_text": "Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers.",
            "cite_ID": "W12-6202",
            "cite_maker_sids": [
                124
            ],
            "cite_sids": [
                124
            ],
            "cite_text": "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Though the main concern with Foma has not been that of efficiency, but of compatibility and extensibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers.",
            "GPT_cite_text": "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional), we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional."
        },
        {
            "Number": 5,
            "refer_ID": "E09-2008",
            "refer_sids": [
                9
            ],
            "refer_text": "Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.",
            "cite_ID": "W12-6211",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49
            ],
            "cite_text": "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Foma is licensed under the GNU General Public License; in keeping with the traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.",
            "GPT_cite_text": "Foma (Hulden, 2009) is a freely available toolkit that allows one to both build and parse FS automata and transducers."
        },
        {
            "Number": 6,
            "refer_ID": "E09-2008",
            "refer_sids": [
                3
            ],
            "refer_text": "Foma is largely compatible with the Xerox/PARC finite-state toolkit.",
            "cite_ID": "W12-6212",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Foma is largely compatible with the Xerox/PARC finite-state toolkit.",
            "GPT_cite_text": "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009)."
        },
        {
            "Number": 7,
            "refer_ID": "E09-2008",
            "refer_sids": [
                3
            ],
            "refer_text": "Foma is largely compatible with the Xerox/PARC finite-state toolkit.",
            "cite_ID": "W12-6212",
            "cite_maker_sids": [
                80
            ],
            "cite_sids": [
                80
            ],
            "cite_text": "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Foma is largely compatible with the Xerox/PARC finite-state toolkit.",
            "GPT_cite_text": "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma toolkit (Hulden, 2009)."
        },
        {
            "Number": 8,
            "refer_ID": "E09-2008",
            "refer_sids": [
                15
            ],
            "refer_text": "This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately.",
            "cite_ID": "W12-6213",
            "cite_maker_sids": [
                77
            ],
            "cite_sids": [
                77
            ],
            "cite_text": "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This makes it straightforward to build spell checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately.",
            "GPT_cite_text": "This can then be used in spell checking applications, for example, by integrating the lexicon with weighted transducers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010)."
        }
    ],
    "W09-0621": [
        {
            "Number": 1,
            "refer_ID": "W09-0621",
            "refer_sids": [
                3
            ],
            "refer_text": "We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.",
            "cite_ID": "D12-1016",
            "cite_maker_sids": [
                83
            ],
            "cite_sids": [
                83,
                84
            ],
            "cite_text": "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases: one based on clustering and the other on pairwise similarity-based matching.",
            "GPT_cite_text": "GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009)."
        },
        {
            "Number": 2,
            "refer_ID": "W09-0621",
            "refer_sids": [
                13
            ],
            "refer_text": "News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News.",
            "cite_ID": "D13-1155",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102
            ],
            "cite_text": "Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service3 was used to identify news.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "News article headlines are abundant on the web and are already grouped by news aggregators such as Google News.",
            "GPT_cite_text": "Similarly to previous work (Dolan et al., 2004; Wubben et al., 2009; Bejan & Harabagiu, 2010, inter alia), the Google News service was used to identify news."
        },
        {
            "Number": 3,
            "refer_ID": "W09-0621",
            "refer_sids": [
                3
            ],
            "refer_text": "We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.",
            "cite_ID": "PS15684-W09",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases: one based on clustering and the other on pairwise similarity-based matching.",
            "GPT_cite_text": "Wubben et al. have compared clustering against pairwise matching for extracting paraphrases from news corpora [9]."
        },
        {
            "Number": 4,
            "refer_ID": "W09-0621",
            "refer_sids": [
                10
            ],
            "refer_text": "Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation.",
            "cite_ID": "PS15684-W09",
            "cite_maker_sids": [
                253
            ],
            "cite_sids": [
                253
            ],
            "cite_text": "The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation.",
            "GPT_cite_text": "The obtained results have been compared with those of the Paraphrase Acquisition system developed by Wubben et al. [9]."
        },
        {
            "Number": 5,
            "refer_ID": "W09-0621",
            "refer_sids": [
                40
            ],
            "refer_text": "Our first approach is to use a clustering algorithm to cluster similar headlines.",
            "cite_ID": "PS15684-W09",
            "cite_maker_sids": [
                288
            ],
            "cite_sids": [
                288
            ],
            "cite_text": "Besides Wubben et al.s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our first approach is to use a clustering algorithm to cluster similar headlines.",
            "GPT_cite_text": "Besides Wubben et al.'s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted."
        },
        {
            "Number": 6,
            "refer_ID": "W09-0621",
            "refer_sids": [
                43
            ],
            "refer_text": "The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.",
            "cite_ID": "PS15684-W09",
            "cite_maker_sids": [
                289
            ],
            "cite_sids": [
                289
            ],
            "cite_text": "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.",
            "GPT_cite_text": "Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering."
        },
        {
            "Number": 7,
            "refer_ID": "W09-0621",
            "refer_sids": [
                60
            ],
            "refer_text": "For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.",
            "cite_ID": "PS15684-W09",
            "cite_maker_sids": [
                349
            ],
            "cite_sids": [
                349
            ],
            "cite_text": "However, the proposed system performs better than Wubben et al.s approaches as well as FCM Clustering for Paraphrase Extraction.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.",
            "GPT_cite_text": "However, the proposed system performs better than Wubben et al.'s approaches as well as FCM Clustering for Paraphrase Extraction."
        },
        {
            "Number": 8,
            "refer_ID": "W09-0621",
            "refer_sids": [
                81
            ],
            "refer_text": "It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.",
            "cite_ID": "PS15684-W09",
            "cite_maker_sids": [
                362
            ],
            "cite_sids": [
                362
            ],
            "cite_text": "The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.",
            "GPT_cite_text": "The proposed system, the existing systems (Wubben et al.), and the FCM clustering approach were tested on this dataset."
        },
        {
            "Number": 9,
            "refer_ID": "W09-0621",
            "refer_sids": [
                68
            ],
            "refer_text": "With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos() = V 1  V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared.",
            "cite_ID": "PS15684-W09",
            "cite_maker_sids": [
                370
            ],
            "cite_sids": [
                368,
                369,
                370
            ],
            "cite_text": "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With this in mind, we adopt two thresholds and the cosine similarity function to calculate the similarity between two sentences: cos() = V1 \u00b7 V2 / (|V1| |V2|) where V1 and V2 are the vectors of the two sentences being compared.",
            "GPT_cite_text": "Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset."
        },
        {
            "Number": 10,
            "refer_ID": "W09-0621",
            "refer_sids": [
                57
            ],
            "refer_text": "We use an F -score with a  of 0.25 as we favour precision over recall.",
            "cite_ID": "W10-4223",
            "cite_maker_sids": [
                33
            ],
            "cite_sids": [
                33,
                34
            ],
            "cite_text": "This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use an F-score with a value of 0.25 as we favor precision over recall.",
            "GPT_cite_text": "This method, described in earlier work by Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus."
        },
        {
            "Number": 11,
            "refer_ID": "W09-0621",
            "refer_sids": [
                1
            ],
            "refer_text": "For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.",
            "cite_ID": "w11-1604",
            "cite_maker_sids": [
                34
            ],
            "cite_sids": [
                34
            ],
            "cite_text": "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.",
            "GPT_cite_text": "So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap, have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009). The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles."
        },
        {
            "Number": 12,
            "refer_ID": "W09-0621",
            "refer_sids": [
                46
            ],
            "refer_text": "We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.",
            "cite_ID": "w11-1604",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                73
            ],
            "cite_text": "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.",
            "GPT_cite_text": "To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8) Ni NISTi) tf.idf scores over headline clusters and cosine similarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines."
        }
    ],
    "C10-1045": [
        {
            "Number": 1,
            "refer_ID": "C10-1045",
            "refer_sids": [
                270
            ],
            "refer_text": "6 Joint Segmentation and Parsing.",
            "cite_ID": "D12-1046",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                39
            ],
            "cite_text": "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "6 Joint Segmentation and Parsing.",
            "GPT_cite_text": "Joint segmentation and parsing were also investigated for Arabic (Green and Manning, 2010)."
        },
        {
            "Number": 2,
            "refer_ID": "C10-1045",
            "refer_sids": [
                270
            ],
            "refer_text": "6 Joint Segmentation and Parsing.",
            "cite_ID": "J13-1007",
            "cite_maker_sids": [
                72
            ],
            "cite_sids": [
                72
            ],
            "cite_text": "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "6 Joint Segmentation and Parsing.",
            "GPT_cite_text": "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010)."
        },
        {
            "Number": 3,
            "refer_ID": "C10-1045",
            "refer_sids": [
                275
            ],
            "refer_text": "But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.",
            "cite_ID": "J13-1007",
            "cite_maker_sids": [
                148
            ],
            "cite_sids": [
                148,
                149
            ],
            "cite_text": "One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "But goal segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.",
            "GPT_cite_text": "One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010)."
        },
        {
            "Number": 4,
            "refer_ID": "C10-1045",
            "refer_sids": [
                270
            ],
            "refer_text": "6 Joint Segmentation and Parsing.",
            "cite_ID": "J13-1007",
            "cite_maker_sids": [
                479
            ],
            "cite_sids": [
                479,
                480
            ],
            "cite_text": "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima\u00e2\u20ac\u2122an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).It is also used for Arabic (Green and Manning 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "6 Joint Segmentation and Parsing.",
            "GPT_cite_text": "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima'an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010)."
        },
        {
            "Number": 5,
            "refer_ID": "C10-1045",
            "refer_sids": [
                6
            ],
            "refer_text": "Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u00e2\u20ac\u201c5% F1.",
            "cite_ID": "J13-1007",
            "cite_maker_sids": [
                511
            ],
            "cite_sids": [
                510,
                511
            ],
            "cite_text": "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Sima\u00e2\u20ac\u2122an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2\u20135% F1.",
            "GPT_cite_text": "Lattice parsing was explored in the context of parsing speech signals by Chappelier et al. (1999), Sima'an (1999), and Hall (2005), and in the context of joint word segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010)."
        },
        {
            "Number": 6,
            "refer_ID": "C10-1045",
            "refer_sids": [
                24
            ],
            "refer_text": "Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00c2\u00a76).",
            "cite_ID": "J13-1007",
            "cite_maker_sids": [
                670
            ],
            "cite_sids": [
                670
            ],
            "cite_text": "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Finally, we provide a realistic evaluation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).",
            "GPT_cite_text": "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford parser and 82% F1 using the PCFG-LA Berkeley Parser, both when assuming gold word segmentation."
        },
        {
            "Number": 7,
            "refer_ID": "C10-1045",
            "refer_sids": [
                24,
                289,
                308
            ],
            "refer_text": "Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation).Table 9: Dev set results for sentences of length \u2264 70.",
            "cite_ID": "J13-1007",
            "cite_maker_sids": [
                672
            ],
            "cite_sids": [
                672
            ],
            "cite_text": "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Finally, we provide a realistic evaluation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76). Parent Head Modifier Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJP R 803 0.64 FRAG 254 72.87 NP NP NP R 2907 0.66 VP 5507 78.83 NP NP S R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG S R R 772 0.86 WHN P 787 96.00 S VP NP L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). Table 9: Dev set results for sentences of length \u2264 70.",
            "GPT_cite_text": "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word segmenter are applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010)."
        },
        {
            "Number": 8,
            "refer_ID": "C10-1045",
            "refer_sids": [
                11
            ],
            "refer_text": "To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.",
            "cite_ID": "J13-1008",
            "cite_maker_sids": [
                195
            ],
            "cite_sids": [
                195
            ],
            "cite_text": "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.",
            "GPT_cite_text": "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)."
        },
        {
            "Number": 9,
            "refer_ID": "C10-1045",
            "refer_sids": [
                21,
                22,
                53,
                141
            ],
            "refer_text": "Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73).We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74).When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct \ufffd ?f iDafa.mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences.",
            "cite_ID": "J13-1008",
            "cite_maker_sids": [
                196
            ],
            "cite_sids": [
                196
            ],
            "cite_text": "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.",
            "label": [
                "Results_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "Next, we show that the ATB is similar to other treebanks in gross statistical terms, but that annotation consistency remains low relative to English (\u00a73). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (\u00a74). When the maSdar lacks a determiner, the constituent as a whole resembles the ubiquitous annexation construct of iDafa. mark-ContainsVerb is especially effective for distinguishing root S nodes of equational sentences.",
            "GPT_cite_text": "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency and introduced an enhanced split-state constituency grammar, including labels for short idiom constructions and verbal or equational clauses."
        },
        {
            "Number": 10,
            "refer_ID": "C10-1045",
            "refer_sids": [
                64
            ],
            "refer_text": "We propose a limit of 70 words for Arabic parsing evaluations.",
            "cite_ID": "J13-1008",
            "cite_maker_sids": [
                665
            ],
            "cite_sids": [
                665
            ],
            "cite_text": "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We propose a limit of 70 words for Arabic parsing evaluations.",
            "GPT_cite_text": "For better comparison with the work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences of up to 70 tokens long."
        },
        {
            "Number": 11,
            "refer_ID": "C10-1045",
            "refer_sids": [
                116,
                117
            ],
            "refer_text": "In our grammar, features are realized as annotations to basic category labels.We start with noun features since written Arabic contains a very high proportion of NPs.",
            "cite_ID": "J13-1009",
            "cite_maker_sids": [
                190
            ],
            "cite_sids": [
                190
            ],
            "cite_text": "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In our grammar, features are realized as annotations to basic category labels. We start with noun features since written Arabic contains a very high proportion of NPs.",
            "GPT_cite_text": "The Arabic grammar features come from Green and Manning (2010), which contain an ablation study similar to Table 2."
        },
        {
            "Number": 12,
            "refer_ID": "C10-1045",
            "refer_sids": [
                128
            ],
            "refer_text": "8 We use head-finding rules specified by a native speaker.",
            "cite_ID": "J13-1009",
            "cite_maker_sids": [
                193
            ],
            "cite_sids": [
                193
            ],
            "cite_text": "For Arabic, we use the head-finding rules from Green and Manning (2010).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We use head-finding rules specified by a native speaker.",
            "GPT_cite_text": "For Arabic, we use the head-finding rules from Green and Manning (2010)."
        },
        {
            "Number": 13,
            "refer_ID": "C10-1045",
            "refer_sids": [
                312
            ],
            "refer_text": "By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.",
            "cite_ID": "J13-1009",
            "cite_maker_sids": [
                316
            ],
            "cite_sids": [
                316
            ],
            "cite_text": "We previously showed that the \u00e2\u20ac\u0153Kulick\u00e2\u20ac\u009d tag set is very effective for basic Arabic parsing (Green and Manning 2010).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.",
            "GPT_cite_text": "We previously showed that the \u201cKulick\u201d tag set is very effective for basic Arabic parsing (Green and Manning 2010)."
        },
        {
            "Number": 14,
            "refer_ID": "C10-1045",
            "refer_sids": [
                301
            ],
            "refer_text": "Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
            "cite_ID": "J13-1009",
            "cite_maker_sids": [
                397
            ],
            "cite_sids": [
                397
            ],
            "cite_text": "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Table 9 shows that MADA produces a high-quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
            "GPT_cite_text": "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning, 2010)."
        },
        {
            "Number": 15,
            "refer_ID": "C10-1045",
            "refer_sids": [
                11
            ],
            "refer_text": "To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.",
            "cite_ID": "J13-1009",
            "cite_maker_sids": [
                406
            ],
            "cite_sids": [
                406
            ],
            "cite_text": "We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.",
            "GPT_cite_text": "We previously showed optimal Berkeley parser (Petrov et al. 2006) parameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets."
        },
        {
            "Number": 17,
            "refer_ID": "C10-1045",
            "refer_sids": [
                11
            ],
            "refer_text": "To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.",
            "cite_ID": "P11-1159",
            "cite_maker_sids": [
                109
            ],
            "cite_sids": [
                109
            ],
            "cite_text": "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.",
            "GPT_cite_text": "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency."
        },
        {
            "Number": 18,
            "refer_ID": "C10-1045",
            "refer_sids": [
                11
            ],
            "refer_text": "To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.",
            "cite_ID": "P11-2037",
            "cite_maker_sids": [
                34
            ],
            "cite_sids": [
                34
            ],
            "cite_text": "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply \u201cArabic\u201d) because of the unusual opportunity it presents for comparison to English parsing results.",
            "GPT_cite_text": "We allow the parser to produce empty elements by means of lattice parsing (Chappelier et al., 1999), a general processing technique (Hall, 2005; Chappelier et al., 1999), and it was recently applied to the task of joint clitic segmentation and syntactic parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010)."
        },
        {
            "Number": 19,
            "refer_ID": "C10-1045",
            "refer_sids": [
                72
            ],
            "refer_text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
            "cite_ID": "P11-2122",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
            "GPT_cite_text": "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)."
        },
        {
            "Number": 20,
            "refer_ID": "C10-1045",
            "refer_sids": [
                21
            ],
            "refer_text": "Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency",
            "cite_ID": "P11-2122",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                73
            ],
            "cite_text": "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Next we show that the ATB is similar to other treebanks in gross statistical terms, but that annotation consistency",
            "GPT_cite_text": "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)."
        },
        {
            "Number": 21,
            "refer_ID": "C10-1045",
            "refer_sids": [
                72
            ],
            "refer_text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
            "cite_ID": "P11-2122",
            "cite_maker_sids": [
                109
            ],
            "cite_sids": [
                109
            ],
            "cite_text": "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u00e2\u20ac\u0153gold\u00e2\u20ac\u009d errors.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).",
            "GPT_cite_text": "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as \u201cgold\u201d errors."
        },
        {
            "Number": 22,
            "refer_ID": "C10-1045",
            "refer_sids": [
                277,
                279
            ],
            "refer_text": "Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.",
            "cite_ID": "P11-2124",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18
            ],
            "cite_text": "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.",
            "GPT_cite_text": "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice parsing for parsing Arabic."
        },
        {
            "Number": 23,
            "refer_ID": "C10-1045",
            "refer_sids": [
                0
            ],
            "refer_text": "Better Arabic Parsing: Baselines, Evaluations, and Analysis",
            "cite_ID": "P12-1016",
            "cite_maker_sids": [
                196
            ],
            "cite_sids": [
                196
            ],
            "cite_text": "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Better Arabic Parsing: Baselines, Evaluations, and Analysis",
            "GPT_cite_text": "The data were pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010)."
        },
        {
            "Number": 24,
            "refer_ID": "C10-1045",
            "refer_sids": [
                24
            ],
            "refer_text": "Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).",
            "cite_ID": "P12-2002",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Finally, we provide a realistic evaluation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).",
            "GPT_cite_text": "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010)."
        },
        {
            "Number": 25,
            "refer_ID": "C10-1045",
            "refer_sids": [
                0
            ],
            "refer_text": "Better Arabic Parsing: Baselines, Evaluations, and Analysis",
            "cite_ID": "P12-2002",
            "cite_maker_sids": [
                34
            ],
            "cite_sids": [
                34,
                35
            ],
            "cite_text": "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).Examples for similar phenomena in Arabic may be found in Green and Manning (2010).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Better Arabic Parsing: Baselines, Evaluations, and Analysis",
            "GPT_cite_text": "The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010)."
        },
        {
            "Number": 26,
            "refer_ID": "C10-1045",
            "refer_sids": [
                24,
                277,
                279
            ],
            "refer_text": "Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).",
            "cite_ID": "P12-2002",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                38,
                39
            ],
            "cite_text": "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. Finally, we provide a realistic evaluation in which segmentation is performed both in a pipeline and jointly with parsing (\u00a76).",
            "GPT_cite_text": "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-Haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010)."
        },
        {
            "Number": 27,
            "refer_ID": "C10-1045",
            "refer_sids": [
                153
            ],
            "refer_text": "Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text.",
            "cite_ID": "W13-4904",
            "cite_maker_sids": [
                115
            ],
            "cite_sids": [
                115
            ],
            "cite_text": "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Preprocessing the raw trees improves parsing performance considerably. We first discard all trees dominated by X, which indicates errors and non-linguistic text.",
            "GPT_cite_text": "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted."
        },
        {
            "Number": 28,
            "refer_ID": "C10-1045",
            "refer_sids": [
                301
            ],
            "refer_text": "Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
            "cite_ID": "W13-4904",
            "cite_maker_sids": [
                234
            ],
            "cite_sids": [
                234
            ],
            "cite_text": "Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Table 9 shows that MADA produces a high-quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
            "GPT_cite_text": "Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs. 79.17 F1)."
        },
        {
            "Number": 29,
            "refer_ID": "C10-1045",
            "refer_sids": [
                301
            ],
            "refer_text": "Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
            "cite_ID": "W13-4904",
            "cite_maker_sids": [
                244
            ],
            "cite_sids": [
                244
            ],
            "cite_text": "Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Table 9 shows that MADA produces a high-quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.",
            "GPT_cite_text": "Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F-score drop in their constituent parsing work."
        },
        {
            "Number": 31,
            "refer_ID": "C10-1045",
            "refer_sids": [
                166,
                167
            ],
            "refer_text": "The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-.able at http://nlp.stanford.edu/projects/arabic.shtml.",
            "cite_ID": "W13-4917",
            "cite_maker_sids": [
                260
            ],
            "cite_sids": [
                260
            ],
            "cite_text": "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The intuition here is that the role of a discourse marker can usually be defined. Both the corpus split and pre-processing code are available at http://nlp.stanford.edu/projects/arabic.shtml.",
            "GPT_cite_text": "The Stanford Arabic Phrase Structure Treebank: In order to stay compatible with the state of the art, we provide the constituency dataset with most of the pre-processing steps of Green and Manning (2010)."
        },
        {
            "Number": 32,
            "refer_ID": "C10-1045",
            "refer_sids": [
                154
            ],
            "refer_text": "At the phrasal level, we remove all function tags and traces.",
            "cite_ID": "W13-4917",
            "cite_maker_sids": [
                262
            ],
            "cite_sids": [
                262
            ],
            "cite_text": "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "At the phrasal level, we remove all function tags and traces.",
            "GPT_cite_text": "We finally remove all traces, but unlike Green and Manning (2010), we keep all function tags."
        },
        {
            "Number": 33,
            "refer_ID": "C10-1045",
            "refer_sids": [
                12
            ],
            "refer_text": "The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).",
            "cite_ID": "W13-4917",
            "cite_maker_sids": [
                218
            ],
            "cite_sids": [
                218
            ],
            "cite_text": "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).",
            "GPT_cite_text": "Data Sets: The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004): the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)."
        }
    ],
    "P98-1046": [
        {
            "Number": 1,
            "refer_ID": "P98-1046",
            "refer_sids": [
                47
            ],
            "refer_text": "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.",
            "cite_ID": "A00-2034",
            "cite_maker_sids": [
                36,
                37
            ],
            "cite_sids": [
                36,
                37
            ],
            "cite_text": "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping together subsets of existing classes with overlapping members.",
            "GPT_cite_text": "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998)."
        },
        {
            "Number": 2,
            "refer_ID": "P98-1046",
            "refer_sids": [
                47
            ],
            "refer_text": "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.",
            "cite_ID": "A00-2034",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                38,
                39
            ],
            "cite_text": "Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping together subsets of existing classes with overlapping members.",
            "GPT_cite_text": "Dang et al. (1998) modified it by adding new classes that remove the overlap between classes from the original scheme."
        },
        {
            "Number": 3,
            "refer_ID": "P98-1046",
            "refer_sids": [
                47
            ],
            "refer_text": "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.",
            "cite_ID": "C00-2118",
            "cite_maker_sids": [
                211
            ],
            "cite_sids": [
                211
            ],
            "cite_text": "\\tVe think that many cases of am\u00c2\u00ad biguous classification of verb types can be ad\u00c2\u00ad dressed with the notion of intersedive sets in\u00c2\u00ad troduced by (Dang ct a!., 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping together subsets of existing classes with overlapping members.",
            "GPT_cite_text": "We think that many cases of ambiguous classification of verb types can be addressed with the notion of intersective sets introduced by (Dang et al., 1998)."
        },
        {
            "Number": 4,
            "refer_ID": "P98-1046",
            "refer_sids": [
                4,
                47
            ],
            "refer_text": "We present a refinement of Levin classes, intersec\u00c2\u00ad tive sets, which are a more fine-grained clas\u00c2\u00ad sification and have more coherent sets of syn\u00c2\u00ad tactic frames and associated semantic compo\u00c2\u00ad nents.We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.",
            "cite_ID": "C00-2148",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18,
                19,
                20
            ],
            "cite_text": "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coher\u00c2\u00ad ent refinement of basic Levin classes.",
            "label": [
                "Results_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping together subsets of existing classes with overlapping members.",
            "GPT_cite_text": "Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang et al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intersective Levin classes, which are a more fine-grained, syntactically and semantically coherent refinement of basic Levin classes."
        },
        {
            "Number": 5,
            "refer_ID": "P98-1046",
            "refer_sids": [
                4
            ],
            "refer_text": "We present a refinement of Levin classes, intersec\u00c2\u00ad tive sets, which are a more fine-grained clas\u00c2\u00ad sification and have more coherent sets of syn\u00c2\u00ad tactic frames and associated semantic compo\u00c2\u00ad nents.",
            "cite_ID": "C08-1002",
            "cite_maker_sids": [
                46
            ],
            "cite_sids": [
                46
            ],
            "cite_text": "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.",
            "GPT_cite_text": "VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically."
        },
        {
            "Number": 6,
            "refer_ID": "P98-1046",
            "refer_sids": [
                40
            ],
            "refer_text": "However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components.",
            "cite_ID": "H01-1010",
            "cite_maker_sids": [
                25
            ],
            "cite_sids": [
                25
            ],
            "cite_text": "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes, [Dang, et al, 98, 00, Kipper, et al. 00].",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components.",
            "GPT_cite_text": "This lexicon exploits the systematic link between syntax and semantics that motivates the Levin classes, and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes [Dang et al., 98, 00; Kipper et al., 00]."
        },
        {
            "Number": 7,
            "refer_ID": "P98-1046",
            "refer_sids": [
                38,
                47
            ],
            "refer_text": "2.1 Ambiguities in Levin classes.We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.",
            "cite_ID": "J01-3003",
            "cite_maker_sids": [
                660,
                661
            ],
            "cite_sids": [
                660,
                661
            ],
            "cite_text": "We think that many cases of ambigu\u00c2\u00ad ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al.(1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "2.1 Ambiguities in Levin classes. We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping together subsets of existing classes with overlapping members.",
            "GPT_cite_text": "We think that many cases of ambiguous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al. (1998)."
        },
        {
            "Number": 8,
            "refer_ID": "P98-1046",
            "refer_sids": [
                40
            ],
            "refer_text": "However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\u00ad tic components.",
            "cite_ID": "J04-1003",
            "cite_maker_sids": [
                59
            ],
            "cite_sids": [
                58,
                59
            ],
            "cite_text": "Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components.",
            "GPT_cite_text": "Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs."
        },
        {
            "Number": 9,
            "refer_ID": "P98-1046",
            "refer_sids": [
                65
            ],
            "refer_text": "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.",
            "cite_ID": "J05-1004",
            "cite_maker_sids": [
                68
            ],
            "cite_sids": [
                68
            ],
            "cite_text": "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin\u00e2\u20ac\u2122s original classes, adding an additional level to the hierarchy (Dang et al. 1998).",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components and could be divided into smaller subclasses.",
            "GPT_cite_text": "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin's original classes, adding an additional level to the hierarchy (Dang et al. 1998)."
        },
        {
            "Number": 10,
            "refer_ID": "P98-1046",
            "refer_sids": [
                70,
                94
            ],
            "refer_text": "The ad\u00ad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more \"pure\" examples of the carry class and always imply the achievement of causation of motion.",
            "cite_ID": "N09-1057",
            "cite_maker_sids": [
                30
            ],
            "cite_sids": [
                30,
                31
            ],
            "cite_text": "participants \u00e2\u20ac\u201d causation, change of state, and others \u00e2\u20ac\u201d are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The adjunct of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise. The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more \"pure\" examples of the carry class and always imply the achievement of causation of motion.",
            "GPT_cite_text": "Participants \u2014 causation, change of state, and others \u2014 are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon as well (e.g., Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)."
        },
        {
            "Number": 11,
            "refer_ID": "P98-1046",
            "refer_sids": [
                28
            ],
            "refer_text": "The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.",
            "cite_ID": "P03-1009",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                5,
                6
            ],
            "cite_text": "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "The fundamental assumption is that the syntactic frames are a direct reflection of the underlying semantics.",
            "GPT_cite_text": "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted considerable research interest in both linguistics and computational linguistics (e.g., Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)."
        },
        {
            "Number": 12,
            "refer_ID": "P98-1046",
            "refer_sids": [
                47
            ],
            "refer_text": "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\u00ad gether subsets of existing classes with over\u00ad lapping members.",
            "cite_ID": "P06-1117",
            "cite_maker_sids": [
                78
            ],
            "cite_sids": [
                78
            ],
            "cite_text": "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin\u00e2\u20ac\u2122s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping together subsets of existing classes with overlapping members.",
            "GPT_cite_text": "This constraint of having the same semantic roles is further ensured inside the VN lexicon, which is constructed based on a more refined version of Levin's classification, called Intersective Levin classes (ILCs) (Dang et al., 1998)."
        },
        {
            "Number": 13,
            "refer_ID": "P98-1046",
            "refer_sids": [
                164
            ],
            "refer_text": "First, since our experi\u00ad ment was based on a translation from English to Portuguese",
            "cite_ID": "P99-1051",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "Levin's study on diathesis alternations has influenced recent work on word sense disam\u00c2\u00ad biguation (Dorr and Jones, 1996), machine transla\u00c2\u00ad tion (Dang et al., 1998), and automatic lexical ac\u00c2\u00ad quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "First, since our experiment was based on a translation from English to Portuguese.",
            "GPT_cite_text": "Levin's study on diathesis alternations has influenced recent work on word sense disambiguation (Dorr and Jones, 1996), machine translation (Dang et al., 1998), and automatic lexical acquisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998)."
        },
        {
            "Number": 14,
            "refer_ID": "P98-1046",
            "refer_sids": [
                158
            ],
            "refer_text": "Intersective Levin sets partition these classes according to more co\u00c2\u00ad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.",
            "cite_ID": "W00-0202",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18
            ],
            "cite_text": "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Intersective Levin sets partition these classes according to more coherent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.",
            "GPT_cite_text": "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associated with each other under a common parent that captures the properties these verbs all share (Dang et al., 1998)."
        },
        {
            "Number": 16,
            "refer_ID": "P98-1046",
            "refer_sids": [
                65
            ],
            "refer_text": "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.",
            "cite_ID": "W02-1108",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                48,
                49
            ],
            "cite_text": "Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components and could be divided into smaller subclasses.",
            "GPT_cite_text": "Dang et al. (1998), for example, have refined the current classification by creating intersective classes for those verbs that share membership in more than one Levin class."
        },
        {
            "Number": 17,
            "refer_ID": "P98-1046",
            "refer_sids": [
                4
            ],
            "refer_text": "We present a refinement of Levin classes, intersec\u00c2\u00ad tive sets, which are a more fine-grained clas\u00c2\u00ad sification and have more coherent sets of syn\u00c2\u00ad tactic frames and associated semantic compo\u00c2\u00ad nents.",
            "cite_ID": "W03-0910",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.",
            "GPT_cite_text": "This reorganization, which was facilitated by the use of intersective Levin classes (Dang et al. 1998), refined the classes to account for semantic and syntactic differences within a class."
        },
        {
            "Number": 18,
            "refer_ID": "P98-1046",
            "refer_sids": [
                28
            ],
            "refer_text": "The fundamental assumption is that the syntactic frames are a direct reflection of the un\u00ad derlying semantics.",
            "cite_ID": "W04-2606",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7,
                8
            ],
            "cite_text": "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "The fundamental assumption is that the syntactic frames are a direct reflection of the underlying semantics.",
            "GPT_cite_text": "Lexical-semantic classes that aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g., Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)."
        },
        {
            "Number": 19,
            "refer_ID": "P98-1046",
            "refer_sids": [
                65
            ],
            "refer_text": "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.",
            "cite_ID": "W06-2611",
            "cite_maker_sids": [
                83
            ],
            "cite_sids": [
                83
            ],
            "cite_text": "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components and could be divided into smaller subclasses.",
            "GPT_cite_text": "This constraint of having the same semantic roles is further ensured inside the VerbNet lexicon that is constructed based on a more refined version of the Levin classification called Intersective Levin classes (Dang et al., 1998)."
        },
        {
            "Number": 20,
            "refer_ID": "P98-1046",
            "refer_sids": [
                3
            ],
            "refer_text": "Current approaches to English classifica\u00ad tion, Levin classes and WordNet, have limita\u00ad tions in their applicability that impede their utility as general classification schemes.",
            "cite_ID": "W99-0503",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                6,
                7
            ],
            "cite_text": "In explormg these quest1ons, we focus on verb clas\u00c2\u00ad Sificatwn for several reasons Verbs are very Impor\u00c2\u00ad tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap\u00c2\u00ad pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Current approaches to English classification, Levin classes and WordNet, have limitations in their applicability that impede their utility as general classification schemes.",
            "GPT_cite_text": "In exploring these questions, we focus on verb classification for several reasons. Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major role in the organization and use of this knowledge. Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dorr, 1997) and document classification (Klavans and Kan, 1998); yet manual classification of large numbers of verbs is a difficult and resource intensive task (Levy, 1993; Miller et al., 1990; Dang et al., 1998)."
        },
        {
            "Number": 21,
            "refer_ID": "P98-1046",
            "refer_sids": [
                65
            ],
            "refer_text": "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.",
            "cite_ID": "W99-0632",
            "cite_maker_sids": [
                46
            ],
            "cite_sids": [
                45,
                46
            ],
            "cite_text": "Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can sim\u00c2\u00ad plify the definition of different verb senses.",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components and could be divided into smaller subclasses.",
            "GPT_cite_text": "Palmer (1999) and Dang et al. (1998) argue that the use of syntactic frames and verb classes can simplify the definition of different verb senses."
        },
        {
            "Number": 22,
            "refer_ID": "P98-1046",
            "refer_sids": [
                20
            ],
            "refer_text": "Two current approaches to English verb classi\u00c2\u00ad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).",
            "cite_ID": "W99-0632",
            "cite_maker_sids": [
                203
            ],
            "cite_sids": [
                203
            ],
            "cite_text": "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Two current approaches to English verb classifications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).",
            "GPT_cite_text": "We also plan to experiment with different classification schemes for verb semantics, such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998)."
        }
    ],
    "C02-1025": [
        {
            "Number": 1,
            "refer_ID": "C02-1025",
            "refer_sids": [
                63
            ],
            "refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "C10-2104",
            "cite_maker_sids": [
                115
            ],
            "cite_sids": [
                115
            ],
            "cite_text": "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "GPT_cite_text": "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help."
        },
        {
            "Number": 2,
            "refer_ID": "C02-1025",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.",
            "cite_ID": "C10-2167",
            "cite_maker_sids": [
                65
            ],
            "cite_sids": [
                65
            ],
            "cite_text": "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data.",
            "GPT_cite_text": "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (MEM) (Chieu et al., 2002), and Conditional Random Fields (CRF) (Lafferty et al., 2001)."
        },
        {
            "Number": 3,
            "refer_ID": "C02-1025",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.",
            "cite_ID": "I05-3013",
            "cite_maker_sids": [
                88
            ],
            "cite_sids": [
                88
            ],
            "cite_text": "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data.",
            "GPT_cite_text": "The latter is currently dominating in NER, amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). From the linguistic perspective, NIL expressions are rather different from named entities in nature."
        },
        {
            "Number": 4,
            "refer_ID": "C02-1025",
            "refer_sids": [
                196
            ],
            "refer_text": "We have shown that the maximum entropy framework is able to use global information directly.",
            "cite_ID": "I05-3030",
            "cite_maker_sids": [
                33
            ],
            "cite_sids": [
                33
            ],
            "cite_text": "model\u00e2\u20ac\u2122s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We have shown that the maximum entropy framework is able to use global information directly.",
            "GPT_cite_text": "The model's conditional probability is defined as another model, which is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002)."
        },
        {
            "Number": 6,
            "refer_ID": "C02-1025",
            "refer_sids": [
                86
            ],
            "refer_text": "Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.",
            "cite_ID": "P02-1061",
            "cite_maker_sids": [
                51
            ],
            "cite_sids": [
                51
            ],
            "cite_text": "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Case and Zone: Similarly, if (or) is initCaps, a feature (initCaps, zone) (or (initCaps, zone)) is set to 1, etc. Token Information: This group consists of 10 features based on the string, as listed in Table 1.",
            "GPT_cite_text": "More details of this mixed-case NER and its performance are given in (Chieu and Ng, 2002)."
        },
        {
            "Number": 7,
            "refer_ID": "C02-1025",
            "refer_sids": [
                6
            ],
            "refer_text": "A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.",
            "cite_ID": "P03-1028",
            "cite_maker_sids": [
                161
            ],
            "cite_sids": [
                161
            ],
            "cite_text": "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.",
            "GPT_cite_text": "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002)."
        },
        {
            "Number": 10,
            "refer_ID": "C02-1025",
            "refer_sids": [
                2,
                3
            ],
            "refer_text": "It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.",
            "cite_ID": "P03-1028",
            "cite_maker_sids": [
                52
            ],
            "cite_sids": [
                52
            ],
            "cite_text": "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence-based classifier.",
            "GPT_cite_text": "Soderland (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only."
        },
        {
            "Number": 12,
            "refer_ID": "C02-1025",
            "refer_sids": [
                63
            ],
            "refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "P05-1045",
            "cite_maker_sids": [
                174
            ],
            "cite_sids": [
                174
            ],
            "cite_text": "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "GPT_cite_text": "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document."
        },
        {
            "Number": 13,
            "refer_ID": "C02-1025",
            "refer_sids": [
                63
            ],
            "refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "P05-1051",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "GPT_cite_text": "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token. Recently, in (Ji and Grishman, 2004) we proposed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names."
        },
        {
            "Number": 15,
            "refer_ID": "C02-1025",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "Such global features enhance the performance of NER (Chieu and Ng, 2002b).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data.",
            "GPT_cite_text": "Such global features enhance the performance of NER (Chieu and Ng, 2002)."
        },
        {
            "Number": 16,
            "refer_ID": "C02-1025",
            "refer_sids": [
                102,
                103,
                104
            ],
            "refer_text": "For all lists except locations, the lists are processed into a list of tokens (unigrams).Location list is processed into a list of unigrams and bigrams (e.g., New York).For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For all lists except locations, the lists are processed into a list of tokens (unigrams). The location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.",
            "GPT_cite_text": "Useful unigrams (UNI) for each name class, words that precede the name class are ranked using a correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list."
        },
        {
            "Number": 17,
            "refer_ID": "C02-1025",
            "refer_sids": [
                61,
                62,
                63
            ],
            "refer_text": "The features we used can be divided into 2 classes: local and global.Local features are features that are based on neighboring tokens, as well as the token itself.Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                44
            ],
            "cite_sids": [
                44
            ],
            "cite_text": "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document.",
            "GPT_cite_text": "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002)."
        },
        {
            "Number": 18,
            "refer_ID": "C02-1025",
            "refer_sids": [
                86
            ],
            "refer_text": "Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                62
            ],
            "cite_sids": [
                62
            ],
            "cite_text": "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Case and Zone: Similarly, if (or) is initCaps, a feature (initCaps, zone) (or (initCaps, zone)) is set to 1, etc. Token Information: This group consists of 10 features based on the string, as listed in Table 1.",
            "GPT_cite_text": "Token Information: These features are based on the string w, such as contains-digits, contains-dollar-sign, etc. (Chieu and Ng, 2002b)."
        },
        {
            "Number": 19,
            "refer_ID": "C02-1025",
            "refer_sids": [
                61,
                62,
                63
            ],
            "refer_text": "The features we used can be divided into 2 classes: local and global.Local features are features that are based on neighboring tokens, as well as the token itself.Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                46
            ],
            "cite_sids": [
                46
            ],
            "cite_text": "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document.",
            "GPT_cite_text": "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002)."
        },
        {
            "Number": 20,
            "refer_ID": "C02-1025",
            "refer_sids": [
                136,
                137
            ],
            "refer_text": "The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.",
            "cite_ID": "W03-0432",
            "cite_maker_sids": [
                44
            ],
            "cite_sids": [
                44
            ],
            "cite_text": "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u00e2\u20ac\u0153weaker\u00e2\u20ac\u009d classifier that did not use case information at all (Chieu and Ng, 2002b).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non-first words in the TXT or TEXT zones) in the same document is initCaps or not initCaps. For a word whose initCaps might be due to its position rather than its meaning (in headlines, the first word of a sentence, etc.), the case information of other occurrences might be more accurate than its own.",
            "GPT_cite_text": "Chieu and Ng used global information such as the occurrence of the same word with other capitalizations in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u201cweaker\u201d classifier that did not use case information at all (Chieu and Ng, 2002b)."
        },
        {
            "Number": 21,
            "refer_ID": "C02-1025",
            "refer_sids": [
                11
            ],
            "refer_text": "We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).",
            "cite_ID": "W04-0705",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).",
            "GPT_cite_text": "Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)"
        },
        {
            "Number": 22,
            "refer_ID": "C02-1025",
            "refer_sids": [
                63
            ],
            "refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "W04-0705",
            "cite_maker_sids": [
                147
            ],
            "cite_sids": [
                147
            ],
            "cite_text": "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "GPT_cite_text": "Other systems have made a second tagging pass that uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002)."
        },
        {
            "Number": 23,
            "refer_ID": "C02-1025",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.",
            "cite_ID": "W06-0119",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data.",
            "GPT_cite_text": "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction."
        }
    ],
    "P06-2124": [
        {
            "Number": 2,
            "refer_ID": "P06-2124",
            "refer_sids": [
                15
            ],
            "refer_text": "In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.",
            "cite_ID": "D11-1084",
            "cite_maker_sids": [
                50
            ],
            "cite_sids": [
                48,
                49,
                50,
                51
            ],
            "cite_text": "There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document pairs.",
            "GPT_cite_text": "There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009)."
        },
        {
            "Number": 3,
            "refer_ID": "P06-2124",
            "refer_sids": [
                2,
                15
            ],
            "refer_text": "Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.",
            "cite_ID": "D11-1084",
            "cite_maker_sids": [
                53
            ],
            "cite_sids": [
                52,
                53,
                54
            ],
            "cite_text": "Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.",
            "label": [
                "Hypothesis_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "Under this formalism, the parallel sentence pairs within a document pair are assumed to constitute a mixture of hidden topics; each word pair follows a topic-specific bilingual translation model. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document pairs.",
            "GPT_cite_text": "Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT."
        },
        {
            "Number": 5,
            "refer_ID": "P06-2124",
            "refer_sids": [
                21,
                37,
                152
            ],
            "refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.The translation lexicon p(f |e) is the key component in this generative process.Topic-specific translation lexicons are learned by a 3-topic BiTAM1.",
            "cite_ID": "P07-1066",
            "cite_maker_sids": [
                28
            ],
            "cite_sids": [
                28,
                29
            ],
            "cite_text": "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. The translation lexicon p(f | e) is the key component in this generative process. Topic-specific translation lexicons are learned by a 3-topic BiTAM.",
            "GPT_cite_text": "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by Zhao and Xing (2006). Basically, the BiTAM model consists of topic-dependent translation lexicons modeling P(r|c, e, k) where c, e, and k denote the source Chinese word, target English word, and the topic index respectively."
        },
        {
            "Number": 7,
            "refer_ID": "P06-2124",
            "refer_sids": [
                152
            ],
            "refer_text": "Topic-specific translation lexicons are learned by a 3-topic BiTAM1.",
            "cite_ID": "P10-2025",
            "cite_maker_sids": [
                84
            ],
            "cite_sids": [
                84
            ],
            "cite_text": "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Topic-specific translation lexicons are learned by a 3-topic BiTAM.",
            "GPT_cite_text": "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006)."
        },
        {
            "Number": 8,
            "refer_ID": "P06-2124",
            "refer_sids": [
                124,
                194
            ],
            "refer_text": "Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA).Inter takes the intersection of the two directions and generates high-precision alignments;",
            "cite_ID": "P10-2025",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "Two word-alignment retrieval schemes are designed for BiTAMs: the uni-directional alignment (UDA) and the bi-directional alignment (BDA). Inter takes the intersection of the two directions and generates high-precision alignments;",
            "GPT_cite_text": "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006)."
        },
        {
            "Number": 9,
            "refer_ID": "P06-2124",
            "refer_sids": [
                115,
                116,
                117
            ],
            "refer_text": "The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).To reduce the data sparsity problem, we introduce two remedies in our models.First: Laplace smoothing.",
            "cite_ID": "P11-2032",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.",
            "label": [
                "Implication_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "The translation lexicons Bf,e,k have a potential size of V^2K, assuming the vocabulary sizes for both languages are V. The data sparsity (i.e., lack of a large volume of document pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing.",
            "GPT_cite_text": "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and uses symmetric Dirichlet priors, but they find the MAP solution."
        },
        {
            "Number": 10,
            "refer_ID": "P06-2124",
            "refer_sids": [
                2,
                3,
                21
            ],
            "refer_text": "Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.",
            "label": [
                "Hypothesis_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "Under this formalism, the parallel sentence pairs within a document pair are assumed to constitute a mixture of hidden topics; each word pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). We propose a new statistical formalism: Bilingual Topic AdMixture Model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "GPT_cite_text": "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection."
        },
        {
            "Number": 11,
            "refer_ID": "P06-2124",
            "refer_sids": [
                7,
                10
            ],
            "refer_text": "Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                149
            ],
            "cite_sids": [
                149
            ],
            "cite_text": "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Our preliminary experiments show that the proposed models improve word alignment accuracy and lead to better translation quality. Beyond the sentence level, corpus-level word correlation and contextual-level topical information may help to disambiguate translation candidates and word alignment choices.",
            "GPT_cite_text": "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006, 2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity."
        },
        {
            "Number": 12,
            "refer_ID": "P06-2124",
            "refer_sids": [
                10,
                83
            ],
            "refer_text": "Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ).",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                160
            ],
            "cite_sids": [
                160
            ],
            "cite_text": "\u00e2\u20ac\u00a2 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u00e2\u20ac\u201d HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Beyond the sentence level, corpus-level word correlation and contextual-level topical information may help to disambiguate translation candidates and word alignment choices. Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f_n | e_{anj}, B_{zn}).",
            "GPT_cite_text": "\u2022 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u2014 HTMM which has a different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking into account topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling."
        },
        {
            "Number": 13,
            "refer_ID": "P06-2124",
            "refer_sids": [
                10,
                13
            ],
            "refer_text": "Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.For example, the word shot in \u00e2\u20ac\u0153It was a nice shot.\u00e2\u20ac\u009d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.",
            "cite_ID": "P12-1079",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Beyond the sentence-level, corpus-level word correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the word shot in \u201cIt was a nice shot.\u201d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.",
            "GPT_cite_text": "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality."
        },
        {
            "Number": 14,
            "refer_ID": "P06-2124",
            "refer_sids": [
                21
            ],
            "refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "cite_ID": "P12-1079",
            "cite_maker_sids": [
                40
            ],
            "cite_sids": [
                40
            ],
            "cite_text": "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).",
            "label": [
                "Hypothesis_Citation",
                "Aim_Citation"
            ],
            "GPT_refer_text": "We propose a new statistical formalism: Bilingual Topic Admixture Model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "GPT_cite_text": "Sentences should be translated in consistency with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007)."
        },
        {
            "Number": 15,
            "refer_ID": "P06-2124",
            "refer_sids": [
                17,
                21
            ],
            "refer_text": "Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "cite_ID": "P12-2023",
            "cite_maker_sids": [
                30
            ],
            "cite_sids": [
                30
            ],
            "cite_text": "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.",
            "label": [
                "Hypothesis_Citation",
                "Aim_Citation"
            ],
            "GPT_refer_text": "Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. We propose a new statistical formalism: Bilingual Topic AdMixture Model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "GPT_cite_text": "Topic modeling has received some use in SMT, for instance, bilingual LSA adaptation (Tam et al., 2007) and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment."
        },
        {
            "Number": 16,
            "refer_ID": "P06-2124",
            "refer_sids": [
                21
            ],
            "refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "cite_ID": "P13-2122",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u00e2\u20ac\u02dcbiTAM\u00e2\u20ac\u2122 (Zhao and Xing, 2006).",
            "label": [
                "Hypothesis_Citation",
                "Aim_Citation"
            ],
            "GPT_refer_text": "We propose a new statistical formalism: Bilingual Topic Admixture Model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "GPT_cite_text": "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u2018biTAM\u2019 (Zhao and Xing, 2006)."
        },
        {
            "Number": 17,
            "refer_ID": "P06-2124",
            "refer_sids": [
                21,
                39,
                45,
                192,
                203
            ],
            "refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entityNotably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE).As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1\u223c3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.",
            "cite_ID": "W07-0722",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13,
                14,
                15
            ],
            "cite_text": "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework. Because of this coupling of sentence pairs (via topic sharing across sentence pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity. Notably, BiTAM allows testing alignments in two directions: English-to-Chinese (EC) and Chinese-to-English (CE). As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direction; the UDA alignments from BiTAM1\u223c3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.",
            "GPT_cite_text": "In (Zhao and Xing, 2006), three fairly sophisticated Bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an English-Chinese task."
        },
        {
            "Number": 18,
            "refer_ID": "P06-2124",
            "refer_sids": [
                116,
                119,
                120
            ],
            "refer_text": "To reduce the data sparsity problem, we introduce two remedies in our models.Second: interpolation smoothing.Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting:",
            "cite_ID": "W07-0722",
            "cite_maker_sids": [
                62
            ],
            "cite_sids": [
                62
            ],
            "cite_text": "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To reduce the data sparsity problem, we introduce two remedies in our models. Second: interpolation smoothing. Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting.",
            "GPT_cite_text": "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006)."
        }
    ],
    "J96-3004": [
        {
            "Number": 1,
            "refer_ID": "J96-3004",
            "refer_sids": [
                89,
                90,
                91
            ],
            "refer_text": "Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexi\u00c2\u00ad cal rule-based approaches, and approaches that combine lexical information with sta\u00c2\u00ad tistical information.The present proposal falls into the last group.Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.",
            "cite_ID": "A00-2032",
            "cite_maker_sids": [
                142
            ],
            "cite_sids": [
                141,
                142
            ],
            "cite_text": "Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub\u00c2\u00ad lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical ap\u00c2\u00ad proach.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexical rule-based approaches, and approaches that combine lexical information with statistical information. The present proposal falls into the last group. Purely statistical approaches have not been very popular, and so far as we are aware, earlier work by Sproat and Shih (1990) is the only published instance of such an approach.",
            "GPT_cite_text": "Chinese According to Sproat et al. (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously published instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical approach."
        },
        {
            "Number": 3,
            "refer_ID": "J96-3004",
            "refer_sids": [
                297,
                298
            ],
            "refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.",
            "cite_ID": "C00-2095",
            "cite_maker_sids": [
                80
            ],
            "cite_sids": [
                80
            ],
            "cite_text": "As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text. Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.",
            "GPT_cite_text": "As (Sproat et al., 1996) testify, several native Chinese speakers do not always agree on one unique tokenization for a given sentence."
        },
        {
            "Number": 4,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109,
                112
            ],
            "refer_text": "The most popular approach to dealing with seg\u00c2\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.",
            "cite_ID": "C02-1049",
            "cite_maker_sids": [
                58
            ],
            "cite_sids": [
                58
            ],
            "cite_text": "Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached. The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.",
            "GPT_cite_text": "Conventionally, a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992; Sproat et al., 1996)."
        },
        {
            "Number": 5,
            "refer_ID": "J96-3004",
            "refer_sids": [
                92,
                93
            ],
            "refer_text": "In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.",
            "cite_ID": "C02-1049",
            "cite_maker_sids": [
                127
            ],
            "cite_sids": [
                124,
                127
            ],
            "cite_text": "Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words. Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.",
            "GPT_cite_text": "Mutual information-like statistics are very often adopted in measuring association strength (MSI, DSI +1) combined (i, i + 1) 1993, Sproat et al, 1996."
        },
        {
            "Number": 6,
            "refer_ID": "J96-3004",
            "refer_sids": [
                51
            ],
            "refer_text": "Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.",
            "cite_ID": "C02-1080",
            "cite_maker_sids": [
                20
            ],
            "cite_sids": [
                20,
                21
            ],
            "cite_text": "Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation boundary assignment and prominence assignment is word segmentation.",
            "GPT_cite_text": "Chinese NE recognition is much more difficult than that in English due to two major problems. The first is the word segmentation problem (Sproat et al. 1996, Palmer 1997)."
        },
        {
            "Number": 7,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108
            ],
            "refer_text": "The most popular approach to dealing with seg\u00c2\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "cite_ID": "C02-1143",
            "cite_maker_sids": [
                107
            ],
            "cite_sids": [
                107
            ],
            "cite_text": "We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "GPT_cite_text": "We used a maximum-matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation."
        },
        {
            "Number": 8,
            "refer_ID": "J96-3004",
            "refer_sids": [
                297,
                325
            ],
            "refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 On",
            "cite_ID": "E09-1063",
            "cite_maker_sids": [
                107
            ],
            "cite_sids": [
                107
            ],
            "cite_text": "First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).",
            "label": [
                "Implication_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text. The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.",
            "GPT_cite_text": "First of all, it is really difficult to build a reliable and objective gold standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996)."
        },
        {
            "Number": 9,
            "refer_ID": "J96-3004",
            "refer_sids": [
                2,
                33
            ],
            "refer_text": "An initial step of any text\u00c2\u00ad analysis task is the tokenization of the input into words.Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "cite_ID": "I05-3031",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                6,
                7
            ],
            "cite_text": "The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An initial step of any text analysis task is the tokenization of the input into words. Thus, if one wants to segment words for any purpose from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "GPT_cite_text": "The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation. As the task is an important precursor to many natural language processing systems, it receives a lot of attention in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996)."
        },
        {
            "Number": 10,
            "refer_ID": "J96-3004",
            "refer_sids": [
                297,
                325
            ],
            "refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15",
            "cite_ID": "J00-3004",
            "cite_maker_sids": [
                42
            ],
            "cite_sids": [
                42
            ],
            "cite_text": "According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the \"correct\" segmentation, and the figure reduces as more people become involved.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text. The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.",
            "GPT_cite_text": "According to Sproat et al. (1996) and Wu and Fung (1994), experiments show that only about 75% agreement between native speakers is to be expected on the \"correct\" segmentation, and the figure reduces as more people become involved."
        },
        {
            "Number": 11,
            "refer_ID": "J96-3004",
            "refer_sids": [
                399
            ],
            "refer_text": "This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "cite_ID": "J00-3004",
            "cite_maker_sids": [
                96
            ],
            "cite_sids": [
                96,
                97
            ],
            "cite_text": "Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "GPT_cite_text": "Sproat et al. (1996) implement special recognizers not only for Chinese names and transliterated foreign names, but also for components of morphologically obtained words as well."
        },
        {
            "Number": 12,
            "refer_ID": "J96-3004",
            "refer_sids": [
                89
            ],
            "refer_text": "Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexi\u00c2\u00ad cal rule-based approaches, and approaches that combine lexical information with sta\u00c2\u00ad tistical information.",
            "cite_ID": "J04-1004",
            "cite_maker_sids": [
                53
            ],
            "cite_sids": [
                53
            ],
            "cite_text": "In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexical rule-based approaches, and approaches that combine lexical information with statistical information.",
            "GPT_cite_text": "In Chinese text segmentation, there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two."
        },
        {
            "Number": 13,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "cite_ID": "J04-1004",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "GPT_cite_text": "There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching (Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996)."
        },
        {
            "Number": 14,
            "refer_ID": "J96-3004",
            "refer_sids": [
                133
            ],
            "refer_text": "Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.",
            "cite_ID": "J04-1004",
            "cite_maker_sids": [
                211
            ],
            "cite_sids": [
                211
            ],
            "cite_text": "In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.",
            "GPT_cite_text": "In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996)."
        },
        {
            "Number": 15,
            "refer_ID": "J96-3004",
            "refer_sids": [
                297
            ],
            "refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.",
            "cite_ID": "J04-1004",
            "cite_maker_sids": [
                321
            ],
            "cite_sids": [
                321
            ],
            "cite_text": "As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.",
            "GPT_cite_text": "As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods."
        },
        {
            "Number": 16,
            "refer_ID": "J96-3004",
            "refer_sids": [
                398
            ],
            "refer_text": "In this paper we have argued that Chinese word segmentation can be modeled ef\u00c2\u00ad fectively using weighted finite-state transducers.",
            "cite_ID": "J05-4005",
            "cite_maker_sids": [
                88
            ],
            "cite_sids": [
                88,
                89
            ],
            "cite_text": "A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers.",
            "GPT_cite_text": "A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs)."
        },
        {
            "Number": 17,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75",
            "cite_ID": "J05-4005",
            "cite_maker_sids": [
                126
            ],
            "cite_sids": [
                125,
                126
            ],
            "cite_text": "As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The average agreement among the human judges is 0.76, and the average agreement between ST and the humans is 0.75.",
            "GPT_cite_text": "As shown in Sproat et al. (1996), the rate of agreement between two human judges is less than 80%."
        },
        {
            "Number": 18,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75",
            "cite_ID": "J05-4005",
            "cite_maker_sids": [
                132
            ],
            "cite_sids": [
                131,
                132
            ],
            "cite_text": "Similarly, Sproat et al.(1996) also uses multiple human judges.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The average agreement among the human judges is 0.76, and the average agreement between ST and the humans is 0.75.",
            "GPT_cite_text": "Similarly, Sproat et al. (1996) also use multiple human judges."
        },
        {
            "Number": 19,
            "refer_ID": "J96-3004",
            "refer_sids": [
                399
            ],
            "refer_text": "This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "cite_ID": "J05-4005",
            "cite_maker_sids": [
                490
            ],
            "cite_sids": [
                489,
                490
            ],
            "cite_text": "The Chinese person-name model is a modified version of that described in Sproat et al.(1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "GPT_cite_text": "The Chinese person-name model is a modified version of that described in Sproat et al. (1996)."
        },
        {
            "Number": 20,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75",
            "cite_ID": "J11-1005",
            "cite_maker_sids": [
                123
            ],
            "cite_sids": [
                123
            ],
            "cite_text": "Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The average agreement among the human judges is 0.76, and the average agreement between ST and the humans is 0.75.",
            "GPT_cite_text": "Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996)."
        },
        {
            "Number": 21,
            "refer_ID": "J96-3004",
            "refer_sids": [
                296,
                297
            ],
            "refer_text": "Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.",
            "cite_ID": "J11-3001",
            "cite_maker_sids": [
                326
            ],
            "cite_sids": [
                326
            ],
            "cite_text": "Gold standards, however, 435 cannot be uni\ufb01ed into a single standard (Fung and Wu 1994; Sproat et al. 1996).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score or else a single precision-recall pair. The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.",
            "GPT_cite_text": "Gold standards, however, cannot be unified into a single standard (Fung and Wu 1994; Sproat et al. 1996)."
        },
        {
            "Number": 23,
            "refer_ID": "J96-3004",
            "refer_sids": [
                21,
                33
            ],
            "refer_text": "In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces;Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "cite_ID": "J97-4004",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi, are written one after another with no intervening spaces; thus, if one wants to segment words\u2014for any purpose\u2014from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "GPT_cite_text": "Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;"
        },
        {
            "Number": 25,
            "refer_ID": "J96-3004",
            "refer_sids": [
                138
            ],
            "refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Trans\u00c2\u00ad ducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "cite_ID": "J97-4004",
            "cite_maker_sids": [
                613
            ],
            "cite_sids": [
                612,
                613
            ],
            "cite_text": "The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Transducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "GPT_cite_text": "The weighted finite-state transducer model developed by Sproat et al. (1996) is another excellent representative example."
        },
        {
            "Number": 26,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "cite_ID": "J97-4004",
            "cite_maker_sids": [
                621
            ],
            "cite_sids": [
                621,
                622
            ],
            "cite_text": "While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are ap\u00c2\u00ad parently employed neither in Sproat et al.(1996) nor in Ma (1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "GPT_cite_text": "While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apparently employed neither in Sproat et al. (1996) nor in Ma (1996)."
        },
        {
            "Number": 27,
            "refer_ID": "J96-3004",
            "refer_sids": [
                138
            ],
            "refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Trans\u00c2\u00ad ducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "cite_ID": "N10-1068",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:\u00e2\u20ac\u00a2 WFSTs provide a uniform knowledge represen tation.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Transducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "GPT_cite_text": "Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: \u2022 WFSTs provide a uniform knowledge representation."
        },
        {
            "Number": 28,
            "refer_ID": "J96-3004",
            "refer_sids": [
                138
            ],
            "refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Trans\u00c2\u00ad ducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "cite_ID": "P03-1035",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41,
                42
            ],
            "cite_text": "One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Transducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "GPT_cite_text": "One example of such approaches is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs)."
        },
        {
            "Number": 29,
            "refer_ID": "J96-3004",
            "refer_sids": [
                419,
                420
            ],
            "refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.",
            "cite_ID": "P03-1035",
            "cite_maker_sids": [
                122
            ],
            "cite_sids": [
                122
            ],
            "cite_text": "Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The method reported in this paper makes use solely of unigram probabilities and is therefore a zeroth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation. However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.",
            "GPT_cite_text": "Because any character strings can be, in principle, named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by Sproat et al."
        },
        {
            "Number": 30,
            "refer_ID": "J96-3004",
            "refer_sids": [
                281
            ],
            "refer_text": "Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.",
            "cite_ID": "P03-1035",
            "cite_maker_sids": [
                155
            ],
            "cite_sids": [
                154,
                155
            ],
            "cite_text": "5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.",
            "GPT_cite_text": "5.2.4 Transliterations of foreign names As described in Sproat et al. (1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name."
        },
        {
            "Number": 31,
            "refer_ID": "J96-3004",
            "refer_sids": [
                283,
                284
            ],
            "refer_text": "Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal Chi\u00ad nese personal name, retains a foreign flavor because of liM.As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\u00ad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "cite_ID": "P06-1010",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                42,
                43
            ],
            "cite_text": "Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal Chinese personal name, retains a foreign flavor because of liM. As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probability of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "GPT_cite_text": "Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names. As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese."
        },
        {
            "Number": 32,
            "refer_ID": "J96-3004",
            "refer_sids": [
                88
            ],
            "refer_text": "There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).",
            "cite_ID": "P06-1126",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).",
            "GPT_cite_text": "Chinese word segmentation is the initial stage of many Chinese language processing tasks and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004)."
        },
        {
            "Number": 33,
            "refer_ID": "J96-3004",
            "refer_sids": [
                283,
                284
            ],
            "refer_text": "Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal Chi\u00ad nese personal name, retains a foreign flavor because of liM. As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\u00ad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "cite_ID": "P07-1015",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal Chinese personal name, retains a foreign flavor because of liM. As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probability of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "GPT_cite_text": "Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three or more characters from the list was taken as a possible candidate for Chinese."
        },
        {
            "Number": 34,
            "refer_ID": "J96-3004",
            "refer_sids": [
                284
            ],
            "refer_text": "As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\u00c2\u00ad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "cite_ID": "P07-1016",
            "cite_maker_sids": [
                70
            ],
            "cite_sids": [
                70
            ],
            "cite_text": "As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probability of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "GPT_cite_text": "As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese; e.g., only 731 Chinese characters are adopted in the E-C corpus."
        },
        {
            "Number": 35,
            "refer_ID": "J96-3004",
            "refer_sids": [
                54
            ],
            "refer_text": "A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.",
            "cite_ID": "P12-1110",
            "cite_maker_sids": [
                105
            ],
            "cite_sids": [
                105
            ],
            "cite_text": "3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.",
            "GPT_cite_text": "3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enable us to investigate the contribution of the syntactic dependency in a more realistic setting."
        },
        {
            "Number": 36,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.",
            "cite_ID": "P12-1111",
            "cite_maker_sids": [
                91
            ],
            "cite_sids": [
                91
            ],
            "cite_text": "In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached.",
            "GPT_cite_text": "In early work, rule-based models find words one by one based on heuristics such as forward maximum matching (Sproat et al., 1996)."
        },
        {
            "Number": 37,
            "refer_ID": "J96-3004",
            "refer_sids": [
                87,
                88
            ],
            "refer_text": "Previous Work.There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).",
            "cite_ID": "P97-1041",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Previous work. There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).",
            "GPT_cite_text": "For a discussion of recent Chinese segmentation work, see Sproat et al. (1996)."
        },
        {
            "Number": 38,
            "refer_ID": "J96-3004",
            "refer_sids": [
                419,
                420
            ],
            "refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.",
            "cite_ID": "P98-1076",
            "cite_maker_sids": [
                145
            ],
            "cite_sids": [
                144,
                145
            ],
            "cite_text": "The actual implementation of the weighted finite\u00c2\u00ad state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The method reported in this paper makes use solely of unigram probabilities and is therefore a zeroth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation. However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.",
            "GPT_cite_text": "The actual implementation of the weighted finite state transducer by Sproat et al. (1996) can be taken as evidence that the hypothesis of one tokenization per source has already been in practical use."
        },
        {
            "Number": 39,
            "refer_ID": "J96-3004",
            "refer_sids": [
                419
            ],
            "refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.",
            "cite_ID": "P98-1076",
            "cite_maker_sids": [
                150
            ],
            "cite_sids": [
                149,
                150
            ],
            "cite_text": "utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.",
            "GPT_cite_text": "Utilizing local and sentential constraints, what Sproat et al. (1996) implemented was simply a token unigram scoring function."
        },
        {
            "Number": 40,
            "refer_ID": "J96-3004",
            "refer_sids": [
                356
            ],
            "refer_text": "The performance was 80.99% recall and 61.83% precision.",
            "cite_ID": "P99-1036",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                5,
                6
            ],
            "cite_text": "In Japanese, around 95% word segmentation ac\u00c2\u00ad curacy is reported by using a word-based lan\u00c2\u00ad guage model and the Viterbi-like dynamic program\u00c2\u00ad ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Mat\u00c2\u00ad sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The performance was 80.99% recall and 61.83% precision.",
            "GPT_cite_text": "In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996)."
        },
        {
            "Number": 41,
            "refer_ID": "J96-3004",
            "refer_sids": [
                415,
                416,
                417
            ],
            "refer_text": "The major problem for our seg\u00c2\u00ad menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.However, there will remain a large number of words that are not readily adduced to any produc\u00c2\u00ad tive pattern and that would simply have to be added to the dictionary.",
            "cite_ID": "P99-1036",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The major problem for our segmenter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]). We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted. However, there will remain a large number of words that are not readily adduced to any productive pattern and that would simply have to be added to the dictionary.",
            "GPT_cite_text": "There are two approaches to solving this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996)."
        },
        {
            "Number": 42,
            "refer_ID": "J96-3004",
            "refer_sids": [
                398,
                399
            ],
            "refer_text": "In this paper we have argued that Chinese word segmentation can be modeled ef\u00c2\u00ad fectively using weighted finite-state transducers.This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "cite_ID": "P99-1036",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "To improve word segmenta\u00c2\u00ad tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers. This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "GPT_cite_text": "To improve word segmentation accuracy, (Nagata, 1996) used a single general-purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words."
        },
        {
            "Number": 43,
            "refer_ID": "J96-3004",
            "refer_sids": [
                128
            ],
            "refer_text": "Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recal",
            "cite_ID": "P99-1036",
            "cite_maker_sids": [
                178
            ],
            "cite_sids": [
                178
            ],
            "cite_text": "Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recall.",
            "GPT_cite_text": "Word segmentation accuracy is expressed in terms of recall and precision, as is done in the previous research (Sproat et al., 1996)."
        },
        {
            "Number": 44,
            "refer_ID": "J96-3004",
            "refer_sids": [
                116
            ],
            "refer_text": "Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).",
            "cite_ID": "W00-0803",
            "cite_maker_sids": [
                29
            ],
            "cite_sids": [
                29
            ],
            "cite_text": "Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).",
            "GPT_cite_text": "Segmentation and morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; Matsui et al., 1997 and many others)."
        },
        {
            "Number": 45,
            "refer_ID": "J96-3004",
            "refer_sids": [
                415,
                417
            ],
            "refer_text": "The major problem for our seg\u00c2\u00ad menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).However, there will remain a large number of words that are not readily adduced to any produc\u00c2\u00ad tive pattern and that would simply have to be added to the dictionary.",
            "cite_ID": "W00-1207",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The major problem for our segmenter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]). However, there will remain a large number of words that are not readily adduced to any productive pattern and that would simply have to be added to the dictionary.",
            "GPT_cite_text": "Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al. 1996, Tung and Lee 1994, Lin et al. 1993, Chiang et al. 1992, Lua, Huang et al., etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low."
        },
        {
            "Number": 46,
            "refer_ID": "J96-3004",
            "refer_sids": [
                20,
                33
            ],
            "refer_text": "Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writ\u00c2\u00ad ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "cite_ID": "W01-0513",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                40,
                41
            ],
            "cite_text": "The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words. Put another way, written Chinese simply lacks orthographic words. Thus, if one wants to segment words\u2014for any purpose\u2014from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "GPT_cite_text": "The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran et al., 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat et al., 1996; Ponte and Croft, 1996; Shimohata, 1997; Teahan et al., 2000; and many others)."
        },
        {
            "Number": 47,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109
            ],
            "refer_text": "The most popular approach to dealing with seg\u00c2\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.",
            "cite_ID": "W02-1117",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached.",
            "GPT_cite_text": "For example, these words should be obtained: The ambiguous string is. There are some methods to resolve this problem: one method is forward maximum matching, backward maximum matching, and minimum matching, which are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990]."
        },
        {
            "Number": 48,
            "refer_ID": "J96-3004",
            "refer_sids": [
                65,
                168,
                169,
                170
            ],
            "refer_text": "In this paper we present a stochastic finite-state model for segmenting Chinese text into wordsWord frequencies are estimated by a re-estimation procedure that involves apply\u00ad ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material.This larger corpus was kindly provided to us by United Informatics Inc.,",
            "cite_ID": "W02-1808",
            "cite_maker_sids": [
                5
            ],
            "cite_sids": [
                5
            ],
            "cite_text": "Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we present a stochastic finite-state model for segmenting Chinese text into words. Word frequencies are estimated by a re-estimation procedure that involves applying the segmentation algorithm presented here to a corpus of 20 million words. Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material. This larger corpus was kindly provided to us by United Informatics Inc.",
            "GPT_cite_text": "Statistical approaches involve language models, mostly finite-state ones, trained on some large-scale corpora, as shown in Fan and Tsai (1988), Chang et al. (1991), Chiang et al. (1992), Sproat et al. (1996)."
        },
        {
            "Number": 49,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement",
            "cite_ID": "W03-1025",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the inter-human agreement.",
            "GPT_cite_text": "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about equal to lower."
        },
        {
            "Number": 50,
            "refer_ID": "J96-3004",
            "refer_sids": [
                0,
                325
            ],
            "refer_text": "A Stochastic Finite-State Word-Segmentation Algorithm for ChineseThe average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement",
            "cite_ID": "W03-1025",
            "cite_maker_sids": [
                180
            ],
            "cite_sids": [
                180
            ],
            "cite_text": "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "A Stochastic Finite-State Word-Segmentation Algorithm for Chinese. The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.",
            "GPT_cite_text": "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996), and it is known that human agreement is relatively low."
        },
        {
            "Number": 51,
            "refer_ID": "J96-3004",
            "refer_sids": [
                138,
                149,
                150
            ],
            "refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Trans\u00ad ducer (WFST) (Pereira, Riley, and Sproat 1994).This FSA I can be segmented into words by composing Id(I) with D*, to form the WFST shown in Figure 2(c), then selecting the best path through this WFST to produce the WFST in Figure 2(d).This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.",
            "cite_ID": "W03-1025",
            "cite_maker_sids": [
                187
            ],
            "cite_sids": [
                186,
                187
            ],
            "cite_text": "Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Transducer (WFST) (Pereira, Riley, and Sproat 1994). This FSA can be segmented into words by composing Id(I) with D*, to form the WFST shown in Figure 2(c), then selecting the best path through this WFST to produce the WFST in Figure 2(d). This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.",
            "GPT_cite_text": "Sproat et al. (1996) employ stochastic finite state machines to find word boundaries."
        },
        {
            "Number": 52,
            "refer_ID": "J96-3004",
            "refer_sids": [
                65
            ],
            "refer_text": "In this paper we present a stochastic finite-state model for segmenting Chinese text into words",
            "cite_ID": "W03-1728",
            "cite_maker_sids": [
                3
            ],
            "cite_sids": [
                3
            ],
            "cite_text": "This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we present a stochastic finite-state model for segmenting Chinese text into words.",
            "GPT_cite_text": "This may sound simple enough, but in reality, identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003)."
        },
        {
            "Number": 53,
            "refer_ID": "J96-3004",
            "refer_sids": [
                419
            ],
            "refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.",
            "cite_ID": "W05-0709",
            "cite_maker_sids": [
                83
            ],
            "cite_sids": [
                83
            ],
            "cite_text": "In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The method reported in this paper makes use solely of unigram probabilities and is therefore a zeroth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.",
            "GPT_cite_text": "In addition to the model based on a dictionary of stems and words, we also experimented with models based on character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996)."
        },
        {
            "Number": 54,
            "refer_ID": "J96-3004",
            "refer_sids": [
                284
            ],
            "refer_text": "As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\u00c2\u00ad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "cite_ID": "W06-1630",
            "cite_maker_sids": [
                118
            ],
            "cite_sids": [
                118
            ],
            "cite_text": "The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probability of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "GPT_cite_text": "The words were stemmed in all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3, if both c2 and c3 are in our suffix and ending list, then this single word generates three possible candidates: c1, c1 c2, and c1 c2 c3. In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996)."
        },
        {
            "Number": 55,
            "refer_ID": "J96-3004",
            "refer_sids": [
                305
            ],
            "refer_text": "A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.",
            "cite_ID": "W10-3212",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A greedy algorithm (or maximum-matching algorithm), GR: proceeds through the sentence, taking the longest match with a dictionary entry at each point.",
            "GPT_cite_text": "In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon-based approaches (ii) Linguistic knowledge-based approaches (iii) Machine learning-based approaches/statistical approaches (Haruechaiyasak et al., 2008). Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon-based approaches."
        },
        {
            "Number": 56,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement",
            "cite_ID": "W10-3708",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the inter-human agreement.",
            "GPT_cite_text": "Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996)."
        },
        {
            "Number": 57,
            "refer_ID": "J96-3004",
            "refer_sids": [
                92,
                93,
                103
            ],
            "refer_text": "In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.",
            "cite_ID": "W11-0823",
            "cite_maker_sids": [
                174
            ],
            "cite_sids": [
                174
            ],
            "cite_text": "There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words. Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary. Church and Hanks [1989], and we have used lists of character pairs ranked by mutual information to expand our own dictionary.",
            "GPT_cite_text": "There are a number of popular dictionary-based solutions such as ChaSen and Juman. Sproat et al. (1996) proposed an alternative solution based on distributional statistics such as mutual information."
        },
        {
            "Number": 58,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement",
            "cite_ID": "W12-1011",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41
            ],
            "cite_text": "Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The average agreement among the human judges is 0.76, and the average agreement between ST and the humans is 0.75, or about 99% of the interhuman agreement.",
            "GPT_cite_text": "Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996)."
        },
        {
            "Number": 59,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76",
            "cite_ID": "W12-1011",
            "cite_maker_sids": [
                204
            ],
            "cite_sids": [
                204
            ],
            "cite_text": "No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The average agreement among the human judges is 0.76.",
            "GPT_cite_text": "No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter-human agreement rate in (Sproat et al., 1996)."
        },
        {
            "Number": 60,
            "refer_ID": "J96-3004",
            "refer_sids": [
                158
            ],
            "refer_text": "The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.",
            "cite_ID": "W12-2303",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                11,
                12
            ],
            "cite_text": "An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.",
            "GPT_cite_text": "An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al. (1996), which utilizes information such as word frequency statistics in a corpus to build the model and is less efficient but more accurate."
        },
        {
            "Number": 61,
            "refer_ID": "J96-3004",
            "refer_sids": [
                398,
                399
            ],
            "refer_text": "In this paper we have argued that Chinese word segmentation can be modeled ef\u00c2\u00ad fectively using weighted finite-state transducers.This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "cite_ID": "W12-2303",
            "cite_maker_sids": [
                157
            ],
            "cite_sids": [
                155,
                156,
                157
            ],
            "cite_text": "There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "In this paper, we have argued that Chinese word segmentation can be modeled effectively using weighted finite-state transducers. This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "GPT_cite_text": "There are many other OOV recognition methods proposed in the literature before the rise of machine learning in the field. For example, the Sproat et al. (1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names and transliterations, using finite-state techniques."
        },
        {
            "Number": 62,
            "refer_ID": "J96-3004",
            "refer_sids": [
                67,
                68,
                69
            ],
            "refer_text": "The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen con\u00ad structions, including morphological derivatives and personal names.We will evaluate various specific aspects of the segmentation, as well as the overall segmentation per\u00ad formance.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                26
            ],
            "cite_sids": [
                26
            ],
            "cite_text": "One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers. It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen constructions, including morphological derivatives and personal names. We will evaluate various specific aspects of the segmentation, as well as the overall segmentation performance.",
            "GPT_cite_text": "One of the major problems in unsupervised word segmentation is the treatment of unseen words. Sproat et al. (1996) wrote lexical rules for each productive morphological process, such as plural noun formation, Chinese personal names, and transliterations of foreign words."
        },
        {
            "Number": 63,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                69
            ],
            "cite_sids": [
                69
            ],
            "cite_text": "We used a simple greedy algorithm described in [Sproat et al., 1996].",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached.",
            "GPT_cite_text": "We used a simple greedy algorithm described in [Sproat et al., 1996]."
        },
        {
            "Number": 64,
            "refer_ID": "J96-3004",
            "refer_sids": [
                190,
                191
            ],
            "refer_text": "The morphological anal\u00adysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.each word in the lexicon whether or not each string is actually an instance of the word in question.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                73
            ],
            "cite_text": "[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The morphological analysis itself can be handled using well-known techniques from finite-state morphol. The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up each word in the lexicon, whether or not each string is actually an instance of the word in question.",
            "GPT_cite_text": "[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus."
        },
        {
            "Number": 65,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it \"maximum matching\", we call this method \"longest match\" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached.",
            "GPT_cite_text": "The problem of the longest match string frequency method is that if a word W1 is a substring of another word W2 and if W1 always appears as a substring of W2 in the training text, just like m1. Although (Sproat et al., 1996) calls it \"maximum matching\", we call this method \"longest match\" according to a review on Chinese word segmentation [Wu and Tseng, 1993] and the literal translation of the Japanese name of the method Hi!."
        },
        {
            "Number": 66,
            "refer_ID": "J96-3004",
            "refer_sids": [
                142,
                143,
                356
            ],
            "refer_text": "We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected.The performance was 80.99% recall and 61.83% precision.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                121
            ],
            "cite_sids": [
                121
            ],
            "cite_text": "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "We can recall that precision is defined to be the number of correct hits divided by the total number of items selected, and that recall is defined to be the number of correct hits divided by the number of items that should have been selected. The performance was 80.99% recall and 61.83% precision.",
            "GPT_cite_text": "Word segmentation accuracy is expressed in terms of recall and precision, as is done for bracketing of partial parses [Nagata, 1994; Sproat et al., 1996]."
        },
        {
            "Number": 67,
            "refer_ID": "J96-3004",
            "refer_sids": [
                65
            ],
            "refer_text": "In this paper we present a stochastic finite-state model for segmenting Chinese text into words",
            "cite_ID": "W97-0316",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                10,
                11
            ],
            "cite_text": "Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we present a stochastic finite-state model for segmenting Chinese text into words.",
            "GPT_cite_text": "Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- are therefore an important and necessary first step to be taken before other analyses can begin. Many researchers have proposed practical methods to resolve this problem, such as (Nie et al., 1995; Wu and Tsang, 1995; Jin & Chen, 1996; Ponte & Croft, 1996; Sproat et al., 1996; Sun et al., 1997)."
        }
    ],
    "P98-2143": [
        {
            "Number": 1,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154,
                155,
                156
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00c2\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.Evaluation shows a success rate of 89.7% for the genre of tech\u00c2\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).",
            "cite_ID": "A00-1020",
            "cite_maker_sids": [
                29
            ],
            "cite_sids": [
                28,
                29
            ],
            "cite_text": "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac\u00c2\u00ad curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of technical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3% success rate) and for Arabic (95.2% success rate).",
            "GPT_cite_text": "Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996), (Kameyama, 1997))."
        },
        {
            "Number": 2,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.",
            "cite_ID": "C02-1027",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors.",
            "GPT_cite_text": "The anaphora resolver is an adaptation for Bulgarian of Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998)."
        },
        {
            "Number": 3,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.",
            "cite_ID": "C02-1027",
            "cite_maker_sids": [
                65
            ],
            "cite_sids": [
                65
            ],
            "cite_text": "This module resolves third-person personal pronouns and is an adaptation of Mitkov\u00e2\u20ac\u2122s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors.",
            "GPT_cite_text": "This module resolves third-person personal pronouns and is an adaptation of Mitkov's robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000)."
        },
        {
            "Number": 4,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15,
                18
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\u00ad mediate reference.",
            "cite_ID": "C02-1027",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                71,
                72,
                73
            ],
            "cite_text": "LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov\u00e2\u20ac\u2122s Anaphora Resolution.System.3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors. The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for immediate reference.",
            "GPT_cite_text": "LINGUA performs the pre-processing needed as an input to the anaphora resolution algorithm: sentence, paragraph, and clause splitters, NP grammar, part-of-speech tagger. MARS stands for Mitkov's Anaphora Resolution. For a detailed procedure on how candidates are handled in the event of a tie, see (Mitkov, 1998)."
        },
        {
            "Number": 5,
            "refer_ID": "P98-2143",
            "refer_sids": [
                16,
                25
            ],
            "refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.",
            "cite_ID": "C02-1027",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                79
            ],
            "cite_text": "Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\"). The antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non-prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance, or to preference of terms.",
            "GPT_cite_text": "Most of the indicators have been adopted in LINGUA without modification from the original English version (see Mitkov, 1998 for more details)."
        },
        {
            "Number": 6,
            "refer_ID": "P98-2143",
            "refer_sids": [
                19,
                47
            ],
            "refer_text": "If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\".",
            "cite_ID": "C04-1074",
            "cite_maker_sids": [
                61
            ],
            "cite_sids": [
                60,
                61
            ],
            "cite_text": "Binding constraints have been in the focus of linguistic research for more than thirty years.They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "If immediate reference has not been identified, then priority is given to the candidate with the best collocation pattern score. The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\".",
            "GPT_cite_text": "Binding constraints have been in the focus of linguistic research for more than thirty years. They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e., parsers (Mitkov, 1998)."
        },
        {
            "Number": 7,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.",
            "cite_ID": "C04-1075",
            "cite_maker_sids": [
                19
            ],
            "cite_sids": [
                19
            ],
            "cite_text": "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors.",
            "GPT_cite_text": "However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998)."
        },
        {
            "Number": 8,
            "refer_ID": "P98-2143",
            "refer_sids": [
                58
            ],
            "refer_text": "Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).",
            "cite_ID": "C04-1143",
            "cite_maker_sids": [
                101
            ],
            "cite_sids": [
                101
            ],
            "cite_text": "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Referential distance: In complex sentences, noun phrases in the previous clause are the best candidates for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).",
            "GPT_cite_text": "The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences, as suggested in (Mitkov, 1998)."
        },
        {
            "Number": 9,
            "refer_ID": "P98-2143",
            "refer_sids": [
                23,
                24
            ],
            "refer_text": "Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00c2\u00ad cedent.",
            "cite_ID": "D09-1101",
            "cite_maker_sids": [
                52
            ],
            "cite_sids": [
                52
            ],
            "cite_text": "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1, or 2) for each indicator; the candidate with the highest aggregate score is proposed as the antecedent.",
            "GPT_cite_text": "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors."
        },
        {
            "Number": 11,
            "refer_ID": "P98-2143",
            "refer_sids": [
                24,
                25
            ],
            "refer_text": "Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\u00ad cedent.The antecedent indicators have been identi\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.",
            "cite_ID": "E99-1031",
            "cite_maker_sids": [
                91
            ],
            "cite_sids": [
                91
            ],
            "cite_text": "We implemented meta-modules to in\u00c2\u00ad terface to the genetic algorithm driver and to combine different salience factors into an over\u00c2\u00ad all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Candidates are assigned a score (-1, 0, 1, or 2) for each indicator; the candidate with the highest aggregate score is proposed as the antecedent. The antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non-prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance, or to preference of terms.",
            "GPT_cite_text": "We implemented meta-modules to interface with the genetic algorithm driver and to combine different salience factors into an overall score (similar to Carbonell and Brown, 1988; Mitkov, 1998)."
        },
        {
            "Number": 12,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.",
            "cite_ID": "H05-1001",
            "cite_maker_sids": [
                59
            ],
            "cite_sids": [
                59
            ],
            "cite_text": "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors.",
            "GPT_cite_text": "The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)."
        },
        {
            "Number": 13,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15,
                16
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "H05-1001",
            "cite_maker_sids": [
                61
            ],
            "cite_sids": [
                61,
                62
            ],
            "cite_text": "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.Mitkov\u00e2\u20ac\u2122s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \u00e2\u20ac\u009dantecedent indicators\u00e2\u20ac\u009d).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. Mitkov's algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."
        },
        {
            "Number": 14,
            "refer_ID": "P98-2143",
            "refer_sids": [
                17
            ],
            "refer_text": "The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\u00c2\u00ad maining candidates (see next section).",
            "cite_ID": "H05-1001",
            "cite_maker_sids": [
                63
            ],
            "cite_sids": [
                63
            ],
            "cite_text": "The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the remaining candidates (see next section).",
            "GPT_cite_text": "The approach works as follows: the system identifies the noun phrases that precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre-specific antecedent indicators to the remaining candidates (Mitkov, 1998)."
        },
        {
            "Number": 17,
            "refer_ID": "P98-2143",
            "refer_sids": [
                16
            ],
            "refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "I05-2040",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and their antecedents."
        },
        {
            "Number": 19,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00c2\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "cite_ID": "I08-3014",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                22,
                23
            ],
            "cite_text": "A lot of work has been done in English for the purpose of anaphora resolution and various algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "GPT_cite_text": "A lot of work has been done in English for the purpose of anaphora resolution, and various algorithms have been devised for this purpose (Aone and Bennett, 1996; Brennan, Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappin and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999)."
        },
        {
            "Number": 20,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00c2\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "cite_ID": "I08-3014",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41
            ],
            "cite_text": "How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "GPT_cite_text": "How these factors are helpful in anaphora resolution in the English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more appreciated."
        },
        {
            "Number": 21,
            "refer_ID": "P98-2143",
            "refer_sids": [
                93,
                113,
                114,
                115
            ],
            "refer_text": "3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).The evaluation carried out was manual to ensure that no added error was gen\u00c2\u00ad erated (e.g. due to possible wrong sentence/clause detection or POS tagging).Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).The evaluation indicated 83.6% success rate.",
            "cite_ID": "J01-4003",
            "cite_maker_sids": [
                5
            ],
            "cite_sids": [
                5
            ],
            "cite_text": "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.",
            "label": [
                "Method_Citation",
                "Implication_Citation"
            ],
            "GPT_refer_text": "3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). The evaluation carried out was manual to ensure that no added error was generated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3). The evaluation indicated an 83.6% success rate.",
            "GPT_cite_text": "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora."
        },
        {
            "Number": 22,
            "refer_ID": "P98-2143",
            "refer_sids": [
                16
            ],
            "refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "J01-4005",
            "cite_maker_sids": [
                318
            ],
            "cite_sids": [
                318
            ],
            "cite_text": "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998])."
        },
        {
            "Number": 23,
            "refer_ID": "P98-2143",
            "refer_sids": [
                94
            ],
            "refer_text": "There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.",
            "cite_ID": "J01-4006",
            "cite_maker_sids": [
                45
            ],
            "cite_sids": [
                45
            ],
            "cite_text": "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u00e2\u20ac\u00a2 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "There were 71 pronouns in the 140-page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.",
            "GPT_cite_text": "Example: (weather) It is raining, (time) It is 5 o'clock, and (ambient environment) It is hot in here. Reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: \u2022 \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872)."
        },
        {
            "Number": 24,
            "refer_ID": "P98-2143",
            "refer_sids": [
                8
            ],
            "refer_text": "For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).",
            "cite_ID": "J02-1001",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & Luper-Foy 1988; Sidner 1979; Webber 1979).",
            "GPT_cite_text": "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and Luper-Foy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedents of anaphors divide into filters and preferences."
        },
        {
            "Number": 25,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154,
                155
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00c2\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.Evaluation shows a success rate of 89.7% for the genre of tech\u00c2\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.",
            "cite_ID": "J05-3004",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                7,
                8
            ],
            "cite_text": "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.",
            "label": [
                "Aim_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of technical manuals and at least in this genre, the approach appears to be more successful than other similar methods.",
            "GPT_cite_text": "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and M\u00fcller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively."
        },
        {
            "Number": 26,
            "refer_ID": "P98-2143",
            "refer_sids": [
                1,
                2,
                154,
                155
            ],
            "refer_text": "Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.One of the disadvantages of developing a knowledge\u00c2\u00ad based system, however, is that it is a very labour\u00c2\u00ad intensive and time-consuming task.We have described a robust, knowledge-poor ap\u00c2\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.",
            "cite_ID": "N01-1008",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                9,
                10,
                11
            ],
            "cite_text": "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of technical manuals and at least in this genre, the approach appears to be more successful than other similar methods.",
            "GPT_cite_text": "The acquisition of extensive linguistic and discourse knowledge necessary for resolving coreference is time-consuming, difficult, and error-prone. Nevertheless, recent results show that knowledge-poor, empirical methods perform with amazing accuracy on certain forms of coreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997))."
        },
        {
            "Number": 27,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00c2\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "cite_ID": "N01-1008",
            "cite_maker_sids": [
                63
            ],
            "cite_sids": [
                63
            ],
            "cite_text": "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "GPT_cite_text": "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCKTAIL filters its most performant rules through massive training data, generated by its AUTOTAGCOFTF component."
        },
        {
            "Number": 28,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15,
                155,
                156
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00ad tic and discourse analysis (which is vital for real\u00ad world applications), we developed a robust, knowl\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.Evaluation shows a success rate of 89.7% for the genre of tech\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).",
            "cite_ID": "N04-1004",
            "cite_maker_sids": [
                45
            ],
            "cite_sids": [
                45
            ],
            "cite_text": "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution that does not parse and analyze the input in order to identify antecedents of anaphors. Evaluation shows a success rate of 89.7% for the genre of technical manuals, and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3% success rate) and for Arabic (95.2% success rate).",
            "GPT_cite_text": "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998)."
        },
        {
            "Number": 29,
            "refer_ID": "P98-2143",
            "refer_sids": [
                16
            ],
            "refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "P00-1022",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis (see Mitkov (1998) for example)."
        },
        {
            "Number": 30,
            "refer_ID": "P98-2143",
            "refer_sids": [
                3,
                6
            ],
            "refer_text": "This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.Evaluation reports a success rate of 89.7% which is better than the suc\u00ad cess rates of the approaches selected for comparison and tested on the same data.",
            "cite_ID": "P00-1022",
            "cite_maker_sids": [
                178
            ],
            "cite_sids": [
                178
            ],
            "cite_text": "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Evaluation reports a success rate of 89.7%, which is better than the success rates of the approaches selected for comparison and tested on the same data.",
            "GPT_cite_text": "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals."
        },
        {
            "Number": 31,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.",
            "cite_ID": "P00-1022",
            "cite_maker_sids": [
                209
            ],
            "cite_sids": [
                209
            ],
            "cite_text": "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors.",
            "GPT_cite_text": "Ruslan Mitkov (1998) Robust pronoun resolution: the evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge."
        },
        {
            "Number": 32,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "cite_ID": "P01-1006",
            "cite_maker_sids": [
                65
            ],
            "cite_sids": [
                65
            ],
            "cite_text": "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev\u00e2\u20ac\u2122s parser- free version of Lappin and Leass\u00e2\u20ac\u2122 RAP (Kennedy and Boguraev, 1996), Baldwin\u00e2\u20ac\u2122s pronoun resolution method (Baldwin, 1997) and Mitkov\u00e2\u20ac\u2122s knowledge-poor pronoun resolution approach (Mitkov, 1998b).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "GPT_cite_text": "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev's parser-free version of Lappin and Leass's RAP (Kennedy and Boguraev, 1996), Baldwin's pronoun resolution method (Baldwin, 1997), and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b)."
        },
        {
            "Number": 33,
            "refer_ID": "P98-2143",
            "refer_sids": [
                3,
                5
            ],
            "refer_text": "This paper pres\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.",
            "cite_ID": "P01-1006",
            "cite_maker_sids": [
                82
            ],
            "cite_sids": [
                82
            ],
            "cite_text": "Mitkov\u00e2\u20ac\u2122s approach Mitkov\u00e2\u20ac\u2122s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Candidates are assigned scores by each indicator, and the candidate with the highest score is returned as the antecedent.",
            "GPT_cite_text": "Mitkov's approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts that is based on a set of boosting and impeding indicators applied to each candidate for antecedent."
        },
        {
            "Number": 34,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "cite_ID": "P01-1006",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \u00e2\u20ac\u009dknowledge-poor philosophy\u00e2\u20ac\u009d: Kennedy and Boguraev\u00e2\u20ac\u2122s (1996) parser-free algorithm, Baldwin\u00e2\u20ac\u2122s (1997) CogNiac and Mitkov\u00e2\u20ac\u2122s (1998b) knowledge-poor approach.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "GPT_cite_text": "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \"knowledge-poor philosophy\": Kennedy and Boguraev's (1996) parser-free algorithm, Baldwin's (1997) CogNiac, and Mitkov's (1998b) knowledge-poor approach."
        },
        {
            "Number": 35,
            "refer_ID": "P98-2143",
            "refer_sids": [
                3,
                5
            ],
            "refer_text": "This paper pres\u00c2\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.",
            "cite_ID": "P03-1023",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "Mitkov\u00e2\u20ac\u2122s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Candidates are assigned scores by each indicator, and the candidate with the highest score is returned as the antecedent.",
            "GPT_cite_text": "Mitkov's knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates."
        },
        {
            "Number": 36,
            "refer_ID": "P98-2143",
            "refer_sids": [
                25
            ],
            "refer_text": "The antecedent indicators have been identi\u00c2\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00c2\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.",
            "cite_ID": "P04-1017",
            "cite_maker_sids": [
                191
            ],
            "cite_sids": [
                191
            ],
            "cite_text": "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non-prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.",
            "GPT_cite_text": "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TF-IDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and M\u00fcller, 2003)."
        },
        {
            "Number": 37,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.",
            "cite_ID": "P04-1018",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors.",
            "GPT_cite_text": "Early work on anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)."
        },
        {
            "Number": 38,
            "refer_ID": "P98-2143",
            "refer_sids": [
                13
            ],
            "refer_text": "It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.",
            "cite_ID": "P05-1021",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.",
            "GPT_cite_text": "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.)."
        },
        {
            "Number": 39,
            "refer_ID": "P98-2143",
            "refer_sids": [
                25
            ],
            "refer_text": "The antecedent indicators have been identi\u00c2\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00c2\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.",
            "cite_ID": "P05-1021",
            "cite_maker_sids": [
                97
            ],
            "cite_sids": [
                97
            ],
            "cite_text": "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non-prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.",
            "GPT_cite_text": "ParalStructmarks whether a candidate and an anaphor have similar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998)."
        },
        {
            "Number": 40,
            "refer_ID": "P98-2143",
            "refer_sids": [
                16
            ],
            "refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "P06-1006",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "These features are calculated by mining the parse trees, and then can be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005)."
        },
        {
            "Number": 41,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15,
                16
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "P07-1068",
            "cite_maker_sids": [
                5
            ],
            "cite_sids": [
                5
            ],
            "cite_text": "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2003))."
        },
        {
            "Number": 43,
            "refer_ID": "P98-2143",
            "refer_sids": [
                25,
                64,
                65
            ],
            "refer_text": "The antecedent indicators have been identi\u00c2\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00c2\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".We should point out that the antecedent indicators are preferences and not absolute factors.",
            "cite_ID": "P13-3012",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non-prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Top symptoms like \"lexical reiteration\" assign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors.",
            "GPT_cite_text": "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weights."
        },
        {
            "Number": 44,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15,
                16
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "S10-1019",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                4,
                5,
                6,
                7
            ],
            "cite_text": "Coreference resolution is a field in which major progress has been made in the last decade.After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf."
        },
        {
            "Number": 45,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154,
                155
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00c2\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.Evaluation shows a success rate of 89.7% for the genre of tech\u00c2\u00ad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.",
            "cite_ID": "W01-0704",
            "cite_maker_sids": [
                83
            ],
            "cite_sids": [
                82,
                83
            ],
            "cite_text": "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998)",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of technical manuals and at least in this genre, the approach appears to be more successful than other similar methods.",
            "GPT_cite_text": "They use limited knowledge (lexical, morphological, and syntactic information sources) for the detection of the correct antecedent. These proposals have reported high success rates for English (89.7%) (Mitkov, 1998)."
        },
        {
            "Number": 47,
            "refer_ID": "P98-2143",
            "refer_sids": [
                154
            ],
            "refer_text": "We have described a robust, knowledge-poor ap\u00c2\u00ad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "cite_ID": "W04-0707",
            "cite_maker_sids": [
                222
            ],
            "cite_sids": [
                222
            ],
            "cite_text": "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov\u00e2\u20ac\u2122s algorithm for pronoun resolution (Mitkov, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have described a robust, knowledge-poor approach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.",
            "GPT_cite_text": "GUITAR (Poesio and Alexandrov Kabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira/Poesio algorithm for definite descriptions and of Mitkov's algorithm for pronoun resolution (Mitkov, 1998)."
        },
        {
            "Number": 49,
            "refer_ID": "P98-2143",
            "refer_sids": [
                63,
                64,
                65
            ],
            "refer_text": "These scores have been determined experimentally on an empirical basis and are constantly being up\u00ad dated.Top symptoms like \"lexical reiteration\" as\u00ad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".We should point out that the antecedent indicators are preferences and not absolute factors.",
            "cite_ID": "W04-2310",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102
            ],
            "cite_text": "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "These scores have been determined experimentally on an empirical basis and are constantly being updated. Top symptoms like \"lexical reiteration\" assign a score of \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\". We should point out that the antecedent indicators are preferences and not absolute factors.",
            "GPT_cite_text": "In most systems (Mitkov, 1998; Lappin and Leass, 1994), the weights that are assigned for different anaphor-antecedent relationships are programmer-dependent."
        },
        {
            "Number": 50,
            "refer_ID": "P98-2143",
            "refer_sids": [
                85,
                139
            ],
            "refer_text": "For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\u00ad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.",
            "cite_ID": "W04-2310",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms), and it is not realistic to expect its performance to be as good as an approach that makes use of syntactic and semantic knowledge in terms of constraints and preferences. It would be fair to say that even though the results show the superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalized automatically for other genres or unrestricted texts, and for a more accurate picture, further extensive tests are necessary.",
            "GPT_cite_text": "Often domain-specific heuristics are used for anaphora resolution and fine-tuned to perform well on a limited corpus, such as in (Mitkov, 1998)."
        },
        {
            "Number": 51,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15,
                16
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "W06-2302",
            "cite_maker_sids": [
                48
            ],
            "cite_sids": [
                48
            ],
            "cite_text": "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic \u00e2\u20ac\u0153it\u00e2\u20ac\u009d occurrences, and assigns animacy.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "The approach is presented as a knowledge-poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking; it tries to individuate pleonastic \"it\" occurrences and assigns animacy."
        },
        {
            "Number": 52,
            "refer_ID": "P98-2143",
            "refer_sids": [
                15,
                16
            ],
            "refer_text": "With a view to avoiding complex syntactic, seman\u00c2\u00ad tic and discourse analysis (which is vital for real\u00c2\u00ad world applications), we developed a robust, knowl\u00c2\u00ad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "cite_ID": "W09-2411",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "With a view to avoiding complex syntactic, semantic, and discourse analysis (which is vital for real-world applications), we developed a robust, knowledge-poor approach to pronoun resolution which does not parse and analyze the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").",
            "GPT_cite_text": "Some of the limitations of the traditional rule-based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora."
        },
        {
            "Number": 53,
            "refer_ID": "P98-2143",
            "refer_sids": [
                3,
                5
            ],
            "refer_text": "This paper pres\u00c2\u00ad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.",
            "cite_ID": "W99-0104",
            "cite_maker_sids": [
                59
            ],
            "cite_sids": [
                59
            ],
            "cite_text": "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Candidates are assigned scores by each indicator, and the candidate with the highest score is returned as the antecedent.",
            "GPT_cite_text": "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), either by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents."
        },
        {
            "Number": 54,
            "refer_ID": "P98-2143",
            "refer_sids": [
                25
            ],
            "refer_text": "The antecedent indicators have been identi\u00c2\u00ad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\u00c2\u00ad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.",
            "cite_ID": "W99-0104",
            "cite_maker_sids": [
                60
            ],
            "cite_sids": [
                60
            ],
            "cite_text": "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non-prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.",
            "GPT_cite_text": "The CogNIAC algorithm (Baldwin, 1997) uses six heuristic rules to resolve coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, local reiteration or immediate reference)."
        },
        {
            "Number": 55,
            "refer_ID": "P98-2143",
            "refer_sids": [
                98,
                99
            ],
            "refer_text": "In order to evaluate the effectiveness of the ap\u00ad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\u00ad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%.",
            "cite_ID": "W99-0207",
            "cite_maker_sids": [
                128
            ],
            "cite_sids": [
                128,
                129
            ],
            "cite_text": "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "In order to evaluate the effectiveness of the approach and to explore how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antecedent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%.",
            "GPT_cite_text": "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pronouns in English technical manuals to the most recent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the antecedent (cf."
        },
        {
            "Number": 56,
            "refer_ID": "P98-2143",
            "refer_sids": [
                12,
                13
            ],
            "refer_text": "Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\u00ad ora resolution.It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.",
            "cite_ID": "W99-0207",
            "cite_maker_sids": [
                131
            ],
            "cite_sids": [
                131
            ],
            "cite_text": "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00c2\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00c2\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Our work is a continuation of these latest trends in the search for inexpensive, fast, and reliable procedures for anaphora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.",
            "GPT_cite_text": "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expensive in the cost of human effort at development time and limited ability to scale to new domains, more recent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge."
        }
    ],
    "H05-1115": [
        {
            "Number": 1,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.",
            "cite_ID": "P09-1083",
            "cite_maker_sids": [
                60
            ],
            "cite_sids": [
                60
            ],
            "cite_text": "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method.",
            "GPT_cite_text": "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar to the Topic-sensitive LexRank (Otterbacher et al., 2005)."
        },
        {
            "Number": 2,
            "refer_ID": "H05-1115",
            "refer_sids": [
                56
            ],
            "refer_text": "In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula:",
            "cite_ID": "P09-1083",
            "cite_maker_sids": [
                85
            ],
            "cite_sids": [
                85
            ],
            "cite_text": "Its weight twij is calculated by tf \u00b7 idf (Otterbacher et al., 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In topic-sensitive LexRank, we first stem all of the sentences in a set of articles and compute word IDFs by the following formula:",
            "GPT_cite_text": "Its weight term is calculated by tf \u00b7 idf (Otterbacher et al., 2005)."
        },
        {
            "Number": 3,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.",
            "cite_ID": "P09-1083",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method.",
            "GPT_cite_text": "Including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008)."
        },
        {
            "Number": 4,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.",
            "cite_ID": "N06-1027",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41
            ],
            "cite_text": "and sentence retrieval for question answering (Otterbacher et al., 2005).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method.",
            "GPT_cite_text": "And sentence retrieval for question answering (Otterbacher et al., 2005)."
        },
        {
            "Number": 5,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.",
            "cite_ID": "C08-1062",
            "cite_maker_sids": [
                83
            ],
            "cite_sids": [
                83
            ],
            "cite_text": "These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method.",
            "GPT_cite_text": "These algorithms are all based on the query-sensitive LexRank (Otterbacher et al., 2005)."
        },
        {
            "Number": 6,
            "refer_ID": "H05-1115",
            "refer_sids": [
                56
            ],
            "refer_text": "In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.",
            "cite_ID": "C10-2049",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005):quiring a specific strategy.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In topic-sensitive LexRank, we first stem all of the sentences in a set of articles and compute word IDFs by the following formula: idfw = log (N + 1 0.5 + sfw) (1) where N is the total number of sentences in the cluster, and sfw is the number of sentences that the word w appears in.",
            "GPT_cite_text": "Component relevance scores are calculated using Term Frequency \u00d7 Inverse Sentence Frequency (TF\u00d7ISF) (Otterbacher et al., 2005): requiring a specific strategy."
        },
        {
            "Number": 7,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.",
            "cite_ID": "D08-1032",
            "cite_maker_sids": [
                24
            ],
            "cite_sids": [
                24,
                25
            ],
            "cite_text": "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method.",
            "GPT_cite_text": "A topic-sensitive LexRank is proposed in (Otterbacher et al., 2005). As in LexRank, the set of sentences in a document cluster is represented as a graph, where nodes are sentences and links between the nodes are induced by a similarity relation between the sentences. Then the system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question."
        },
        {
            "Number": 8,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36,
                40,
                41,
                46,
                47,
                48,
                49
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.The output of our system, a ranked list of sentences relevant to the user\u2019s question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences.Alternatively, the sentences canbe returned to the user as a question-focused summary.To apply LexRank, a similarity graph is producedfor the sentences in an input document set.In thegraph, each node represents a sentence.There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.",
            "cite_ID": "D08-1032",
            "cite_maker_sids": [
                94
            ],
            "cite_sids": [
                94
            ],
            "cite_text": "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method. The output of our system, a ranked list of sentences relevant to the user\u2019s question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences. Alternatively, the sentences can be returned to the user as a question-focused summary. To apply LexRank, a similarity graph is produced for the sentences in an input document set. In the graph, each node represents a sentence. There are edges between nodes for which the cosine similarity between the respective pair of sentences exceeds a given threshold. The degree of a given node is an indication of how much information the respective sentence has in common with other sentences.",
            "GPT_cite_text": "To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005)."
        },
        {
            "Number": 9,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.",
            "cite_ID": "P08-2003",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method.",
            "GPT_cite_text": "A topic-sensitive LexRank is proposed in (Otterbacher et al., 2005)."
        },
        {
            "Number": 10,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.",
            "cite_ID": "P08-2003",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method.",
            "GPT_cite_text": "To apply LexRank to query-focused contexts, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005)."
        },
        {
            "Number": 11,
            "refer_ID": "H05-1115",
            "refer_sids": [
                36
            ],
            "refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.",
            "cite_ID": "P10-2055",
            "cite_maker_sids": [
                108
            ],
            "cite_sids": [
                104,
                108
            ],
            "cite_text": "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRank method.",
            "GPT_cite_text": "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."
        }
    ],
    "P05-1053": [
        {
            "Number": 2,
            "refer_ID": "P05-1053",
            "refer_sids": [
                193
            ],
            "refer_text": "Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.",
            "cite_ID": "C08-1088",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "However, detailed research (Zhou et al., 2005) shows that it\u00e2\u20ac\u2122s difficult to extract new effective features to further improve the extraction accuracy.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Instead of exploring the full parse tree information directly as in previous related work, we incorporate the base phrase chunking information performance improvement from a syntactic aspect, while the further incorporation of the parse tree and dependency tree information only slightly improves the performance.",
            "GPT_cite_text": "However, detailed research (Zhou et al., 2005) shows that it\u2019s difficult to extract new effective features to further improve the extraction accuracy."
        },
        {
            "Number": 3,
            "refer_ID": "P05-1053",
            "refer_sids": [
                51,
                52
            ],
            "refer_text": "Moreover, we only apply the simple linear kernel, although other kernels can peform better.The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available.",
            "cite_ID": "C08-1088",
            "cite_maker_sids": [
                180
            ],
            "cite_sids": [
                180
            ],
            "cite_text": "Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Moreover, we only apply the simple linear kernel, although other kernels can perform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of-the-art in the machine learning research community, and there are good implementations of the algorithm available.",
            "GPT_cite_text": "Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-art feature-based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al."
        },
        {
            "Number": 5,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "C10-1018",
            "cite_maker_sids": [
                42
            ],
            "cite_sids": [
                42
            ],
            "cite_text": "Most of the features used in our system are based on the work in (Zhou et al., 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "Most of the features used in our system are based on the work of (Zhou et al., 2005)."
        },
        {
            "Number": 6,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "C10-1018",
            "cite_maker_sids": [
                45
            ],
            "cite_sids": [
                45
            ],
            "cite_text": "Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "Due to space limitations, we only describe the collocational features and refer the reader to (Zhou et al., 2005) for the rest of the features."
        },
        {
            "Number": 8,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17,
                20
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.",
            "cite_ID": "D07-1076",
            "cite_maker_sids": [
                14
            ],
            "cite_sids": [
                14
            ],
            "cite_text": "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Evaluation shows that the incorporation of diverse features enables our system to achieve the best reported performance.",
            "GPT_cite_text": "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity-related information to syntactic parse trees, dependency trees, and semantic information."
        },
        {
            "Number": 9,
            "refer_ID": "P05-1053",
            "refer_sids": [
                196,
                197
            ],
            "refer_text": "Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins\u2019 parser used in our system achieves the state-of-the-art performance.Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.",
            "cite_ID": "D07-1076",
            "cite_maker_sids": [
                14
            ],
            "cite_sids": [
                14
            ],
            "cite_text": "How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Second, it is well known that full parsing is always prone to long-distance parsing errors, although the Collins parser used in our system achieves state-of-the-art performance. Therefore, state-of-the-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Prepositional Phrase) attachment.",
            "GPT_cite_text": "However, it is difficult for them to effectively capture structured parse tree information (Zhou et al. 2005), which is critical for further performance improvement in relation extraction."
        },
        {
            "Number": 10,
            "refer_ID": "P05-1053",
            "refer_sids": [
                51,
                52
            ],
            "refer_text": "Moreover, we only apply the simple linear kernel, although other kernels can peform better.The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available.",
            "cite_ID": "D07-1076",
            "cite_maker_sids": [
                161
            ],
            "cite_sids": [
                161
            ],
            "cite_text": "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Moreover, we only apply the simple linear kernel, although other kernels can perform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-of-the-art in the machine learning research community, and there are good implementations of the algorithm available.",
            "GPT_cite_text": "Composite Kernel: In this paper, a composite kernel via polynomial interpolation, as described by Zhang et al. (2006), is applied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al. 2005)."
        },
        {
            "Number": 11,
            "refer_ID": "P05-1053",
            "refer_sids": [
                34
            ],
            "refer_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.",
            "cite_ID": "D07-1076",
            "cite_maker_sids": [
                167
            ],
            "cite_sids": [
                166,
                167
            ],
            "cite_text": "7 Here, we use the same set of flat features (i.e. word,.entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from words, entity types, mention levels, overlap, dependency trees, and parse trees.",
            "GPT_cite_text": "Here, we use the same set of flat features (i.e. word, entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree, and semantic information) as Zhou et al. (2005)."
        },
        {
            "Number": 12,
            "refer_ID": "P05-1053",
            "refer_sids": [
                33
            ],
            "refer_text": "Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.",
            "cite_ID": "D07-1076",
            "cite_maker_sids": [
                177
            ],
            "cite_sids": [
                176,
                177
            ],
            "cite_text": " dependency kernel Zhou et al.(2005)",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Culotta et al. (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved a 63.2 F-measure in relation detection and a 45.8 F-measure in relation classification on the 5 ACE relation types.",
            "GPT_cite_text": "Dependency kernel Zhou et al. (2005)"
        },
        {
            "Number": 14,
            "refer_ID": "P05-1053",
            "refer_sids": [
                60,
                17
            ],
            "refer_text": "For each pair of mentions3, we compute various lexical, syntactic and semantic features.This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "D09-1149",
            "cite_maker_sids": [
                53
            ],
            "cite_sids": [
                53
            ],
            "cite_text": "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each pair of mentions, we compute various lexical, syntactic, and semantic features. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005)."
        },
        {
            "Number": 15,
            "refer_ID": "P05-1053",
            "refer_sids": [
                121,
                122
            ],
            "refer_text": "ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d.It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate.",
            "cite_ID": "D09-1149",
            "cite_maker_sids": [
                81
            ],
            "cite_sids": [
                81
            ],
            "cite_text": "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d. It also shows that the ACE RDC task defines some difficult subtypes such as the subtypes \u201cBased-In\u201d, \u201cLocated\u201d and \u201cResidence\u201d under the type \u201cAT\u201d, which are difficult even for human experts to differentiate.",
            "GPT_cite_text": "It is well known that the number of instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus."
        },
        {
            "Number": 16,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "D12-1074",
            "cite_maker_sids": [
                100
            ],
            "cite_sids": [
                100
            ],
            "cite_text": "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) toward a higher absolute score, but rather to serve as a point of comparison to the models that rely on syntactic information."
        },
        {
            "Number": 17,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "E06-2012",
            "cite_maker_sids": [
                59
            ],
            "cite_sids": [
                59
            ],
            "cite_text": "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al., 2003; Zhou et al., 2005)."
        },
        {
            "Number": 18,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17,
                20
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.",
            "cite_ID": "E12-1020",
            "cite_maker_sids": [
                54
            ],
            "cite_sids": [
                54,
                55
            ],
            "cite_text": "For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Evaluation shows that the incorporation of diverse features enables our system to achieve the best reported performance.",
            "GPT_cite_text": "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have state-of-the-art performance (Sun et al., 2011)."
        },
        {
            "Number": 19,
            "refer_ID": "P05-1053",
            "refer_sids": [
                37,
                40
            ],
            "refer_text": "Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "E12-1020",
            "cite_maker_sids": [
                109
            ],
            "cite_sids": [
                109
            ],
            "cite_text": "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Tree kernel-based approaches proposed by Zelenko et al. (2003) and Culotta et al. (2004) are able to explore the implicit feature space without much feature engineering. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones, and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction."
        },
        {
            "Number": 20,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "E12-1020",
            "cite_maker_sids": [
                225
            ],
            "cite_sids": [
                224,
                225
            ],
            "cite_text": "We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "We use SVM as our learning algorithm with the full feature set from Zhou et al. (2005)."
        },
        {
            "Number": 21,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "I08-1004",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al. 2005), semantic relation extraction (Zhou et al. 2005), and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al. 2004; Luo and Zitouni 2005; Bergsma and Lin 2006)."
        },
        {
            "Number": 22,
            "refer_ID": "P05-1053",
            "refer_sids": [
                193
            ],
            "refer_text": "Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Instead of exploring the full parse tree information directly as in previous related work, we incorporate the base phrase chunking information performance improvement from a syntactic aspect, while the further incorporation of the parse tree and dependency tree information only slightly improves the performance.",
            "GPT_cite_text": "However, it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contribute less to performance improvement."
        },
        {
            "Number": 23,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                31,
                32
            ],
            "cite_text": "Zhou et al.(2005) explore various features in relation extraction using SVM.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "Zhou et al. (2005) explore various features in relation extraction using SVM."
        },
        {
            "Number": 24,
            "refer_ID": "P05-1053",
            "refer_sids": [
                34,
                40
            ],
            "refer_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                36
            ],
            "cite_sids": [
                35,
                36
            ],
            "cite_text": "The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree, and parse tree. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually."
        },
        {
            "Number": 25,
            "refer_ID": "P05-1053",
            "refer_sids": [
                193
            ],
            "refer_text": "Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                38,
                39
            ],
            "cite_text": "Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Instead of exploring the full parse tree information directly as in previous related work, we incorporate the base phrase chunking information performance improvement from a syntactic aspect, while the further incorporation of the parse tree and dependency tree information only slightly improves the performance.",
            "GPT_cite_text": "Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features."
        },
        {
            "Number": 26,
            "refer_ID": "P05-1053",
            "refer_sids": [
                34,
                40
            ],
            "refer_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                136
            ],
            "cite_sids": [
                135,
                136
            ],
            "cite_text": "we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree, and parse tree. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "We call the features used in Zhou et al. (2005) and Kambhatla (2004) a flat feature set."
        },
        {
            "Number": 27,
            "refer_ID": "P05-1053",
            "refer_sids": [
                110
            ],
            "refer_text": "In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.",
            "cite_ID": "N06-1037",
            "cite_maker_sids": [
                137
            ],
            "cite_sids": [
                137
            ],
            "cite_text": "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited numbers.",
            "GPT_cite_text": "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in the annotation of implicit relations and their limited numbers."
        },
        {
            "Number": 28,
            "refer_ID": "P05-1053",
            "refer_sids": [
                34
            ],
            "refer_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.",
            "cite_ID": "N07-1015",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from words, entity types, mention levels, overlap, dependency trees, and parse trees.",
            "GPT_cite_text": "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)."
        },
        {
            "Number": 29,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "N07-1015",
            "cite_maker_sids": [
                34
            ],
            "cite_sids": [
                33,
                34
            ],
            "cite_text": "Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "Zhao and Grishman (2005) and Zhou et al. (2005) explored a large set of features that are potentially useful for relation extraction."
        },
        {
            "Number": 31,
            "refer_ID": "P05-1053",
            "refer_sids": [
                67,
                68
            ],
            "refer_text": "This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1 3 In ACE, each mention has a head annotation and an.extent annotation.",
            "cite_ID": "N07-1015",
            "cite_maker_sids": [
                124
            ],
            "cite_sids": [
                122,
                123,
                124
            ],
            "cite_text": "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: \u2022 WM1: bag-of-words in M1 \u2022 HM1: head word of M1. In ACE, each mention has a head annotation and an extent annotation.",
            "GPT_cite_text": "Bag-of-Words: These features have also been explored by Zhao and Grishman (2005) and Zhou et al. (2005)."
        },
        {
            "Number": 32,
            "refer_ID": "P05-1053",
            "refer_sids": [
                93
            ],
            "refer_text": "This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.",
            "cite_ID": "N07-1015",
            "cite_maker_sids": [
                132
            ],
            "cite_sids": [
                130,
                131,
                132
            ],
            "cite_text": "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This category of features includes information about the words, parts of speech, and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.",
            "GPT_cite_text": "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et al. (2005)."
        },
        {
            "Number": 33,
            "refer_ID": "P05-1053",
            "refer_sids": [
                20
            ],
            "refer_text": "Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.",
            "cite_ID": "N09-3012",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Evaluation shows that the incorporation of diverse features enables our system to achieve the best reported performance.",
            "GPT_cite_text": "Although, in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction."
        },
        {
            "Number": 34,
            "refer_ID": "P05-1053",
            "refer_sids": [
                51
            ],
            "refer_text": "Moreover, we only apply the simple linear kernel, although other kernels can peform better.",
            "cite_ID": "N13-1093",
            "cite_maker_sids": [
                108
            ],
            "cite_sids": [
                107,
                108
            ],
            "cite_text": "These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Moreover, we only apply the simple linear kernel, although other kernels can perform better.",
            "GPT_cite_text": "These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of the proposed KH F kernel, and KH hybrid kernel."
        },
        {
            "Number": 35,
            "refer_ID": "P05-1053",
            "refer_sids": [
                53
            ],
            "refer_text": "In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).",
            "cite_ID": "N13-1093",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                112,
                113
            ],
            "cite_text": "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we use the binary-class SVMLight2 developed by Joachims (1998).",
            "GPT_cite_text": "We also performed 5-fold cross-validation experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005)."
        },
        {
            "Number": 36,
            "refer_ID": "P05-1053",
            "refer_sids": [
                121
            ],
            "refer_text": "ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d.",
            "cite_ID": "P06-1016",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "ACE corpus suffers from a small amount of annotated data for a few subtypes, such as the subtype \u201cFounder\u201d under the type \u201cROLE.\u201d",
            "GPT_cite_text": "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al., 2005)."
        },
        {
            "Number": 37,
            "refer_ID": "P05-1053",
            "refer_sids": [
                53,
                121
            ],
            "refer_text": "In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d.",
            "cite_ID": "P06-1016",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18
            ],
            "cite_text": "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we use the binary-class SVMLight2 developed by Joachims (1998). The ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \u201cFounder\u201d under the type \u201cROLE\u201d.",
            "GPT_cite_text": "While various machine learning approaches, such as generative modeling (Miller et al. 2000), maximum entropy (Kambhatla 2004), and support vector machines (Zhao and Grisman 2005; Zhou et al. 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations."
        },
        {
            "Number": 39,
            "refer_ID": "P05-1053",
            "refer_sids": [
                130
            ],
            "refer_text": "It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.",
            "cite_ID": "P06-1016",
            "cite_maker_sids": [
                45
            ],
            "cite_sids": [
                45
            ],
            "cite_text": "Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "It shows that our system achieves the best performance of 63.1%/49.5%/55.5% in precision/recall/F-measure when combining diverse lexical, syntactic, and semantic features.",
            "GPT_cite_text": "Zhou et al. (2005) further systematically explored diverse lexical, syntactic, and semantic features through support vector machines and achieved an F-measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus, respectively."
        },
        {
            "Number": 40,
            "refer_ID": "P05-1053",
            "refer_sids": [
                110,
                58,
                59
            ],
            "refer_text": "In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.The semantic relation is determined between two mentions.In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.",
            "cite_ID": "P06-1016",
            "cite_maker_sids": [
                128
            ],
            "cite_sids": [
                128
            ],
            "cite_text": "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g., M1-Parent-Of-M2 vs. M2-Parent-Of-M1.",
            "GPT_cite_text": "Same as Zhou et al. (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved."
        },
        {
            "Number": 43,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "P06-1017",
            "cite_maker_sids": [
                207
            ],
            "cite_sids": [
                206,
                207
            ],
            "cite_text": "In the future, we would like to use more effective feature sets Zhou et al.(2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "In the future, we would like to use more effective feature sets (Zhou et al., 2005)."
        },
        {
            "Number": 45,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "P06-1104",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                39
            ],
            "cite_text": "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic, and semantic features."
        },
        {
            "Number": 47,
            "refer_ID": "P05-1053",
            "refer_sids": [
                18,
                155
            ],
            "refer_text": "Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.",
            "cite_ID": "P08-2023",
            "cite_maker_sids": [
                29
            ],
            "cite_sids": [
                29,
                30
            ],
            "cite_text": "Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.",
            "label": [
                "Results_Citation",
                "Implication_Citation"
            ],
            "GPT_refer_text": "Our study illustrates that the base phrase chunking information contributes to most of the performance improvement from a syntactic aspect, while additional full parsing information does not contribute much, largely due to the fact that most of the relations defined in the ACE corpus are within a very short distance. This is largely due to the incorporation of two semantic resources, i.e., the country name list and the personal relative trigger word list.",
            "GPT_cite_text": "Based on his work, Zhou et al. (2005) further incorporated the base phrase chunking information and semi-automatically collected a country name list and a personal relative trigger word list."
        },
        {
            "Number": 48,
            "refer_ID": "P05-1053",
            "refer_sids": [
                18
            ],
            "refer_text": "Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.",
            "cite_ID": "P09-1113",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                39
            ],
            "cite_text": "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Our study illustrates that the base phrase chunking information contributes to most of the performance improvement from a syntactic aspect, while additional full parsing information does not contribute much, largely due to the fact that most of the relations defined in the ACE corpus are within a very short distance.",
            "GPT_cite_text": "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE."
        },
        {
            "Number": 49,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "P09-1113",
            "cite_maker_sids": [
                52
            ],
            "cite_sids": [
                50,
                51,
                52,
                53
            ],
            "cite_text": "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al.(2005), and work in the ACE paradigm such as Zhou et al.(2005) and Zhou et al.(2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007)."
        },
        {
            "Number": 50,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "P09-1114",
            "cite_maker_sids": [
                25
            ],
            "cite_sids": [
                23,
                24,
                25
            ],
            "cite_text": "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction."
        },
        {
            "Number": 51,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17,
                20
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.",
            "cite_ID": "P09-1114",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Evaluation shows that the incorporation of diverse features enables our system to achieve the best reported performance.",
            "GPT_cite_text": "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and S\u00f8rensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008)."
        },
        {
            "Number": 52,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "P11-1053",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "Then the feature-based method explicitly extracts a variety of lexical, syntactic, and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007)."
        },
        {
            "Number": 53,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40,
                52
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.The reason why we choose SVMs for this purpose is that SVMs represent the state-of\u2013the-art in the machine learning research community, and there are good implementations of the algorithm available.",
            "cite_ID": "P11-1053",
            "cite_maker_sids": [
                81
            ],
            "cite_sids": [
                80,
                81
            ],
            "cite_text": "We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information. The reason why we choose SVMs for this purpose is that SVMs represent the state-of-the-art in the machine learning research community, and there are good implementations of the algorithm available.",
            "GPT_cite_text": "We first adopted the full feature set from Zhou et al. (2005), a state-of-the-art feature-based relation extraction system."
        },
        {
            "Number": 54,
            "refer_ID": "P05-1053",
            "refer_sids": [
                108
            ],
            "refer_text": "This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.",
            "cite_ID": "P11-1053",
            "cite_maker_sids": [
                193
            ],
            "cite_sids": [
                193,
                194
            ],
            "cite_text": "Zhou et al.(2005) tested their system on the ACE 2003 data;.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper uses the ACE corpus provided by the LDC to train and evaluate our feature-based relation extraction system.",
            "GPT_cite_text": "Zhou et al. (2005) tested their system on the ACE 2003 data."
        },
        {
            "Number": 55,
            "refer_ID": "P05-1053",
            "refer_sids": [
                59
            ],
            "refer_text": "In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.",
            "cite_ID": "P11-1056",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "However, most approaches to RE have assumed that the relations\u00e2\u20ac\u2122 arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g., M1-Parent-of-M2 vs. M2-Parent-of-M1.",
            "GPT_cite_text": "However, most approaches to RE have assumed that the relations' arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem."
        },
        {
            "Number": 56,
            "refer_ID": "P05-1053",
            "refer_sids": [
                128
            ],
            "refer_text": "In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.",
            "cite_ID": "P11-1056",
            "cite_maker_sids": [
                58
            ],
            "cite_sids": [
                58
            ],
            "cite_text": "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we only measure the performance of relation extraction on \u201ctrue\u201d mentions with \u201ctrue\u201d chaining of coreference (i.e., as annotated by the corpus annotators) in the ACE corpus.",
            "GPT_cite_text": "Most prior RE evaluations on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005)."
        },
        {
            "Number": 57,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "cite_ID": "P11-3012",
            "cite_maker_sids": [
                34
            ],
            "cite_sids": [
                34
            ],
            "cite_text": "We used Zhou et al.\u00e2\u20ac\u2122s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).",
            "GPT_cite_text": "We used Zhou et al.'s lexical features (Zhou et al., 2005) as the basis for the features of our system, similar to what other researchers have done (Chan and Roth, 2010)."
        },
        {
            "Number": 58,
            "refer_ID": "P05-1053",
            "refer_sids": [
                130
            ],
            "refer_text": "It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.",
            "cite_ID": "P11-3012",
            "cite_maker_sids": [
                47
            ],
            "cite_sids": [
                47
            ],
            "cite_text": "Although a bit lower than Zhou et al.\u00e2\u20ac\u2122s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "It shows that our system achieves the best performance of 63.1%/49.5%/55.5% in precision/recall/F-measure when combining diverse lexical, syntactic, and semantic features.",
            "GPT_cite_text": "Although a bit lower than Zhou et al.'s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and not having used the semantic information features."
        },
        {
            "Number": 59,
            "refer_ID": "P05-1053",
            "refer_sids": [
                17,
                126
            ],
            "refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related.",
            "cite_ID": "P13-1147",
            "cite_maker_sids": [
                167
            ],
            "cite_sids": [
                167
            ],
            "cite_text": "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a \u201cNONE\u201d class for the case where the two mentions are not related.",
            "GPT_cite_text": "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e., a phrase chunker, dependency parser, and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance; they use 40)."
        },
        {
            "Number": 60,
            "refer_ID": "P05-1053",
            "refer_sids": [
                53
            ],
            "refer_text": "In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).",
            "cite_ID": "W06-1634",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we use the binary-class SVMLight2 developed by Joachims (1998).",
            "GPT_cite_text": "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted information extraction."
        },
        {
            "Number": 61,
            "refer_ID": "P05-1053",
            "refer_sids": [
                4
            ],
            "refer_text": "This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.",
            "cite_ID": "W06-1634",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "This suggests that most of the useful information in full parse trees for relation extraction is shallow and can be captured by chunking.",
            "GPT_cite_text": "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005)."
        },
        {
            "Number": 63,
            "refer_ID": "P05-1053",
            "refer_sids": [
                193
            ],
            "refer_text": "Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.",
            "cite_ID": "W06-1667",
            "cite_maker_sids": [
                174
            ],
            "cite_sids": [
                174
            ],
            "cite_text": "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Instead of exploring the full parse tree information directly as in previous related work, we incorporate the base phrase chunking information performance improvement from a syntactic aspect, while the further incorporation of the parse tree and dependency tree information only slightly improves the performance.",
            "GPT_cite_text": "Especially, although we did not consider the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, can still provide complementary information for capturing the characteristics of entity pairs."
        },
        {
            "Number": 64,
            "refer_ID": "P05-1053",
            "refer_sids": [
                30
            ],
            "refer_text": "The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.",
            "cite_ID": "W08-0602",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                39
            ],
            "cite_text": "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The relation extraction task was formulated at the 7th Message Understanding Conference (MUC-7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.",
            "GPT_cite_text": "This follows on from the success of these methods in general NLP (see, for example, Zhou et al. (2005))."
        },
        {
            "Number": 65,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "W08-0602",
            "cite_maker_sids": [
                136
            ],
            "cite_sids": [
                136
            ],
            "cite_text": "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "We use features developed in part from those described in Zhou et al. (2005) and Wang et al. (2006)."
        },
        {
            "Number": 66,
            "refer_ID": "P05-1053",
            "refer_sids": [
                40
            ],
            "refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.",
            "cite_ID": "W11-1101",
            "cite_maker_sids": [
                203
            ],
            "cite_sids": [
                203
            ],
            "cite_text": "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic, and semantic information.",
            "GPT_cite_text": "A variety of features has been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000)."
        },
        {
            "Number": 68,
            "refer_ID": "P05-1053",
            "refer_sids": [
                12
            ],
            "refer_text": "Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.).",
            "cite_ID": "W11-1815",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "Entities can be of five types: persons, organizations, locations, facilities, and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g., countries, states, cities, etc.).",
            "GPT_cite_text": "BB task example: Ureaplasma parvum is a mycoplasma and poses pathogenic biological challenges (Kim et al., 2010) in geographical locations (Zhou et al., 2005)."
        }
    ],
    "P05-1004": [
        {
            "Number": 1,
            "refer_ID": "P05-1004",
            "refer_sids": [
                2
            ],
            "refer_text": "Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.",
            "cite_ID": "C10-2101",
            "cite_maker_sids": [
                74
            ],
            "cite_sids": [
                74
            ],
            "cite_text": "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organize their manual insertion into WORDNET.",
            "GPT_cite_text": "Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006)."
        },
        {
            "Number": 3,
            "refer_ID": "P05-1004",
            "refer_sids": [
                147
            ],
            "refer_text": "Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.",
            "cite_ID": "J07-4005",
            "cite_maker_sids": [
                229
            ],
            "cite_sids": [
                229
            ],
            "cite_text": "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our approach uses voting across the known supersenses of automatically extracted synonyms to select a supersense for the unknown nouns.",
            "GPT_cite_text": "Although we could adapt our method for use with an automatically induced inventory, our method, which uses WordNet, might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns."
        },
        {
            "Number": 4,
            "refer_ID": "P05-1004",
            "refer_sids": [
                13
            ],
            "refer_text": "Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.",
            "cite_ID": "J09-3004",
            "cite_maker_sids": [
                446
            ],
            "cite_sids": [
                446
            ],
            "cite_text": "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Some specialist topics are better covered in WORDNET than others, e.g., dog has finer-grained distinctions than cat and worm, although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.",
            "GPT_cite_text": "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005) and typically overlaps to a rather limited extent with the output of automatic acquisition methods."
        },
        {
            "Number": 5,
            "refer_ID": "P05-1004",
            "refer_sids": [
                147
            ],
            "refer_text": "Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.",
            "cite_ID": "N06-1017",
            "cite_maker_sids": [
                26
            ],
            "cite_sids": [
                26
            ],
            "cite_text": "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our approach uses voting across the known supersenses of automatically extracted synonyms to select a supersense for the unknown nouns.",
            "GPT_cite_text": "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection."
        },
        {
            "Number": 6,
            "refer_ID": "P05-1004",
            "refer_sids": [
                228
            ],
            "refer_text": "Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor.",
            "cite_ID": "N06-1017",
            "cite_maker_sids": [
                189
            ],
            "cite_sids": [
                189
            ],
            "cite_text": "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Each synonym votes for each of its supersenses from WordNet 1.6 using the similarity score from our synonym extractor.",
            "GPT_cite_text": "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses."
        },
        {
            "Number": 7,
            "refer_ID": "P05-1004",
            "refer_sids": [
                231
            ],
            "refer_text": "Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.",
            "cite_ID": "N07-1024",
            "cite_maker_sids": [
                83
            ],
            "cite_sids": [
                83
            ],
            "cite_text": "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense-tag.",
            "GPT_cite_text": "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ciaramita 2003; Curran 2005)"
        },
        {
            "Number": 8,
            "refer_ID": "P05-1004",
            "refer_sids": [
                20
            ],
            "refer_text": "Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u00e2\u20ac\u2122s hierarchical structure to create many annotated training instances from the synset glosses.",
            "cite_ID": "P12-2050",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00c3\u0178 and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET's hierarchical structure to create many annotated training instances from the synset glosses.",
            "GPT_cite_text": "More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa\u00df and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet. In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon. In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1."
        },
        {
            "Number": 9,
            "refer_ID": "P05-1004",
            "refer_sids": [
                65
            ],
            "refer_text": "Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.",
            "cite_ID": "S07-1032",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "Thus, some research has been focused on deriving different sense groupings to overcome the fine\u00e2\u20ac\u201c grained distinctions of WN (Hearst and Schu\u00c2\u00a8 tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Ciaramita and Johnson (2003) implement a super-sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocations, spelling, and syntactic features common in WSD and named entity recognition systems.",
            "GPT_cite_text": "Thus, some research has been focused on deriving different sense groupings to overcome the fine-grained distinctions of WN (Hearst and Sch\u00fctze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006)."
        },
        {
            "Number": 10,
            "refer_ID": "P05-1004",
            "refer_sids": [
                65
            ],
            "refer_text": "Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.",
            "cite_ID": "S10-1090",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Ciaramita and Johnson (2003) implement a super-sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocations, spelling, and syntactic features common in WSD and named entity recognition systems.",
            "GPT_cite_text": "In contrast, some research has been focused on using predefined sets of sense groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005), and (Ciaramita and Altun, 2006)."
        },
        {
            "Number": 11,
            "refer_ID": "P05-1004",
            "refer_sids": [
                87
            ],
            "refer_text": "Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.",
            "cite_ID": "S12-1011",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Similarity vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.",
            "GPT_cite_text": "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005)."
        },
        {
            "Number": 12,
            "refer_ID": "P05-1004",
            "refer_sids": [
                20
            ],
            "refer_text": "Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u00e2\u20ac\u2122s hierarchical structure to create many annotated training instances from the synset glosses.",
            "cite_ID": "S12-1011",
            "cite_maker_sids": [
                50
            ],
            "cite_sids": [
                50
            ],
            "cite_text": "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model\u00e2\u20ac\u2122s ability to cluster words by their semantics.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET's hierarchical structure to create many annotated training instances from the synset glosses.",
            "GPT_cite_text": "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model's ability to cluster words by their semantics."
        },
        {
            "Number": 13,
            "refer_ID": "P05-1004",
            "refer_sids": [
                65
            ],
            "refer_text": "Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.",
            "cite_ID": "S12-1023",
            "cite_maker_sids": [
                234
            ],
            "cite_sids": [
                234
            ],
            "cite_text": "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Ciaramita and Johnson (2003) implement a super-sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocations, spelling, and syntactic features common in WSD and named entity recognition systems.",
            "GPT_cite_text": "A concept analogous to our notion of meta-sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well."
        },
        {
            "Number": 14,
            "refer_ID": "P05-1004",
            "refer_sids": [
                20
            ],
            "refer_text": "Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET\u00e2\u20ac\u2122s hierarchical structure to create many annotated training instances from the synset glosses.",
            "cite_ID": "W06-1670",
            "cite_maker_sids": [
                94
            ],
            "cite_sids": [
                94
            ],
            "cite_text": "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET's hierarchical structure to create many annotated training instances from the synset glosses.",
            "GPT_cite_text": "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word-type classification rather than tagging."
        }
    ],
    "N06-2049": [
        {
            "Number": 1,
            "refer_ID": "N06-2049",
            "refer_sids": [
                25,
                104
            ],
            "refer_text": "It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.",
            "cite_ID": "D13-1031",
            "cite_maker_sids": [
                274
            ],
            "cite_sids": [
                273,
                274
            ],
            "cite_text": "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword-based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary-based approach.",
            "GPT_cite_text": "CRF + Rule system represents a combination of the CRF model and the rule-based model presented in Zhang et al. (2006)."
        },
        {
            "Number": 2,
            "refer_ID": "N06-2049",
            "refer_sids": [
                3
            ],
            "refer_text": "In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.",
            "cite_ID": "I08-4009",
            "cite_maker_sids": [
                19
            ],
            "cite_sids": [
                19
            ],
            "cite_text": "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.",
            "GPT_cite_text": "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005; Zhang et al., 2006a)."
        },
        {
            "Number": 3,
            "refer_ID": "N06-2049",
            "refer_sids": [
                58,
                59
            ],
            "refer_text": "We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.",
            "cite_ID": "I08-4009",
            "cite_maker_sids": [
                20
            ],
            "cite_sids": [
                20
            ],
            "cite_text": "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We define a confidence measure, CM(tiob | w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary-based word segmentation.",
            "GPT_cite_text": "Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006)."
        },
        {
            "Number": 4,
            "refer_ID": "N06-2049",
            "refer_sids": [
                104
            ],
            "refer_text": "Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.",
            "cite_ID": "I08-4015",
            "cite_maker_sids": [
                36
            ],
            "cite_sids": [
                36
            ],
            "cite_text": "After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary-based approach.",
            "GPT_cite_text": "After we get word-based segmentation results, we use them to revise the CRF tagging result similar to (Zhang et al., 2006)."
        },
        {
            "Number": 5,
            "refer_ID": "N06-2049",
            "refer_sids": [
                66,
                67
            ],
            "refer_text": "A confidence measure threshold, t, was defined for making a decision based on the value.If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.",
            "cite_ID": "I08-4030",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.",
            "GPT_cite_text": "If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006)."
        },
        {
            "Number": 6,
            "refer_ID": "N06-2049",
            "refer_sids": [
                66,
                67,
                104,
                105
            ],
            "refer_text": "A confidence measure threshold, t, was defined for making a decision based on the value.If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.However, the R-iv rates were getting worse in return for higher R-oov rates.",
            "cite_ID": "I08-4030",
            "cite_maker_sids": [
                51
            ],
            "cite_sids": [
                51
            ],
            "cite_text": "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary-based approach. However, the R-iv rates were getting worse in return for higher R-oov rates.",
            "GPT_cite_text": "According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Entropy performs well on IV words, so a model combining the advantages of these two methods is appealing."
        },
        {
            "Number": 7,
            "refer_ID": "N06-2049",
            "refer_sids": [
                8
            ],
            "refer_text": "In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "cite_ID": "J11-1005",
            "cite_maker_sids": [
                277
            ],
            "cite_sids": [
                277
            ],
            "cite_text": "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we propose a subword-based IOB tagging system, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "GPT_cite_text": "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the subword-based model of Zhang, Kikui, and Sumita (2006) for comparison."
        },
        {
            "Number": 8,
            "refer_ID": "N06-2049",
            "refer_sids": [
                25
            ],
            "refer_text": "It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.",
            "cite_ID": "N09-1007",
            "cite_maker_sids": [
                134
            ],
            "cite_sids": [
                134
            ],
            "cite_text": "Z06-a and Z06-b represents the pure sub- word CRF model and the con\ufb01dence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword-based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.",
            "GPT_cite_text": "Z06-a and Z06-b represent the pure sub-word CRF model and the confidence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);"
        },
        {
            "Number": 9,
            "refer_ID": "N06-2049",
            "refer_sids": [
                8
            ],
            "refer_text": "In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "cite_ID": "P07-1106",
            "cite_maker_sids": [
                94
            ],
            "cite_sids": [
                93,
                94
            ],
            "cite_text": "One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we propose a subword-based IOB tagging system, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "GPT_cite_text": "One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model."
        },
        {
            "Number": 10,
            "refer_ID": "N06-2049",
            "refer_sids": [
                8
            ],
            "refer_text": "In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "cite_ID": "P07-1106",
            "cite_maker_sids": [
                172
            ],
            "cite_sids": [
                172,
                173
            ],
            "cite_text": "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we propose a subword-based IOB tagging system, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "GPT_cite_text": "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the subword-based model of Zhang et al. (2006) for comparison."
        },
        {
            "Number": 11,
            "refer_ID": "N06-2049",
            "refer_sids": [
                25
            ],
            "refer_text": "It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.",
            "cite_ID": "P12-1027",
            "cite_maker_sids": [
                189
            ],
            "cite_sids": [
                188,
                189
            ],
            "cite_text": "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword-based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.",
            "GPT_cite_text": "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents a confidence-based combination of CRF and rule-based models, presented in Zhang et al. (2006)."
        },
        {
            "Number": 12,
            "refer_ID": "N06-2049",
            "refer_sids": [
                8
            ],
            "refer_text": "In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "cite_ID": "W06-0118",
            "cite_maker_sids": [
                14
            ],
            "cite_sids": [
                14
            ],
            "cite_text": "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we propose a subword-based IOB tagging system, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "GPT_cite_text": "Also, the CRF model uses maximum subword-based tagging (Zhang et al., 2006)."
        },
        {
            "Number": 13,
            "refer_ID": "N06-2049",
            "refer_sids": [
                8,
                153,
                72
            ],
            "refer_text": "In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.We also successfully employed the confidence measure to make a confidence-dependent word segmentation.We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.",
            "cite_ID": "W06-0118",
            "cite_maker_sids": [
                24
            ],
            "cite_sids": [
                24
            ],
            "cite_text": "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. We also successfully employed the confidence measure to make a confidence-dependent word segmentation. We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.",
            "GPT_cite_text": "Recently, Zhang et al. (2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach, which obtains a very high accuracy on the shared task data from previous SIGHAN competitions."
        },
        {
            "Number": 14,
            "refer_ID": "N06-2049",
            "refer_sids": [
                151,
                72
            ],
            "refer_text": "In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.",
            "cite_ID": "W08-0335",
            "cite_maker_sids": [
                55
            ],
            "cite_sids": [
                55,
                56
            ],
            "cite_text": "Part of the work using this tool was described by (Zhang et al., 2006).The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.",
            "GPT_cite_text": "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff."
        },
        {
            "Number": 15,
            "refer_ID": "N06-2049",
            "refer_sids": [
                78,
                79
            ],
            "refer_text": "For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.Tri- gram LMs were generated using the SRI LM toolkit for disambiguation.",
            "cite_ID": "W08-0335",
            "cite_maker_sids": [
                160
            ],
            "cite_sids": [
                160
            ],
            "cite_text": "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \u00e2\u20ac\u0153dict-hybrid.\u00e2\u20ac\u009d (Zhang et al., 2006) We used the \u00e2\u20ac\u0153dict-hybrid\u00e2\u20ac\u009d to segment the SMT training corpus and test data.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri-gram LMs were generated using the SRI LM toolkit for disambiguation.",
            "GPT_cite_text": "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the \"dict-hybrid.\" (Zhang et al., 2006) We used the \"dict-hybrid\" to segment the SMT training corpus and test data."
        },
        {
            "Number": 16,
            "refer_ID": "N06-2049",
            "refer_sids": [
                8
            ],
            "refer_text": "In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "cite_ID": "W10-4128",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we propose a subword-based IOB tagging system, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "GPT_cite_text": "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literature (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focuses on employing lexical words or subwords as tagging units."
        },
        {
            "Number": 17,
            "refer_ID": "N06-2049",
            "refer_sids": [
                1,
                2
            ],
            "refer_text": "We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.",
            "cite_ID": "W10-4135",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "We proposed two approaches to improve Chinese word segmentation: a subword-based tagging approach and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.",
            "GPT_cite_text": "For this purpose, our system is based on a combination of subword-based tagging methods (Zhang et al., 2006) and accessory variety-based new word recognition methods (Feng et al., 2004)."
        },
        {
            "Number": 18,
            "refer_ID": "N06-2049",
            "refer_sids": [
                8
            ],
            "refer_text": "In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "cite_ID": "W10-4135",
            "cite_maker_sids": [
                25
            ],
            "cite_sids": [
                25
            ],
            "cite_text": "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we propose a subword-based IOB tagging system, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.",
            "GPT_cite_text": "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter The procedure for constructing a subword list is similar to the one used in (Zhang et al., 2006)."
        },
        {
            "Number": 19,
            "refer_ID": "N06-2049",
            "refer_sids": [
                8,
                9
            ],
            "refer_text": "In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.",
            "cite_ID": "W10-4135",
            "cite_maker_sids": [
                30
            ],
            "cite_sids": [
                30
            ],
            "cite_text": "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, we propose a subword-based IOB tagging system, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded to a character-based one.",
            "GPT_cite_text": "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)."
        },
        {
            "Number": 20,
            "refer_ID": "N06-2049",
            "refer_sids": [
                38,
                39
            ],
            "refer_text": "For a character-based IOB tagger, there is only one possibility of re-segmentation.However, there are multiple choices for a subword-based IOB tagger.",
            "cite_ID": "W10-4138",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                21,
                22,
                23
            ],
            "cite_text": "Thus, the bigram \u00e2\u20ac\u0153RAIL ENQUIRIES\u00e2\u20ac\u009d gives a misleading probability that \u00e2\u20ac\u0153RAIL\u00e2\u20ac\u009d is followed by \u00e2\u20ac\u0153ENQUIRIES\u00e2\u20ac\u009d irrespective of what precedes it.This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "For a character-based IOB tagger, there is only one possibility of re-segmentation. However, there are multiple choices for a subword-based IOB tagger.",
            "GPT_cite_text": "Thus, the bigram \u201cRAIL ENQUIRIES\u201d gives a misleading probability that \u201cRAIL\u201d is followed by \u201cENQUIRIES\u201d irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N-grams still appear; therefore, corresponding solutions such as those of Zhang et al. (2006) were proposed."
        }
    ],
    "D10-1083": [
        {
            "Number": 1,
            "refer_ID": "D10-1083",
            "refer_sids": [
                99
            ],
            "refer_text": "TheFigure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration",
            "cite_ID": "D11-1056",
            "cite_maker_sids": [
                264
            ],
            "cite_sids": [
                263,
                264
            ],
            "cite_text": "Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration.",
            "GPT_cite_text": "Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the F-score, in addition to inferred values."
        },
        {
            "Number": 3,
            "refer_ID": "D10-1083",
            "refer_sids": [
                22
            ],
            "refer_text": "In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                10,
                11
            ],
            "cite_text": "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "In this way, we restrict the parameterization of a language: original case English, Danish, Dutch, German, Spanish, Swedish, Portuguese. Table 1: Upper bound on tagging accuracy, assuming each word type is assigned to the majority POS tag.",
            "GPT_cite_text": "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al. (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages."
        },
        {
            "Number": 4,
            "refer_ID": "D10-1083",
            "refer_sids": [
                239
            ],
            "refer_text": "Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                16,
                17
            ],
            "cite_text": "More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers, which employ more sophisticated learning mechanisms to exploit similar constraints.",
            "GPT_cite_text": "More recently, Lee et al. (2010) presented a new type-based model and also reported very good results."
        },
        {
            "Number": 6,
            "refer_ID": "D10-1083",
            "refer_sids": [
                52
            ],
            "refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "GPT_cite_text": "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token-based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features."
        },
        {
            "Number": 7,
            "refer_ID": "D10-1083",
            "refer_sids": [
                112
            ],
            "refer_text": "For all languages we do not make use of a tagging dictionary.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                91
            ],
            "cite_sids": [
                90,
                91
            ],
            "cite_text": "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For all languages, we do not make use of a tagging dictionary.",
            "GPT_cite_text": "One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al. (2010)."
        },
        {
            "Number": 8,
            "refer_ID": "D10-1083",
            "refer_sids": [
                6
            ],
            "refer_text": "Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                130
            ],
            "cite_sids": [
                129,
                130
            ],
            "cite_text": "Following Lee et al.(2010) we used only the training sections for each language.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.",
            "GPT_cite_text": "Following Lee et al. (2010), we used only the training sections for each language."
        },
        {
            "Number": 9,
            "refer_ID": "D10-1083",
            "refer_sids": [
                9
            ],
            "refer_text": "Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.",
            "cite_ID": "D12-1086",
            "cite_maker_sids": [
                74
            ],
            "cite_sids": [
                74
            ],
            "cite_text": "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.",
            "GPT_cite_text": "Given that close to 95% of the word occurrences in human-labeled data are tagged with their most frequent part of speech (Lee et al., 2010)"
        },
        {
            "Number": 10,
            "refer_ID": "D10-1083",
            "refer_sids": [
                52
            ],
            "refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "cite_ID": "D12-1125",
            "cite_maker_sids": [
                213
            ],
            "cite_sids": [
                213
            ],
            "cite_text": "vised POS induction algorithm (Lee et al., 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "GPT_cite_text": "Revised POS induction algorithm (Lee et al., 2010)"
        },
        {
            "Number": 11,
            "refer_ID": "D10-1083",
            "refer_sids": [
                52
            ],
            "refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "cite_ID": "D12-1127",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00c2\u00b8a et al., 2011; Lee et al., 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "GPT_cite_text": "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems and is not suitable for most applications (Berg-Kirkpatrick et al., 2010; Gra\u00e7a et al., 2011; Lee et al., 2010)."
        },
        {
            "Number": 12,
            "refer_ID": "D10-1083",
            "refer_sids": [
                243
            ],
            "refer_text": "We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.",
            "cite_ID": "D13-1004",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.",
            "GPT_cite_text": "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)."
        },
        {
            "Number": 13,
            "refer_ID": "D10-1083",
            "refer_sids": [
                243
            ],
            "refer_text": "We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.",
            "cite_ID": "N12-1045",
            "cite_maker_sids": [
                14
            ],
            "cite_sids": [
                14
            ],
            "cite_text": "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.",
            "GPT_cite_text": "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)."
        },
        {
            "Number": 14,
            "refer_ID": "D10-1083",
            "refer_sids": [
                27,
                85,
                97
            ],
            "refer_text": "First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution \u03c8 over tag assignments drawn from DIRICHLET(\u03b2, K ).During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior:",
            "cite_ID": "P11-1087",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                40,
                41,
                42,
                43
            ],
            "cite_text": "Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)\u00e2\u20ac\u2122s one-class HMM.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type-level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns). Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution \u03c8 over tag assignments drawn from DIRICHLET(\u03b2, K). During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior:",
            "GPT_cite_text": "Recently, Lee et al. (2010) combined the one-class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However, this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model that underperformed Brown et al. (1992)\u2019s one-class HMM."
        },
        {
            "Number": 15,
            "refer_ID": "D10-1083",
            "refer_sids": [
                155
            ],
            "refer_text": "5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5).",
            "cite_ID": "P11-1087",
            "cite_maker_sids": [
                153
            ],
            "cite_sids": [
                152,
                153,
                154
            ],
            "cite_text": "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5).",
            "GPT_cite_text": "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al. (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in an accuracy of 66.4%."
        },
        {
            "Number": 16,
            "refer_ID": "D10-1083",
            "refer_sids": [
                236
            ],
            "refer_text": "We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.",
            "cite_ID": "P13-1150",
            "cite_maker_sids": [
                55
            ],
            "cite_sids": [
                55
            ],
            "cite_text": "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have presented a method for unsupervised part-of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.",
            "GPT_cite_text": "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)."
        },
        {
            "Number": 17,
            "refer_ID": "D10-1083",
            "refer_sids": [
                20,
                21
            ],
            "refer_text": "The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.",
            "cite_ID": "W11-0301",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102
            ],
            "cite_text": "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token-level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.",
            "GPT_cite_text": "Here, W_t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W_t which are generated earlier (Lee et al., 2010)."
        },
        {
            "Number": 18,
            "refer_ID": "D10-1083",
            "refer_sids": [
                236
            ],
            "refer_text": "We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.",
            "cite_ID": "W12-1914",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                7,
                8
            ],
            "cite_text": "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "We have presented a method for unsupervised part-of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.",
            "GPT_cite_text": "Second, learning categories has been cast as an unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al."
        }
    ],
    "P07-1040": [
        {
            "Number": 1,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "C08-1014",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                16,
                17
            ],
            "cite_text": "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b)."
        },
        {
            "Number": 2,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "C08-1014",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "Confusion networks and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b)."
        },
        {
            "Number": 3,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "C08-1014",
            "cite_maker_sids": [
                92
            ],
            "cite_sids": [
                88,
                89,
                90,
                91,
                92
            ],
            "cite_text": "Bangalore et al.(2001), Sim et al.(2007), Rosti et al.(2007a), and Rosti et al.(2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "Bangalore et al. (2001), Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) chose the hypothesis that best agrees with other hypotheses on average as the skeleton."
        },
        {
            "Number": 4,
            "refer_ID": "P07-1040",
            "refer_sids": [
                89
            ],
            "refer_text": "The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems.",
            "cite_ID": "C08-1014",
            "cite_maker_sids": [
                97
            ],
            "cite_sids": [
                93,
                94,
                95,
                96,
                97
            ],
            "cite_text": "Bangalore et al.(2001) used a WER based alignment and Sim et al. (2007), Rosti et al.(2007a), and Rosti et al.(2007b) used minimum Translation Error Rate (TER) based alignment to build the confusion network.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4), where n is the number of systems.",
            "GPT_cite_text": "Bangalore et al. (2001) used a WER-based alignment, and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER)-based alignment to build the confusion network."
        },
        {
            "Number": 5,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "D09-1115",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "In recent years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks."
        },
        {
            "Number": 6,
            "refer_ID": "P07-1040",
            "refer_sids": [
                124,
                125,
                126
            ],
            "refer_text": "All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs.The final arcs have a probability of one.",
            "cite_ID": "D09-1115",
            "cite_maker_sids": [
                133,
                134
            ],
            "cite_sids": [
                133,
                134
            ],
            "cite_text": "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "All confusion networks are connected to a single start node with NULL arcs that contain the prior probability from the system used as the skeleton for that network. All confusion networks are connected to a common end node with NULL arcs. The final arcs have a probability of one.",
            "GPT_cite_text": "While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by the lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word, and the number of all non-null words."
        },
        {
            "Number": 7,
            "refer_ID": "P07-1040",
            "refer_sids": [
                80,
                81,
                82
            ],
            "refer_text": "Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.The other hypotheses are aligned against the skeleton.Either votes or some form of confidences are assigned to each word in the network.",
            "cite_ID": "D09-1115",
            "cite_maker_sids": [
                148
            ],
            "cite_sids": [
                147,
                148
            ],
            "cite_text": "Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Confusion network decoding in MT has to pick one hypothesis as the skeleton, which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidence is assigned to each word in the network.",
            "GPT_cite_text": "Each arc has different confidences associated with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008)."
        },
        {
            "Number": 8,
            "refer_ID": "P07-1040",
            "refer_sids": [
                119,
                120,
                121
            ],
            "refer_text": "On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.This -best list is then re-scored with the higher order -gram.The second set of weights is used to find the final -best from the re-scored -best list.",
            "cite_ID": "N09-2003",
            "cite_maker_sids": [
                24,
                25
            ],
            "cite_sids": [
                24,
                25
            ],
            "cite_text": "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "On a test set, the first set of weights is used to generate an n-best list from the bi-gram expanded lattice. This n-best list is then re-scored with the higher order n-gram. The second set of weights is used to find the final n-best from the re-scored n-best list.",
            "GPT_cite_text": "Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g., Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network."
        },
        {
            "Number": 9,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "P08-2021",
            "cite_maker_sids": [
                2,
                3
            ],
            "cite_sids": [
                2,
                3
            ],
            "cite_text": "We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "We build our confusion networks using the method of Rosti et al. (2007), but instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize inv-WER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings."
        },
        {
            "Number": 10,
            "refer_ID": "P07-1040",
            "refer_sids": [
                196,
                197
            ],
            "refer_text": "Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.This guarantees that the best path will not be found from a network generated for a system with zero weight.",
            "cite_ID": "P08-2021",
            "cite_maker_sids": [
                23,
                24
            ],
            "cite_sids": [
                23,
                24
            ],
            "cite_text": "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Also, confusion networks generated by using the best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight.",
            "GPT_cite_text": "The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality and uses an estimate of Translation Error Rate (TER) to guide the alignment."
        },
        {
            "Number": 11,
            "refer_ID": "P07-1040",
            "refer_sids": [
                64,
                70
            ],
            "refer_text": "Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.ric since it is based on the rate of edits required to transform the hypothesis into the reference.",
            "cite_ID": "P08-2021",
            "cite_maker_sids": [
                29,
                30
            ],
            "cite_sids": [
                28,
                29,
                30
            ],
            "cite_text": "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Translation edit rate (TER) (Snover et al., 2006) has been proposed as a more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the reference.",
            "GPT_cite_text": "In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings. For this, Rosti et al. (2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions, and block shifts) that convert an input string to another."
        },
        {
            "Number": 12,
            "refer_ID": "P07-1040",
            "refer_sids": [
                6,
                144,
                145,
                146
            ],
            "refer_text": "A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR.The algorithm explores better weights iteratively starting from a set of initial weights.First, each dimension is optimized using a grid-based line minimization algorithm.Then, a new direction based on the changes in the objective function is estimated to speed up the search.",
            "cite_ID": "P08-2021",
            "cite_maker_sids": [
                69,
                70
            ],
            "cite_sids": [
                69,
                70
            ],
            "cite_text": "ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU, and METEOR. The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search.",
            "GPT_cite_text": "ITG-based alignments and TERCOM-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al. (2007)."
        },
        {
            "Number": 13,
            "refer_ID": "P07-1040",
            "refer_sids": [
                93
            ],
            "refer_text": "When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.",
            "cite_ID": "P08-2021",
            "cite_maker_sids": [
                78,
                79
            ],
            "cite_sids": [
                78,
                79
            ],
            "cite_text": "Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "When using best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.",
            "GPT_cite_text": "Note that the algorithm of Rosti et al. (2007) used N-best lists in the combination."
        },
        {
            "Number": 14,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "P11-1125",
            "cite_maker_sids": [
                47,
                48
            ],
            "cite_sids": [
                46,
                47,
                48
            ],
            "cite_text": "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "This large grammatical difference may produce a longer sentence with spuriously inserted words, as in \"I saw the blue trees,\" which was found in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network."
        },
        {
            "Number": 15,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "P11-1125",
            "cite_maker_sids": [
                127
            ],
            "cite_sids": [
                127
            ],
            "cite_text": "Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "Our baseline confusion network system has an additional penalty feature, hp(m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b)."
        },
        {
            "Number": 16,
            "refer_ID": "P07-1040",
            "refer_sids": [
                119,
                120,
                121
            ],
            "refer_text": "On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.This -best list is then re-scored with the higher order -gram.The second set of weights is used to find the final -best from the re-scored -best list.",
            "cite_ID": "P39_p07",
            "cite_maker_sids": [
                24,
                25
            ],
            "cite_sids": [
                24,
                25
            ],
            "cite_text": "@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "On a test set, the first set of weights is used to generate an n-best list from the bi-gram expanded lattice. This n-best list is then re-scored with the higher order n-gram. The second set of weights is used to find the final n-best from the re-scored n-best list.",
            "GPT_cite_text": "@2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network."
        },
        {
            "Number": 17,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "P101121_p07",
            "cite_maker_sids": [
                20,
                21
            ],
            "cite_sids": [
                19,
                20,
                21
            ],
            "cite_text": "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this, and might even learn to produce translations which show the good features without actually being good translations. For example, Rosti et al.(2007) report such an effect.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "If measures with this property are used to tune a typical statistical MT system, it can sometimes be observed that the MT system learns to play against this and might even learn to produce translations that show the good features without actually being good translations. For example, Rosti et al. (2007) report such an effect."
        },
        {
            "Number": 18,
            "refer_ID": "P07-1040",
            "refer_sids": [
                36
            ],
            "refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "cite_ID": "Pling_p07",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                12,
                13
            ],
            "cite_text": "In this paper, a system combination based on confusion network (CN) is described.This approach is not new, and numerous publications are available on that subject, see for example, (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008) and (Leusch et al., 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.",
            "GPT_cite_text": "In this paper, a system combination based on a confusion network (CN) is described. This approach is not new, and numerous publications are available on that subject; see for example (Rosti et al., 2007), (Shen et al., 2008), (Karakos et al., 2008), and (Leusch et al., 2009)."
        },
        {
            "Number": 19,
            "refer_ID": "P07-1040",
            "refer_sids": [
                199,
                200,
                201,
                202,
                203
            ],
            "refer_text": "The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks.Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR.The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese.It also seems like METEOR should not be used in tuning due to high insertion rate and low precision.",
            "cite_ID": "Pling_p07",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41
            ],
            "cite_text": "This diers from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU, and METEOR. The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese. It also seems like METEOR should not be used in tuning due to high insertion rate and low precision.",
            "GPT_cite_text": "This differs from the result of (Rosti et al., 2007) where the nearest hypothesis is computed at each step, which is supposed to be better."
        },
        {
            "Number": 20,
            "refer_ID": "P07-1040",
            "refer_sids": [
                140
            ],
            "refer_text": "A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.",
            "cite_ID": "Psem_p07",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A confusion network may be represented by a word lattice, and standard tools may be used to generate -best hypothesis lists, including word confidence scores, language model scores, and other features.",
            "GPT_cite_text": "The handicap of using a single reference can be addressed by constructing a lattice of reference translations; this technique has been used to combine the output of multiple translation systems (Rosti et al. 2007)."
        },
        {
            "Number": 21,
            "refer_ID": "P07-1040",
            "refer_sids": [
                64,
                70
            ],
            "refer_text": "Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.ric since it is based on the rate of edits required to transform the hypothesis into the reference.",
            "cite_ID": "Psem_p07",
            "cite_maker_sids": [
                116
            ],
            "cite_sids": [
                116
            ],
            "cite_text": "In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Translation edit rate (TER) (Snover et al., 2006) has been proposed as a more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the reference.",
            "GPT_cite_text": "In addition, it may also be used as a general-purpose string alignment tool. TER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited."
        },
        {
            "Number": 22,
            "refer_ID": "P07-1040",
            "refer_sids": [
                86
            ],
            "refer_text": "The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.",
            "cite_ID": "W08-0329",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The modified Levenshtein alignment as used in TER is more natural than simple edit distances such as word error rates since machine translation hypotheses may have different word orders while having the same meaning.",
            "GPT_cite_text": "The recent approaches used pairwise alignment algorithms based on symmetric alignments from an HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007)."
        },
        {
            "Number": 23,
            "refer_ID": "P07-1040",
            "refer_sids": [
                123,
                124,
                125
            ],
            "refer_text": "To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs.",
            "cite_ID": "W08-0329",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To prevent the best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion networks are connected to a common end node with NULL arcs.",
            "GPT_cite_text": "As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice that is expanded and re-scored with language models."
        },
        {
            "Number": 24,
            "refer_ID": "P07-1040",
            "refer_sids": [
                40
            ],
            "refer_text": "In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.",
            "cite_ID": "W08-0329",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                39
            ],
            "cite_text": "Other scores for the word arc are set as in (Rosti et al., 2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.",
            "GPT_cite_text": "Other scores for the word arc are set as in (Rosti et al., 2007)."
        },
        {
            "Number": 25,
            "refer_ID": "P07-1040",
            "refer_sids": [
                95,
                96
            ],
            "refer_text": "Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned.Similar approach to estimate word posteriors is adopted in this work.",
            "cite_ID": "W08-0329",
            "cite_maker_sids": [
                88
            ],
            "cite_sids": [
                87,
                88
            ],
            "cite_text": "The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Due to the computational burden of the TER alignment, only the best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. A similar approach to estimate word posteriors is adopted in this work.",
            "GPT_cite_text": "The first, syscomb pw, corresponds to BLEU System de-en fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the German-English (de-en) and French-English (fren) Europarl test 2008 set to the pairwise TER alignment described in (Rosti et al., 2007)."
        },
        {
            "Number": 26,
            "refer_ID": "P07-1040",
            "refer_sids": [
                140
            ],
            "refer_text": "A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.",
            "cite_ID": "W09-0441",
            "cite_maker_sids": [
                36
            ],
            "cite_sids": [
                35,
                36
            ],
            "cite_text": "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A confusion network may be represented by a word lattice, and standard tools may be used to generate -best hypothesis lists, including word confidence scores, language model scores, and other features.",
            "GPT_cite_text": "The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007)."
        }
    ],
    "W95-0104": [
        {
            "Number": 1,
            "refer_ID": "W95-0104",
            "refer_sids": [
                32,
                25
            ],
            "refer_text": "\\Ve treat context-sensitive spelling correction as a task of word disambiguation.\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "A00-2019",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                23
            ],
            "cite_text": "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "We treat context-sensitive spelling correction as a task of word disambiguation. We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're."
        },
        {
            "Number": 2,
            "refer_ID": "W95-0104",
            "refer_sids": [
                25
            ],
            "refer_text": "\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "A97-1025",
            "cite_maker_sids": [
                29
            ],
            "cite_sids": [
                29
            ],
            "cite_text": "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992)."
        },
        {
            "Number": 3,
            "refer_ID": "W95-0104",
            "refer_sids": [
                63
            ],
            "refer_text": "Table 1 shows the performance of the baseline method for 18 confusion sets.",
            "cite_ID": "A97-1025",
            "cite_maker_sids": [
                125
            ],
            "cite_sids": [
                125
            ],
            "cite_text": "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Table 1 shows the performance of the baseline method for 18 confusion sets.",
            "GPT_cite_text": "The results described in this section are based on the 18 confusion sets selected by Golding (1995, 1996)."
        },
        {
            "Number": 4,
            "refer_ID": "W95-0104",
            "refer_sids": [
                17,
                25
            ],
            "refer_text": "This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "C04-1131",
            "cite_maker_sids": [
                46
            ],
            "cite_sids": [
                46
            ],
            "cite_text": "We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).",
            "label": [
                "Hypothesis_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper takes Yarowsky's method as a starting point and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence but all the available evidence. We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "We have also selected a decision list classifier (DL) that is similar to the classifier used by Yarowsky (1994) for words having two senses and extended for more senses by Golding (1995)."
        },
        {
            "Number": 5,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18,
                24
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.\\Ve then apply each of the two component methods mentioned above\u00c2\u00ad context words and collocations.",
            "cite_ID": "D07-1012",
            "cite_maker_sids": [
                71
            ],
            "cite_sids": [
                71
            ],
            "cite_text": "Golding (1995) builds a classifier based on a rich set of context features.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers. We then apply each of the two component methods mentioned above: context words and collocations.",
            "GPT_cite_text": "Golding (1995) builds a classifier based on a rich set of contextual features."
        },
        {
            "Number": 6,
            "refer_ID": "W95-0104",
            "refer_sids": [
                25
            ],
            "refer_text": "\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "D11-1119",
            "cite_maker_sids": [
                40
            ],
            "cite_sids": [
                40
            ],
            "cite_text": "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), and decision lists (Golding, 1995)."
        },
        {
            "Number": 7,
            "refer_ID": "W95-0104",
            "refer_sids": [
                58,
                59,
                63
            ],
            "refer_text": "The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].Table 1 shows the performance of the baseline method for 18 confusion sets.",
            "cite_ID": "E06-1030",
            "cite_maker_sids": [
                164
            ],
            "cite_sids": [
                164
            ],
            "cite_text": "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. Table 1 shows the performance of the baseline method for 18 confusion sets.",
            "GPT_cite_text": "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus."
        },
        {
            "Number": 8,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18,
                19
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.",
            "cite_ID": "E99-1024",
            "cite_maker_sids": [
                34
            ],
            "cite_sids": [
                33,
                34
            ],
            "cite_text": "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold\u00c2\u00ad ing and Schabes, 1996).",
            "label": [
                "Method_Citation",
                "Aim_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction.",
            "GPT_cite_text": "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded (Golding, 1995; Golding and Schabes, 1996)."
        },
        {
            "Number": 9,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18,
                33,
                34,
                35
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.The ambiguity among words is modelled by confusion sets.A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.Thus if C = {deser\u00c2\u00b7t, desser\u00c2\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.",
            "cite_ID": "H01-1052",
            "cite_maker_sids": [
                20
            ],
            "cite_sids": [
                20,
                21,
                22
            ],
            "cite_text": "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers. The ambiguity among words is modeled by confusion sets. A confusion set C = { w1, ..., wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus, if C = {desert, dessert}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.",
            "GPT_cite_text": "The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose."
        },
        {
            "Number": 10,
            "refer_ID": "W95-0104",
            "refer_sids": [
                85,
                86,
                87,
                88
            ],
            "refer_text": "The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k., c_ 1, c1, ..., cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.",
            "cite_ID": "J98-1006",
            "cite_maker_sids": [
                108
            ],
            "cite_sids": [
                108,
                109
            ],
            "cite_text": "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf ...,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p(c_k, c_1, c_1, ..., c_k|w_i), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a severe sparse-data problem. Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.",
            "GPT_cite_text": "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_k | Ck | si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then"
        },
        {
            "Number": 11,
            "refer_ID": "W95-0104",
            "refer_sids": [
                17,
                19,
                333,
                334
            ],
            "refer_text": "This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.A method for doing this, based on Bayesian classifiers, was presented.It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.",
            "cite_ID": "N03-2035",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                23,
                24
            ],
            "cite_text": "Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "This paper takes Yarowsky's method as a starting point and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence but all the available evidence. The work reported here was applied not to accent restoration but to a related lexical disambiguation task: context-sensitive spelling correction. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction and was found to outperform the component methods as well as decision lists.",
            "GPT_cite_text": "Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sensitive spelling correction and was reported to be superior to decision lists."
        },
        {
            "Number": 12,
            "refer_ID": "W95-0104",
            "refer_sids": [
                25
            ],
            "refer_text": "\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "N03-2035",
            "cite_maker_sids": [
                56
            ],
            "cite_sids": [
                56
            ],
            "cite_text": "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "Hybrid approach [3, 12] combines the strengths of other techniques such as the Bayesian classifier, n-gram, and decision lists."
        },
        {
            "Number": 13,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.",
            "cite_ID": "N03-2035",
            "cite_maker_sids": [
                89
            ],
            "cite_sids": [
                89
            ],
            "cite_text": "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers.",
            "GPT_cite_text": "In the experiment, we classify the data into three groups depending on types of text ambiguity according to section 2: CDSA, CISA, and Homograph, and compare the results from different approaches: Winnow, Bayesian hybrid [3], and POS trigram."
        },
        {
            "Number": 14,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18,
                24
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.\\Ve then apply each of the two component methods mentioned above\u00c2\u00ad context words and collocations.",
            "cite_ID": "N04-1016",
            "cite_maker_sids": [
                98
            ],
            "cite_sids": [
                98,
                99
            ],
            "cite_text": "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).Despite their differences, most approaches use two types of features: context words and collocations.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers. We then apply each of the two component methods mentioned above: context words and collocations.",
            "GPT_cite_text": "These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations."
        },
        {
            "Number": 15,
            "refer_ID": "W95-0104",
            "refer_sids": [
                63
            ],
            "refer_text": "Table 1 shows the performance of the baseline method for 18 confusion sets.",
            "cite_ID": "N04-1016",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102
            ],
            "cite_text": "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Table 1 shows the performance of the baseline method for 18 confusion sets.",
            "GPT_cite_text": "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995)."
        },
        {
            "Number": 16,
            "refer_ID": "W95-0104",
            "refer_sids": [
                19,
                58
            ],
            "refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.",
            "cite_ID": "N04-1016",
            "cite_maker_sids": [
                103
            ],
            "cite_sids": [
                103
            ],
            "cite_text": "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71\u00e2\u20ac\u00a0\u00e2\u20ac\u00a1 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24\u00e2\u20ac\u00a0\u00e2\u20ac\u00a1 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.",
            "label": [
                "Aim_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera].",
            "GPT_cite_text": "Most methods are trained and tested on Model Alta BNC. Model Alta BNC f(t) 72.98 70.00 f(w1, t, w2)/f(t) 87.77 76.33 f(w1, t) 84.40 83.02 f(w1, w2, t)/f(t) 86.27 74.47 f(t, w1) 84.89 82.74 f(t, w2, w2)/f(t) 84.94 74.23 f(w1, t, w2) 89.24 77.13 f(w1, t, w2)/f(w1, t) 80.70 73.69 f(t, w1, w2) 84.68 75.08 f(w1, w2, t)/f(w2, t) 72.11 69.28 f(w1, t)/f(t) 82.81 77.84 f(t, w1, w2)/f(t, w1) 75.65 72.57 f(t, w1)/f(t) 77.49 80.71. Table 5: Performance of Altavista counts and BNC counts for context-sensitive spelling correction (data from Cucerzan and Yarowsky 2002). Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23. Table 6: Performance comparison with the literature for context-sensitive spelling correction using the Brown corpus, using 80% for training and 20% for testing. We devised a simple, unsupervised method for performing spelling correction using web counts."
        },
        {
            "Number": 17,
            "refer_ID": "W95-0104",
            "refer_sids": [
                58,
                59
            ],
            "refer_text": "The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].",
            "cite_ID": "N04-1016",
            "cite_maker_sids": [
                114
            ],
            "cite_sids": [
                114
            ],
            "cite_text": "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].",
            "GPT_cite_text": "Table 6 shows an exception: Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing."
        },
        {
            "Number": 18,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.",
            "cite_ID": "N04-1016",
            "cite_maker_sids": [
                116
            ],
            "cite_sids": [
                116
            ],
            "cite_text": "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers.",
            "GPT_cite_text": "A comparison with the literature shows that the best Altavista model outperforms Golding (1995) and Jones and Martin (1997); the highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999)."
        },
        {
            "Number": 19,
            "refer_ID": "W95-0104",
            "refer_sids": [
                35
            ],
            "refer_text": "Thus if C = {deser\u00c2\u00b7t, desser\u00c2\u00b7t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.",
            "cite_ID": "N10-1019",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Thus if C = {desert, dessert}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.",
            "GPT_cite_text": "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996)."
        },
        {
            "Number": 20,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18,
                162,
                163
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;.An ambiguous target word is then classified by finding all collocations that match its context.",
            "cite_ID": "P01-1005",
            "cite_maker_sids": [
                19
            ],
            "cite_sids": [
                19,
                20
            ],
            "cite_text": "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers. The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each wi. An ambiguous target word is then classified by finding all collocations that match its context.",
            "GPT_cite_text": "The more recent set of techniques includes multiplicative weight-update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993; Golding, 1995; Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose."
        },
        {
            "Number": 21,
            "refer_ID": "W95-0104",
            "refer_sids": [
                333,
                334
            ],
            "refer_text": "A method for doing this, based on Bayesian classifiers, was presented.It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.",
            "cite_ID": "P96-1010",
            "cite_maker_sids": [
                25
            ],
            "cite_sids": [
                25
            ],
            "cite_text": "Feature-based approaches, such as Bayesian clas\u00c2\u00ad sifiers (Gale, Church, and Yarowsky, 1993), deci\u00c2\u00ad sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc\u00c2\u00ad cess for the problem of context-sensitive spelling correction.",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction and was found to outperform the component methods as well as decision lists.",
            "GPT_cite_text": "Feature-based approaches, such as Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of success for the problem of context-sensitive spelling correction."
        },
        {
            "Number": 22,
            "refer_ID": "W95-0104",
            "refer_sids": [
                322,
                325
            ],
            "refer_text": "Trigrams are at their worst when the words in the confusion set have the same part of speech.In such cases, the Bayesian hybrid method is clearly better.",
            "cite_ID": "P96-1010",
            "cite_maker_sids": [
                38
            ],
            "cite_sids": [
                38
            ],
            "cite_text": "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Trigrams are at their worst when the words in the confusion set have the same part of speech. In such cases, the Bayesian hybrid method is clearly better.",
            "GPT_cite_text": "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech."
        },
        {
            "Number": 23,
            "refer_ID": "W95-0104",
            "refer_sids": [
                25
            ],
            "refer_text": "\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "P96-1010",
            "cite_maker_sids": [
                78
            ],
            "cite_sids": [
                78
            ],
            "cite_text": "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996)."
        },
        {
            "Number": 24,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.",
            "cite_ID": "P96-1010",
            "cite_maker_sids": [
                80
            ],
            "cite_sids": [
                79,
                80
            ],
            "cite_text": "We adopt the Bayesian hybrid methodThis method has been described elsewhere (Golding, 1995)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers.",
            "GPT_cite_text": "We adopt the Bayesian hybrid method. This method has been described elsewhere (Golding, 1995)."
        },
        {
            "Number": 25,
            "refer_ID": "W95-0104",
            "refer_sids": [
                19
            ],
            "refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.",
            "cite_ID": "P98-2138",
            "cite_maker_sids": [
                95
            ],
            "cite_sids": [
                95
            ],
            "cite_text": "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction.",
            "GPT_cite_text": "For English, a number of methods have been proposed to cope with real-world errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996)."
        },
        {
            "Number": 26,
            "refer_ID": "W95-0104",
            "refer_sids": [
                24
            ],
            "refer_text": "\\Ve then apply each of the two component methods mentioned above\u00c2\u00ad context words and collocations.",
            "cite_ID": "P98-2138",
            "cite_maker_sids": [
                143
            ],
            "cite_sids": [
                143
            ],
            "cite_text": "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We then apply each of the two component methods mentioned above: context words and collocations.",
            "GPT_cite_text": "Following previous works (Golding, 1995; Meknawin et al., 1997), we have tried two types of features: context words and collocations."
        },
        {
            "Number": 27,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18,
                19
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.",
            "cite_ID": "W00-0701",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                23
            ],
            "cite_text": "This general scheme has been used to de\u00c2\u00ad rive classifiers for a variety of natural lan\u00c2\u00ad guage applications including speech applica\u00c2\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00c2\u00ad sensitive spelling correction (Gol95).",
            "label": [
                "Method_Citation",
                "Aim_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction.",
            "GPT_cite_text": "This general scheme has been used to derive classifiers for a variety of natural language applications including speech applications (Rab89), pos tagging (Kup92; Sch95), word-sense disambiguation (GCY93) and context-sensitive spelling correction (Gol95)."
        },
        {
            "Number": 28,
            "refer_ID": "W95-0104",
            "refer_sids": [
                17
            ],
            "refer_text": "This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.",
            "cite_ID": "W00-0701",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).",
            "label": [
                "Hypothesis_Citation"
            ],
            "GPT_refer_text": "This paper takes Yarowsky's method as a starting point and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence but all the available evidence.",
            "GPT_cite_text": "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature rather than a combination (Gol95)."
        },
        {
            "Number": 29,
            "refer_ID": "W95-0104",
            "refer_sids": [
                18
            ],
            "refer_text": "A method is presented for doing this, based on Bayesian classifiers.",
            "cite_ID": "W01-0502",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A method is presented for doing this, based on Bayesian classifiers.",
            "GPT_cite_text": "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995)."
        },
        {
            "Number": 30,
            "refer_ID": "W95-0104",
            "refer_sids": [
                19,
                25
            ],
            "refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "W02-1005",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction. We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)."
        },
        {
            "Number": 32,
            "refer_ID": "W95-0104",
            "refer_sids": [
                19,
                58
            ],
            "refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.",
            "cite_ID": "W02-1005",
            "cite_maker_sids": [
                180
            ],
            "cite_sids": [
                180
            ],
            "cite_text": "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)",
            "label": [
                "Aim_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera].",
            "GPT_cite_text": "For CSSC, we tested our system on the identical data from the Brown Corpus used by Golding (1995)."
        },
        {
            "Number": 33,
            "refer_ID": "W95-0104",
            "refer_sids": [
                19
            ],
            "refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.",
            "cite_ID": "W04-3238",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction.",
            "GPT_cite_text": "A different body of work (e.g., Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors in the framework of context-sensitive spelling correction (CSSC)."
        },
        {
            "Number": 34,
            "refer_ID": "W95-0104",
            "refer_sids": [
                216
            ],
            "refer_text": "Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)",
            "cite_ID": "W06-1624",
            "cite_maker_sids": [
                84
            ],
            "cite_sids": [
                84
            ],
            "cite_text": "We use the metric described in (Yarowsky, 1994; Golding, 1995).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability.",
            "GPT_cite_text": "We use the metric described in (Yarowsky, 1994; Golding, 1995)."
        },
        {
            "Number": 35,
            "refer_ID": "W95-0104",
            "refer_sids": [
                32
            ],
            "refer_text": "\\Ve treat context-sensitive spelling correction as a task of word disambiguation.",
            "cite_ID": "W06-3604",
            "cite_maker_sids": [
                146
            ],
            "cite_sids": [
                146
            ],
            "cite_text": "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We treat context-sensitive spelling correction as a task of word disambiguation.",
            "GPT_cite_text": "More generally, as a precursor to the above-mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;"
        },
        {
            "Number": 37,
            "refer_ID": "W95-0104",
            "refer_sids": [
                19,
                25
            ],
            "refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "W12-0304",
            "cite_maker_sids": [
                33
            ],
            "cite_sids": [
                33
            ],
            "cite_text": "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction. We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "There are also other studies (Yarowsky, 1994; Golding, 1995; or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection."
        },
        {
            "Number": 38,
            "refer_ID": "W95-0104",
            "refer_sids": [
                19,
                25
            ],
            "refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.",
            "cite_ID": "W96-0108",
            "cite_maker_sids": [
                28
            ],
            "cite_sids": [
                28
            ],
            "cite_text": "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction. We try two ways of combining these components: decision lists and Bayesian classifiers.",
            "GPT_cite_text": "Golding [1995] has applied a hybrid Bayesian method for real-world error correction, and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose."
        },
        {
            "Number": 39,
            "refer_ID": "W95-0104",
            "refer_sids": [
                19
            ],
            "refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disam\u00c2\u00ad biguation task: context-sensitive spelling correction.",
            "cite_ID": "W98-1234",
            "cite_maker_sids": [
                4
            ],
            "cite_sids": [
                4
            ],
            "cite_text": "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The work reported here was applied not to accent restoration, but to a related lexical disambiguation task: context-sensitive spelling correction.",
            "GPT_cite_text": "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2], Golding [3], Golding and Schabes [4], and Powers [5]."
        }
    ],
    "C98-1097": [
        {
            "Number": 1,
            "refer_ID": "C98-1097",
            "refer_sids": [
                45
            ],
            "refer_text": "To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.",
            "cite_ID": "C02-1033",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To automatically detect lexical cohesion ties between pairwise words, three linguistic features were considered: word repetition, collocation, and relation weights.",
            "GPT_cite_text": "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations, and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues."
        },
        {
            "Number": 2,
            "refer_ID": "C98-1097",
            "refer_sids": [
                115
            ],
            "refer_text": "Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62.",
            "cite_ID": "C02-1033",
            "cite_maker_sids": [
                136
            ],
            "cite_sids": [
                136
            ],
            "cite_text": "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62.",
            "GPT_cite_text": "Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach to text segmentation."
        },
        {
            "Number": 3,
            "refer_ID": "C98-1097",
            "refer_sids": [
                45
            ],
            "refer_text": "To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.",
            "cite_ID": "ICDAR99",
            "cite_maker_sids": [
                31
            ],
            "cite_sids": [
                31,
                32
            ],
            "cite_text": "In earlier work [11] a text segmentation algorithm was described that captured all types of lexical cohesion ties.To automatically find ties between pairwise words three features were developed: word repetition, collocation and relation weights.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To automatically detect lexical cohesion ties between pairwise words, three linguistic features were considered: word repetition, collocation, and relation weights.",
            "GPT_cite_text": "In earlier work [11], a text segmentation algorithm was described that captured all types of lexical cohesion ties. To automatically find ties between pairwise words, three features were developed: word repetition, collocation, and relation weights."
        },
        {
            "Number": 4,
            "refer_ID": "C98-1097",
            "refer_sids": [
                45
            ],
            "refer_text": "To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.",
            "cite_ID": "P07-1061",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To automatically detect lexical cohesion ties between pairwise words, three linguistic features were considered: word repetition, collocation, and relation weights.",
            "GPT_cite_text": "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, co-occurrences, and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues."
        },
        {
            "Number": 5,
            "refer_ID": "C98-1097",
            "refer_sids": [
                20,
                45
            ],
            "refer_text": "Another approach extracted semantic information from Roget's Thesaurus (RT).To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.",
            "cite_ID": "P07-1061",
            "cite_maker_sids": [
                30
            ],
            "cite_sids": [
                30,
                31,
                32
            ],
            "cite_text": "When no external knowledge is used, this similarity is only based on the strict reiteration of words.But it can be enhanced by taking into account semantic relations between words.This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another approach extracted semantic information from Roget's Thesaurus (RT). To automatically detect lexical cohesion ties between pairwise words, three linguistic features were considered: word repetition, collocation, and relation weights.",
            "GPT_cite_text": "When no external knowledge is used, this similarity is only based on the strict reiteration of words. But it can be enhanced by taking into account semantic relations between words. This was done, for instance, in (Jobbins and Evett, 1998) by taking semantic relations from Roget's Thesaurus."
        },
        {
            "Number": 6,
            "refer_ID": "C98-1097",
            "refer_sids": [
                41,
                47,
                50
            ],
            "refer_text": "Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing).Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem.",
            "cite_ID": "P07-1061",
            "cite_maker_sids": [
                90
            ],
            "cite_sids": [
                89,
                90
            ],
            "cite_text": "The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb), which includes near-synonym and hyponym, superordinate (e.g. ascent and task), and general word (e.g. ascent and thing). Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem.",
            "GPT_cite_text": "The cohesion in the part of the text delimited by this window is evaluated by measuring the word reiteration between its two sides. This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998)."
        },
        {
            "Number": 7,
            "refer_ID": "C98-1097",
            "refer_sids": [
                123,
                124,
                125
            ],
            "refer_text": "The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69.When used in isolation, the performance of each feature was inferior to a combined approach.This fact provides evidence that different lexical relations are detected by each linguistic feature considered.",
            "cite_ID": "P07-1061",
            "cite_maker_sids": [
                98
            ],
            "cite_sids": [
                97,
                98
            ],
            "cite_text": "This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The combination of features, word repetition and relation weights, produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that different lexical relations are detected by each linguistic feature considered.",
            "GPT_cite_text": "This evaluation is also a weak point as card(Wl, Wr) only relies on word reiteration. As a consequence, two different words that respectively belong to Wl and Wr but also belong to the same text topic cannot contribute lr to the identification of a possible topical similarity. This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998)."
        },
        {
            "Number": 8,
            "refer_ID": "C98-1097",
            "refer_sids": [
                45
            ],
            "refer_text": "To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.",
            "cite_ID": "P07-1061",
            "cite_maker_sids": [
                203
            ],
            "cite_sids": [
                203,
                204
            ],
            "cite_text": "In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.In both cases the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To automatically detect lexical cohesion ties between pairwise words, three linguistic features were considered: word repetition, collocation, and relation weights.",
            "GPT_cite_text": "In fact, the way we use relations between words is closer to Jobbins and Evett (1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics. In both cases, the similarity of two text units is determined by the proportion of their words that are part of a relation across the two units."
        },
        {
            "Number": 9,
            "refer_ID": "C98-1097",
            "refer_sids": [
                62
            ],
            "refer_text": "The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity.",
            "cite_ID": "P07-1061",
            "cite_maker_sids": [
                216
            ],
            "cite_sids": [
                216
            ],
            "cite_text": "This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity.",
            "GPT_cite_text": "This network could also be used more directly for topic segmentation as in (Jobbins and Evett, 1998)."
        },
        {
            "Number": 10,
            "refer_ID": "C98-1097",
            "refer_sids": [
                45
            ],
            "refer_text": "To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.",
            "cite_ID": "P04470",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "In other words, meaning of UW can be found generally through co- occurrence words [5].",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To automatically detect lexical cohesion ties between pairwise words, three linguistic features were considered: word repetition, collocation, and relation weights.",
            "GPT_cite_text": "In other words, the meaning of UW can generally be found through co-occurring words [5]."
        },
        {
            "Number": 11,
            "refer_ID": "C98-1097",
            "refer_sids": [
                7,
                8
            ],
            "refer_text": "The Wall Street Journal archives, for example, consist of a series of articles about different subject areas.Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved.",
            "cite_ID": "P06128",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved.",
            "GPT_cite_text": "In information retrieval, segmenting a long document into distinct topics is useful because only the topical segments relevant to the user's needs are retrieved [1]."
        },
        {
            "Number": 12,
            "refer_ID": "C98-1097",
            "refer_sids": [
                36,
                37
            ],
            "refer_text": "Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.Identifying semantic relations in a text can be a useful indicator of its conceptual structure.",
            "cite_ID": "S0885",
            "cite_maker_sids": [
                110
            ],
            "cite_sids": [
                109,
                110
            ],
            "cite_text": "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure.",
            "GPT_cite_text": "Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words. These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998)."
        }
    ],
    "J00-3003": [
        {
            "Number": 1,
            "refer_ID": "J00-3003",
            "refer_sids": [
                214,
                358,
                392,
                394
            ],
            "refer_text": "The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here.Table 9 Combined utterance classification accuracies (chance = 35%).Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%).",
            "cite_ID": "W13-4047",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13,
                14
            ],
            "cite_text": "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here. Table 9 Combined utterance classification accuracies (chance = 35%). Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%).",
            "GPT_cite_text": "(Stolcke et al., 2000) use HMMs for dialogue modeling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus."
        },
        {
            "Number": 2,
            "refer_ID": "J00-3003",
            "refer_sids": [
                68,
                71
            ],
            "refer_text": "For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech.",
            "cite_ID": "W12-1634",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech.",
            "GPT_cite_text": "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010)."
        },
        {
            "Number": 3,
            "refer_ID": "J00-3003",
            "refer_sids": [
                188,
                192
            ],
            "refer_text": "The computation of likelihoods P(EIU ) depends on the types of evidence used.Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F I U).",
            "cite_ID": "W12-1634",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18
            ],
            "cite_text": "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The computation of likelihoods P(EIU) depends on the types of evidence used. Prosodic features evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F | U).",
            "GPT_cite_text": "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored."
        },
        {
            "Number": 4,
            "refer_ID": "J00-3003",
            "refer_sids": [
                64,
                68
            ],
            "refer_text": "The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods.For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.",
            "cite_ID": "W10-1012",
            "cite_maker_sids": [
                185
            ],
            "cite_sids": [
                185
            ],
            "cite_text": "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on non-lexical information correlated with DA identity.",
            "GPT_cite_text": "There have also been dialogue act modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpora (Shawar and Atwell, 2005)."
        },
        {
            "Number": 5,
            "refer_ID": "J00-3003",
            "refer_sids": [
                155,
                156
            ],
            "refer_text": "A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking.These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970).",
            "cite_ID": "W13-4011",
            "cite_maker_sids": [
                1
            ],
            "cite_sids": [
                1,
                2
            ],
            "cite_text": "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation.Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking. These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970).",
            "GPT_cite_text": "Conversational feedback is mostly performed through short utterances such as yeah, mh, okay not produced by the main speaker but by one of the other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000)."
        },
        {
            "Number": 6,
            "refer_ID": "J00-3003",
            "refer_sids": [
                38
            ],
            "refer_text": "Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department.",
            "cite_ID": "W01-1627",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18
            ],
            "cite_text": "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YES-NO QUESTION NONVERBAL YES ANSWERS CONVENTIONAL CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-NO QUESTION OTHER BACKCHANNEL QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION DIRECTIVE COLLABORATIVE COMPLETION REPEAT PHRASE OPEN QUESTION RHETORICAL QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL OPENING OR CLAUSE DISPREFERRED ANSWERS 3RD PARTY TALK OFFERS, OPTIONS ~ COMMITS SELF TALK DOWNPLAYER MAYBE/ACCEPT PART TAG QUESTION DECLARATIVE WH QUESTION APOLOGY THANKING Example % Me, I'm in the legal department.",
            "GPT_cite_text": "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored."
        },
        {
            "Number": 7,
            "refer_ID": "J00-3003",
            "refer_sids": [
                21,
                22,
                27
            ],
            "refer_text": "The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue. While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs).Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria.",
            "cite_ID": "N13-1099",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                17,
                18
            ],
            "cite_text": "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue. While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs). Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria.",
            "GPT_cite_text": "Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialogue. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000))."
        },
        {
            "Number": 8,
            "refer_ID": "J00-3003",
            "refer_sids": [
                64,
                67,
                81
            ],
            "refer_text": "The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods.However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence.The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium.",
            "cite_ID": "H01-1001",
            "cite_maker_sids": [
                95
            ],
            "cite_sids": [
                95,
                96
            ],
            "cite_text": "dialogue acts such as statements, questions, backchannels, ... are detected using a language model based detec\u00adtor trained on Switchboard similar to Stolcke et al.(2000",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The goal of this article is twofold: on the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium.",
            "GPT_cite_text": "Dialogue acts such as statements, questions, backchannels, ... are detected using a language model-based detector trained on Switchboard, similar to Stolcke et al. (2000)."
        },
        {
            "Number": 9,
            "refer_ID": "J00-3003",
            "refer_sids": [
                66,
                214,
                215
            ],
            "refer_text": "For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui).",
            "cite_ID": "N06-2021",
            "cite_maker_sids": [
                66
            ],
            "cite_sids": [
                64,
                65,
                66
            ],
            "cite_text": "The HMM has been widely used in many tagging problems.Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil | Ui).",
            "GPT_cite_text": "The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation."
        },
        {
            "Number": 10,
            "refer_ID": "J00-3003",
            "refer_sids": [
                32,
                33
            ],
            "refer_text": "Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand.In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.",
            "cite_ID": "W12-1616",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.",
            "GPT_cite_text": "By representing a higher level intention of utterances during human conversation, dialogue act labels are being used to enrich the information provided by spoken words (Stolcke et al., 2000)."
        }
    ],
    "E03-1020": [
        {
            "Number": 1,
            "refer_ID": "E03-1020",
            "refer_sids": [
                37
            ],
            "refer_text": "To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).",
            "cite_ID": "W11-1104",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To detect the different areas of meaning in our local graphs, we use a clustering algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).",
            "GPT_cite_text": "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs."
        },
        {
            "Number": 2,
            "refer_ID": "E03-1020",
            "refer_sids": [
                20,
                37
            ],
            "refer_text": "Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).",
            "cite_ID": "S13-2038",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18,
                19
            ],
            "cite_text": "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.They perform Markov clustering on this graph.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times. To detect the different areas of meaning in our local graphs, we use a clustering algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).",
            "GPT_cite_text": "Dorow and Widdows (2003) use the BNC to build a co-occurrence graph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph."
        },
        {
            "Number": 5,
            "refer_ID": "E03-1020",
            "refer_sids": [
                20
            ],
            "refer_text": "Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.",
            "cite_ID": "P04-1080",
            "cite_maker_sids": [
                199
            ],
            "cite_sids": [
                199,
                200,
                201
            ],
            "cite_text": "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word.Their algorithm required a threshold as input, which controlled the number of senses.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun, and two nodes have an edge between them if they co-occur in lists more than a given number of times.",
            "GPT_cite_text": "The algorithm in (Dorow and Widdows, 2003) represented the target noun word, its neighbors, and their relationships using a graph in which each node denoted a noun, and two nodes had an edge between them if they co-occurred more than a given number of times. Then, the senses of the target word were iteratively learned by clustering the local graph of similar words around the target word. Their algorithm required a threshold as input, which controlled the number of senses."
        },
        {
            "Number": 6,
            "refer_ID": "E03-1020",
            "refer_sids": [
                20
            ],
            "refer_text": "Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.",
            "cite_ID": "D10-1073",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35,
                36,
                37
            ],
            "cite_text": "Another graph-based method is presented in(Dorow and Widdows, 2003).They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.Additionally, theyextract second-order co-occurrences.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun, and two nodes have an edge between them if they co-occur in lists more than a given number of times.",
            "GPT_cite_text": "Another graph-based method is presented in (Dorow and Widdows, 2003). They extract only noun neighbors that appear in conjunctions or disjunctions with the target word. Additionally, they extract second-order co-occurrences."
        },
        {
            "Number": 7,
            "refer_ID": "E03-1020",
            "refer_sids": [
                37
            ],
            "refer_text": "To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).",
            "cite_ID": "C04-1194",
            "cite_maker_sids": [
                21
            ],
            "cite_sids": [
                21
            ],
            "cite_text": "The last trend, explored by (V\u00e9ronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To detect the different areas of meaning in our local graphs, we use a clustering algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).",
            "GPT_cite_text": "The last trend, explored by (V\u00e9ronis, 2003), (Dorow and Widdows, 2003), and (Rapp, 2003), starts from the co-occurrents of a word recorded from a corpus and builds its senses by gathering its co-occurrents according to their similarity or their dissimilarity."
        },
        {
            "Number": 8,
            "refer_ID": "E03-1020",
            "refer_sids": [
                2,
                3
            ],
            "refer_text": "The algorithm is based on a graph model representing words and relationships between them.Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.",
            "cite_ID": "C04-1194",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "This method, as the ones presented in (V\u00e9ronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.",
            "GPT_cite_text": "This method, as the ones presented in (V\u00e9ronis, 2003), (Dorow and Widdows, 2003), and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the co-occurrents of a word, the number of relations between the co-occurrents defining a sense is higher than the number of relations that these co-occurrents have with those defining the other senses of the considered word."
        },
        {
            "Number": 9,
            "refer_ID": "E03-1020",
            "refer_sids": [
                93
            ],
            "refer_text": "Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used.",
            "cite_ID": "C04-1194",
            "cite_maker_sids": [
                157
            ],
            "cite_sids": [
                157
            ],
            "cite_text": "As they rely on the detection of high-density areas in a network of cooccurrences, (V\u00e9ronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Preliminary observations show that the different neighbors in Table 1 can be used to indicate with great accuracy which of the senses is being used.",
            "GPT_cite_text": "As they rely on the detection of high-density areas in a network of co-occurrences, (V\u00e9ronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours."
        },
        {
            "Number": 10,
            "refer_ID": "E03-1020",
            "refer_sids": [
                20
            ],
            "refer_text": "Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.",
            "cite_ID": "C04-1194",
            "cite_maker_sids": [
                160
            ],
            "cite_sids": [
                160
            ],
            "cite_text": "In our case, we chose a more general approach by working at the level of a simi\u00adlarity graph: when the similarity of two words is given by their relation of cooccurrence, our situa\u00adtion is comparable to the one of (V\u00e9ronis, 2003) and (Dorow and Widdows, 2003)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun, and two nodes have an edge between them if they co-occur in lists more than a given number of times.",
            "GPT_cite_text": "In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of co-occurrence, our situation is comparable to that of (V\u00e9ronis, 2003) and (Dorow and Widdows, 2003)."
        },
        {
            "Number": 11,
            "refer_ID": "E03-1020",
            "refer_sids": [
                19
            ],
            "refer_text": "Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. \"genomic DNA from rat, mouse and dog\".",
            "cite_ID": "C04-1194",
            "cite_maker_sids": [
                165
            ],
            "cite_sids": [
                165
            ],
            "cite_text": "From a global viewpoint, these two differences lead (V\u00e9ronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g., \"genomic DNA from rat, mouse, and dog.\"",
            "GPT_cite_text": "From a global viewpoint, these two differences lead (V\u00e9ronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours."
        },
        {
            "Number": 12,
            "refer_ID": "E03-1020",
            "refer_sids": [
                37
            ],
            "refer_text": "To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).",
            "cite_ID": "N07-3010",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                73
            ],
            "cite_text": "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To detect the different areas of meaning in our local graphs, we use a clustering algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).",
            "GPT_cite_text": "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between them)."
        },
        {
            "Number": 13,
            "refer_ID": "E03-1020",
            "refer_sids": [
                72
            ],
            "refer_text": "The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1.",
            "cite_ID": "W11-2214",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The local graph in step 1 consists of w, the n_i neighbours of w and the n_j neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1.",
            "GPT_cite_text": "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003)."
        },
        {
            "Number": 14,
            "refer_ID": "E03-1020",
            "refer_sids": [
                90
            ],
            "refer_text": "This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated.",
            "cite_ID": "W11-2214",
            "cite_maker_sids": [
                80
            ],
            "cite_sids": [
                80
            ],
            "cite_text": "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "This gives rise to an automatic, unsupervised word sense disambiguation algorithm that is trained on the data to be disambiguated.",
            "GPT_cite_text": "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as context and all words with a dependency path of length 3 or less, with the last word and its relation as a feature."
        },
        {
            "Number": 15,
            "refer_ID": "E03-1020",
            "refer_sids": [
                37,
                2,
                3
            ],
            "refer_text": "To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).The algorithm is based on a graph model representing words and relationships between them.Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.",
            "cite_ID": "W06-3812",
            "cite_maker_sids": [
                176
            ],
            "cite_sids": [
                176,
                178
            ],
            "cite_text": "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To detect the different areas of meaning in our local graphs, we use a clustering algorithm for graphs (Markov Clustering, MCL) developed by van Dongen (2000). The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.",
            "GPT_cite_text": "Similar to the approach presented in (Dorow and Widdows, 2003), we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the subgraph induced by the neighborhood of w (without w) and clustering it with MCL."
        }
    ],
    "W08-2222": [
        {
            "Number": 2,
            "refer_ID": "W08-2222",
            "refer_sids": [
                3,
                144
            ],
            "refer_text": "The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.The numerical and date expressions got correct representations.",
            "cite_ID": "P13-1138",
            "cite_maker_sids": [
                69
            ],
            "cite_sids": [
                69
            ],
            "cite_text": "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate neo-Davidsonian representations for events, using the VerbNet inventory of thematic roles. The numerical and date expressions got correct representations.",
            "GPT_cite_text": "We use the Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT and DATE."
        },
        {
            "Number": 3,
            "refer_ID": "W08-2222",
            "refer_sids": [
                17
            ],
            "refer_text": "Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.",
            "cite_ID": "Q13-1015",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.",
            "GPT_cite_text": "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008)."
        },
        {
            "Number": 4,
            "refer_ID": "W08-2222",
            "refer_sids": [
                17
            ],
            "refer_text": "Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.",
            "cite_ID": "S12-1040",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "In this paper we present and evaluate a system that transforms texts into logical formulas \u00e2\u20ac\u201c using the C&C tools and Boxer (Bos, 2008) \u00e2\u20ac\u201c in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.",
            "GPT_cite_text": "In this paper, we present and evaluate a system that transforms texts into logical formulas \u2013 using the C&C tools and Boxer (Bos, 2008) \u2013 in the context of the shared task on recognizing negation in English texts (Morante and Blanco, 2012)."
        },
        {
            "Number": 5,
            "refer_ID": "W08-2222",
            "refer_sids": [
                17,
                11
            ],
            "refer_text": "Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts.",
            "cite_ID": "S13-1002",
            "cite_maker_sids": [
                42
            ],
            "cite_sids": [
                42
            ],
            "cite_text": "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts.",
            "GPT_cite_text": "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993)."
        },
        {
            "Number": 6,
            "refer_ID": "W08-2222",
            "refer_sids": [
                11,
                164
            ],
            "refer_text": "Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts.It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer",
            "cite_ID": "W10-1750",
            "cite_maker_sids": [
                56
            ],
            "cite_sids": [
                56
            ],
            "cite_text": "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called \u201cboxes\u201d because of the way they are graphically displayed) for English sentences and texts. It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer.",
            "GPT_cite_text": "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008)."
        },
        {
            "Number": 7,
            "refer_ID": "W08-2222",
            "refer_sids": [
                17,
                23
            ],
            "refer_text": "Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992)",
            "cite_ID": "W11-2408",
            "cite_maker_sids": [
                124
            ],
            "cite_sids": [
                124
            ],
            "cite_text": "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.",
            "label": [
                "Method_Citation",
                "Implication_Citation"
            ],
            "GPT_refer_text": "Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. We choose DRT because it has established itself as a well-documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992).",
            "GPT_cite_text": "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into the generic present tense from other tenses, and other issues that affect the quality of the statements."
        },
        {
            "Number": 8,
            "refer_ID": "W08-2222",
            "refer_sids": [
                10
            ],
            "refer_text": "Boxer is an open-domain tool for computing and reasoning with semantic representations.",
            "cite_ID": "W13-2101",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Boxer is an open-domain tool for computing and reasoning with semantic representations.",
            "GPT_cite_text": "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process and consider such semantic representations as input to a surface realization component."
        },
        {
            "Number": 9,
            "refer_ID": "W08-2222",
            "refer_sids": [
                156
            ],
            "refer_text": "It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic.",
            "cite_ID": "W13-3209",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35
            ],
            "cite_text": "This line of research converts logical representations obtained from syntactic parses using Bos\u00e2\u20ac\u2122 Boxer (Bos, 2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "It takes as input a CCG derivation of a natural language expression and produces formally interpretable semantic representations, either in the form of DRSs or as formulas of first-order logic.",
            "GPT_cite_text": "This line of research converts logical representations obtained from syntactic parses using Bos' Boxer (Bos, 2008)."
        }
    ],
    "X96-1048": [
        {
            "Number": 1,
            "refer_ID": "X96-1048",
            "refer_sids": [
                357
            ],
            "refer_text": "In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.",
            "cite_ID": "A97-1028",
            "cite_maker_sids": [
                3
            ],
            "cite_sids": [
                3
            ],
            "cite_text": "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "In addition, there are plans to put evaluations online, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.",
            "GPT_cite_text": "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of information extraction tasks (Sundheim, 1995b)."
        },
        {
            "Number": 4,
            "refer_ID": "X96-1048",
            "refer_sids": [
                4,
                113
            ],
            "refer_text": "The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.",
            "cite_ID": "J00-4003",
            "cite_maker_sids": [
                20
            ],
            "cite_sids": [
                12,
                20
            ],
            "cite_text": "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.",
            "GPT_cite_text": "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions."
        },
        {
            "Number": 5,
            "refer_ID": "X96-1048",
            "refer_sids": [
                129,
                132,
                133
            ],
            "refer_text": "In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.",
            "cite_ID": "J00-4003",
            "cite_maker_sids": [
                72
            ],
            "cite_sids": [
                71,
                72
            ],
            "cite_text": "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "In the middle of the effort of preparing the test data for the formal evaluation, an inter-annotator variability test was conducted. There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.",
            "GPT_cite_text": "The third main result was that we found very little agreement between our subjects on identifying bridging descriptions: in our second experiment, the agreement on 5 previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for example, in the coreference annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them."
        },
        {
            "Number": 6,
            "refer_ID": "X96-1048",
            "refer_sids": [
                13
            ],
            "refer_text": "\u2022 Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.",
            "cite_ID": "W97-1307",
            "cite_maker_sids": [
                29
            ],
            "cite_sids": [
                29,
                30
            ],
            "cite_text": "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "\u2022 Coreference (CO) -- Insert SGML tags into the text to link strings that represent coreferring noun phrases.",
            "GPT_cite_text": "This algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995), and produced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recognize coreference among noun phrases (Sundheim, 1995)."
        },
        {
            "Number": 7,
            "refer_ID": "X96-1048",
            "refer_sids": [
                174
            ],
            "refer_text": "5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..",
            "cite_ID": "P06-1059",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision.",
            "GPT_cite_text": "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004)1, whereas the best performance at MUC-6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995)."
        },
        {
            "Number": 8,
            "refer_ID": "X96-1048",
            "refer_sids": [
                27,
                28,
                31,
                227
            ],
            "refer_text": "CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.",
            "cite_ID": "C04-1126",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49,
                50,
                51
            ],
            "cite_text": "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an\u00adnotators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator\u2019s templates were assumed to be correct and compared with the other.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to the degree of match with a keyword search query. Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.",
            "GPT_cite_text": "The MUC organizers provided strict guidelines about what constituted a succession event and how the templates should be filled, which the annotators sometimes found difficult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotator\u2019s templates were assumed to be correct and compared with the other."
        },
        {
            "Number": 9,
            "refer_ID": "X96-1048",
            "refer_sids": [
                212,
                213
            ],
            "refer_text": "In this article, the management succession scenario will be used as the basis for discussion.The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.",
            "cite_ID": "C04-1126",
            "cite_maker_sids": [
                34
            ],
            "cite_sids": [
                34
            ],
            "cite_text": "The test corpus consists of 100 Wall Street Jour\u00adnal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "In this article, the management succession scenario will be used as the basis for discussion. The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.",
            "GPT_cite_text": "The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995)."
        },
        {
            "Number": 10,
            "refer_ID": "X96-1048",
            "refer_sids": [
                54,
                57
            ],
            "refer_text": "When the outputs are scored in \"key-to-response\" mode, as though one annotator's output represented the \"key\" and the other the \"response,\" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA \"fast\" configuration 95.66, SRA \"fastest\" configuration 92.61, SRA \"nonames\" configuration 94.92, SRI 94.0, Sterling Software 92.74..",
            "cite_ID": "W99-0612",
            "cite_maker_sids": [
                142
            ],
            "cite_sids": [
                141,
                142
            ],
            "cite_text": "It is not clear what resources are required to adapt systems to new languages.\"It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "When the outputs are scored in \"key-to-response\" mode, as though one annotator's output represented the \"key\" and the other the \"response,\" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%. Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-measure (P&R): 1. Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA \"fast\" configuration 95.66, SRA \"fastest\" configuration 92.61, SRA \"nonames\" configuration 94.92, SRI 94.0, Sterling Software 92.74.",
            "GPT_cite_text": "It is not clear what resources are required to adapt systems to new languages. It is important to mention that the F-measure for human performance on this task is about 96% (Sundheim 1995)."
        },
        {
            "Number": 11,
            "refer_ID": "X96-1048",
            "refer_sids": [
                72
            ],
            "refer_text": "Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..",
            "cite_ID": "E99-1001",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc.",
            "GPT_cite_text": "In an article on the Named Entity Recognition competition (part of MUC-6), Sundheim (1995) remarks that \"common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16)."
        },
        {
            "Number": 12,
            "refer_ID": "X96-1048",
            "refer_sids": [
                245,
                246
            ],
            "refer_text": "No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.",
            "cite_ID": "M98-1003",
            "cite_maker_sids": [
                3
            ],
            "cite_sids": [
                3
            ],
            "cite_text": "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks. The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.",
            "GPT_cite_text": "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks, saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995]."
        }
    ],
    "W11-0815": [
        {
            "Number": 1,
            "refer_ID": "W11-0815",
            "refer_sids": [
                217
            ],
            "refer_text": "This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.",
            "cite_ID": "D13-1060",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This work consists of investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing of these expressions.",
            "GPT_cite_text": "The task of identifying MWEs is relevant not only to lexical semantics applications, but also to machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002)."
        },
        {
            "Number": 2,
            "refer_ID": "W11-0815",
            "refer_sids": [
                18
            ],
            "refer_text": "The selection of appropriate indexing terms is a key factor for the quality of IR systems.",
            "cite_ID": "P09207_w11",
            "cite_maker_sids": [
                625,
                626
            ],
            "cite_sids": [
                625,
                626
            ],
            "cite_text": "Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al. (2011).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The selection of appropriate indexing terms is a key factor for the quality of IR systems.",
            "GPT_cite_text": "Joining the words of MWEs before indexing is a simple idea that was put into practice by Acosta et al. (2011)."
        },
        {
            "Number": 3,
            "refer_ID": "W11-0815",
            "refer_sids": [
                1
            ],
            "refer_text": "The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions.",
            "cite_ID": "Pproc9_w11",
            "cite_maker_sids": [
                15,
                16
            ],
            "cite_sids": [
                15,
                16
            ],
            "cite_text": "Our research in compositionality is motivated by the hypothesis that a special treatment of se\u00ad mantically non-compositional expressions can im\u00ad prove results in various Natural Language Process\u00ad ing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Re\u00ad trieval (IR).",
            "label": [
                "Method_CItation"
            ],
            "GPT_refer_text": "The extensive use of Multiword Expressions (MWEs) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions.",
            "GPT_cite_text": "Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NLP) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Retrieval (IR)."
        },
        {
            "Number": 4,
            "refer_ID": "W11-0815",
            "refer_sids": [
                47
            ],
            "refer_text": "This task aimed to explore the contribution of the disambiguation of words to bilingual or monolingual IR.",
            "cite_ID": "Pproc14_w11",
            "cite_maker_sids": [
                16,
                17,
                18,
                19
            ],
            "cite_sids": [
                16,
                17,
                18,
                19
            ],
            "cite_text": "As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti\u00ad cal machine translation. They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional\u00ad ity ofMWEs into their system, they could improve translation quality. Acosta et al. (2011) showed that treating non-compositional MWEs as a sin\u00ad gle unit in information retrieval improves retrieval effectiveness.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This task aimed to explore the contribution of the disambiguation of words to bilingual and monolingual IR.",
            "GPT_cite_text": "As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statistical machine translation. They show that even a large-scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositionality of MWEs into their system, they could improve translation quality. Acosta et al. (2011) showed that treating non-compositional MWEs as a single unit in information retrieval improves retrieval effectiveness."
        },
        {
            "Number": 5,
            "refer_ID": "W11-0815",
            "refer_sids": [
                6
            ],
            "refer_text": "One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.",
            "cite_ID": "S13-1039",
            "cite_maker_sids": [
                5
            ],
            "cite_sids": [
                5
            ],
            "cite_text": "Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improving results and ultimately leads to more precise man-machine interaction.",
            "GPT_cite_text": "Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006)."
        },
        {
            "Number": 6,
            "refer_ID": "W11-0815",
            "refer_sids": [
                21
            ],
            "refer_text": "For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101109, Portland, Oregon, USA, 23 June 2011.",
            "cite_ID": "W12-3311",
            "cite_maker_sids": [
                31
            ],
            "cite_sids": [
                31
            ],
            "cite_text": " Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For example, if the query was \"pop star meaning celebrity,\" and the terms were indexed individually, the relevant documents may not be retrieved, and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101-109, Portland, Oregon, USA, 23 June 2011.",
            "GPT_cite_text": "Information retrieval: when MWEs like pop stars are indexed as a unit, the accuracy of the system improves on multi-word queries (Acosta et al., 2011)."
        },
        {
            "Number": 7,
            "refer_ID": "W11-0815",
            "refer_sids": [
                15
            ],
            "refer_text": "In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.",
            "cite_ID": "W13-1006",
            "cite_maker_sids": [
                15,
                16
            ],
            "cite_sids": [
                15,
                16
            ],
            "cite_text": "Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Retrieval (IR).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an information retrieval (IR) system.",
            "GPT_cite_text": "Our research in compositionality is motivated by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NLP) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Retrieval (IR)."
        },
        {
            "Number": 8,
            "refer_ID": "W11-0815",
            "refer_sids": [
                17
            ],
            "refer_text": "Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system.",
            "cite_ID": "W13-3208",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997), and in this scenario, NLP techniques may contribute to the selection of MWEs for indexing as single units in the IR system.",
            "GPT_cite_text": "Biemann's idea and motivation is that non-compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010)."
        },
        {
            "Number": 9,
            "refer_ID": "W11-0815",
            "refer_sids": [
                30
            ],
            "refer_text": "The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.",
            "cite_ID": "W15-0909",
            "cite_maker_sids": [
                4,
                5,
                6
            ],
            "cite_sids": [
                4,
                5,
                6
            ],
            "cite_text": "(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.",
            "GPT_cite_text": "(2002), Baldwin and Kim (2009) has been shown to be useful in various NLP applications (Ramisch, 2012). Recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility in applications including information retrieval (\u201cIR\u201d: Acosta et al. (2011)) and machine translation (\u201cMT\u201d: Weller et al. (2014), Carpuat and Diab (2010), and Venkatapathy and Joshi (2006))."
        },
        {
            "Number": 10,
            "refer_ID": "W11-0815",
            "refer_sids": [
                62,
                64
            ],
            "refer_text": "We used Zettair to generate the ranked list of documents retrieved in response to each query.We used the cosine metric to calculate the scores and rank the documents.",
            "cite_ID": "W15-0909",
            "cite_maker_sids": [
                7,
                8
            ],
            "cite_sids": [
                7,
                8
            ],
            "cite_text": "For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We used Zettair to generate the ranked list of documents retrieved in response to each query. We used the cosine metric to calculate the scores and rank the documents.",
            "GPT_cite_text": "For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality."
        }
    ],
    "N09-1025": [
        {
            "Number": 1,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "C10-2052",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "GPT_cite_text": "We explore the different options for context and feature selection. See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454-462, Beijing, August 2010, the influence of different preprocessing methods, and different levels of sense granularity."
        },
        {
            "Number": 2,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We add more than 250 features to improve a syntax-based MT system, already the highest-scoring single system in the NIST 2008 Chinese-English common-data track by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "GPT_cite_text": "Recent work has shown that SMT benefits a lot from exploiting large amounts of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009)."
        },
        {
            "Number": 3,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "GPT_cite_text": "Overfitting problems often occur when training many features on a small dataset (Watanabe et al., 2007; Chiang et al., 2009)."
        },
        {
            "Number": 4,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                25
            ],
            "cite_sids": [
                25
            ],
            "cite_text": "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "GPT_cite_text": "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. Generally, this is done by deleting a node X0,1, leading to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally."
        },
        {
            "Number": 5,
            "refer_ID": "N09-1025",
            "refer_sids": [
                32,
                33
            ],
            "refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                159
            ],
            "cite_sids": [
                159
            ],
            "cite_text": "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "GPT_cite_text": "Following Chiang et al. (2009), we only use the 100 most frequent words for the word context feature."
        },
        {
            "Number": 6,
            "refer_ID": "N09-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                239
            ],
            "cite_sids": [
                239
            ],
            "cite_text": "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system.",
            "GPT_cite_text": "Lexicalize operator is used more frequently mainly due to the fact that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008)."
        },
        {
            "Number": 7,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "GPT_cite_text": "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009)."
        },
        {
            "Number": 8,
            "refer_ID": "N09-1025",
            "refer_sids": [
                14
            ],
            "refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                78,
                79
            ],
            "cite_sids": [
                77,
                78,
                79
            ],
            "cite_text": "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "GPT_cite_text": "The most prominent example of a tuning method that performs well on high-dimensional candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009)."
        },
        {
            "Number": 9,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                208
            ],
            "cite_sids": [
                207,
                208,
                209
            ],
            "cite_text": "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We add more than 250 features to improve a syntax-based MT system, already the highest-scoring single system in the NIST 2008 Chinese-English common-data track by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "GPT_cite_text": "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features. We used the following feature classes in SBMT extended scenarios only (cf."
        },
        {
            "Number": 10,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                210
            ],
            "cite_sids": [
                210,
                211
            ],
            "cite_text": "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We add more than 250 features to improve a syntax-based MT system, already the highest-scoring single system in the NIST 2008 Chinese-English common-data track by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "GPT_cite_text": "Chiang et al. (2009), Section 4.1: 10 \u2022 Rule overlap features \u2022 Node count features 9 For Chinese-English and Urdu-English SBMT, these features only fired when the inserted target word was unaligned to any source word."
        },
        {
            "Number": 11,
            "refer_ID": "N09-1025",
            "refer_sids": [
                14
            ],
            "refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                227
            ],
            "cite_sids": [
                227,
                228,
                229
            ],
            "cite_text": "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "GPT_cite_text": "5.4.1 MERT We used David Chiang\u2019s CMERT implementation. We, for the most part, follow the MIRA algorithm for machine translation as described by Chiang et al. (2009) but instead of using the 10-best of each of the best hw, hw + g, and hw - g, we use the 30-best according to hw. We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009)."
        },
        {
            "Number": 12,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "N12-1006",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102,
                103
            ],
            "cite_text": "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al.  (2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "GPT_cite_text": "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009)."
        },
        {
            "Number": 13,
            "refer_ID": "N09-1025",
            "refer_sids": [
                14
            ],
            "refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "cite_ID": "P12-1001",
            "cite_maker_sids": [
                147
            ],
            "cite_sids": [
                147
            ],
            "cite_text": "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "GPT_cite_text": "Alternatively, by using the large-margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (line 49), one can get an online algorithm such as PMOMIRA."
        },
        {
            "Number": 14,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "P13-1110",
            "cite_maker_sids": [
                36
            ],
            "cite_sids": [
                36
            ],
            "cite_text": "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "GPT_cite_text": "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009)."
        },
        {
            "Number": 15,
            "refer_ID": "N09-1025",
            "refer_sids": [
                14
            ],
            "refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "cite_ID": "P13-1110",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102
            ],
            "cite_text": "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "GPT_cite_text": "Task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012)."
        },
        {
            "Number": 16,
            "refer_ID": "N09-1025",
            "refer_sids": [
                65
            ],
            "refer_text": "Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.",
            "cite_ID": "P13-1110",
            "cite_maker_sids": [
                115
            ],
            "cite_sids": [
                115
            ],
            "cite_text": "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.",
            "GPT_cite_text": "The bound constraint B was set to 1.4. The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set."
        },
        {
            "Number": 17,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "P13-1110",
            "cite_maker_sids": [
                120
            ],
            "cite_sids": [
                120
            ],
            "cite_text": "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "GPT_cite_text": "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan, 2004). We also conducted an investigation into the setting of the B parameter."
        },
        {
            "Number": 18,
            "refer_ID": "N09-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.",
            "cite_ID": "P28_n09",
            "cite_maker_sids": [
                51
            ],
            "cite_sids": [
                51
            ],
            "cite_text": "Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system.",
            "GPT_cite_text": "Recently, a simple method was presented in (Chiang et al., 2009), which keeps partial English and Urdu words in the training data for alignment training."
        },
        {
            "Number": 19,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "P134_n09",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27,
                28
            ],
            "cite_text": "First, we used features proposed by Chiang et al. (2009): \u2022 phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) \u2022 target word insertion features \u2022 source word deletion features \u2022 word translation features \u2022 phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We add more than 250 features to improve a syntax-based MT system, already the highest-scoring single system in the NIST 2008 Chinese-English common-data track by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "GPT_cite_text": "First, we used features proposed by Chiang et al. (2009): \u2022 phrase pair count bin features (bins 1, 2, 3, 4, 5, 6, 10+) \u2022 target word insertion features \u2022 source word deletion features \u2022 word translation features \u2022 phrase length feature (source, target, both) Table 4: Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5)."
        },
        {
            "Number": 20,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "Pmert_n09",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We add more than 250 features to improve a syntax-based MT system, already the highest-scoring single system in the NIST 2008 Chinese-English common-data track by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "GPT_cite_text": "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and naturally incorporate regularization."
        },
        {
            "Number": 21,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "Pmert_n09",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We add more than 250 features to improve a syntax-based MT system, already the highest-scoring single system in the NIST 2008 Chinese-English common-data track by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "GPT_cite_text": "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation."
        },
        {
            "Number": 22,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "PMTS_n09",
            "cite_maker_sids": [
                218
            ],
            "cite_sids": [
                218
            ],
            "cite_text": "This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "GPT_cite_text": "This observation confirms previous findings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters."
        },
        {
            "Number": 23,
            "refer_ID": "N09-1025",
            "refer_sids": [
                65
            ],
            "refer_text": "Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.",
            "cite_ID": "PMTS_n09",
            "cite_maker_sids": [
                245
            ],
            "cite_sids": [
                245
            ],
            "cite_text": "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.",
            "GPT_cite_text": "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009)."
        },
        {
            "Number": 24,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "PMTS_n09",
            "cite_maker_sids": [
                252
            ],
            "cite_sids": [
                252
            ],
            "cite_text": "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "GPT_cite_text": "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm, to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009)."
        },
        {
            "Number": 25,
            "refer_ID": "N09-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.",
            "cite_ID": "PSMPT_n09",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                73,
                74
            ],
            "cite_text": "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system.",
            "GPT_cite_text": "Chiang et al. (2009) define new translational features using neighboring word contexts of the source phrase, which are directly integrated into the translation model of the Hiero system."
        },
        {
            "Number": 26,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "PTASL_n09",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                79
            ],
            "cite_text": "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "GPT_cite_text": "For example, Chiang et al. [3] designed many target-side syntactic features to improve the string-to-tree translation."
        },
        {
            "Number": 27,
            "refer_ID": "N09-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.",
            "cite_ID": "PTASL_n09",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system.",
            "GPT_cite_text": "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model."
        },
        {
            "Number": 28,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "W10-1757",
            "cite_maker_sids": [
                48
            ],
            "cite_sids": [
                48
            ],
            "cite_text": "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "GPT_cite_text": "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail."
        },
        {
            "Number": 29,
            "refer_ID": "N09-1025",
            "refer_sids": [
                32,
                33
            ],
            "refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "cite_ID": "W10-1757",
            "cite_maker_sids": [
                209
            ],
            "cite_sids": [
                209
            ],
            "cite_text": "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "GPT_cite_text": "Recent work by Chiang et al. (2009) describes new features for hierarchical phrase-based MT, while Collins and Koo (2005) describe features for parsing."
        },
        {
            "Number": 30,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "W10-1761",
            "cite_maker_sids": [
                53
            ],
            "cite_sids": [
                53,
                54
            ],
            "cite_text": "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We add more than 250 features to improve a syntax-based MT system, already the highest-scoring single system in the NIST 2008 Chinese-English common-data track by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "GPT_cite_text": "Chiang et al. (2009) added thousands of linguistically motivated features to hierarchical and syntactic systems; however, the source syntax features are derived from the research above."
        },
        {
            "Number": 31,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "W10-1761",
            "cite_maker_sids": [
                92
            ],
            "cite_sids": [
                92
            ],
            "cite_text": "A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "GPT_cite_text": "A non-exhaustive sample is given below: [X Ls, i, i] Terminal Symbol (X Ls) G X ADJ A1 Akt # X1 Act N P N E1 X2 # X1 X2 T OP N E1 letzter X2 # X1 Last X2 [X Fj,k Ls, i, j] [X Fj,k Ls, i, j + 1] Non-Terminal Symbol [X Fj,k Ls, i, j] [X, j, Rj,k] [X Fj,k Ls, i, Rj,k] [X Ls, i, Ri,j] [Fi,j = Ls] the log function (hm = log hm) [X Ls, i, Ri,j] log p(tPIPEs) = m hm(t, s) (3) m Goal [X Ls, 0, PIPEV PIPE 1] An advantage of our model over (Martin and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature functions remains the same. This model allows translation rules to take advantage of both syntactic label and word context."
        },
        {
            "Number": 32,
            "refer_ID": "N09-1025",
            "refer_sids": [
                32,
                33
            ],
            "refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "cite_ID": "W10-1761",
            "cite_maker_sids": [
                97
            ],
            "cite_sids": [
                97
            ],
            "cite_text": "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "GPT_cite_text": "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule nonterminals and requiring them to match the source span label."
        }
    ],
    "P00-1025": [
        {
            "Number": 1,
            "refer_ID": "P00-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.",
            "cite_ID": "J06-1004",
            "cite_maker_sids": [
                104
            ],
            "cite_sids": [
                104
            ],
            "cite_text": "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (FST), as shown in Figure 4, that implements the relation computationally.",
            "GPT_cite_text": "Beesley and Karttunen (2000) describe a technique called compile-replace for constructing FSTs, which involves reapplying the regular expression compiler to its own output."
        },
        {
            "Number": 2,
            "refer_ID": "P00-1025",
            "refer_sids": [
                123
            ],
            "refer_text": "This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.",
            "cite_ID": "P55-62_p00",
            "cite_maker_sids": [
                52
            ],
            "cite_sids": [
                52
            ],
            "cite_text": "Beesleyl7l presents a finite-state morphological analyzer for Arabic, which displays the root, pattern and prefixes/suffixes.",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.",
            "GPT_cite_text": "Beesley presents a finite-state morphological analyzer for Arabic, which displays the root, pattern, and prefixes/suffixes."
        },
        {
            "Number": 3,
            "refer_ID": "P00-1025",
            "refer_sids": [
                203
            ],
            "refer_text": "Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation.",
            "cite_ID": "P9852_p00",
            "cite_maker_sids": [
                200,
                201
            ],
            "cite_sids": [
                200,
                201
            ],
            "cite_text": "Thus, we employ the com pile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbi trarily complex sublanguages by specifying the brackets '\"'[ \" and \"A 1 \" to mark the domain of re duplication.",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation.",
            "GPT_cite_text": "Thus, we employ the compile-replace feature in xfst (Beesley & Karttunen, 2000). This feature allows the repetition of arbitrarily complex sublanguages by specifying the brackets '\"'[ \" and \"A1 \" to mark the domain of re duplication."
        },
        {
            "Number": 5,
            "refer_ID": "P00-1025",
            "refer_sids": [
                62
            ],
            "refer_text": "The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.",
            "cite_ID": "PE2006_p00",
            "cite_maker_sids": [
                104
            ],
            "cite_sids": [
                104
            ],
            "cite_text": "Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The compile-replace algorithm then reapplies the regular-expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.",
            "GPT_cite_text": "Beesley and Karttunen (2000) describe a technique called compile-replace for constructing FSTs, which involves reapplying the regular expression compiler to its own output."
        },
        {
            "Number": 6,
            "refer_ID": "P00-1025",
            "refer_sids": [
                119
            ],
            "refer_text": "In work more directly related to the current solution, it was Kataja and Koskenniemi (1988) who first demonstrated that Semitic (Akkadian) roots and patterns5 could be formalized as regular languages, and that the non-concatenative interdigitation of stems could be elegantly formalized as the intersection of those regular languages.",
            "cite_ID": "Plex_p00",
            "cite_maker_sids": [
                31
            ],
            "cite_sids": [
                31
            ],
            "cite_text": "Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003).",
            "label": [
                "Result Citation"
            ],
            "GPT_refer_text": "In work more directly related to the current solution, it was Kataja and Koskenniemi (1988) who first demonstrated that Semitic (Akkadian) roots and patterns could be formalized as regular languages, and that the non-concatenative interdigitation of stems could be elegantly formalized as the intersection of those regular languages.",
            "GPT_cite_text": "Challenging non-concatenative morphological phenomena, such as circumfixation and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003)."
        },
        {
            "Number": 7,
            "refer_ID": "P00-1025",
            "refer_sids": [
                16
            ],
            "refer_text": "This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below.",
            "cite_ID": "Plex_p00",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen 2000).",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full-stem reduplication and Arabic stem interdigitation, which will be described below.",
            "GPT_cite_text": "Even total reduplication can be characterized without going outside the regular languages (Beesley and Karttunen, 2000)."
        },
        {
            "Number": 8,
            "refer_ID": "P00-1025",
            "refer_sids": [
                96
            ],
            "refer_text": "In the regular expression calculus there are several operators that involve concatenation.",
            "cite_ID": "Pmorph_p00",
            "cite_maker_sids": [
                67
            ],
            "cite_sids": [
                67
            ],
            "cite_text": "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "In the regular expression calculus, there are several operators that involve concatenation.",
            "GPT_cite_text": "In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as defined in Beesley and Karttunen [4]."
        },
        {
            "Number": 10,
            "refer_ID": "P00-1025",
            "refer_sids": [
                131
            ],
            "refer_text": "3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.",
            "cite_ID": "Pstat_p00",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.",
            "GPT_cite_text": "The application of the merge algorithm to the lower side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000), and the result is shown in Figure 7."
        },
        {
            "Number": 11,
            "refer_ID": "P00-1025",
            "refer_sids": [
                202
            ],
            "refer_text": "The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.",
            "cite_ID": "W02-0503-parscit130908",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The technique described here, implemented in the compile-replace algorithm, allows the regular expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.",
            "GPT_cite_text": "Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular expression compiler to its own output."
        },
        {
            "Number": 12,
            "refer_ID": "P00-1025",
            "refer_sids": [
                123
            ],
            "refer_text": "This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.",
            "cite_ID": "W07-0802-parscit130908",
            "cite_maker_sids": [
                134
            ],
            "cite_sids": [
                134
            ],
            "cite_text": "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.",
            "GPT_cite_text": "A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphological Analyzer and Generator (Beesley and Karttunen, 2000; Beesley, 2001)."
        },
        {
            "Number": 13,
            "refer_ID": "P00-1025",
            "refer_sids": [
                50
            ],
            "refer_text": "All the finite-state operations (concatenation, union, intersection, composition, etc.) can be simulated by an apply routine at runtime.",
            "cite_ID": "W08-0703",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "All the finite-state operations (concatenation, union, intersection, composition, etc.) can be simulated by an apply routine at runtime.",
            "GPT_cite_text": "In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm."
        },
        {
            "Number": 14,
            "refer_ID": "P00-1025",
            "refer_sids": [
                205
            ],
            "refer_text": "The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific",
            "cite_ID": "W09-0802",
            "cite_maker_sids": [
                207
            ],
            "cite_sids": [
                207
            ],
            "cite_text": "The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).",
            "label": [
                "Method Citation"
            ],
            "GPT_refer_text": "The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specifics.",
            "GPT_cite_text": "The interdigitation is handled using a compile-replace process with the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995)."
        }
    ],
    "J98-2005": [
        {
            "Number": 1,
            "refer_ID": "J98-2005",
            "refer_sids": [
                17
            ],
            "refer_text": "We will show that in both cases the estimated probability is tight.",
            "cite_ID": "J01-2004",
            "cite_maker_sids": [
                72
            ],
            "cite_sids": [
                72
            ],
            "cite_text": "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We will show that in both cases the estimated probability is tight.",
            "GPT_cite_text": "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight."
        },
        {
            "Number": 2,
            "refer_ID": "J98-2005",
            "refer_sids": [
                45
            ],
            "refer_text": "(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator.",
            "cite_ID": "N03-1027",
            "cite_maker_sids": [
                56
            ],
            "cite_sids": [
                56
            ],
            "cite_text": "Chi and Geman (1998) proved that this con\u00addition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "(8fl)ea ~(B --/3) = ~=lf(B --/3; cai) (3) c~ s.t. H <B-e~ =lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator.",
            "GPT_cite_text": "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus."
        },
        {
            "Number": 3,
            "refer_ID": "J98-2005",
            "refer_sids": [
                17
            ],
            "refer_text": "We will show that in both cases the estimated probability is tight.",
            "cite_ID": "P01-1017",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                79
            ],
            "cite_text": "When a PCFG probability distribu\u00adtion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum\u00adming to one) probability distribution over strings [5], thus making them appropriate for language models.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "We will show that in both cases the estimated probability is tight.",
            "GPT_cite_text": "When a PCFG probability distribution is estimated from training data (in our case the Penn Treebank), PCFGs define a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models."
        },
        {
            "Number": 6,
            "refer_ID": "J98-2005",
            "refer_sids": [
                16,
                17
            ],
            "refer_text": "If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?We will show that in both cases the estimated probability is tight.",
            "cite_ID": "P13-1102",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                23
            ],
            "cite_text": "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "If the corpus is unparsed, then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm\u2014again, see Section 2), and the same question arises: do we get actual probabilities, or do the estimated PCFGs assign some mass to infinite trees? We will show that in both cases the estimated probability is tight.",
            "GPT_cite_text": "Chi and Geman (1998) studied the question for Maximum Likelihood (ML) estimation and showed that ML estimates are always tight for both the supervised case (where the input consists of parse trees) and the unsupervised case (where the input consists of yields or terminal strings)."
        }
    ],
    "D09-1023": [
        {
            "Number": 1,
            "refer_ID": "D09-1023",
            "refer_sids": [
                17
            ],
            "refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.",
            "cite_ID": "D09-1086",
            "cite_maker_sids": [
                78
            ],
            "cite_sids": [
                78
            ],
            "cite_text": "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel approach. To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219-228, Singapore, 6-7 August 2009.",
            "GPT_cite_text": "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009)."
        },
        {
            "Number": 3,
            "refer_ID": "D09-1023",
            "refer_sids": [
                17
            ],
            "refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.",
            "cite_ID": "D11-1044",
            "cite_maker_sids": [
                1
            ],
            "cite_sids": [
                1
            ],
            "cite_text": "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel approach. To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219-228, Singapore, 6-7 August 2009.",
            "GPT_cite_text": "We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words, as in previous work (Gimpel and Smith, 2009)."
        },
        {
            "Number": 4,
            "refer_ID": "D09-1023",
            "refer_sids": [
                17
            ],
            "refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.",
            "cite_ID": "D11-1044",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel approach. To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219-228, Singapore, 6-7 August 2009.",
            "GPT_cite_text": "Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009)."
        },
        {
            "Number": 5,
            "refer_ID": "D09-1023",
            "refer_sids": [
                17
            ],
            "refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.",
            "cite_ID": "D11-1044",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                23
            ],
            "cite_text": "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel approach. To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219-228, Singapore, 6-7 August 2009.",
            "GPT_cite_text": "We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level."
        },
        {
            "Number": 6,
            "refer_ID": "D09-1023",
            "refer_sids": [
                17
            ],
            "refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 67 August 2009.",
            "cite_ID": "D11-1044",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41
            ],
            "cite_text": "We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We compare lexical phrase and dependency syntax features, as well as a novel approach. To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219-228, Singapore, 6-7 August 2009.",
            "GPT_cite_text": "We denote this grammar by Gs,s; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009)."
        },
        {
            "Number": 7,
            "refer_ID": "D09-1023",
            "refer_sids": [
                260
            ],
            "refer_text": "Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle non-local features using generic techniques that also support efficient parameter estimation.",
            "cite_ID": "D11-1044",
            "cite_maker_sids": [
                146
            ],
            "cite_sids": [
                146
            ],
            "cite_text": "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle non-local features using generic techniques that also support efficient parameter estimation.",
            "GPT_cite_text": "For a QPDG model, decoding consists of finding the highest-scoring tuple (t, a) for an input sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s. We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to-fine strategy (Petrov, 2009) to speed up decoding."
        },
        {
            "Number": 9,
            "refer_ID": "D09-1023",
            "refer_sids": [
                204
            ],
            "refer_text": "We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura Phrase Syntactic Features: features: +f att  f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU).",
            "cite_ID": "Pjournal",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                79
            ],
            "cite_text": "(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configuration Phrase Syntactic Features: features: +f att f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU).",
            "GPT_cite_text": "(2007) ArEn[UN, NIST 06][L] SL: lexical, morphological, and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL: syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features integrated into Berger et al."
        },
        {
            "Number": 10,
            "refer_ID": "D09-1023",
            "refer_sids": [
                1
            ],
            "refer_text": "We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.",
            "cite_ID": "Pjournal",
            "cite_maker_sids": [
                139
            ],
            "cite_sids": [
                139
            ],
            "cite_text": "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.",
            "GPT_cite_text": "Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5)."
        },
        {
            "Number": 12,
            "refer_ID": "D09-1023",
            "refer_sids": [
                123
            ],
            "refer_text": "Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.",
            "cite_ID": "Pproc_d09",
            "cite_maker_sids": [
                205
            ],
            "cite_sids": [
                205
            ],
            "cite_text": "Gimpel & Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.",
            "GPT_cite_text": "Gimpel and Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding."
        },
        {
            "Number": 16,
            "refer_ID": "D09-1023",
            "refer_sids": [
                11
            ],
            "refer_text": "Here we take first steps toward such a universal decoder, making the following contributions:Arbitrary feature model (2): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.",
            "cite_ID": "W10-1730-parscit",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Here we take first steps toward such a universal decoder, making the following contributions: Arbitrary feature model (2): We define a single, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.",
            "GPT_cite_text": "Log-linear translation models (instead of MLE) with rich feature sets are also used in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997)."
        },
        {
            "Number": 18,
            "refer_ID": "D09-1023",
            "refer_sids": [
                254
            ],
            "refer_text": "Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism.",
            "cite_ID": "W11-2139",
            "cite_maker_sids": [
                37
            ],
            "cite_sids": [
                37
            ],
            "cite_text": "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).",
            "label": [
                "Result_Citation"
            ],
            "GPT_refer_text": "Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to-tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non-isomorphism.",
            "GPT_cite_text": "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008)."
        },
        {
            "Number": 20,
            "refer_ID": "D09-1023",
            "refer_sids": [
                191
            ],
            "refer_text": "Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.",
            "cite_ID": "N10-1040",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.",
            "GPT_cite_text": "Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to-right on the target sentence."
        }
    ],
    "P98-1081": [
        {
            "Number": 2,
            "refer_ID": "P98-1081",
            "refer_sids": [
                116,
                117
            ],
            "refer_text": "A next step is to examine them in pairs.We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.",
            "cite_ID": "W01-0712",
            "cite_maker_sids": [
                130
            ],
            "cite_sids": [
                130
            ],
            "cite_text": "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.",
            "GPT_cite_text": "And finally, TAGPAIR uses classification pair weights based on the probability of a classification for some predicted classification pair (van Halteren et al., 1998)."
        },
        {
            "Number": 3,
            "refer_ID": "P98-1081",
            "refer_sids": [
                80,
                89
            ],
            "refer_text": "In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.We accept that we are measuring quality in relation to a specific tagging",
            "cite_ID": "W01-0712",
            "cite_maker_sids": [
                135
            ],
            "cite_sids": [
                134,
                135
            ],
            "cite_text": "Like Van Halteren et al.(1998), we evaluated two features combinations.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In order to see whether the combination of the component taggers is likely to lead to improvements in tagging quality, we first examine the results of the individual taggers when applied to Tune. We accept that we are measuring quality in relation to a specific tagging.",
            "GPT_cite_text": "Like Van Halteren et al. (1998), we evaluated two feature combinations."
        },
        {
            "Number": 5,
            "refer_ID": "P98-1081",
            "refer_sids": [
                116,
                117,
                124
            ],
            "refer_text": "A next step is to examine them in pairs.We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).",
            "cite_ID": "W02-1004",
            "cite_maker_sids": [
                105
            ],
            "cite_sids": [
                105,
                106,
                107,
                108,
                109,
                110,
                111
            ],
            "cite_text": "Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout\u00adputs s2, P(sls i(xd)=ss j(xd)=s2), is com\u00adputed on development data, and the posterior prob\u00adability is estimated as N P(slx,d)e\u00c6(s,sAk(x,d))+\u00c6(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi\u00adments presented in van Halteren et al.(1998), this method was the best performer among the presented methods.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, but does not yet approach the level where all tying majority votes are handled correctly (98.31%).",
            "GPT_cite_text": "Van Halteren et al. (1998) introduce a modified version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classifier i outputs s and classifier j outputs s2, P(s | i(xd) = s1, j(xd) = s2), is computed on development data, and the posterior probability is estimated as N P(s | x, d) e^(\u00c6(s, sA_k(x, d)) + \u00c6(s, sA_j(x, d))) (7) k..j where s_c;,j(xfd) = argmax_t P(t | s_c;(xfd), s_cj(xfd)). Each classifier votes for its classification and every pair of classifiers votes for the sense that is most likely given the joint classification. In the experiments presented in Van Halteren et al. (1998), this method was the best performer among the presented methods."
        },
        {
            "Number": 7,
            "refer_ID": "P98-1081",
            "refer_sids": [
                100,
                102,
                116,
                118
            ],
            "refer_text": "The most democratic option is to give each tagger one vote (Majority).This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).A next step is to examine them in pairs.When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.",
            "cite_ID": "E99-1025",
            "cite_maker_sids": [
                118
            ],
            "cite_sids": [
                117,
                118
            ],
            "cite_text": "We consider three voting strategies suggested by van Halteren et al.(1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The most democratic option is to give each tagger one vote (Majority). This can be general quality, e.g., each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g., each tagger votes its precision on the suggested tag (Tag-Precision). A next step is to examine them in pairs. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e., not just the ones suggested by the component taggers.",
            "GPT_cite_text": "We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pairwise voting."
        },
        {
            "Number": 9,
            "refer_ID": "P98-1081",
            "refer_sids": [
                100
            ],
            "refer_text": "The most democratic option is to give each tagger one vote (Majority).",
            "cite_ID": "P06-2060",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35
            ],
            "cite_text": "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The most democratic option is to give each tagger one vote (majority).",
            "GPT_cite_text": "Halteren et al. (1998) compare a number of voting methods, including a majority vote scheme, with other combination methods for part-of-speech tagging."
        },
        {
            "Number": 10,
            "refer_ID": "P98-1081",
            "refer_sids": [
                158
            ],
            "refer_text": "Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems.",
            "cite_ID": "A00-1024",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                42,
                43
            ],
            "cite_text": "Thirdly, this approach is compatible with in\u00ad corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Our experiment shows that, at least for the task at hand, the combination of several different systems allows us to raise the performance ceiling for data-driven systems.",
            "GPT_cite_text": "Thirdly, this approach is compatible with incorporating multiple components of the same type to improve performance (cf. van Halteren et al. (1998), who found that combining the results of several parts of speech taggers increased performance)."
        },
        {
            "Number": 11,
            "refer_ID": "P98-1081",
            "refer_sids": [
                101,
                102
            ],
            "refer_text": "However, it appears more useful to give more weight to taggers which have proved their quality.This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10,
                11
            ],
            "cite_text": "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).In both cases the investigators were able to achieve significant improvements over the previous best tagging results.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, it appears more useful to give more weight to taggers that have proved their quality. This can be general quality, e.g., each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g., each tagger votes its precision on the suggested tag (Tag-Precision).",
            "GPT_cite_text": "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases, the investigators were able to achieve significant improvements over the previous best tagging results."
        },
        {
            "Number": 12,
            "refer_ID": "P98-1081",
            "refer_sids": [
                145,
                146,
                147
            ],
            "refer_text": "The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.Also of note is the improvement yielded by the best combination.The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                82
            ],
            "cite_sids": [
                81,
                82,
                83
            ],
            "cite_text": "Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on the test, a 19.1% reduction in error rate over the best individual system, viz.",
            "GPT_cite_text": "Van Halteren et al. (1998) have generalized this approach for a higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by its respective accuracy."
        },
        {
            "Number": 13,
            "refer_ID": "P98-1081",
            "refer_sids": [
                101,
                102
            ],
            "refer_text": "However, it appears more useful to give more weight to taggers which have proved their quality.This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                90
            ],
            "cite_sids": [
                90
            ],
            "cite_text": "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "However, it appears more useful to give more weight to taggers that have proved their quality. This can be general quality, e.g., each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g., each tagger votes its precision on the suggested tag (Tag-Precision).",
            "GPT_cite_text": "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers: Memory-Based and Decision-Tree-Based."
        },
        {
            "Number": 14,
            "refer_ID": "P98-1081",
            "refer_sids": [
                130,
                137
            ],
            "refer_text": "The first choice for this is to use a Memory- Based second level learner.To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.",
            "cite_ID": "W05-1518",
            "cite_maker_sids": [
                111
            ],
            "cite_sids": [
                111
            ],
            "cite_text": "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The first choice for this is to use a memory-based second-level learner. To examine if the overtraining effects are specific to this particular second-level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.",
            "GPT_cite_text": "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used."
        },
        {
            "Number": 15,
            "refer_ID": "P98-1081",
            "refer_sids": [
                116,
                117
            ],
            "refer_text": "A next step is to examine them in pairs.We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.",
            "cite_ID": "W00-0733",
            "cite_maker_sids": [
                26
            ],
            "cite_sids": [
                26
            ],
            "cite_text": "The most advanced voting method ex\u00ad amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag\u00ad Pair, Van Halteren et al., (1998)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.",
            "GPT_cite_text": "The most advanced voting method examines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag-Pair, Van Halteren et al., 1998)."
        },
        {
            "Number": 16,
            "refer_ID": "P98-1081",
            "refer_sids": [
                94,
                101
            ],
            "refer_text": "Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers.However, it appears more useful to give more weight to taggers which have proved their quality.",
            "cite_ID": "W00-0733",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18,
                19,
                20
            ],
            "cite_text": "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).Five are so-called voting methods.They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Simple Voting: There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. However, it appears more useful to give more weight to taggers that have proved their quality.",
            "GPT_cite_text": "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag."
        },
        {
            "Number": 17,
            "refer_ID": "P98-1081",
            "refer_sids": [
                129
            ],
            "refer_text": "The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern.",
            "cite_ID": "W00-0733",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "For this purpose we have used the part-of-speech tag of the cur\u00ad rent word as compressed representation of the first stage input (Van Halteren et al., 1998).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The second stage can be provided with the first-level outputs and with additional information, e.g., about the original input pattern.",
            "GPT_cite_text": "For this purpose, we have used the part-of-speech tag of the current word as a compressed representation of the first stage input (Van Halteren et al., 1998)."
        },
        {
            "Number": 18,
            "refer_ID": "P98-1081",
            "refer_sids": [
                136,
                145,
                146,
                147,
                161
            ],
            "refer_text": "This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.Also of note is the improvement yielded by the best combination.The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages.",
            "cite_ID": "J01-2002",
            "cite_maker_sids": [
                40
            ],
            "cite_sids": [
                40,
                41,
                42
            ],
            "cite_text": "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "This is most likely an overtraining effect: Tune is probably too small to collect case bases that can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tag sets and/or languages.",
            "GPT_cite_text": "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, work. In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally."
        },
        {
            "Number": 19,
            "refer_ID": "P98-1081",
            "refer_sids": [
                75,
                90,
                136
            ],
            "refer_text": "The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune.This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.",
            "cite_ID": "J01-2002",
            "cite_maker_sids": [
                183
            ],
            "cite_sids": [
                183,
                184,
                185,
                186
            ],
            "cite_text": "Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998).This consisted of 114K.tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The second part, Tune, consists of 10% of the data (every ninth utterance, 114,479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods. All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune. This is most likely an overtraining effect: Tune is probably too small to collect case bases that can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.",
            "GPT_cite_text": "Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners."
        },
        {
            "Number": 20,
            "refer_ID": "P98-1081",
            "refer_sids": [
                3,
                4,
                125
            ],
            "refer_text": "Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.After comparison, their outputs are combined using several voting strategies and second stage classifiers.Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.",
            "cite_ID": "J01-2002",
            "cite_maker_sids": [
                570
            ],
            "cite_sids": [
                570,
                571,
                573
            ],
            "cite_text": "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).In both approaches, different tagger gen\u00ad erators were applied to the same training data and their predictions combined using different combination methods, including stacking.As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules, and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second-stage classifiers. Stacked classifiers, from the measurements so far, appear to lead to a better accuracy improvement.",
            "GPT_cite_text": "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger generators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison."
        },
        {
            "Number": 21,
            "refer_ID": "P98-1081",
            "refer_sids": [
                116,
                117,
                127,
                128,
                134
            ],
            "refer_text": "A next step is to examine them in pairs.We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.is usually called stacking (Wolpert 1992).Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.",
            "cite_ID": "J01-2002",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102,
                103,
                104
            ],
            "cite_text": "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele\u00ad mans 1998) is the TagPair method.It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. The practice of feeding the outputs of a number of classifiers as features for a next learner is significantly better than the runner-up (Precision-Recall) with p=0. is usually called stacking (Wolpert 1992). Surprisingly, none of the Memory-Based methods reaches the quality of TagPair.",
            "GPT_cite_text": "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daelemans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers."
        },
        {
            "Number": 22,
            "refer_ID": "P98-1081",
            "refer_sids": [
                124,
                134
            ],
            "refer_text": "When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.",
            "cite_ID": "J01-2002",
            "cite_maker_sids": [
                487
            ],
            "cite_sids": [
                487,
                488
            ],
            "cite_text": "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Surprisingly, none of the Memory-Based methods reaches the quality of TagPair.",
            "GPT_cite_text": "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed."
        },
        {
            "Number": 23,
            "refer_ID": "P98-1081",
            "refer_sids": [
                63
            ],
            "refer_text": "The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986).",
            "cite_ID": "J01-2002",
            "cite_maker_sids": [
                176
            ],
            "cite_sids": [
                176
            ],
            "cite_text": "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The data we use for our experiment consists of the tagged LOB corpus (Johansson, 1986).",
            "GPT_cite_text": "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proven to be a good testing ground."
        },
        {
            "Number": 24,
            "refer_ID": "P98-1081",
            "refer_sids": [
                3,
                107
            ],
            "refer_text": "Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.Table 2: Accuracy of individual taggers and combination methods.",
            "cite_ID": "J01-2002",
            "cite_maker_sids": [
                309
            ],
            "cite_sids": [
                309
            ],
            "cite_text": "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im\u00ad plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules, and Maximum Entropy) are trained on the same corpus data. Table 2: Accuracy of individual taggers and combination methods.",
            "GPT_cite_text": "In van Halteren, Zavrel, and Daelemans (1998), we used a straightforward implementation of HMMs, which turned out to have the worst accuracy of the four competing methods."
        },
        {
            "Number": 25,
            "refer_ID": "P98-1081",
            "refer_sids": [
                130,
                134,
                137,
                138
            ],
            "refer_text": "The first choice for this is to use a Memory- Based second level learner.Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.1\u00b0 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.",
            "cite_ID": "J01-2002",
            "cite_maker_sids": [
                398
            ],
            "cite_sids": [
                398
            ],
            "cite_text": "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The first choice for this is to use a memory-based second-level learner. Surprisingly, none of the memory-based methods reaches the quality of TagPair. To examine if the overtraining effects are specific to this particular second-level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material. Because C5.0 prunes the decision tree, the overfitting of training material (tune) is less than with memory-based learning, but the results on test are also worse.",
            "GPT_cite_text": "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word."
        }
    ],
    "N01-1011": [
        {
            "Number": 1,
            "refer_ID": "N01-1011",
            "refer_sids": [
                84,
                101,
                161
            ],
            "refer_text": "Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems.Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coe\u00c6cient.While the accuracy of this approach was as good as any previously published results, the learned models were complex and di\u00c6cult to interpret, in e?ect acting as very accurate black boxes.",
            "cite_ID": "W02-0812",
            "cite_maker_sids": [
                131
            ],
            "cite_sids": [
                131,
                132,
                133
            ],
            "cite_text": "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the ear\u00adlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the log\u2013likelihood ratio.This earlier approach was evaluated on the SENSEVAL\u00ad1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems. Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coefficient. While the accuracy of this approach was as good as any previously published results, the learned models were complex and difficult to interpret, in effect acting as very accurate black boxes.",
            "GPT_cite_text": "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bigrams, according to the log-likelihood ratio. This earlier approach was evaluated on the SENSEVAL-1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data."
        },
        {
            "Number": 2,
            "refer_ID": "N01-1011",
            "refer_sids": [
                18,
                19,
                61
            ],
            "refer_text": "The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.",
            "cite_ID": "W04-0813",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "We also obtain salient bigrams in the con\u00adtext, with the methods and the software de\u00adscribed in (Pedersen, 2001).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The approach in this paper relies upon a feature set made up of bigrams, two-word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.",
            "GPT_cite_text": "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001)."
        },
        {
            "Number": 3,
            "refer_ID": "N01-1011",
            "refer_sids": [
                189
            ],
            "refer_text": "This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.",
            "cite_ID": "W02-1011",
            "cite_maker_sids": [
                136
            ],
            "cite_sids": [
                136
            ],
            "cite_text": "in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.",
            "GPT_cite_text": "In fact, Pedersen (2001) found that bigrams alone can be effective features for word sense disambiguation."
        },
        {
            "Number": 4,
            "refer_ID": "N01-1011",
            "refer_sids": [
                189
            ],
            "refer_text": "This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.",
            "cite_ID": "N03-3004",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49
            ],
            "cite_text": "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder\u00adsen, 2001).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.",
            "GPT_cite_text": "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001)."
        },
        {
            "Number": 7,
            "refer_ID": "N01-1011",
            "refer_sids": [
                40,
                42
            ],
            "refer_text": "However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.",
            "cite_ID": "J02-2003",
            "cite_maker_sids": [
                148
            ],
            "cite_sids": [
                148
            ],
            "cite_text": "In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "However, Cressie and Read (1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. Unfortunately, it is usually not clear which test is most appropriate for a particular sample of data.",
            "GPT_cite_text": "In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic."
        },
        {
            "Number": 8,
            "refer_ID": "N01-1011",
            "refer_sids": [
                18,
                29,
                37
            ],
            "refer_text": "The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen.A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic.",
            "cite_ID": "W08-0611",
            "cite_maker_sids": [
                103
            ],
            "cite_sids": [
                103
            ],
            "cite_text": "\u2022 Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The approach in this paper relies upon a feature set made up of bigrams, two-word sequences that occur in a text. Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen. A number of well-known statistics belong to this family, including the likelihood ratio statistic G\u00b2 and Pearson's X\u00b2 statistic.",
            "GPT_cite_text": "\u2022 Salient bigrams: Salient bigrams within the abstract have high log-likelihood scores, as described by Pedersen (2001)."
        }
    ],
    "D10-1058": [
        {
            "Number": 1,
            "refer_ID": "D10-1058",
            "refer_sids": [
                3,
                4,
                6
            ],
            "refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.",
            "cite_ID": "C16-1060",
            "cite_maker_sids": [
                48
            ],
            "cite_sids": [
                48
            ],
            "cite_text": "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves a lower alignment error rate than the hidden Markov model, but also runs faster. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.",
            "GPT_cite_text": "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations."
        },
        {
            "Number": 2,
            "refer_ID": "D10-1058",
            "refer_sids": [
                18,
                26,
                33,
                34
            ],
            "refer_text": "Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.Our model is a coherent generative model that combines the HMM and IBM Model 4.Our model is much faster than IBM Model 4.In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                24
            ],
            "cite_sids": [
                24
            ],
            "cite_text": "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Our model is a coherent generative model that combines the HMM and IBM Model 4. Our model is much faster than IBM Model 4. In fact, we will show that it is also faster than the HMM and has a lower alignment error rate than the HMM.",
            "GPT_cite_text": "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation."
        },
        {
            "Number": 3,
            "refer_ID": "D10-1058",
            "refer_sids": [
                6,
                46
            ],
            "refer_text": "We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                33
            ],
            "cite_sids": [
                33
            ],
            "cite_text": ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j = 1 (a_j, i) which has nice probabilistic guarantees.",
            "GPT_cite_text": "`, fJ and word alignment vectors are used to estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).`"
        },
        {
            "Number": 4,
            "refer_ID": "D10-1058",
            "refer_sids": [
                86
            ],
            "refer_text": "Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J PIPEe2I +1); 1 1 1 1 are further away from the mean have low probability.",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P(I, aJ, fJ | e2I + 1); those that are further away from the mean have low probability.",
            "GPT_cite_text": "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution."
        },
        {
            "Number": 5,
            "refer_ID": "D10-1058",
            "refer_sids": [
                88,
                90
            ],
            "refer_text": "Our model has only one parameter for each target word, which can be learned more reliably.i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                82
            ],
            "cite_sids": [
                82
            ],
            "cite_text": "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our model has only one parameter for each target word, which can be learned more reliably. The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).",
            "GPT_cite_text": "Prior work addressed this by using the single-parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010)."
        },
        {
            "Number": 6,
            "refer_ID": "D10-1058",
            "refer_sids": [
                110
            ],
            "refer_text": "1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                99
            ],
            "cite_sids": [
                99
            ],
            "cite_text": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each target sentence e2I +1 and source sentence fJ, we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.",
            "GPT_cite_text": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010)."
        },
        {
            "Number": 7,
            "refer_ID": "D10-1058",
            "refer_sids": [
                113,
                114,
                115
            ],
            "refer_text": "This Gibbs sampling method updates parameters constantly, so it is an online learning algorithm.However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.Instead, we do batch learning: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).",
            "cite_ID": "P59105ca",
            "cite_maker_sids": [
                45
            ],
            "cite_sids": [
                45
            ],
            "cite_text": "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This Gibbs sampling method updates parameters constantly, so it is an online learning algorithm. However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel. Instead, we do batch learning: we fix the parameters, scan through the entire corpus, and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M-step).",
            "GPT_cite_text": "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any priors on the parameters."
        },
        {
            "Number": 8,
            "refer_ID": "D10-1058",
            "refer_sids": [
                14,
                26,
                27
            ],
            "refer_text": "Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.Our model is a coherent generative model that combines the HMM and IBM Model 4.It is easier to understand than IBM Model 4 (see Section 3).",
            "cite_ID": "P87-94",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                79
            ],
            "cite_text": "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3).",
            "GPT_cite_text": "Zhao proposes a brief fertility-based HMM model, which also decreases the complexity of Model A: Fully Bayesian Inference for Word Alignment."
        },
        {
            "Number": 9,
            "refer_ID": "D10-1058",
            "refer_sids": [
                29,
                30
            ],
            "refer_text": "We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.Qc 2010 Association for Computational Linguistics estimation.",
            "cite_ID": "Pbulletin",
            "cite_maker_sids": [
                104
            ],
            "cite_sids": [
                103,
                104
            ],
            "cite_text": "For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter estimation. 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596-605, MIT, Massachusetts, USA, 9-11 October 2010. \u00a9 2010 Association for Computational Linguistics.",
            "GPT_cite_text": "For models with fertility computing, the expectations instead become intractable, and previous authors have solved this by using approximative methods. The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors). R. Stling, J. Tiedemann, Efficient Word Alignment with MCMC (125-146), greedy optimization techniques (Brown et al., 1993), or local Gibbs sampling (Zhao and Gildea, 2010)."
        },
        {
            "Number": 10,
            "refer_ID": "D10-1058",
            "refer_sids": [
                29,
                30,
                107,
                108
            ],
            "refer_text": "We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.Qc 2010 Association for Computational Linguistics estimation.Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.",
            "cite_ID": "Pbulletin",
            "cite_maker_sids": [
                131
            ],
            "cite_sids": [
                131
            ],
            "cite_text": "Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter estimation. Although we can estimate the parameters by using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977), in order to compute the expected counts, we have to sum over all possible alignments, which is, unfortunately, exponential.",
            "GPT_cite_text": "Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for an HMM model with fertility."
        },
        {
            "Number": 11,
            "refer_ID": "D10-1058",
            "refer_sids": [
                3
            ],
            "refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.",
            "cite_ID": "Pcoling_D10",
            "cite_maker_sids": [
                232
            ],
            "cite_sids": [
                232
            ],
            "cite_text": "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We built a fertility-hidden Markov model by adding fertility to the hidden Markov model.",
            "GPT_cite_text": "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010), who added a fertility distribution to the HMM."
        },
        {
            "Number": 12,
            "refer_ID": "D10-1058",
            "refer_sids": [
                3,
                46
            ],
            "refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                24
            ],
            "cite_sids": [
                24
            ],
            "cite_text": "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j = 1 (a_j, i) which has nice probabilistic guarantees.",
            "GPT_cite_text": "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation."
        },
        {
            "Number": 13,
            "refer_ID": "D10-1058",
            "refer_sids": [
                86
            ],
            "refer_text": "Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J PIPEe2I +1); 1 1 1 1 are further away from the mean have low probability.",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                38
            ],
            "cite_sids": [
                38
            ],
            "cite_text": "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P(I, aJ, fJ | e2I + 1); those that are further away from the mean have low probability.",
            "GPT_cite_text": "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution."
        },
        {
            "Number": 14,
            "refer_ID": "D10-1058",
            "refer_sids": [
                6,
                46
            ],
            "refer_text": "We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                39
            ],
            "cite_text": "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j = 1 (a_j, i) which has nice probabilistic guarantees.",
            "GPT_cite_text": "I Pr(f, ale) = p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010)."
        },
        {
            "Number": 15,
            "refer_ID": "D10-1058",
            "refer_sids": [
                88,
                90
            ],
            "refer_text": "Our model has only one parameter for each target word, which can be learned more reliably.i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our model has only one parameter for each target word, which can be learned more reliably. The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).",
            "GPT_cite_text": "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010)."
        },
        {
            "Number": 16,
            "refer_ID": "D10-1058",
            "refer_sids": [
                110
            ],
            "refer_text": "1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                104
            ],
            "cite_sids": [
                104
            ],
            "cite_text": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For each target sentence e2I +1 and source sentence fJ, we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.",
            "GPT_cite_text": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010)."
        },
        {
            "Number": 17,
            "refer_ID": "D10-1058",
            "refer_sids": [
                3,
                4,
                5,
                6
            ],
            "refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.It is similar in some ways to IBM Model 4, but is much easier to understand.We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.",
            "cite_ID": "Q13-1024",
            "cite_maker_sids": [
                60
            ],
            "cite_sids": [
                60
            ],
            "cite_text": "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves a lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.",
            "GPT_cite_text": "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based models can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011)."
        },
        {
            "Number": 18,
            "refer_ID": "D10-1058",
            "refer_sids": [
                46
            ],
            "refer_text": "We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.",
            "cite_ID": "Q13-1024",
            "cite_maker_sids": [
                130
            ],
            "cite_sids": [
                130
            ],
            "cite_text": "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j = 1 (a_j, i) which has nice probabilistic guarantees.",
            "GPT_cite_text": "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source-side dependency tree."
        }
    ],
    "H89-2014": [
        {
            "Number": 1,
            "refer_ID": "H89-2014",
            "refer_sids": [
                27
            ],
            "refer_text": "The work described here also makes use of a hidden Markov model.",
            "cite_ID": "W93-0111",
            "cite_maker_sids": [
                178
            ],
            "cite_sids": [
                178
            ],
            "cite_text": "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The work described here also makes use of a hidden Markov model.",
            "GPT_cite_text": "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989)."
        },
        {
            "Number": 2,
            "refer_ID": "H89-2014",
            "refer_sids": [
                124,
                125
            ],
            "refer_text": "A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words).A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary.",
            "cite_ID": "A92-1018",
            "cite_maker_sids": [
                47
            ],
            "cite_sids": [
                47
            ],
            "cite_text": "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A model containing all of the refinements described was tested using a magazine article containing 146 sentences (3,822 words). A 30,000-word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary.",
            "GPT_cite_text": "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first-order model, rather than using uniform second-order dependencies."
        },
        {
            "Number": 3,
            "refer_ID": "H89-2014",
            "refer_sids": [
                87,
                88,
                89
            ],
            "refer_text": "An alternative to uniformly increasing the order of the conditioning is to extend it selectively.Mixed higher- order context can be modeled by introducing explicit state sequences.In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.",
            "cite_ID": "A92-1018",
            "cite_maker_sids": [
                108
            ],
            "cite_sids": [
                108
            ],
            "cite_text": "adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher-order context can be modeled by introducing explicit state sequences. In the arrangement, the basic first-order network remains, permitting all possible category sequences and modeling first-order dependency.",
            "GPT_cite_text": "Adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a]."
        },
        {
            "Number": 4,
            "refer_ID": "H89-2014",
            "refer_sids": [
                1
            ],
            "refer_text": "The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.",
            "cite_ID": "J93-2006",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                40,
                41
            ],
            "cite_text": "We report in Section 2 on our experiments on the assignment of part of speech to words in text.The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.",
            "GPT_cite_text": "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)."
        },
        {
            "Number": 5,
            "refer_ID": "H89-2014",
            "refer_sids": [
                88,
                90
            ],
            "refer_text": "Mixed higher- order context can be modeled by introducing explicit state sequences.The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.",
            "cite_ID": "H91-1046",
            "cite_maker_sids": [
                21
            ],
            "cite_sids": [
                21
            ],
            "cite_text": "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Mixed higher-order context can be modeled by introducing explicit state sequences. The basic network is then augmented with the extra state sequences, which model certain category sequences in more detail.",
            "GPT_cite_text": "Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies."
        },
        {
            "Number": 6,
            "refer_ID": "H89-2014",
            "refer_sids": [
                29,
                30
            ],
            "refer_text": "In this regard, word equivalence classes were used (Kupiec, 1989).There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.",
            "cite_ID": "H91-1046",
            "cite_maker_sids": [
                60
            ],
            "cite_sids": [
                60
            ],
            "cite_text": "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this regard, word equivalence classes were used (Kupiec, 1989). There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.",
            "GPT_cite_text": "The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989)."
        },
        {
            "Number": 7,
            "refer_ID": "H89-2014",
            "refer_sids": [
                69,
                70
            ],
            "refer_text": "In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.",
            "cite_ID": "C00-1081",
            "cite_maker_sids": [
                85
            ],
            "cite_sids": [
                85
            ],
            "cite_text": "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In a ranked list of words in the corpus, the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably. The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.",
            "GPT_cite_text": "In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized."
        },
        {
            "Number": 8,
            "refer_ID": "H89-2014",
            "refer_sids": [
                2
            ],
            "refer_text": "The model has the advantage that a pre-tagged training corpus is not required.",
            "cite_ID": "H92-1022",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18
            ],
            "cite_text": "The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "The model has the advantage that a pre-tagged training corpus is not required.",
            "GPT_cite_text": "The parameters of the model can be estimated from tagged [1, 3, 4, 6, 12] or untagged [2, 9, 11] text."
        },
        {
            "Number": 9,
            "refer_ID": "H89-2014",
            "refer_sids": [
                145
            ],
            "refer_text": "A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.",
            "cite_ID": "H92-1022",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "One area in which the statistical approach has done par\u00ad ticularly well is automatic part of speech tagging, as\u00ad signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.",
            "GPT_cite_text": "One area in which the statistical approach has done particularly well is automatic part of speech tagging, assigning each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12)."
        },
        {
            "Number": 10,
            "refer_ID": "H89-2014",
            "refer_sids": [
                29,
                30
            ],
            "refer_text": "In this regard, word equivalence classes were used (Kupiec, 1989).There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.",
            "cite_ID": "C92-1060",
            "cite_maker_sids": [
                124
            ],
            "cite_sids": [
                124
            ],
            "cite_text": "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In this regard, word equivalence classes were used (Kupiec, 1989). There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.",
            "GPT_cite_text": "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)."
        }
    ],
    "W04-0213": [
        {
            "Number": 1,
            "refer_ID": "W04-0213",
            "refer_sids": [
                6
            ],
            "refer_text": "At present, the \u00e2\u20ac\u02dcPotsdam Commentary Corpus\u00e2\u20ac\u2122 (henceforth \u00e2\u20ac\u02dcPCC\u00e2\u20ac\u2122 for short) consists of 170 commentaries from Ma\u00c2\u00a8rkische Allgemeine Zeitung, a German regional daily.",
            "cite_ID": "C04-1061",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from M\u00e4rkische Allgemeine Zeitung, a German regional daily.",
            "GPT_cite_text": "Two examples of such corpora are the RST Treebank Corpus (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German."
        },
        {
            "Number": 2,
            "refer_ID": "W04-0213",
            "refer_sids": [
                3
            ],
            "refer_text": "A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.",
            "cite_ID": "C04-1061",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A corpus of German newspaper commentaries has been assembled at Potsdam University and annotated with different linguistic information to different degrees.",
            "GPT_cite_text": "However, when we trained two experienced students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004), and upon completion of the task asked them about their experiences, a very different picture emerged."
        },
        {
            "Number": 3,
            "refer_ID": "W04-0213",
            "refer_sids": [
                17,
                18,
                113
            ],
            "refer_text": "The corpus has been annotated with six different types of information, which are characterized in the following subsections.Not all the layers have been produced for all the texts yet.Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase.",
            "cite_ID": "C04-1061",
            "cite_maker_sids": [
                47
            ],
            "cite_sids": [
                47
            ],
            "cite_text": "Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. Thus, it is possible, for illustration, to look for a noun phrase (syntax tier) marked as a topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase.",
            "GPT_cite_text": "Annotators have to also make syntactic judgments, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004))."
        },
        {
            "Number": 4,
            "refer_ID": "W04-0213",
            "refer_sids": [
                68
            ],
            "refer_text": "For the \u00e2\u20ac\u02dccore\u00e2\u20ac\u2122 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u00e2\u20ac\u201d but see Sections 3.2 and 3.3 below.",
            "cite_ID": "C08-2009",
            "cite_maker_sids": [
                4
            ],
            "cite_sids": [
                4
            ],
            "cite_text": "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "For the 'core' portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective. When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below.",
            "GPT_cite_text": "Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyze them completely."
        },
        {
            "Number": 5,
            "refer_ID": "W04-0213",
            "refer_sids": [
                3
            ],
            "refer_text": "A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.",
            "cite_ID": "P06-3008",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A corpus of German newspaper commentaries has been assembled at Potsdam University and annotated with different linguistic information to different degrees.",
            "GPT_cite_text": "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004)."
        },
        {
            "Number": 6,
            "refer_ID": "W04-0213",
            "refer_sids": [
                42
            ],
            "refer_text": "We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures.",
            "cite_ID": "P08-2062",
            "cite_maker_sids": [
                3
            ],
            "cite_sids": [
                3,
                4
            ],
            "cite_text": "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the ANNOTATE3 tool for interactive construction of tree structures.",
            "GPT_cite_text": "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarization (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser."
        },
        {
            "Number": 7,
            "refer_ID": "W04-0213",
            "refer_sids": [
                125
            ],
            "refer_text": "That is, we can use the discourse parser on PCC texts, emulating for instance a \u00e2\u20ac\u0153co-reference oracle\u00e2\u20ac\u009d that adds the information from our co-reference annotations.",
            "cite_ID": "P08-2062",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "That is, we can use the discourse parser on PCC texts, emulating for instance a \u201cco-reference oracle\u201d that adds the information from our co-reference annotations.",
            "GPT_cite_text": "Following annotation schemes like the one by Stede (2004), we model discourse structures as binary trees."
        },
        {
            "Number": 8,
            "refer_ID": "W04-0213",
            "refer_sids": [
                5,
                6,
                7
            ],
            "refer_text": "This paper, however, provides a comprehensive overview of the data collection effort and its current state.At present, the \u00e2\u20ac\u02dcPotsdam Commentary Corpus\u00e2\u20ac\u2122 (henceforth \u00e2\u20ac\u02dcPCC\u00e2\u20ac\u2122 for short) consists of 170 commentaries from Ma\u00c2\u00a8rkische Allgemeine Zeitung, a German regional daily.The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.",
            "cite_ID": "W06-2709",
            "cite_maker_sids": [
                72
            ],
            "cite_sids": [
                72
            ],
            "cite_text": "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from M\u00e4rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.",
            "GPT_cite_text": "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyntax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure."
        },
        {
            "Number": 9,
            "refer_ID": "W04-0213",
            "refer_sids": [
                3
            ],
            "refer_text": "A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.",
            "cite_ID": "W07-1525",
            "cite_maker_sids": [
                170
            ],
            "cite_sids": [
                170
            ],
            "cite_text": "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A corpus of German newspaper commentaries has been assembled at Potsdam University and annotated with different linguistic information to different degrees.",
            "GPT_cite_text": "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)."
        },
        {
            "Number": 10,
            "refer_ID": "W04-0213",
            "refer_sids": [
                3,
                4
            ],
            "refer_text": "A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization).",
            "cite_ID": "W07-1530",
            "cite_maker_sids": [
                29
            ],
            "cite_sids": [
                29,
                30
            ],
            "cite_text": "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A corpus of German newspaper commentaries has been assembled at Potsdam University and annotated with different linguistic information to different degrees. Two aspects of the corpus have been presented in previous papers (Reitter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization.",
            "GPT_cite_text": "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12-15 sentences), with 33,000 tokens in total."
        },
        {
            "Number": 11,
            "refer_ID": "W04-0213",
            "refer_sids": [
                2,
                6
            ],
            "refer_text": "The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.At present, the \u00e2\u20ac\u02dcPotsdam Commentary Corpus\u00e2\u20ac\u2122 (henceforth \u00e2\u20ac\u02dcPCC\u00e2\u20ac\u2122 for short) consists of 170 commentaries from Ma\u00c2\u00a8rkische Allgemeine Zeitung, a German regional daily.",
            "cite_ID": "W07-1530",
            "cite_maker_sids": [
                40
            ],
            "cite_sids": [
                40
            ],
            "cite_text": "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The paper explains the design decisions taken in the annotations and describes a number of applications using this corpus with its multi-layer annotation. At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from M\u00e4rkische Allgemeine Zeitung, a German regional daily.",
            "GPT_cite_text": "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing."
        },
        {
            "Number": 12,
            "refer_ID": "W04-0213",
            "refer_sids": [
                6,
                17,
                18,
                68
            ],
            "refer_text": "At present, the \u00e2\u20ac\u02dcPotsdam Commentary Corpus\u00e2\u20ac\u2122 (henceforth \u00e2\u20ac\u02dcPCC\u00e2\u20ac\u2122 for short) consists of 170 commentaries from Ma\u00c2\u00a8rkische Allgemeine Zeitung, a German regional daily.The corpus has been annotated with six different types of information, which are characterized in the following subsections.Not all the layers have been produced for all the texts yet.For the \u00e2\u20ac\u02dccore\u00e2\u20ac\u2122 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u00e2\u20ac\u201d but see Sections 3.2 and 3.3 below.",
            "cite_ID": "W11-0401",
            "cite_maker_sids": [
                42
            ],
            "cite_sids": [
                42,
                43,
                44,
                45,
                48,
                49
            ],
            "cite_text": "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).This corpus includes 173 texts on politics from the online newspaper M\u00c3\u00a4rkische Allgemeine Zeitung.It contains 32,962 words and 2,195 sentences.It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure.This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online).The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.",
            "label": [
                "Method_Citation",
                "Implication_Citation"
            ],
            "GPT_refer_text": "At present, the \u2018Potsdam Commentary Corpus\u2019 (henceforth \u2018PCC\u2019 for short) consists of 170 commentaries from M\u00e4rkische Allgemeine Zeitung, a German regional daily. The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. For the \u2018core\u2019 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective. When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright \u2014 but see Sections 3.2 and 3.3 below.",
            "GPT_cite_text": "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper M\u00e4rkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data types: morphology, syntax, rhetorical structure, connectors, coreference, and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria), and the inter-annotator agreement is not given."
        },
        {
            "Number": 13,
            "refer_ID": "W04-0213",
            "refer_sids": [
                44
            ],
            "refer_text": "All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).",
            "cite_ID": "W12-3205",
            "cite_maker_sids": [
                98
            ],
            "cite_sids": [
                98
            ],
            "cite_text": "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "All commentaries have been annotated with rhetorical structure, using RSTTool and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).",
            "GPT_cite_text": "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portuguese (Pardo et al., 2008) and German (Stede, 2004)."
        },
        {
            "Number": 14,
            "refer_ID": "W04-0213",
            "refer_sids": [
                8
            ],
            "refer_text": "Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.",
            "cite_ID": "W13-2708",
            "cite_maker_sids": [
                161
            ],
            "cite_sids": [
                161
            ],
            "cite_text": "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Commentaries argue in favor of a specific point of view toward some political issue, often discussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.",
            "GPT_cite_text": "We first extracted opinionated and objective texts from the DeReKo corpus (Stede, 2004; Kupietz, Figure 4: 10 most used verbs (lemmas) in indirect speech)."
        },
        {
            "Number": 15,
            "refer_ID": "W04-0213",
            "refer_sids": [
                1
            ],
            "refer_text": "A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.",
            "cite_ID": "W13-3306",
            "cite_maker_sids": [
                31
            ],
            "cite_sids": [
                31
            ],
            "cite_text": "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (Pe\u00c2\u00b4ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.",
            "GPT_cite_text": "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (P\u00e9ry-Woodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)."
        }
    ],
    "W06-3909": [
        {
            "Number": 1,
            "refer_ID": "W06-3909",
            "refer_sids": [
                8
            ],
            "refer_text": "From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts.",
            "cite_ID": "D09-1025",
            "cite_maker_sids": [
                93
            ],
            "cite_sids": [
                93
            ],
            "cite_text": "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "From one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations to more specific and domain-oriented ones like chemical reactants in a chemistry domain and position succession in political texts.",
            "GPT_cite_text": "We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives; see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns."
        },
        {
            "Number": 2,
            "refer_ID": "W06-3909",
            "refer_sids": [
                121
            ],
            "refer_text": "Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.",
            "cite_ID": "D09-1025",
            "cite_maker_sids": [
                120
            ],
            "cite_sids": [
                119,
                120
            ],
            "cite_text": "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.",
            "GPT_cite_text": "Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006)."
        },
        {
            "Number": 3,
            "refer_ID": "W06-3909",
            "refer_sids": [
                196
            ],
            "refer_text": "We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.",
            "cite_ID": "P09-1045",
            "cite_maker_sids": [
                127
            ],
            "cite_sids": [
                127
            ],
            "cite_text": "Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We proposed a weakly supervised bootstrapping algorithm called Espresso for automatically extracting a wide variety of binary semantic relations from raw text.",
            "GPT_cite_text": "Typically, algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008)."
        },
        {
            "Number": 4,
            "refer_ID": "W06-3909",
            "refer_sids": [
                19
            ],
            "refer_text": "Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.",
            "cite_ID": "P09-1113",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.",
            "GPT_cite_text": "A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008)."
        },
        {
            "Number": 5,
            "refer_ID": "W06-3909",
            "refer_sids": [
                51
            ],
            "refer_text": "We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((AdjPIPENoun)+PIPE((AdjPIPENoun)*(NounPrep)?)(AdjPIPENoun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point).",
            "cite_ID": "P170300_w06",
            "cite_maker_sids": [
                82
            ],
            "cite_sids": [
                82
            ],
            "cite_text": "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((AdjPIPENoun)+PIPE((AdjPIPENoun)*(NounPrep)?)(AdjPIPENoun)*)Noun. We operationally extend the definition of Adj to include present and past participles, as most noun phrases composed of them are usually intended as terms (e.g., boiling point).",
            "GPT_cite_text": "To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel, 2006)."
        },
        {
            "Number": 6,
            "refer_ID": "W06-3909",
            "refer_sids": [
                20,
                21
            ],
            "refer_text": "Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.",
            "cite_ID": "P846406_w06",
            "cite_maker_sids": [
                104
            ],
            "cite_sids": [
                104
            ],
            "cite_text": "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.",
            "GPT_cite_text": "In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text-harvesting paradigm."
        },
        {
            "Number": 7,
            "refer_ID": "W06-3909",
            "refer_sids": [
                30
            ],
            "refer_text": "The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.",
            "cite_ID": "P846406_w06",
            "cite_maker_sids": [
                105
            ],
            "cite_sids": [
                105
            ],
            "cite_text": "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "The approach gives good results on specific relations such as birthdates; however, it has low precision on generic ones like is-a and part-of.",
            "GPT_cite_text": "In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006), they report the results."
        },
        {
            "Number": 9,
            "refer_ID": "W06-3909",
            "refer_sids": [
                196
            ],
            "refer_text": "We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.",
            "cite_ID": "Pfram_w06",
            "cite_maker_sids": [
                596
            ],
            "cite_sids": [
                596
            ],
            "cite_text": "Pennacchiotti and Pantel [32] describes a system called Espresso.",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "We proposed a weakly supervised bootstrapping algorithm called Espresso for automatically extracting a wide variety of binary semantic relations from raw text.",
            "GPT_cite_text": "Pennacchiotti and Pantel [32] describe a system called Espresso."
        },
        {
            "Number": 10,
            "refer_ID": "W06-3909",
            "refer_sids": [
                68
            ],
            "refer_text": "The recall of a pattern p can be approximated by the fraction of input instances in I' that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision).",
            "cite_ID": "Pfram_w06",
            "cite_maker_sids": [
                601
            ],
            "cite_sids": [
                601
            ],
            "cite_text": "Pantel and Pennacchiotti [31] extends Espresso by treating patterns with high recall differently from patterns with high precision.",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "The recall of a pattern p can be approximated by the fraction of input instances in I' that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are wary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision).",
            "GPT_cite_text": "Pantel and Pennacchiotti [31] extend Espresso by treating patterns with high recall differently from patterns with high precision."
        },
        {
            "Number": 11,
            "refer_ID": "W06-3909",
            "refer_sids": [
                33
            ],
            "refer_text": "Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks.",
            "cite_ID": "Ponto_w06",
            "cite_maker_sids": [
                18
            ],
            "cite_sids": [
                18
            ],
            "cite_text": "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision; KnowItAll [7], which performs large-scale extraction of facts from the Web; Mann [15] and Fleischman et al. [9], who used part-of-speech patterns to extract a subset of is-a relations involving proper nouns; and Downey et al. [6], who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks.",
            "GPT_cite_text": "As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalized is-a or part-of relationships."
        },
        {
            "Number": 12,
            "refer_ID": "W06-3909",
            "refer_sids": [
                19
            ],
            "refer_text": "Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.",
            "cite_ID": "Ponto_w06",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.",
            "label": [
                "Method citation"
            ],
            "GPT_refer_text": "Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.",
            "GPT_cite_text": "Hearst's method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping."
        },
        {
            "Number": 13,
            "refer_ID": "W06-3909",
            "refer_sids": [
                122,
                123,
                124,
                125,
                126,
                127,
                128,
                129,
                130
            ],
            "refer_text": "For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations:  succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus.  reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus.  production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus.",
            "cite_ID": "PSS07_w06",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49
            ],
            "cite_text": "Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006).",
            "label": [
                "Method citation |"
            ],
            "GPT_refer_text": "For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus. reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts with oxygen gas and zinc reacts with hydrochloric acid. We evaluate this relation on the CHEM corpus. production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus.",
            "GPT_cite_text": "Pennacchiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the TREC-9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti & Pantel 2006)."
        }
    ],
    "C04-1089": [
        {
            "Number": 1,
            "refer_ID": "C04-1089",
            "refer_sids": [
                53,
                61
            ],
            "refer_text": "In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document.We use backoff and linear interpolation for probability estimation:",
            "cite_ID": "C10-1070",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                78,
                79
            ],
            "cite_text": "To our knowledge, this association measure has not been used yet in translation spotting.It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document. We use backoff and linear interpolation for probability estimation:",
            "GPT_cite_text": "To our knowledge, this association measure has not been used yet in translation spotting. It is computed as: (O11 + 1)(O22 + 1) described in (Shao and Ng, 2004)."
        },
        {
            "Number": 2,
            "refer_ID": "C04-1089",
            "refer_sids": [
                135
            ],
            "refer_text": "To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.",
            "cite_ID": "C10-2164",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u00e2\u20ac\u201c1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone) on those Chinese words that have translations in the English part of the comparable corpus.",
            "GPT_cite_text": "At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionaries, transliteration, or parallel corpora to the intermediate pattern based on comparable corpora (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435\u20131443, Beijing, August 2010 then became a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006)."
        },
        {
            "Number": 3,
            "refer_ID": "C04-1089",
            "refer_sids": [
                10,
                11
            ],
            "refer_text": "Comparable corpora refer to texts that are not direct translation but are about the same topic.For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.",
            "cite_ID": "D10-1042",
            "cite_maker_sids": [
                54
            ],
            "cite_sids": [
                53,
                54
            ],
            "cite_text": "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "Comparable corpora refer to texts that are not direct translations but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.",
            "GPT_cite_text": "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, Fung and Yee (1998) and Shao and Ng (2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event."
        },
        {
            "Number": 4,
            "refer_ID": "C04-1089",
            "refer_sids": [
                32
            ],
            "refer_text": "We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c . C (e* ) is the this retrieval problem.",
            "cite_ID": "D12-1003",
            "cite_maker_sids": [
                38
            ],
            "cite_sids": [
                38
            ],
            "cite_text": "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to the document translation of c. C(e*) is the retrieval problem.",
            "GPT_cite_text": "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models."
        },
        {
            "Number": 5,
            "refer_ID": "C04-1089",
            "refer_sids": [
                17
            ],
            "refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.",
            "cite_ID": "D12-1003",
            "cite_maker_sids": [
                68
            ],
            "cite_sids": [
                68
            ],
            "cite_text": "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (De\u00c2\u00b4jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora by combining both context and transliteration information.",
            "GPT_cite_text": "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (Dejean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004)."
        },
        {
            "Number": 6,
            "refer_ID": "C04-1089",
            "refer_sids": [
                17
            ],
            "refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.",
            "cite_ID": "N09-1048",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora by combining both context and transliteration information.",
            "GPT_cite_text": "The other is multilingual parallel and comparable corpora (e.g., Wikipedia), wherein features such as co-occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008)."
        },
        {
            "Number": 7,
            "refer_ID": "C04-1089",
            "refer_sids": [
                17
            ],
            "refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.",
            "cite_ID": "N09-1048",
            "cite_maker_sids": [
                50
            ],
            "cite_sids": [
                50
            ],
            "cite_text": "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora by combining both context and transliteration information.",
            "GPT_cite_text": "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and contextual information."
        },
        {
            "Number": 8,
            "refer_ID": "C04-1089",
            "refer_sids": [
                17
            ],
            "refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.",
            "cite_ID": "P06-1011",
            "cite_maker_sids": [
                147
            ],
            "cite_sids": [
                147
            ],
            "cite_text": "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora by combining both context and transliteration information.",
            "GPT_cite_text": "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004)."
        },
        {
            "Number": 9,
            "refer_ID": "C04-1089",
            "refer_sids": [
                90
            ],
            "refer_text": "We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm.",
            "cite_ID": "P13-1059",
            "cite_maker_sids": [
                208
            ],
            "cite_sids": [
                208
            ],
            "cite_text": "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We used a list of 1,580 Chinese-English name pairs as training data for the EM algorithm.",
            "GPT_cite_text": "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007)."
        },
        {
            "Number": 10,
            "refer_ID": "C04-1089",
            "refer_sids": [
                30
            ],
            "refer_text": "So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.",
            "cite_ID": "P13-1062",
            "cite_maker_sids": [
                164
            ],
            "cite_sids": [
                163,
                164
            ],
            "cite_text": "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "So we use the context of c, we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C(e*) that matches the query and try to retrieve the most similar best matches to the query.",
            "GPT_cite_text": "Evaluation in EC. We processed news articles for an entire year in the context of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si, Sj), EB) 2008 by Xinhua News, which publishes news in EC in both English and Chinese, and which were also used with edge weights that are defined by TS. The maximum bipartite matching finds a subset as described by Kim et al. (2011) and Shao and Ng (2004)."
        },
        {
            "Number": 12,
            "refer_ID": "C04-1089",
            "refer_sids": [
                17
            ],
            "refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.",
            "cite_ID": "P13-2036",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora by combining both context and transliteration information.",
            "GPT_cite_text": "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011)."
        },
        {
            "Number": 13,
            "refer_ID": "C04-1089",
            "refer_sids": [
                107
            ],
            "refer_text": "Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).",
            "cite_ID": "P13-2036",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Finally, the English candidate word with the smallest average rank position that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).",
            "GPT_cite_text": "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank."
        },
        {
            "Number": 14,
            "refer_ID": "C04-1089",
            "refer_sids": [
                17
            ],
            "refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.",
            "cite_ID": "W11-1215",
            "cite_maker_sids": [
                38
            ],
            "cite_sids": [
                38
            ],
            "cite_text": "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "In this paper, we propose a new approach for the task of mining new word translations from comparable corpora by combining both context and transliteration information.",
            "GPT_cite_text": "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009)."
        },
        {
            "Number": 16,
            "refer_ID": "C04-1089",
            "refer_sids": [
                7
            ],
            "refer_text": "In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.",
            "cite_ID": "W13-2501",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                23
            ],
            "cite_text": "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.",
            "GPT_cite_text": "The traditional approach to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language."
        },
        {
            "Number": 17,
            "refer_ID": "C04-1089",
            "refer_sids": [
                135
            ],
            "refer_text": "To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.",
            "cite_ID": "W13-2512",
            "cite_maker_sids": [
                66
            ],
            "cite_sids": [
                66
            ],
            "cite_text": "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone) on those Chinese words that have translations in the English part of the comparable corpus.",
            "GPT_cite_text": "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006)."
        }
    ],
    "C08-1098": [
        {
            "Number": 1,
            "refer_ID": "C08-1098",
            "refer_sids": [
                5,
                6,
                127
            ],
            "refer_text": "Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.",
            "cite_ID": "C10-2023",
            "cite_maker_sids": [
                122
            ],
            "cite_sids": [
                122
            ],
            "cite_text": "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).",
            "label": [
                "Implication_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "Tagsets of this size contain little or no information about number, gender, case, and similar morphosyntactic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.",
            "GPT_cite_text": "Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.)."
        },
        {
            "Number": 2,
            "refer_ID": "C08-1098",
            "refer_sids": [
                100
            ],
            "refer_text": "The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.",
            "cite_ID": "D12-1133",
            "cite_maker_sids": [
                208
            ],
            "cite_sids": [
                208
            ],
            "cite_text": "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The tagger may use an external lexicon that supplies entries for additional words not found in the training corpus and additional tags for words that did occur in the training data.",
            "GPT_cite_text": "For German, we obtain a tagging accuracy of 97.24%, which is close to the 97.39% achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German."
        },
        {
            "Number": 3,
            "refer_ID": "C08-1098",
            "refer_sids": [
                167
            ],
            "refer_text": "3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
            "cite_ID": "D12-1133",
            "cite_maker_sids": [
                228
            ],
            "cite_sids": [
                228
            ],
            "cite_text": "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
            "GPT_cite_text": "For German, finally, we see the greatest improvement with k = 3 additional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008)."
        },
        {
            "Number": 4,
            "refer_ID": "C08-1098",
            "refer_sids": [
                4
            ],
            "refer_text": "A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u00cb\u2020N = t\u00cb\u20201, ..., t\u00cb\u2020N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.",
            "cite_ID": "D13-1032",
            "cite_maker_sids": [
                212
            ],
            "cite_sids": [
                212
            ],
            "cite_text": "We use the following baselines: SVMTool (Gime\u00c2\u00b4nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t^N = t^1, ..., t^N for a given word sequence w^N. POS taggers are usually trained on corpora with between 50 and 150 different POS tags.",
            "GPT_cite_text": "We use the following baselines: SVMTool (Gim\u00e9nez and M\u00e1rquez, 2004), an SVM-based discriminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;"
        },
        {
            "Number": 5,
            "refer_ID": "C08-1098",
            "refer_sids": [
                95
            ],
            "refer_text": "Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.",
            "cite_ID": "D13-1033",
            "cite_maker_sids": [
                176
            ],
            "cite_sids": [
                176
            ],
            "cite_text": "For German, we show results for RFTagger (Schmid and Laws, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our tagger is an HMM tagger that decomposes the context probabilities into a product of attribute probabilities.",
            "GPT_cite_text": "For German, we show results for RFTagger (Schmid and Laws, 2008)."
        },
        {
            "Number": 6,
            "refer_ID": "C08-1098",
            "refer_sids": [
                59
            ],
            "refer_text": "Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.",
            "cite_ID": "E09-1079",
            "cite_maker_sids": [
                121
            ],
            "cite_sids": [
                121
            ],
            "cite_text": "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our tagger generates a predictor for each feature (such as base POS, number, gender, etc.). Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.",
            "GPT_cite_text": "The decision tree uses different context features for the prediction of different attributes (Schmid and Laws, 2008)."
        },
        {
            "Number": 7,
            "refer_ID": "C08-1098",
            "refer_sids": [
                59,
                68
            ],
            "refer_text": "Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.A typical context attribute is \u00e2\u20ac\u01531:ART.Nom\u00e2\u20ac\u009d which states that the preceding tag is an article with the attribute \u00e2\u20ac\u0153Nom\u00e2\u20ac\u009d.",
            "cite_ID": "P10-1020",
            "cite_maker_sids": [
                159
            ],
            "cite_sids": [
                159
            ],
            "cite_text": "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our tagger generates a predictor for each feature (such as base POS, number, gender, etc.). Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value. A typical context attribute is \"1:ART.Nom\" which states that the preceding tag is an article with the attribute \"Nom.\"",
            "GPT_cite_text": "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological cases."
        },
        {
            "Number": 8,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "P10-1068",
            "cite_maker_sids": [
                42
            ],
            "cite_sids": [
                42
            ],
            "cite_text": "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "With respect to morphosyntactic annotations (parts of speech, POS) and morphological annotations (morph), five annotation models for German are currently available: STTS (Schiller et al., 1999, POS), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, POS, morph), RFTagger (Schmid and Laws, 2008, POS, morph)."
        },
        {
            "Number": 9,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets",
            "cite_ID": "P10-1068",
            "cite_maker_sids": [
                74
            ],
            "cite_sids": [
                74
            ],
            "cite_text": "Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tag sets.",
            "GPT_cite_text": "Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model, such as in (6)."
        },
        {
            "Number": 10,
            "refer_ID": "C08-1098",
            "refer_sids": [
                5,
                224
            ],
            "refer_text": "Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "P10-1068",
            "cite_maker_sids": [
                81
            ],
            "cite_sids": [
                81
            ],
            "cite_text": "(iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Tagsets of this size contain little or no information about number, gender, case, and similar morphosyntactic features. We presented a HMM POS tagger for fine-grained tagsets, which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "(iii) the RFTagger that performs part-of-speech and morphological analysis (Schmid and Laws, 2008)"
        },
        {
            "Number": 11,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W10-1704",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tag set containing approximately 800 tags."
        },
        {
            "Number": 12,
            "refer_ID": "C08-1098",
            "refer_sids": [
                127
            ],
            "refer_text": "It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.",
            "cite_ID": "W10-1727",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41
            ],
            "cite_text": "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "It is annotated with POS tags from the coarse-grained STTS tag set and with additional features encoding information about number, gender, case, person, degree, tense, and mood.",
            "GPT_cite_text": "For German, we used morphologically rich tags from RFTagger (Schmid and Laws, 2008) that contain morphological information such as case, number, and gender for nouns and tense for verbs."
        },
        {
            "Number": 13,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W11-2135",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49
            ],
            "cite_text": "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "For German, the fine-grained POS information used for preprocessing was computed by the RFTagger (Schmid and Laws, 2008)."
        },
        {
            "Number": 14,
            "refer_ID": "C08-1098",
            "refer_sids": [
                12,
                127
            ],
            "refer_text": "The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.",
            "cite_ID": "W11-2145",
            "cite_maker_sids": [
                101
            ],
            "cite_sids": [
                101,
                102
            ],
            "cite_text": "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall). It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.",
            "GPT_cite_text": "The parts of speech were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produce more fine-grained tags that also include person, gender, and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus."
        },
        {
            "Number": 15,
            "refer_ID": "C08-1098",
            "refer_sids": [
                4,
                5
            ],
            "refer_text": "A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t\u00cb\u2020N = t\u00cb\u20201, ..., t\u00cb\u2020N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.",
            "cite_ID": "W11-2147",
            "cite_maker_sids": [
                26
            ],
            "cite_sids": [
                26
            ],
            "cite_text": "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence t^N = t^1, ..., t^N for a given word sequence wN. POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case, and similar morphosyntactic features.",
            "GPT_cite_text": "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008), which provides morphological information such as case, number, and gender for nouns and tense for verbs."
        },
        {
            "Number": 16,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W12-3141",
            "cite_maker_sids": [
                89
            ],
            "cite_sids": [
                89
            ],
            "cite_text": "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes using decision trees.",
            "GPT_cite_text": "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for preprocessing and were obtained using the RFTagger (Schmid and Laws, 2008)."
        },
        {
            "Number": 17,
            "refer_ID": "C08-1098",
            "refer_sids": [
                127
            ],
            "refer_text": "It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.",
            "cite_ID": "W12-3144",
            "cite_maker_sids": [
                127
            ],
            "cite_sids": [
                127
            ],
            "cite_text": "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.",
            "label": [
                "Results_Citation"
            ],
            "GPT_refer_text": "It is annotated with POS tags from the coarse-grained STTS tag set and with additional features encoding information about number, gender, case, person, degree, tense, and mood.",
            "GPT_cite_text": "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender, and case information."
        },
        {
            "Number": 18,
            "refer_ID": "C08-1098",
            "refer_sids": [
                124,
                224
            ],
            "refer_text": "We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter CWe presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W12-3402",
            "cite_maker_sids": [
                78
            ],
            "cite_sids": [
                78
            ],
            "cite_text": "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We took standard features from a 5-word window and M4LRL training without optimization of the regularization parameter. We presented an HMM POS tagger for fine-grained tagsets, which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger)."
        },
        {
            "Number": 19,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W13-2204",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35
            ],
            "cite_text": "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for preprocessing and were obtained using the RFTagger (Schmid and Laws, 2008)."
        },
        {
            "Number": 20,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W13-2210",
            "cite_maker_sids": [
                37
            ],
            "cite_sids": [
                37
            ],
            "cite_text": "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment."
        },
        {
            "Number": 21,
            "refer_ID": "C08-1098",
            "refer_sids": [
                54
            ],
            "refer_text": "The probability of an attribute (such as \u00e2\u20ac\u0153Nom\u00e2\u20ac\u009d) is always conditioned on the respective base POS (such as \u00e2\u20ac\u0153N\u00e2\u20ac\u009d) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns.",
            "cite_ID": "W13-2210",
            "cite_maker_sids": [
                85
            ],
            "cite_sids": [
                85
            ],
            "cite_text": "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "The probability of an attribute (such as \"Nom\") is always conditioned on the respective base POS (such as \"N\") (unless the predicted attribute is the Figure 1: Probability estimation tree for the nominative case of nouns.",
            "GPT_cite_text": "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German."
        },
        {
            "Number": 22,
            "refer_ID": "C08-1098",
            "refer_sids": [
                0
            ],
            "refer_text": "Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging",
            "cite_ID": "W13-2211",
            "cite_maker_sids": [
                84
            ],
            "cite_sids": [
                84
            ],
            "cite_text": "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging",
            "GPT_cite_text": "We lemmatized German articles, adjectives (only positive form), some pronouns, and nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine-grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008)."
        },
        {
            "Number": 23,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W13-2228",
            "cite_maker_sids": [
                134
            ],
            "cite_sids": [
                134
            ],
            "cite_text": "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "6.1.1 POS Tagging: We use RFTagger (Schmid and Laws, 2008) for POS tagging."
        },
        {
            "Number": 24,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W13-2230",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35
            ],
            "cite_text": "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille\u00c2\u00b4 et al., 2003).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008), which was trained on the French treebank (Abeille et al., 2003)."
        },
        {
            "Number": 25,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W13-2230",
            "cite_maker_sids": [
                58
            ],
            "cite_sids": [
                58
            ],
            "cite_text": "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes using decision trees.",
            "GPT_cite_text": "Morphological analysis and resources. The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008)."
        },
        {
            "Number": 26,
            "refer_ID": "C08-1098",
            "refer_sids": [
                224
            ],
            "refer_text": "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "cite_ID": "W13-2230",
            "cite_maker_sids": [
                83
            ],
            "cite_sids": [
                83
            ],
            "cite_text": "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We presented an HMM POS tagger for fine-grained tagsets that splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.",
            "GPT_cite_text": "Tagging and tagging errors: For tagging, we use a version of RFTagger (Schmid and Laws, 2008)."
        },
        {
            "Number": 28,
            "refer_ID": "C08-1098",
            "refer_sids": [
                1,
                167
            ],
            "refer_text": "We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
            "cite_ID": "W13-2302",
            "cite_maker_sids": [
                87
            ],
            "cite_sids": [
                87
            ],
            "cite_text": "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "We present an HMM part-of-speech tagging method that is particularly suited for POS tagsets with a large number of fine-grained tags. Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.",
            "GPT_cite_text": "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)."
        },
        {
            "Number": 29,
            "refer_ID": "C08-1098",
            "refer_sids": [
                1
            ],
            "refer_text": "We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.",
            "cite_ID": "W13-2708",
            "cite_maker_sids": [
                111
            ],
            "cite_sids": [
                111
            ],
            "cite_text": "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We present an HMM part-of-speech tagging method that is particularly suited for POS tagsets with a large number of fine-grained tags.",
            "GPT_cite_text": "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatization (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)."
        }
    ],
    "N04-1038": [
        {
            "Number": 1,
            "refer_ID": "N04-1038",
            "refer_sids": [
                8
            ],
            "refer_text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.",
            "cite_ID": "E12-1054",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and De\u00c2\u00b4silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.",
            "GPT_cite_text": "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and Desilets, 2005), optical character recognition (Wick et al., 2007), and co-reference resolution (Bean and Riloff, 2004)."
        },
        {
            "Number": 2,
            "refer_ID": "N04-1038",
            "refer_sids": [
                18,
                19,
                201
            ],
            "refer_text": "We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.BABAR employs information extraction techniques to represent and learn role relationships.We evaluated BABAR on two domains: terrorism and natural disasters.",
            "cite_ID": "H05-1003",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35,
                36
            ],
            "cite_text": "Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. We evaluated BABAR on two domains: terrorism and natural disasters.",
            "GPT_cite_text": "Recently, Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction. Their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters)."
        },
        {
            "Number": 3,
            "refer_ID": "N04-1038",
            "refer_sids": [
                68
            ],
            "refer_text": "Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.",
            "cite_ID": "N13-1104",
            "cite_maker_sids": [
                109
            ],
            "cite_sids": [
                109
            ],
            "cite_text": "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our representation of contextual roles is based on information extraction patterns that are converted into simple case frames.",
            "GPT_cite_text": "The dependency from the event head to an event argument depi, j, our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004)."
        },
        {
            "Number": 4,
            "refer_ID": "N04-1038",
            "refer_sids": [
                18,
                19,
                20
            ],
            "refer_text": "We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.BABAR employs information extraction techniques to represent and learn role relationships.Each pattern represents the role that a noun phrase plays in the surrounding context.",
            "cite_ID": "N13-1110",
            "cite_maker_sids": [
                207
            ],
            "cite_sids": [
                207,
                208,
                209
            ],
            "cite_text": "Another source of inspiration is the work by Bean and Riloff (2004).They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.However, they use a very small corpus (two domains) and do not aim to build a dictionary.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context.",
            "GPT_cite_text": "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary."
        },
        {
            "Number": 5,
            "refer_ID": "N04-1038",
            "refer_sids": [
                158
            ],
            "refer_text": "Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.",
            "cite_ID": "P05-1020",
            "cite_maker_sids": [
                202
            ],
            "cite_sids": [
                202
            ],
            "cite_text": "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Finally, a Dempster-Shafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.",
            "GPT_cite_text": "(2001) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004))."
        },
        {
            "Number": 6,
            "refer_ID": "N04-1038",
            "refer_sids": [
                49,
                63
            ],
            "refer_text": "Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved.Ex: Mr. Bush disclosed the policy by reading it",
            "cite_ID": "P06-1005",
            "cite_maker_sids": [
                124
            ],
            "cite_sids": [
                124
            ],
            "cite_text": "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each other. 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved. Ex: Mr. Bush disclosed the policy by reading it.",
            "GPT_cite_text": "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), and manually defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)."
        },
        {
            "Number": 7,
            "refer_ID": "N04-1038",
            "refer_sids": [
                2
            ],
            "refer_text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.",
            "cite_ID": "P06-1005",
            "cite_maker_sids": [
                177
            ],
            "cite_sids": [
                177,
                178,
                179
            ],
            "cite_text": "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.They give the example \u00e2\u20ac\u0153Mr.Bush disclosed the policy by reading it.\u00e2\u20ac\u009d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.",
            "GPT_cite_text": "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily resolved anaphors and antecedents. They give the example \u201cMr. Bush disclosed the policy by reading it.\u201d Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model."
        },
        {
            "Number": 8,
            "refer_ID": "N04-1038",
            "refer_sids": [
                1,
                8,
                70,
                76,
                77,
                137
            ],
            "refer_text": "We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. The focus of our work is on the use of contextual role knowledge for coreference resolution.Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found.One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves.",
            "cite_ID": "P07-1067",
            "cite_maker_sids": [
                46
            ],
            "cite_sids": [
                46,
                47,
                48,
                49
            ],
            "cite_text": "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.They apply an IE component to unannotated texts to generate a set of extraction caseframes.Each caseframe represents a linguistic expression and a syntactic position, e.g. \u00e2\u20ac\u0153murder of <NP>\u00e2\u20ac\u009d, \u00e2\u20ac\u0153killed <patient>\u00e2\u20ac\u009d.From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ],
            "GPT_refer_text": "We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. The focus of our work is on the use of contextual role knowledge for coreference resolution. Next, we describe four contextual role knowledge sources that are created from the training examples and the case frames. We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain. Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found. One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves.",
            "GPT_cite_text": "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. \u201cmurder of <NP>\u201d, \u201ckilled <patient>\u201d. From the caseframes, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes."
        },
        {
            "Number": 9,
            "refer_ID": "N04-1038",
            "refer_sids": [
                9
            ],
            "refer_text": "A contextual role represents the role that a noun phrase plays in an event or relationship.",
            "cite_ID": "P07-1068",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).",
            "label": [
                "Implication_Citation"
            ],
            "GPT_refer_text": "A contextual role represents the role that a noun phrase plays in an event or relationship.",
            "GPT_cite_text": "(2004) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff, 2004)."
        },
        {
            "Number": 10,
            "refer_ID": "N04-1038",
            "refer_sids": [
                68,
                87
            ],
            "refer_text": "Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions.",
            "cite_ID": "P08-1090",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35,
                38
            ],
            "cite_text": "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our representation of contextual roles is based on information extraction patterns that are converted into simple case frames. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies case frames that co-occur in anaphor/antecedent resolutions.",
            "GPT_cite_text": "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knowledge for anaphora resolution. Bean and Riloff learned these networks from two topic-specific texts and applied them to the problem of anaphora resolution."
        },
        {
            "Number": 11,
            "refer_ID": "N04-1038",
            "refer_sids": [
                68
            ],
            "refer_text": "Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.",
            "cite_ID": "P09-1068",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our representation of contextual roles is based on information extraction patterns that are converted into simple case frames.",
            "GPT_cite_text": "In this paper, we extend this work to represent sets of situation-specific events not unlike scripts and case frames (Bean and Riloff, 2004)."
        },
        {
            "Number": 12,
            "refer_ID": "N04-1038",
            "refer_sids": [
                8
            ],
            "refer_text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.",
            "cite_ID": "P09-1074",
            "cite_maker_sids": [
                223
            ],
            "cite_sids": [
                223
            ],
            "cite_text": "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.",
            "GPT_cite_text": "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g., Ng and Cardie (2002a) and Bean and Riloff (2004))."
        },
        {
            "Number": 13,
            "refer_ID": "N04-1038",
            "refer_sids": [
                158
            ],
            "refer_text": "Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.",
            "cite_ID": "P10-1142",
            "cite_maker_sids": [
                72
            ],
            "cite_sids": [
                72
            ],
            "cite_text": "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Finally, a Dempster-Shafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.",
            "GPT_cite_text": "The Dempster-Shafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition."
        },
        {
            "Number": 14,
            "refer_ID": "N04-1038",
            "refer_sids": [
                2
            ],
            "refer_text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.",
            "cite_ID": "P11-1082",
            "cite_maker_sids": [
                129
            ],
            "cite_sids": [
                129
            ],
            "cite_text": "However, the use of related verbs is similar in spirit to Bean and Riloff\u00e2\u20ac\u2122s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.",
            "GPT_cite_text": "However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006)."
        },
        {
            "Number": 15,
            "refer_ID": "N04-1038",
            "refer_sids": [
                68
            ],
            "refer_text": "Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.",
            "cite_ID": "P13-1121",
            "cite_maker_sids": [
                63
            ],
            "cite_sids": [
                63
            ],
            "cite_text": "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Our representation of contextual roles is based on information extraction patterns that are converted into simple case frames.",
            "GPT_cite_text": "Caseframes do not consider the dependents of the semantic role approximations. The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization, such as coreference resolution (Bean and Riloff, 2004)."
        },
        {
            "Number": 16,
            "refer_ID": "N04-1038",
            "refer_sids": [
                0,
                3,
                201
            ],
            "refer_text": "Unsupervised Learning of Contextual Role Knowledge for Coreference ResolutionThese knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.We evaluated BABAR on two domains: terrorism and natural disasters.",
            "cite_ID": "P13-2015",
            "cite_maker_sids": [
                46
            ],
            "cite_sids": [
                46,
                47
            ],
            "cite_text": "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. We evaluated BABAR on two domains: terrorism and natural disasters.",
            "GPT_cite_text": "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves."
        },
        {
            "Number": 17,
            "refer_ID": "N04-1038",
            "refer_sids": [
                2
            ],
            "refer_text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.",
            "cite_ID": "W05-0612",
            "cite_maker_sids": [
                33
            ],
            "cite_sids": [
                33
            ],
            "cite_text": "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.",
            "GPT_cite_text": "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role knowledge (Bean and Riloff, 2004)."
        },
        {
            "Number": 18,
            "refer_ID": "N04-1038",
            "refer_sids": [
                2,
                3
            ],
            "refer_text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.",
            "cite_ID": "W06-0106",
            "cite_maker_sids": [
                181
            ],
            "cite_sids": [
                181
            ],
            "cite_text": "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.",
            "label": [
                "Method_Citation"
            ],
            "GPT_refer_text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.",
            "GPT_cite_text": "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs."
        },
        {
            "Number": 19,
            "refer_ID": "N04-1038",
            "refer_sids": [
                8
            ],
            "refer_text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.",
            "cite_ID": "W06-0206",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.",
            "GPT_cite_text": "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), and coreference resolution (Bean and Riloff, 2004)."
        },
        {
            "Number": 20,
            "refer_ID": "N04-1038",
            "refer_sids": [
                100,
                101
            ],
            "refer_text": "For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor\u00e2\u20ac\u2122s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.If so, the CF Network reports that the anaphor and candidate may be coreferent.",
            "cite_ID": "W10-3909",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                22,
                23
            ],
            "cite_text": "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u00e2\u20ac\u201c68, Beijing, August 2010 recent work on anaphora resolution.Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.",
            "label": [
                "Method_Citation",
                "Implication_Citation"
            ],
            "GPT_refer_text": "For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor's caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions. If so, the CF Network reports that the anaphor and candidate may be coreferent.",
            "GPT_cite_text": "Methods for acquiring and using such knowledge are receiving increasing attention in Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60\u201368, Beijing, August 2010. Recent work on anaphora resolution by Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task."
        },
        {
            "Number": 21,
            "refer_ID": "N04-1038",
            "refer_sids": [
                8
            ],
            "refer_text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.",
            "cite_ID": "W10-3909",
            "cite_maker_sids": [
                36
            ],
            "cite_sids": [
                36,
                37,
                38
            ],
            "cite_text": "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.They got substantial gains on articles in two specific domains, terrorism and natural disasters.",
            "label": [
                "Aim_Citation"
            ],
            "GPT_refer_text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.",
            "GPT_cite_text": "Bean and Riloff (2004) present a system that uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They achieved substantial gains on articles in two specific domains: terrorism and natural disasters."
        },
        {
            "Number": 22,
            "refer_ID": "N04-1038",
            "refer_sids": [
                64,
                151,
                156,
                222,
                223
            ],
            "refer_text": "Table 1: Syntactic Seeding Heuristics BABAR\u00e2\u20ac\u2122s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.The confidence level is then used as the belief value for the knowledge source.Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision.The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.",
            "cite_ID": "W10-3909",
            "cite_maker_sids": [
                50
            ],
            "cite_sids": [
                50,
                51
            ],
            "cite_text": "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ],
            "GPT_refer_text": "Table 1: Syntactic Seeding Heuristics BABAR's reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. The confidence level is then used as the belief value for the knowledge source. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. The F-measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision. The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.",
            "GPT_cite_text": "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters."
        }
    ]
}